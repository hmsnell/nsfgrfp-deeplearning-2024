title,text
1.0,"Background:Children who are victims of interpersonalviolence have an elevated risk of engaging inaggressive behavior and perpetrating violence in adolescence and adulthood1. Youth currently in the fostercare system are particularly vulnerable to this cycle2,and are considerably more likely than theircounterparts to have contact with the criminal justice system. This has created substantial public costs forthe United States3. However, while research has shownhigh rates of criminal involvement within fosterpopulations4, not all victimized children engage inviolent behavior. In fact, a significant portion of thispopulation demonstrates relatively uncompromised, or “resilient” functioning5. Factors such as presenceof supportive adults, satisfaction in school, and participation in extracurricular activities were found to beprotective factors in high-risk populations6. However,access to and enhancement of these resources dueto foster placement and stability has been less well studied. Similarly, while risks associated with negativeoutcomes have been identified, such as previous physical abuse and delinquency in youths’ originalfamilies7, an examination of how these factors areamplified and affected by the foster care system has nottaken place. So while adverse childhood experiences often predict aggressive behavior4, there isvariability in delinquency rates within this population and little agreement about what is responsible forincreased or decreased risk of violent behavior. The lack of understanding of contributions to thisvariability complicates the development of effective social interventions within the foster care system andadds greatly to economic burden in the United States3.Proposed Research:To better understand this variability,I aim to examine the relationship betweenhistory of violence and rates of delinquency, focusing on the factors that potentially moderate thisassociation. The proposed study will focus on factors which mayplace children at increasedrisk, butalsofactors that may explain resiliencein this population.My analytic strategy will incorporatebothquantitative and qualitative techniques. Combiningthese approaches will provide multidimensionalunderstanding of complex issues that cannot be obtained through one method alone8. Large scalequantitative research will provide precise foundational information used to conduct the study, analyze thedata, identify factors that can be intervened on, and verify the findings. Smaller scale qualitative analyseswill expand understanding by exploring subjective factors, identifying other factors not captured in thequantitative data, gaining insight into social processes, and giving voice to participants in the study.First, I will analyze the Midwest Evaluation of the Adult Functioning of Former Foster Youth atChapin Hall9, self-reported survey data collectedfrom 732 study participants when they were 17 or 18years old. Within this data set, I will examinehistoryof maltreatment, delinquency, foster care servicefactors, access to protective factors, and placement satisfaction.History of maltreatmentwill beassessed by The Lifetime Experiences Questionnaire.Delinquencywill be measured by surveys regardinghistory of arrest, conviction for committing a crime, and overnight stay in a correctional facility. Thisanalysis will also include surveys collected regarding victimization in the past 12 months, as well asperpetrator status in the past 12 months.Foster servicefactorswill be measured by data regarding age atentry into foster care system, number of placements, type of placements, and total length of stay in fostercare system.Access to protective factorswill beassessed by The MOS Social Support Survey, surveysregarding the impact of foster care on ability to attend school, and information regarding educationalattainment, such as last grade level completed. Lastly,placement satisfactionwill be analyzed by a surveyconcerning attitudes and satisfaction with most recent placement situations, and a survey regarding thelikelihood of turning to the child welfare system for support in the future. A series of linear regressionmodels will explore the effects of maltreatment, foster care service factors, protective access, andsatisfaction on delinquency.I aim to(1) comparedelinquency rates for maltreated andnon-maltreated foster youth, (2) within maltreated foster youth, identify which factors areassociated with increased or decreased delinquency,and(3) determine whether foster care servicefactors, access to protective factors, and placement satisfaction moderate the relationship betweenviolence exposure and violent behavior within the foster care system.Second, I will conduct qualitative analysis using a smaller sample of foster youth, recruited atUniversity of Illinois at Chicago. I will interview foster care youth (n=50), ages 16 to 18 with a history ofmaltreatment and delinquency history tounderstandself-reported protective and risk factors in thecurrent operation of foster care.Example questionsare ""What do you think could be improved in yourexperience with foster care?"", and ""what do you feel has been useful to you during your foster careexperience?"". After transcription of these interviews, a code book will then be developed for theidentification and interpretation of patterns and themes in the textual data. This qualitative research willallow for better insight into social interactions, foster care delivery processes, and subjective factorsthat may not have been included in the surveys of the Midwest Evaluation. This qualitative researchwill generate explanatory models and theories, which will also be useful in the creation of interventions.Working collaboratively with the quantitative data, these interviews will provide valuable insight, allowfoster youth to have a voice, and add a more comprehensive analysis to numeric methods.HypothesesI hypothesize thatHypothesis (1)Maltreatedyouth will experience higher rates ofdelinquency.Hypothesis (2)Within the populationof maltreated youth, higher violence exposure will beassociated with lower rates of foster care service factors, access to protective factors, and placementsatisfaction.Hypothesis (3)Differences in thesefactors will identify which victims of abuse are morelikely to engage in delinquency, and thus strengthen or weaken the association between violence exposureand future violence.Hypothesis (4)The qualitativeinterviews will provide unique insight on complexsocial issues that will give voice to children in foster care and generate explanatory models that can serveto devise new kinds of interventions.Intellectual MeritThis study will address the relationshipbetween maltreatment and incarceration in anovel way that will advance understanding of variabilities in violent behavior among maltreated fosteryouth. By interpreting these factors that contribute to variabilities, I will add to research that willdetermine what is responsible for increased or decreased risk of delinquency and incarceration. As thistopic has never been addressed from both qualitative and quantitative perspectives, this study will offercomprehensive and unprecedented data that will begin to address the enormity of cycles of violenceamong foster youth.FeasibilityThe Midwest Evaluation is a longitudinaldataset that has already been collected and ispublicly available with data use agreements. My proposed qualitative interviews will add an unexploredconstruct to ongoing studies with minimal burden to participants. Preparation and analysis of this datawill be conducted with support from Dr. Kathryn Grant, an expert in maltreatment, or with Dr. ElizabethRaposa, who has worked extensively with foster youth and juvenile delinquents. This NSF award willpermit me to pursue coursework that will be critical to the success of my project and allow for dedicatedmentored training. The success of the research will be assessed via communication of these results bypublications of peer-reviewed first-authored manuscripts, presentations at research conferences such asthe International Association for Child and Adolescent Psychology and the International Society forTraumatic Stress Studies, and the development of interventions based on this research.Broader ImpactsThe estimated economic burden resultingfrom cases of child maltreatment in theUnited States is approximately $124 billion3. An additional $5.1 billion is used annually to incarcerateformer foster youth in State and Federal prisons10.Not only are we ethically bound to serve and providefor these underrepresented children, but we are also bound to advance the progress of science and reduceeconomic burdens for our nation. This research will identify which factors strengthen or weaken cycles ofviolence, and outline how to specifically support children within the system who have been exposed toviolence. This work will enhance and improve foster care procedures that specifically decrease rates ofviolent behavior and incarceration. My goal is to ultimately improve foster care service factors, access toprotective factors, and placement satisfaction. This research will lead to better interventions, decreasedrates of incarceration, and therefore a decreased economic burden for maltreatment in the United States.References1. Widom,Science1989 2. McMillen et al.Child Psychiatry2005 3. Fang et al.,Child AbuseNegl. 2012 4. Font et al.Crime Delinquency2021 5.Jones,Soc. Work J.2012 6. Luthar,Developmentalpsychopathology2006 7. Datta et al. 2019 8. Bartunek& Seo,Journal of Organizational Behavior20029. Courtney, Terao, Bost,Midwest Evaluation of theAdult Functioning of Former Foster Youth:Conditions of Youth Preparing to Leave State Care2011 10.Administration for Children and Families,Preliminary Fiscal Year 2009U.S. Department of Healthand Human Services 2010"
2.0,"PRIMARY RESEARCH OBJECTIVE: The primary objective of my PhD research is to formulatenovel statistical methods for accurately quantifying uncertainty and variability in life-cycle assessments(LCA), the primary method used to assess embodied carbon (greenhouse gas emissions associated withthe production, transport, construction,andend-of-lifeofbuildingmaterials).Theresultsofmyworkwilladvance LCA research by enabling more robust decision-making,moreaccuratesensitivityanalyses,andenhanced comparability between whole-building LCAs. Academics, practitioners, and policymakerswillthus be empowered to more effectively understand, quantify, and ultimately reduce embodied carbon.Immediate embodied carbon reduction is necessary for curbing the catastrophic effects of theclimate crisis, yet the current body of research is severely limited, and few policies or industrystandards incorporate embodied carbon into emission reduction strategies.JUSTIFICATION: Although whole-building LCA methodology is standardized by an internationalcoalition of technical standard-setting bodies (ISO 14040/14044),thereisconsiderablevariabilityindataquality, impact assumptions, and scope. This variability is particularly appreciable for biogenic carbon,the physical carbon that is stored in biological materials such as wood, hemp, and straw. Negativebiogenic carbon emissions due to carbon storage are treated inconsistently across whole-building LCAsbecause these assumptions are not standardized. In a five-building case study series, embodied carbonnormalized by floor arearangedfrom-936to207kgCO e/m2whenbiogeniccarbonstoragewasincluded2in theanalysisand132to557kgCO e/m2whenbiogeniccarbonstoragewasexcluded[1].Scientistshave2investigated several approaches to incorporate uncertainty in whole-building LCA, including but notlimited to: probability density functions to model building element service-life [2], probability densityfunctions to model manufacturing emissions [3], and more advanced, exhaustive methods that involveconducting aglobalsensitivityanalysisacrossparameterspacesdeterminedviaLatinhypercubesampling[4]. Unfortunately, these modeling practices are not yet standardized or integrated into whole-buildingLCA. From 2014-2019, 44% of published LCA studies did not mention uncertainty and 36%mentioneduncertainty but did not incorporate it in the analysis [5]. In summary, there is no consensus amongavailable methods to incorporate uncertainty in whole-building LCA, and available methods are seldomincorporated in LCA research. I aim to build upon theseavailablemethodsbyintroducingaprobabilisticframework to standardize uncertainty characterization in biogenic carbon accounting, enabling betterdecision-making and comparability between whole-building LCAs.Objective 1: Literature Review. Intent: To assess sources of uncertainty and uncertaintycharacterization methods in scientific literature and industry case studies. Methods: I will conduct aliterature review of academic LCA studies,whole-buildingLCAs,material-specificLCAs,andembodiedcarbon benchmarks. I will select the most appropriate sources and characterization methods for LCAuncertainty. I hypothesize that Monte Carlo simulation with probabilistic modeling will most accuratelycharacterize emission uncertainty for a linear model like whole-building LCA.Objective 2: A Probabilistic Framework for Whole-Building LCA. Intent: To increase the potentialfor synthesis between data sets for whole-building LCA in academic research and in practice. Methods:Based on my findings from the literature review, I will develop a statistical framework for classifying,characterizing, andquantifyinguncertaintyinLCA.Ihypothesizethatrunningwhole-buildingLCAswithmy proposed probabilistic approach will more accurately modelthisuncertainty,whichwillallowme(1)to standardize uncertainty quantification in LCA research and (2) to enable more accurate comparisonsand better decision-making.Objective 3: StandardizingBiogenicCarbonAccounting.Intentandbackground:Currentapproachesto biogenic carbon accounting are deterministicandquantifyneitheruncertaintynorvariabilitythatcomefrom widely varying assumptions. The ‘0/0approach’assumesnetcarbonsequestrationfromtreegrowthbalances with end-of-life carbon emissions, and thus, the biogenic carbon storage and its subsequentrelease are ignored [6]. The ‘-1/+1 approach’ considers biogenic carbon as a negative carbon emissionduring Life Cycle Stage A (the production stage) with carbon released during Life Cycle Stage C (theend-of-life stage) [6]. Dynamic LCA relies on the regrowth of biogenic carbon put into a building byaccounting for forest rotation periods over a period leading up to or during the building life [7].Carbondiscounting methods calculate a net present value of carbon emissions avoided with biogenic carbonstorage [8]. Ton-year accounting converts the time-value of biogenic carbon storage into a carbon offsetequivalent that yields anestimateofnegativeemissionsperton-yearofstorage[9].Insummary,thereisavital need toformulateanaccurateandstandardizedmodelingmethodologythatproperlyaccountsforthebenefits of biogenic carbon storage in buildings while also quantifying variability and uncertainty in thecalculation. Methods: I will formulate stochastic models for biogenic carbon that producemoreaccurateand consistent whole-building LCA results. I hypothesize that introducing these stochastic models todynamic LCA will yield the most accurate and descriptive results but will becomputationallyexpensiveand difficult to implement for regular use. Therefore, I posit that a stochastic model based on ton-yearaccounting will yield a viable, but easy-to-implement approach. My proposed method (likely to bestochastic ton-year accounting) will enable practitioners to circumvent end-of-life assumptions forbiogenic carbon, which are often the source of significant uncertainty.INTELLECTUAL MERIT: Despite being a necessary component to solving the climate crisis,embodied carbon reduction is not widely studied in theUS.LCAistheprimarymethodusedtoresearch,understand, and reduce embodiedcarbon,butdisparatedatasetswithwidelyvariedassumptionsprecludecomparison ofexistingstudiesandmoreadvancedanalysis.Withmyresearch,Iwillsteerresearchersandpractitioners towards a more complete understanding of the most important data needed to describe thetotal embodied carbon footprint of a building. With support from the National Renewable EnergyLaboratory (NREL) and the University of Colorado, Boulder’s Center for Research Data & DigitalScholarship, I will elevate the standards for embodied carbon and LCA research.BROADER IMPACTS: The two most impactful ways to facilitate widespread, effective embodiedcarbon reduction are (1) establishingembodiedcarbonbenchmarkingmethodologyand(2)disseminatingstatistically rigorous embodied carbon reduction tools. Benchmarks: Designers need science-basedembodied benchmarks to inform effective target setting, though none currently exist. These benchmarksmust describe expectedrangesofembodiedcarbonperusablefloorareaandaccountforvariablessuchaslateral design requirements, building geometry, building type, and soil quality. My research will beintegrated with ongoing research in Dr. Srubar’s Living Materials Laboratory to establish embodiedcarbon benchmarking methodology. Our lab group has collaborators at NREL who are particularlyinterested in integrating this methodology with their analyses of operational carbon (greenhouse gasemissions associated with building energy use). Embodied carbon reduction tools: Industry-standardLCA software programs are proprietary and provide conflicting results with ill-constrained uncertainty.My research will culminate in publishing an open-source, easy-to-implement software packagetoensurebroad implementation by researchers and practitioners. I will then distribute my findings throughprominent academic and industry organizations (e.g., CLF, AIA, SEI, UNFCCC, USGBC, WGBC).CONCLUSION: I am uniquely suited to conduct this research because of (1) my first-hand experiencewith LCA, engineering design work, and computational research, (2) the exemplary leadership I havedemonstrated in the embodied carbon space, and (3) my position in a leading embodied carbon researchgroupwithimportantrelationshipssuchasthatwithNREL.Myresearchwillelevatethestandardforhowacademic and industry practitioners conductLCA,whichwillbeessentialformoreeffectivelycombatingthe climate crisis. I will ensure the findings of my research are disseminated to salient technicalcommunities and so that they are applied ubiquitously in industry and academia.[1] TallWood Design Institute CLT Case Studies (2020); [2]K.Goulotiet.al.,BuildingandEnvironment(2020); [3] M.A. DeRousseau et. al., Journal of Cleaner Production (2020); [4] E. Igos et. al., TheInternational Journal ofLifeCycleAssessment(2019);[5]N.Bamberet.al.,TheInternationalJournalofLife Cycle Assessment (2020); [6] Hoxha et. al., Buildings & Cities (2020); [7] A. Levasseur et. al.,Environmental Science &Technology(2010);[8]L.Marshall,A.Kelly,WorldResourcesInstitute(2010);[9]P. Moura Costa, C. Wilson,Mitigation and AdaptationStrategies for Global Change(2000)"
3.0,"The problem: While most plants rely on soil nitrogen (N), plants capable of symbiotic Nfixation (SNF) can acquire N directly from the atmospheric N . Because N is inexhaustible,2 2SNF is convenient. However, SNF has a high cost2 due to the need to break the triple bond of N .2Following previous literature1,2, I define the C cost of SNF as the respiration (CO flux) needed2to drive SNF divided by SNF itself (N flux). Biochemical calculations estimate the C cost of2SNF to be slightly higher than using nitrate and much higher than using ammonium1.Measurements of these costs in nodules (the root structures that house symbiotic bacteria) havebeen close to the biochemical predictions2. However, these measurements have been carried outat constant temperatures. As explained below, the cost might vary widely across temperature.The cost of SNF helps determine its effectiveness, both within a plant (using SNF vs. soilN) and across species (competition between N-fixing and non-fixing plants). A lower cost makesSNF viable even when soil N is abundant, whereas a higher cost makes SNF untenable evenwhen soil N is scarce. Therefore, variation in the cost of SNF across temperature would have far-reaching implications. For example, it could help explain why N-fixing trees are successful inwarm areas3, and could also affect how SNF will change with climate. Despite its importance tofundamental biology, research on temperature responses of SNF has long been beset bytechnological constraints. Using a novel method that overcomes these constraints, I will ask onemain question: What is the temperature response of the C costs of SNF? I will address thisquestion using the tree Robinia pseudoacacia, which lives across a wide climatic range, accountsfor 64% of tree-based SNF in the contiguous USA5, and is common across Eurasia3.Hypotheses: My hypotheses are based on previous measurements of the components ofthe cost: respiration and SNF itself. Previous work6 has observed that SNF plummets at low(near 0°C) and high (near 50°C) temperatures. There are few data on nodule respiration atdifferent temperatures, but leaf respiration continues across 0-50°C7, suggesting nodulerespiration might too. Respiration rates well above 0 dividedby SNF rates near 0 mean that (H1) the C cost of SNF will bewell above the biochemical predictions at low and hightemperatures.My hypotheses about temperature optima stem frommeasurements from my lab, which recently developed a systemfor non-destructive, extremely sensitive, and continuousmeasurements of nitrogenase activity6. Preliminary researchusing this system has shown that the optimal temperature forSNF is much higher (29-36°C) than previously assumed(25°C)4, as shown in Fig. 1a. I do not know how nodulerespiration will change with temperature, so I have competinghypotheses. (H2a) If respiration peaks near the sametemperature as SNF (green curve, Fig. 1b), then the C costwill have a similar temperature optimum as SNF (greencurve, Fig. 1c). (H2b) Alternatively, if respiration risescontinually (blue curve, Fig. 1b), as leaf respiration does6,then the temperature optimum of the C cost will be lower thanthe optimum of SNF (blue curve, Fig. 1c).Hypotheses H1, H2a, and H2b are represented in Fig.1c, which also shows the equation for the C cost of SNF that is used in manymodels8(black curve). This equation assumes that the change in C cost of SNF with temperatureis inversely proportional to the SNF rate and is scaled to the biochemical C cost of SNF (7.5-12.5g C g N−1). As explained above, I believe this model is flawed as it does not account for hownodule respiration changes with temperature.Methods: Using growth chambers at Columbia University, I will grow 30 Robiniapseudoacacia seedlings (from seed) under a temperature regime of 26°C during day and 20°Cduring night using a 14-hour light and 10-hour dark photoperiod with relative humidity and CO₂concentrations of 70% and 400 ppm to emulate controlled climate conditions6. The seedlings willbe inoculated with slurries of crushed nodules as well as bacteria cultured from these nodules toensure the plants can establish symbiotic partnerships, and will be fertilized with limited levelsof N (1.5 g N m−2 yr−1) but ample amounts of all other nutrients to promote SNF.The nodules will be measured for SNF and respiration continuously across 1-50°C overthe course of 3 hours. The excised nodules will be placed in a sealed chamber with 2% acetylene(the concentration at which the system measures nitrogenase activity most precisely andaccurately6). After accounting for leakage and other factors6, the rate at which acetylene isreduced to ethylene (measured with a Picarro G2106 laser) gives a measurement of nitrogenaseactivity9. Preliminary work in our lab has shown that Robinia nodules have stable nitrogenaseactivity at least 6 hours after excision, and that the ratio of 15N to acetylene reduction is stable2across the temperature range of our study (TA Bytnerowicz, pers. comm.).CO₂ flux in the chamber will be synchronously measured by a Licor LI-62626 todetermine the temperature response of nodule respiration. I will process and analyze the data bymodifying R scripts previously developed in the Menge lab (ref. 4 for processing, TABytnerowicz, pers. comm. for temperature responses of SNF). The analysis will yieldtemperature response curves for SNF, respiration, and the ratio of the two (the C cost of SNF).Intellectual Merit: This research will answer questions fundamental to the biology of thesymbiotic relationship between legumes and N-fixing bacteria. At the level of plantecophysiology, at what temperatures is it energetically favorable for Robinia pseudoacacia to fixN? At the level of community ecology, how does Robinia pseudoacacia compete against non-fixing plants if it relies on SNF?Broader Impacts: The paucity of knowledge on how SNF and its C cost respond totemperature has been a major constraint on global biogeochemistry and climate modeling. Asdescribed above, temperature response functions for SNF and for the C cost of SNF are alreadyin use in terrestrial biosphere models, despite few data for the temperature response of SNF itselfand zero data for the temperature response of the C cost of SNF. My work will lead to directimprovements in the representation of SNF in these models, and thus will directly influence ourability to predict global biogeochemistry and climate change.In addition to publishing in academic journals, I will present my work at academicconferences, such as SACNAS’ National Diversity in STEM Conference, and outreachprograms, such as the Ecological Society of America’s SEEDS program and Women In Scienceat Columbia (WISC). As a former McNair Scholar, I am well aware of the disparity of resourceswithin underrepresented populations. Because of this, I willalso contribute my mentorship to theEnvironmental Justice and Urban Ecology Summer Research Program, a funded program forhigh school students at the Washington Heights Expeditionary Learning School.1Gutschick 1981, The American Naturalist. 2Tjepkema & Winship 1980, Science. 3Steidinger et al. 2019,Nature. 4Houlton et al. 2008, Nature. 5Staccone et al. 2020, Global Biogeochemical Cycles. 6Bytnerowiczet al. 2019, Methods in Ecology & Evolution. 7Heskel et al. 2016, PNAS. 8Fisher et al. 2010, GlobalBiogeochemical Cycles. 9Hardy et al. 1968, Plant Physiology."
4.0,"Key Terms: wildfire, water stress, land surface temperatureMotivation: The primary wildfire monitoring system in the United States is the National FireDanger Rating System (NFDRS)1,2. NFDRS maps are routinely used by land managers andregional governments to allocate fire mitigation resources and track fire risk. Although NFDRS isa standard risk assessment tool, it faces several disadvantages. Calculation of a NFDRS ratingrequires advance knowledge of site conditions and manual input of user parameters into closedsource software. For non-experts, NFDRS is difficult to use. Despite the complexity of NFDRS,its primary fire danger metric is a coarse 5-level categorical scale (from “low” to “extreme”) thatcannot be forecasted beyond 24 hours. This project will improve wildfire modeling withempirical techniques that enable proactive wildfire mitigation and long-term forecasting.Objectives: Climate change is expected to produce more intense wildfires more frequently in theAmerican West3. Wildfires are intensified by high plant biomass (i.e. fuel load) and low fuelmoisture. Both of these factors can be remotely sensed over large areas4,5. Given that vasculareffects of water stress linger in plants weeks to months after a drought6, drought conditions earlyin spring may predispose water-stressed forests to wildfire the following summer.This project will produce remote sensing data streams of plant water stress and vegetationgrowth as inputs for an open source wildfire predictive model covering forested regions of thewestern United States at 1 km2 spatial resolution. I hypothesize that (1) water stress in earlyspring increases wildfire intensity the following summer, (2) fuel load can be estimated froma time series of a vegetation growth index and (3) by measuring these variables in early spring(Fig. 1a), wildfire-prone regions can be identified weeks before ignition (Fig. 1b). In short,water-stressed locations with high plant biomass will be identified as wildfire hotspots before thefire season begins. I will perform computationally-intensive spatial analysis using open sourcecloud infrastructure to ensure usability by non-experts.Aim 1: Calculate and validate water stressindex. Water stress can be estimatedquantitatively from canopy temperature (i.e.leaf temperature) and vapor pressure deficitusing the crop water stress index (CWSI)5.CWSI is related to evapotranspiration and isapplicable to all leaved plants. To calculateCWSI, I will use vapor pressure deficit atdaily temporal resolution and 1 km2 spatialresolution from Daymet, a continuous,Figure 1. (a) Raster stacks of CWSI and GSIgridded meteorological product covering thebecome a time series for each grid cell. (b) Earlycontiguous United States7. To determineplant stress may indicate wildfire hotspots.canopy temperature, I will use land surfacetemperature (LST) calibrated with the normalized difference vegetation index (NDVI)8. My sourceof LST and NDVI data will be the Terra MODIS satellite, for which cloud-free, gap-filled LSTdata have recently been developed9. This method of determining canopy temperature requires onlyoccasional NDVI values, so clouds will not prevent canopy temperature measurement8. AlthoughMODIS is approaching retirement, its 20-year data archive is desirable. For recent fire years I willalso work with the current-generation LST sensor ECOSTRESS. A field campaign will beperformed in fire-prone western forests to calibrate CWSI, to validate canopy temperaturemeasurements, and to determine the relationship between CWSI and plant water potential.Aim 2: Calculate and validate fuel load index. I will employ the growing season index (GSI)to estimate fuel load over time. The GSI quantifies how plant growth is limited or unconstrainedby humidity, air temperature, or photoperiod4. GSI therefore remains measurable under all weatherconditions using Daymet data, unlike spectral biomass indices such as the leaf area index. GSI willbe validated against in situ measurements of fuel load as part of the field campaign in Aim 1.Aim 3: Model wildfire likelihood. I will use an existing dataset of wildfire occurrence from 2000-2019 to produce annual wildfire presence maps aligned on the same grid as the CWSI and GSIcalculations. For each year of data, I will calculate CWSI and GSI for each grid cell at dailytemporal resolution from February 1 to May 31 to produce a stack of CWSI and GSI grids throughtime (Fig. 1a). Possibly, the end of the data collection period will be adjusted later or earlier in theyear to optimize model performance and parsimony. Each cell of the raster stack will be insertedinto a high-dimensional dataset where each row is a time series of CWSI and GSI values and asingle column indicating whether the cell experienced fire that year. I will use the CWSI and GSItime series as predictors in a partial least squares regression (PLSR) model with a logarithm linkfunction. PLSR reprojects the predictor variables in a typical linear regression to a lower-dimensional space that is maximally correlated with the response matrix. PLSR therefore resolvesissues with correlated predictor variables and, via weights, identifies which CWSI and GSImeasurements contribute most to the model prediction. Wildfire occurrence is a binary responsevariable, so a logarithm link function will enable calculation of wildfire likelihood for each cell inthe modeling area. A variant of PLSR for binary classification tasks, partial least squaresdiscriminant analysis, is also a candidate modeling approach.I will evaluate the proposed model with standard measures for a binary classifier with animbalanced response variable. I will also compare the proposed model against NFDRS mapsproduced on May 31 the same year. Project success is defined as accurate prediction of 1 km2pixels as fire-present or fire-absent, emphasizing a low false negative rate, and a model whichland managers prefer over existing NFDRS maps for allocating management resources.Intellectual Merit: This study will improve wildfire prediction by employing empirical methodsand longer forecasting times. Improved wildfire modeling will enable proactive wildfiremitigation and clarify factors that drive wildfire occurrence. In particular, this project will producethe most spatially extensive measurement of forest water stress to date and determine how wildfireis influenced by water stress early in the growing season. This study will also demonstrate howremote sensing datasets can be combined to model ecosystem processes.Broader Impacts: Wildfire intensity and frequency is expected to increase in the American Westas climate change continues3. Wildland firefighters must effectively allocate tens of billions ofdollars to mitigate this annual emergency. Advance warning of fire risk will enable proactivemanagement that utilizes resources effectively. Such warnings also enable the public to avoidinjury and property damage. In society at large, less wildfire smoke reduces respiratory illness andavoids air travel disruption. Knowledge of forest response to drought conditions will also improvetimber production in the logging industry. All computer code and data products generated by thisproject will be open source. Model results will be distributed via a non-technical online dashboard.I will demonstrate the analysis workflow at workshops and reach out to potential users who wouldbe interested in using the wildfire model produced by this work.References: [1] U.S. Forest Service p. INT-GTR-169. [2] W.M. Jolly, National NFDRS 2016 RolloutWorkshop (2018). [3] A.P. Williams et al., Earths Future 7, 892 (2019). [4] W.M. Jolly et al., Glob. ChangeBiol. 11, 619 (2005). [5] S.B. Idso et al., Agric. Meteorol. 24, 45 (1981). [6] C.R. Brodersen et al., Annu.Rev. Plant Biol. 70, 407 (2019). [7] J.T. Abatzoglou, Int. J. Climatol. 33, 121 (2013).[8] M. Blum et al., Agric. For. Meteorol. 176, 90 (2013). [9] S. Shiff et al., Sci. Data 8, 74 (2021)."
5.0,"IntroductionMany metabolites are reactive and unstable, making them prone to undesired chemicalmodification outside of the intended pathways1. While metabolism as a whole is well-studied, thebiochemical mechanisms of managing such reactive metabolites are not. Proteins which are closelymetabolically involved with each other can frequently be found in protein-protein interactions which areessential for nearly all cellular function. It is currently believed that most, if not all, proteins participate inprotein-protein interaction networks2. Such a network suggests the possibility of metabolic substratechanneling, in which a metabolite travels from one enzymatic active site to another without freely diffusinginto the surrounding medium. Evidence for substrate channeling has been observed in enzymes of manymajor biochemical pathways and is being increasingly recognized as foundational to metabolic regulation3.Through substrate channeling, an intermediate metabolite may be retained for use in a specific pathway,protected from degradation, or prevented from causing damage to the cell4.The goal of my proposed research is to discover mechanisms by which unstable metabolitesare managed in biochemical systems. This research will provide insight into cellular control overreactive metabolites, protein-protein interactions, and substrate channeling. To achieve this goal, Iwill uncover mechanisms of substrate channeling for the reactive metabolite Δ1-pyrroline-5-carboxylate(P5C). P5C is an unstable intermediate at the intersection of proline, glutamate, and ornithine metabolicpathways (Figure 1). P5C has been shown to react with other metabolites and inhibit enzymes and is linkedwith human disease including hyperprolinemia type II5.Under physiological conditions, P5C existsin dynamic equilibrium with glutamic semialdehyde(GSA) via spontaneous nonenzymatic hydrolysis andcondensation reactions. In the proline biosyntheticpathway, P5C serves as an intermediate forproduction of proline from glutamate. Glutamate isreduced to GSA by P5C synthetase (P5CS), then P5Cis reduced to proline by P5C reductase (P5CR).Similarly, the glutamate biosynthetic pathway usesP5C as an intermediate in the production of Figure 1. Involvement of P5C with proline,glutamate from proline. Proline is oxidized to P5C by glutamate, and ornithine metabolism.proline dehydrogenase (PRODH), then GSA is Adapted from Stránská et al6.oxidized to glutamate by P5C dehydrogenase(P5CDH). In an alternative pathway, P5C can be produced or consumed by ornithine aminotransferase(OAT), which uses ornithine as a reactant or product for P5C production or consumption. Under normalphysiological conditions, OAT functions in the “forward” direction from ornithine to P5C, however, underextreme dysregulation, OAT may catalyze the opposite “reverse” reaction from P5C to ornithine6.Aim 1: Protein-protein interactions among enzymes of the proline/glutamate/ornithine pathways.Existing research already supports protein-protein interactions between PRODH and P5CDH7.However, the involvement of other enzymes in the proline, glutamate, and ornithine pathways is unknown.This aim will focus on potential protein-protein interactions between P5CS–P5CR, OAT–PRODH, andOAT–P5CDH. Because of the current discord surrounding the cellular locations of P5CS and P5CR (e.g.,mitochondrial vs. cytosolic), OAT–P5CS and OAT–P5CR complexes will also be considered. In fact, theexpression in some organisms of ornithine cyclodeaminase, which directly catalyzes ornithine to proline,supports potential protein-protein interactions between OAT and P5CR8.Enzymes used in these studies will be expressed as His-tagged proteins, as done previously withPRODH and P5CDH7. Stable protein-protein interactions will be examined through coimmunoprecipitationwith anti-His-tag antibodies and supplemented with pull-down assays via Ni-NTA chromatography. In eachexperiment, the identification of specific proteins will be accomplished through Western blot.Transient protein-protein interactions will be observed through surface plasmon resonance (SPR)at the BIAcore 3000 instrument maintained by the Nanomaterials Characterization Core Facility at theUniversity of Nebraska Medical Center. For SPR, an enzyme’s His-tag will be used to anchor the proteinto a Ni-NTA sensor chip. SPR will allow the characterization of protein-protein interaction association anddissociation constants. For cellular characterization of protein-protein interactions, fluorescence resonanceenergy transfer and yeast two-hybrid system experiments will be carried out2,7.Aim 2: Kinetics of P5C channeling among enzymes of the proline/glutamate/ornithine pathways.Even in the case of weak, transient protein-protein interactions, substrate channeling may occur.Substrate channeling of P5C has been identified in the glutamate biosynthetic pathway but has yet to becharacterized among other protein pairs of the proline, glutamate, and ornithine pathways7.Previous studies with PRODH and P5CDH have used a free diffusion two-enzyme model tosimulate reaction progress curves. In these models, the presence of substrate channeling can be inferred bycomparing theoretical non-channeling versus experimental differences in transient time7. In addition, P5Ctrapping studies will be carried out to support results from the simulated progress curves. In theseexperiments, ortho-aminobenzaldehyde (oAB) is conveniently used to “trap” P5C in aspectrophotometrically-monitored oAB–P5C complex. In the presence of P5C channeling, less P5C willfreely diffuse to be complexed with oAB as compared to a negative non-channeling control. In addition, Iwill use stopped-flow kinetics to measure the transient time of P5C channeling between the enzymes.Intellectual MeritMy proposed graduate research plan to examine unstable metabolite management is a naturalcontinuation of research I have been involved with in the Becker lab at the University of Nebraska-Lincoln.Previously, I conducted research investigating P5C channeling in the glutamate biosynthetic pathway, so Iam well-prepared to expand into protein-protein interactions and broader aspects of substrate channeling. Ihave experience with fundamental methods used for protein and metabolic research, including proteinoverexpression, purification, UV-visible spectroscopy, stopped-flow, and steady state enzyme kinetics.This research will be well-supported in the Becker lab at the University of Nebraska-Lincoln, homeof the Center for Biological Chemistry and the Redox Biology Center. My projects will be pursued incollaboration with existing structural biology partners in the Tanner lab at the University of Missouri.Broader ImpactsIt is estimated that over 80% of proteins rely on protein-protein interactions2, and substratechanneling has been identified in many major metabolic pathways3. My research will provide specificknowledge in unstudied interactions and channeling between proteins of the proline, glutamate, andornithine pathways, offering insight into the biochemical methods of unstable and reactive metabolitemanagement. Additionally, this research will serve as a case study to lay the foundation for metabolitemanagement experiments in other major pathways. Research surrounding P5C has implications forbiological mechanisms of metabolism relevant to all life.Outside of strict academics, I will leverage this graduate research in a way that benefits thescientific community and the general public. I will present results at regional and national conferences andpublish using accessible language in open-access journals. Through this research, I will take advantage ofimportant mentorship opportunities. Mentoring undergraduate or beginning graduate students in the labwill allow me to help develop the next generation of scientists as I provide a rigorous yet supportiveenvironment to foster academic, professional, and personal growth.References1Lerma-Ortiz et al. Biochem. Soc. Trans. 2016. 2Berggård et al. Proteomics. 2007. 3Sweetlove et al. Nat.Commun. 2018. 4Liu et al. Arch. of Biochem. & Biophys. 2017. 5Farrant et al. J. Biol. Chem. 2001. 6Stránskáet al. Plant Sig. & Behav. 2008. 7Sanyal et al. J. Biol. Chem. 2015. 8Goodman et al. Biochem. 2004."
6.0,"billion breeding birds11. Among the species that showed some of the steepest declines were migratoryshorebirds, one of which is the Lesser Yellowlegs (LEYE, Tringa flavipes). LEYE, a once common birdthat breeds in the boreal forest, has declined by 80% range-wide since 19661,3 and is estimated to lose anadditional 50% of its global population within 11 years9. As a result, LEYE has been designated asfederally threatened in Canada and a species of high conservation concern in the U.S.9 This species likelyencounters multiple threats during its 8000-mile migration journey3, but agricultural practices in one oftheir most critical stopover regions, the Prairie Pothole Region (PPR) have the potential to impact muchof the breeding population3. Reductions in survival due to exposure to agricultural insecticides in the PPRis one novel hypothesis that has been proposed to explain why many shorebirds in North America havedeclined, including LEYE. However, this hypothesis has not been thoroughly explored. Investigatingpopulation-level threats to rapidly declining species like LEYE is a critical conservation priority.The migratory period may be the most critical to annual survival due to the high energetic demands thatif not fulfilled can lead to reduced survival and reproductive success10. However, to ensure a timely arrivalat breeding and wintering sites, migratory birds must balance their time spent refueling at stopover siteswith their migration speed10. The optimal bird migration theory predicts that migrants constrained bytime should adjust their stopover duration to their refueling rate, and thus minimize time spent on migrationto maximize their fitness2. Research has shown that individuals with low refueling rates depart later fromtheir stopover sites relative to individuals with higher refueling rates, indicating that birds wait until theyreach a threshold of fuel stores before departing10. This suggests that the quality of stopover habitat affectsthe decision of when to leave a stopover site, which is of critical importance for migration success.During migration, shorebirds are exposed to neonicotinoids6, the most widely used class of insecticidesin the world, which pose significant risks to birds and other wildlife2,4. Neonicotinoids cause impairedimmune function, rapid reduction in food consumption, and lower reproductive success, which can resultin greater energetic demand, reduced fat stores, delayed migration and low survival1,4. Because migrationdelays can carry over to affect survival and reproduction1, neonicotinoids have the potential to imposepopulation-level impacts. Although their adverse impacts have been established in songbirds1,4, we havelittle information regarding their effect on shorebirds, highlighting a critical information gap. In a recentstudy, GPS transmitters were deployed on over 100 LEYE in Alaska and Canada3. Of the birds that bredwest of James Bay, Ontario, 90% stopped in the PPR to refuel during their migration to South America,with stopover duration times varying from a few days to over a month, indicating the importance of thisregion during migration. High presence of neonicotinoids has been reported in these prairie wetlandsand agricultural fields5, which are important foraging habitats for migrating shorebirds.Proposed Research: Using the optimal bird migration theory as a framework, I will investigate the threatof neonicotinoids on the fitness and migration of fourteen shorebird species of high conservation concern9that heavily rely on the PPR. This study will investigate a potential contributor to the observed populationdeclines of shorebirds and will help guide on-the-ground management decisions for agricultural solutions.Hypothesis: Migrating shorebirds with high plasma concentrations of neonicotinoids will bephysiologically and behaviorally impaired relative to birds with low concentrations. Similar to whathas been observed in studies of captive birds1, I predict that wild shorebirds with high neonicotinoidconcentrations will exhibit: A) lower plasma triglyceride and higher uric acid levels, indicating lowerfueling rates and fat deposition, B) poorer body condition (measured by body mass and fat scores), C)reduced foraging behavior, D) prolonged migration stopovers, and E) later migration departure dates.Research Plan: To establish an environmental gradient in pesticide contamination, I will pre-screenwetlands by measuring neonicotinoid concentrations in water samples. At sites with low and highconcentrations of this pesticide, I will capture LEYE and thirteen other shorebird species, collect bloodsamples, and measure body mass and fat over two fall and two spring migration seasons. I will measurethe concentrations of neonicotinoids in bird plasma as well as key metabolites in blood using cutting-edge LC-MS/MS techniques8. Prediction A: I will measure plasma concentrations of triglycerides anduric acid and correlate them to plasma neonicotinoid concentrations7, thereby testing for a link betweenpesticides and fuel deposition rates. Prediction B: I will compare body mass and fat scores of birds withhigh, moderate, and low neonicotinoid concentrations to better understand how neonicotinoids affectbody condition. Prediction C: I will conduct behavioral surveys on shorebirds at high and lowcontamination wetlands to determine if there is a relationship between neonicotinoid exposure andforaging behavior. After randomly selecting an individual, I will record the length of time spent indifferent behavior categories (foraging, resting, etc.) for a duration of 5 minutes. This will be repeated for10 individuals per wetland. To account for time and weather, I will conduct surveys in the morning andwill record temperature, wind, and cloud cover. Predictions D & E: To understand if neonicotinoids areimpairing migratory ability and causing migration delays, I will deploy Lotek PinPoint GPS transmittersthat will allow me to track the migration, departure dates, and stopover durations of birds with varyinglevels of neonicotinoid exposure. The results of this study will provide critical information on howenvironmental contaminants interfere with optimal migration. To minimize confounding factors, I willonly capture adults, and will stratify results by sex, species, and migration season.Facilities & Mentorship: I have two mentors: Dr. Christy Morrissey at the University of Saskatchewanand Dr. Courtney Conway at the University of Idaho (UI) where I will matriculate. Dr. Morrissey is aglobal leader in avian ecotoxicology and has developed novel and extremely sensitive methods forneonicotinoid analysis8. Dr. Conway is a renowned expert in ecology and migration of birds.Intellectual Merit: Regional efforts to study neonicotinoids in songbirds1 and wetlands6 in the PPR areongoing and our project expands on this by investigating the effects of neonicotinoids on shorebirdhealth, a novel yet timely research topic. This project would build upon an existing and growingpartnership among 8 state agencies, federal agencies, South American agencies, and universities in boththe U.S. and Canada, as well as farmers and landowners in both countries. My findings will advance thefields of migration ecology and ecotoxicology and will be highly applicable to developing conservationstrategies for shorebirds in the PPR because it will improve our understanding of the effects ofagricultural insecticides. This project aligns with the 3-Billion Birds Campaign11 to reverse populationdeclines and is part of an international effort to understand threats impacting LEYE throughout theirannual cycle. This study fills a critical information gap by investigating a major threat during migrationthat may have carry over effects to survival and reproduction and will inform managers and farmingcommunities about the effects of agricultural insecticides on birds.Broader Impacts: To increase participation of underrepresented minorities in STEM, I will developan internship opportunity through the Doris Duke Conservation Scholars Program at UI that will engagestudents from diverse backgrounds to participate in my research and develop their own independentprojects. To improve STEM education and outreach, I will create a program called Backyard BirdBanding for underrepresented students from rural schools and tribal communities to watch how wecapture and band shorebirds and participate when deemed appropriate. I will enhance the experience withengaging kid-friendly games and shorebird ID cards for teachers and students to use while out in the field.This event will be recorded and made publicly available world-wide on social media. I plan to developthis program with the following rural ND schools: Glenburn, Kenmare, and Turtle Mountain CommunityHigh School. In addition to hands-on field activities, I will use real-time shorebird migration data to linkschools through social media platforms in ND and Alaska, and through the Outreach InternationalEnvironmental program in South America. Students will be able to track the migrations of birds tagged inor passing through their neighborhoods via Movebank, an animal tracking database. Our outreach goal isto engage with at least 200 students in our programs. To increase public engagement, I will develophigh-impact outreach and educational materials about shorebird friendly agricultural practices andalternative biological pesticides in the PPR. I will work closely with the Lesser Yellowlegs workinggroup, the Coalition for Conservation & Environmental Education, farmers, and landowners in the PPR tofind practical, long-term solutions that will benefit bird populations and farming communities.1Eng et al. 2019. Sci. 365:1177; 2Alerstam et al. 1990. Bird Mig. 331-351 ; 3McDuffie et al. 2021. Pro. 1-134; 4Gibbons et al. 2015. Env. Sci. Poll. Res. 22:103; 5Main et al. 2014. PLOS 9:1; 6Malaj et al. 2020.Sci. Tot. Env. 1-10. 7Li et al. 2020. Nat. Sus. 8Bianchini et al. 2018. Env. Sci. & Tech. 52:13562; 9U.S.Shore. Cons. Plan. 2016. 10Zhao et al. 2017. Move. Eco. 5-23; 11Rosenberg et al. 2019. Sci. 120-124."
7.0,"Hypothesis: Ocean wave energy has vast potential as a renewable power source, but traditionalsequential design methods perpetuate prohibitively high device costs. Applying systems optimizationtechniques to wave energy would unlock new architectures that are cost-competitive at utility scale.Introduction: Climate change is the most critical problem facing humanity. It threatens warming,flooding, erosion, and storms that damage infrastructure, agriculture, health, and biodiversity. The fatalpotential for 4oC of warming can be limited to 1.4oC if complete decarbonization occurs by 2055 [1]. Theelectricity sector contributes 40% of global CO emissions, representing perhaps the largest challenge and2opportunity for decarbonization [2]. Existing renewables have limitations: wind and solar are intermittentand unpredictable; hydropower and geothermal have few suitable sites. To reach 100% clean electricity,we must supplement these sources by developing diverse renewables to technical and economic maturity.Ocean wave energy is uniquely attractive due to its predictability, geographic abundance, and continuousavailability [3]. Despite an ability to fulfill 34% of US electricity demand [4], wave energy technology’shigh cost, long design cycles, and risk-intensive investment have prevented full-scale deployment [3].The wave energy converter (WEC) industry has so far followed a trajectory similar to aerospace,focusing on technical risk reduction with an expectation that costs will fall after the technology matures.However, recent analysis indicates that this path is infeasible, and success hinges on a reinvention of thetypical design cycle to emphasize early cost and performance innovation before deploying expensiveprototypes [5]. Multidisciplinary Design Optimization (MDO) and Control Co-Design (CCD)techniques can provide the design paradigm shift that is required for dramatic cost reduction.MDO and CCD are emerging techniques that depart from the standard sequential design processby considering subsystem interactions early on. MDO is an optimization framework for interdisciplinarydesign problems. MDO has been used successfully in the automotive, energy, and aerospace sectors butnever attempted for wave energy due to novelty and computational costs [6]. CCD, the use of controlprinciples to inspire device design, has never been applied to a utility-scale WEC for similar reasons.CCD offers significant benefits: in offshore wind turbines, CCD decreased structural loading by 99% [7],and one estimate predicts 30% cost reduction potential for wave energy [8]. Overall, MDO and CCD arepromising methods to advance wave energy design towards full decarbonization of the electricity sector.Research Plan: This project will be completed over the course of my PhD studies at CornellUniversity in the Symbiotic Engineering Analysis (SEA) Lab, led by Professor Maha Haji.Objective 1: Develop a novel WEC design framework using principles of MDO and CCD.I will create a multidisciplinary model of WEC dynamics and performance. As shown in Fig. 1, the modelwill bridge previously disparate simulations in controls, structures, powertrain, and hydrodynamics,which are recognized as the four most impactful subsystems to drive down WEC costs [9]. I willprioritize appropriate model fidelity, simplifyingwherever feasible while still capturing importantsubsystem interactions. One key tradeoff is thedecision to model in the stochastic, frequency,or time domain. Degrees of freedom will beselected strategically, balancing computationaltractability against flexibility to describe diversearchitectures. Objective 1 is attained when adesign optimization process can be clearlyarticulated and validation efforts show that themodel can correctly predict performance ofexisting WECs. This framework forms thefoundation for the second research stage.Objective 2: Utilize the design framework to obtain a novel WEC architecture that is cost-competitive at utility-scale. The MDO-CCD model and process developed in Objective 1 will beimplemented and iterated upon, focusing on interactions between device controls and structures. Initially,optimization will identify the best combination of existing configurations; ultimately, model degrees offreedom will be extended, allowing optimization to yield new architectures altogether. Sensitivities willquantify impacts to inform further innovation. Objective 2 is complete when a WEC design with levelizedenergy cost below $0.30/kWh is found, representing a 75% cost reduction from current technology [10].Objective 3: Validate key features of the optimal design through wave tank testing. The finalstage of research involves the detail-design of a scaled prototype of the optimal solution, followed by themanufacturing, testing, and data analysis of the prototype. The testing investigates real-time controlimplementation, validates model robustness, and provides a practical industry-relevant realization of thenew design process and design. Cornell University is equipped with the facilities to enable this testingthrough the DeFrees Hydraulics Lab wave tank, and collaborations with nearby institutions including theUniversity of New Hampshire or the University of Maine could be leveraged for larger wave tank access.Objective 3 is complete when any major differences between model and test are identified and explained.Intellectual Merit: Wave energy is an under-utilized and under-researched renewable sourcewith great potential. Multidisciplinary integration is expected to be a key enabler of future cost-competitive wave energy. The proposed work will be the first to apply MDO and CCD techniques to autility-scale wave energy converter, potentially unlocking radical cost savings. My design process will bethe first to unify disparate WEC domains; my optimization will provide new insights to advance the fieldtoward design convergence; and my test data will confirm understanding and applicability of these novelcontributions. My undergraduate background provides relevant technical depth in mechanical design,power electronics, hydrodynamics, and controls, and above all I have the systems mindset to effectivelyunify these fields. This experience, coupled with the SEA Lab’s expertise exploiting synergies in marinetechnologies, uniquely qualifies me to succeed at the proposed research.Broader Impacts: Wave energy has the potential to meet 34% of national electricity demand [4].My research will enable cost-competitive wave energy, adding to the renewables mix to combat climatechange. The integrated MDO-CCD design process applies widely to any system with multidisciplinaryinteractions and embedded dynamic controllers. Thus, the proposed research advances knowledge in bothrenewable energy alternatives and systems optimization methods, both of which broadly benefit society.To circulate my research to the academic community, I will publish in journals such as OceanEngineering and Renewable Energy and present at diverse venues such as IEEE OCEANS and CESUN.Leveraging my advisor’s connections, I will initiate collaborations with the National Renewable EnergyLab and companies such as CalWave. These partnerships will amplify the impact of my research byensuring its alignment with current industry goals and its rapid dissemination to others upon completion.Finally, I will engage in outreach to both the general public and students of all backgrounds.Societal acceptance is key for widespread adoption of wave energy technology, and education about theopportunities and challenges of renewable energy encourages sustainable habits as well as an interest inengineering. My contributions to STEM outreach and curriculum development through SWE, Splash, andGRASSHOPR are detailed in my personal statement. In sum, my work as an NSF fellow wouldpromote STEM engagement, advance systems design techniques, and enable a carbon-free future.References:[1] “Climate change 2021,” IPCC, 2021. [2] “Net zero by 2050,” IEA, 2021. [3] F. Mwasilu and J. Jung,IET Renewable Power Gener, 2019. [4] L. Kilcher, et al., NREL, TP-5700-78773, 2021. [5] D. Bull etal., Sandia, SAND2017-4507, 2017. [6] J. Sobieszczanski-Sobieski and R. Haftka, Struct. Optim., 1997.[7] X. Du, et al., ASME 2020 Int. Mech. Eng. Congr. and Expo., 2021. [8] M. Garcia-Sanz, ARPA-E,2018. [9] D. Bull, et al., Sandia, SAND2013-7204, 2013. [10] G. Chang, et al., Renewable Energy, 2018."
8.0,"of bacteria (~99%), followed by archaea, then eukaryota, collectively forming the human microbiome.1,2Over the past decade, research has revealed impacts of colon microbiota on human physiology, includingroles in digestion, metabolism, immune system regulation, hormone signaling, and development.3,4 Colonmicrobiota reside in a specialized niche—a layer of mucus secreted by the colon epithelium, heavilycomprised of the gel-forming protein MUC2.5 Adenomatous Polyposis Coli (APC) is a scaffolding proteinthat has long been studied as a tumor suppressor antagonist of the Wnt/-catenin signaling pathway, withadditional roles less defined. In Dr. Kristi Neufeld’s lab at the University of Kansas, we have accumulatedevidence suggesting multiple novel functions of APC, including in the expression of MUC2. Given thisevidence, I hypothesize that APC directly or indirectly promotes MUC2 expression, and therefore hasa role in colonic mucus generation, microbiome homeostasis, and colon function. Uncovering roles ofAPC in MUC2 expression would fill a critical knowledge gap not only in our understanding of the humanmicrobiome and its homeostasis, but also in the cellular function of highly conserved APC-like orthologsfound nearly universally across invertebrates and vertebrates.6In a recent study, using Human Colon Epithelial Cells(HCECs), we found that reduction of APC led to a 75% decreasein IL-1R, an integral membrane receptor protein that, when boundto a ligand, promotes MUC2 expression.7 We found a similar trendin the IL-1R ligand, IL-1. To further investigate the relationshipbetween APC, IL-1 signaling, and MUC2 expression, we modifieda human colon cancer cell line, DLD-1, using CRISPR/Cas9. Weinserted wildtype (WT) APC under control of a doxycycline (Dox)responsive promoter, which allowed us to treat cells with Dox andinduce expression of WT APC. Using unmodified (parental) DLD-1s as a control, we found that IL-1 and WT APC individuallyincreased MUC2 expression 2-fold, but when expressedsimultaneously, MUC2 expression increased 15-fold (Fig. 1).7 Figure 1. RT-qPCR for MUC2 inThis evidence suggests that IL-1 signaling and APC act Parental and APC-Inducible DLD-1synergistically to promote MUC2. This evidence also suggests that cells. (Two-way ANOVA withIL-1R activation increases MUC2 expression, which may occur Tukey’s range test, **P <0.005,through signaling of PKC-, a protein previously correlated with IL- ***P<0.0005, ****P<0.0001)1R activation and MUC2 expression.8 To investigate DNA-binding of APC that might be involved in MUC2 promotion,we used a dataset from a recent ChIP-seq study9 and foundthat APC binds to the promoter of the MUC2 gene.Combined with our APC gain and loss of function studies, Icreated a simple model to reflect hypothesized interactionsbetween IL-1R, APC, PKC-, and MUC2 (Fig. 2).Aim 1: Elucidate the Molecular Roles of APC in MUC2Expression. As evidenced by our research, APC expressionFigure 2. Model of relationships that wouldleads to an increase in MUC2 mRNA levels and APC lossallow IL-1R and APC to increase MUC2leads to reduction in IL-1R. However, whether APC inducesexpression individually and synergistically.*translation of MUC2 or IL-1R is yet to be determined. Withthe help of Dr. Yoshiaki Azuma, a CRISPR/Cas9 expert at the University of Kansas, I have designed andbegun creation of two novel cell lines to uncover the molecular roles of APC. By modifying HCEC andDLD-1 cells, we will be able to perform quick, cheap, efficient, and consistent 1) APC KO using an Auxin-Inducible-Degron (AID), 2) basal-level APC expression with no treatment, and 3) APC overexpressionusing Tetracycline-Inducible-Expression (TIE). The AID system uses OsTIR1, an auxin-responsiveubiquitin ligase, to polyubiquitinate AID-tagged proteins for degradation in the proteosome.10 I am taggingendogenous WT APC with an AID for APC KO. The TIE system initiates transcription of a gene througha tetracycline (TET)-responsive promoter11 and will be paired with the endogenous AID-tagged APC. I willuse MUC2 promoter/Luciferase Reporter Assays (LRAs) and our in-lab luminometer to confirm responseto various levels of cellular APC. An increase in luciferase indicates that APC increases MUC2 promoteractivity, a neutral result indicates no effect of APC, and a negative result indicates APC reduces MUC2promoter activity. This design can be repeated with reporters for all my candidate gene promotors, includingIL-1R, MUC2, synergism between APC and IL-1 for MUC, etc. Regardless of my findings, these resultswill direct my further studies on APC and the other proteins of interest in this system, includinginvestigation on protein localization, protein modifications, and signaling pathways. I will use co-immunoprecipitation followed by mass spectrometry performed at KU’s Core Mass Spec Lab to identifysignaling proteins, florescence microscopy using our in-lab microscope to understand protein localizationsand treatment phenotypes, SDS-PAGE and western blots for exposing signaling pathway patterns, mobilityshift assays for uncovering post-translational modifications and protein activation, and much more tounderstand my proteins of interest and their signaling pathways.Aim 2. Monitor Impacts of APC on Colon Microbiome Homeostasis. In another recent study, we createdtransgenic mice with an APC allele containing mutations in the protein’s Nuclear Localization Sequences(APC-mNLS), preventing APC from entering the nucleus, and thus, inhibiting potential direct APC-drivennuclear promotion of gene expression. In this study, we uncovered that APC-mNLS mice had a >90%decrease in MUC2 expression following colon epithelial injury compared to control mice.12 I hypothesizethat APC-mNLS mice have lower levels of MUC2, reducing the amount of mucus habitable by microbiotain the colon and thereby decreasing microbiome diversity and microbiota count. Using this mouse strain,with our extensive mouse care facilities and IACUC approval (137-01), I will collect fecal and colonicmucus swab samples of both mutant mice and control littermates for Zymo Research’s MicrobiomeAnalysis Service,13 which provides publication-ready data of a list of microbiomic data, including absolutemicrobiota counts, multi-kingdom accounts of microbiome diversity within and between samples, andmore. In addition to these data, I will use immunohistochemistry to visualize MUC2 in the colon epitheliumusing anti-MUC2 antibodies, comparing mutant and control littermates. This will provide comprehensiveevidence of any changes to the microbiome and the colonic mucus layer in response to loss of nuclear APC,providing insight into APC’s physiological function and connection to the microbiome in vivo.Broader Impacts: The proposed research will expand our understanding of non-Wnt functions of APC,the function of APC in the generation of the colon mucus layer, the impact of APC on the microbiome, andprovide insight on the function of APC orthologs across the animal kingdom. These data have the potentialto uncover numerous novel functions of APC and better our understanding of microbiome homeostasis inrelation to physiological function, a field of study that is still in its infancy. The cell lines I will create anddata I will collect will be powerful research tools for myself, the Neufeld lab, and other researchers studyingAPC and colon microbiomics. I will communicate the conclusions of my research through publications, aswell as through posters and oral presentations at regional and national conferences. I will also continue todevelop the Rural Scientist Initiative (RSI) outlined in my personal statement; the NSF GRF would allowme to dedicate more time to the RSI while attending graduate school. Development of the RSI includespresenting this research to rural students along with information on scientific careers to ~15-30 of the 70high school students at Norwich High School per year, based on student interest. I will continuecommunicating my experience as a scientist from several underrepresented communities to these students,encouraging rural students to pursue careers in scientific research.References. 1. Sender et al (2016) PLOS Biol 14(8):e1002533; 2. Qin et al (2010) Nature 464:59-65; 3.Heintz-Buschart et al (2018) Trends Microbiol 26(7):563-74; 4. Schroeder et al (2016) Nat Med 22:1079-89; 5. Johansson et al (2016) Nat Rev Immunol 16:639-49; 6. Bienz et al (2002) Nat Rev Mol Cell Biol3:328-38; 7. Gomez et al (2020) Exp Physiol 105(12):2154-67; 8. Tiwari et al (2011) J Immunol187(5):2632-45; 9. Hankey et al (2018) Oncotarget 9(58):31214-30; 10. Natsume et al (2016) Cell Reports15, 210-18; 11. Das et al (2016) Curr Gene Ther 16(3):156-67; 12. Zeineldin et al (2014) Carcinogenesis35(8):1881-90; 13. Zymobiomics (2020) 5:159-63; *Created with BioRender.com"
9.0,"Introduction:In recent decades, reports of re-emergingand novel phytopathogens have increaseddramatically in forests.1These pathogens threatenforest health and pose serious risks to plantbiodiversity. Studies indicate climate change (e.g. warmer temperatures, wetter growing seasons) hasaccelerated forest decline within the United States by expanding plant pathogen ranges.2The effectsofclimate change have heightened and extended the infection period for pathogens, making treesmore vulnerable to outbreaks of less aggressive phytopathogens.3Plant pathogens in the familyNectriaceae, including undescribed species, have been indirectly linked to climate change.4In addition,these changes in temperature are known to increase sporulation and virulence of fungal pathogens, as coldperiods would ordinarily reduce the populations of pathogens by arresting their growth. Trees at highelevations including red spruce, Fraser magnolia, yellow birch, striped maple and mountain ash arebuffered from many pathogenic fungi due to persistent cold temperatures in their habitat; however,warmer winters have increased the risk for biological invasion of these species.5Although significant progress has been made regarding the taxonomy of these nectriaceous fungi,additional data are needed to clarify species boundaries and their evolutionary relationships. Likewise,these fungi pose risks that must be fully assessed by more robust studies on host range and pathogenicity.Long studied forests are now experiencing epidemics of these emergent plant pathogens (EPPs).6Whilebeech bark disease andFusarium-associated diseasesare highly-studied pathosystems, native, often lessvirulent, nectriaceous fungi are becoming more abundant. My objective is to protect Appalachian forestsby 1) drawing connections between abiotic stressors and the prevalence of nectriaceous fungal pathogens,2) identifying these fungal pathogens, and 3) assessing the effects of temperature on the aggressiveness ofthese pathogens.I propose to expound upon currentclimate change models and forest pestpredictions, particularly for nectriaceous fungi on these high-elevation tree hosts.Aim 1:Assessing abiotic stressors contributing tothe emergence of fungal pathogens.Using the ‘Climate by Forest’ tool provided by the U.S. Forest Service, I will review changes in foresthealth and climate projections for forests throughout the Appalachian region. The ‘Climate by Forest’ toolis a novel interface in which users can select regions of national forests and look at various climate trendsand variables.7From these projections, forests thatare predicted to have significantly warmer winters willbe selected for sampling. High-elevation tree species will be selected based on their known range acrosshigh elevation zones throughout Appalachia. Symptomatic tissues and conspicuous fungal fruiting bodiesfrom these species in our sample sites will be surveyed, collected and processed for culture- andDNA-based studies. I will also compile and analyze temperature data across the Appalachian region toquantify and evaluate the abiotic stress these forests have endured.Hypothesis: Fungal pathogens andabiotic stresses are synergizing declines in native tree species.Aim 2: Characterizing known and unknown nectriaceousfungal diversity in Appalachian forests.Hypothesis: Despite known diversity of Nectriaceous fungal pathogens across Appalachian forests,many remain undetected.Molecular tools must be usedin combination with existing morphologicalmethods to capture the full diversity of phytopathogenic fungi. Sanger sequencing will be used for purecultures of my suspected fungal pathogens recovered from trees sampled inAim 1. Targeted loci (LSUand EF1-α) are widely used for phylogenetic inference in Nectriaceae.8Illumina amplicon sequencing—amultiplexed PCR approach—will be used to identify asymptomatic fungi that may also be contributing toforest decline.Aim 3: Determining the interaction between individualnectriaceous fungi and targeted tree species incentral Appalachia along a temperature gradient. Hypothesis:Nectriaceousfungi have contributeddifferentially to tree disease epidemics, which are driven in part by changes in temperaturethroughout our Appalachian forests. To simulate globalwarming and to assess the effects oftemperature on fungal growth and pathogenicity, temperature-dependent pathogenicity assays will beconducted. In climate-controlled growth chambers, saplings of the aforementioned five species will begrown at varying temperature ranges (0°C, 10°C, 20°C, and 30°C). Trees will be inoculated with selectnectriaceous fungi discovered inAim 2and tree healthwill be monitored at 6-MPI and 12-MPI. Anynotable canker formation will be measured at the end of the inoculation period. To fulfill Koch’spostulates, trees will also be sampled to see if the original inoculum can be recovered. This experimentwill quantify the aggressiveness of suspected nectriaceous pathogens at varying temperatures, allowingme to infer the impact novel phytopathogens will have on our forests as global warming worsens.Intellectual Merit:As a member of Dr. Kasson’s forestpathology lab since mid-2020, I have directexperience identifying and characterizing diverse fungal phytopathogens in West Virginia. During myefforts to delimit the species boundaries ofNeonectriamagnoliae,I have already identified numerousnectriaceous fungi on a wide range of hosts, including some novel species we are describing. Putativepathogens in the Nectriaceae are abundant and appear to be emerging as the result of the unique overlapof biotic and abiotic factors. I have and will continue to collaborate with forest pathologists throughoutthe Appalachian region to compare DNA sequences, host range, and pathogenicity of these fungi undersupervision of Dr. Matt Kasson (WVU), my current advisor and forest pathology expert. With his supportand the support of my forest pathology colleagues, I will not only unravel the contributions of thesefungal phytopathogens to the decline of our forests, but provide novel information to our Appalachiancommunities, our foresters, and our scientists.WestVirginia is the “black box” of biodiversity:severely understudied with much to discover. There are an estimated 150 different tree species in WV:more than anywhere else in North America. My contributions to forest pathology will revolutionize howwe look at and care for our trees in Appalachia.Broader Impacts:West Virginia (WV), my homestate, is suffering from educational neglect. It hasthe lowest number of Bachelor’s degrees (20.6%) per capita of any state, and we have the second lowestper capita graduate degrees.9In Appalachia—and WV specifically—we deal with low science literacy anda fear of science. Our region once powered the country with coal, but the profits of this mono-culturaleconomy were not reinvested in our communities. As coal has faded away and global temperatures rise,there is much skepticism and fear around climate action in West Virginia. West Virginians needscientists from our own communities trained to identify the challenges we face, develop solutions tothese problems and share them with our own.Thesescientists, like myself, will have a broad andimmensely positive impact on my community.As science outreach has been an integral part of my undergraduate career, I will curate my ownenvironmental science outreach program to invest my project in our Appalachian youth. Through myAppalachian Children’s Environmental Research program(ACER), I will recruit fellow youngscientists to bring presentations to K-12 students and also offer field trips to teach a variety ofenvironmental concepts. In this novel program, I will provide resources (i.e., guides, lessons, andaccessible information) on climate change, forest health, mycology, and other topics pertaining topreserving the integrity of our ecosystems.My lifetimeof learning has prepared me to fulfill this nextstage of my career, one that will ensure West Virginians are not left behind.References:[1] Karunarathna et al. 2021.Front CellInfect Microbiol. [2] Kasson et al. 2009.Mycologia.[3] Dukes et al. 2008.Can J For Res. [4] Pavlov etal. 2020.Sci Rep. [5] Pauchard et al. 2015.BiolInvasions. [6] Corredor-Moreno et al. 2019.New Phytologist.[7] U.S. Forest Service. 2018. Climate byForest. [8] Stauder et al. 2020.Fungal Ecol. [9]West Virginia QuickFacts. U.S. Census Bureau."
10.0,"actions allow them to control their external environment. Studiesoncausallearningshowthatasearlyasinfancy, humans learn the relations between actions and their outcomes1 and are able to use thisknowledgetoactassuccessfulagentsintheirenvironment.Mostexistingliteratureonagencyemphasizescomparing the outcome of one’s own actions with their internal predictions of thoseactions2-5.However,these processes have been limited to sensory perception and stimulus-response learning, precluding theability to explore the effects of agency on episodic memory6 and how sequences of information whichunfold due to causal actions drive brain regions to support memory.Background. Exercising agency over learning environments has been shown to improvememory7,8, even when the choices do not relate to the content of the to-be-learned items. Previous workfrom ourlabgaveparticipantsasimplechoicebetweentwo‘cards’whichwouldrevealanunrelateditem.Participants better remembered items that appeared as a result of their choice9,10. Further, this memoryenhancement was driven by an interaction between anticipatory activation within the striatum, a regionassociated with causal actions and motivation, and hippocampal (HPC) engagement during encoding.While this shows how agency over a choice can positively affect memory fortheoutcomeofachoice,itdoes not shed much light onto memory for the overall decision sequence.To explore how agency over a series of events affectsassociativememoryforthecomponentsofthe sequence, I developed a task where participant's agency was manipulated via a choice. In the “gameshow” task, participants assisted contestants in choosing one of three doors which reveal a hiddenprize.On each trial, participants saw a trial-unique contestant and either freely chose between the doors(“agency” trials) or selected a highlighted door (“forced-choice”trials).Unbeknownsttotheparticipants,the prize image presented was predetermined. After completion of the task, participants completed asurprise retrieval taskwhichtestedmemoryforthecontestantspresentedintheencodingtask,whichdoorthey selected, and the outcome hidden behind the door in three separate, consecutive memory tasks.Across two studies (study 1 n = 28; study 2 n = 131), which serve as the foundation for thisproposal, participants showed enhanced memory for the contestants (p<0.001), constant-prize pairs(p<0.05), contestant-door pairs (p<0.01), and prize-door pairs (p<0.01). These results show that bymanipulating participant’s agency to select which door to open, we are able toenhancememoryforcuesas well as associative memory between cues and outcomes. However, these results do not discriminatewhether agency enhances memory separately for each individual pair, or whether agency facilitates thebinding of all associations into one integrated sequence. Follow-up analysis explored whether agencyfacilitates memory integration by examining whether there was an inter-dependence upon memorymeasures such that memory for one pair was dependent onmemoryfortheotherpairs.Indeed,wefoundmemory for the contestant-prize pair was modulated by memory for recalling both the contestant-doorand door-prize pairs (p<0.01). Further, this effect was significantly higher for pairs that occurred inagency trials vs the forced-choice trials.Intellectual Merit. While I have established a paradigm that modulates associative memory viaagency, it isstillunclearhowmesolimbic-hippocampalengagementsupportsthislearning.Understandingthe timescale of how these systems interact will inform us on how agency modulates encoding andconnect human and animal research. Much of the existing human literature exploring mesolimbiccontributions to memory focus onhowphasicdopaminedriveslearninginresponsetorewardfeedback11.However, rodent research has shown hippocampal engagement during exploration prompts sustainedventral tegmental area (VTA) engagement leading to greater response feedback to the hippocampus12. Ipropose exploring these temporal dynamics within the same paradigm to address the gaps in these twolines of research and contribute to the translation of rodent-to-human research. Using neuroimagingtechniques, the current research seeks to: 1) examine sustained mesolimbic during encoding, 2)examineevent-evoked VTA and hippocampal activationduringencodinganditseffectsonmemory,3)exploretheinteractions between engagement at these different timescales. I hypothesize sustained mesolimbicactivity during learning when an individual has agency increases cue and outcome basedmesolimbic-hippocampal interactions. The relationship with the sustained and event-evoked activitywillbias encoding to promote associative learning across and within decision sequences.Methodology. Eighty healthy participants will berecruitedtoparticipateinastudyattheTempleUniversity Brain Research and Imaging Center, which houses a Siemens Prisma 3T MRI scanner.Participants will complete a modified version of the game show task. On each trial, they will see atrial-unique contestant (2s), and then will seeandselectoneofthethreedoors(2-4s).Uponselection,thedoor will be highlighted, then removed to presentatrial-uniqueprizeimage(2s).Again,participantswilleither get to freely choose one of the three doors (agency trials) or be forcedtoselectahighlighteddoor(forced-choice trials). Participants will complete three runs of each condition, each run containing 20trials. Runs will be pseudo-randomized across participants so no more than 2 runs ofthesameconditionappear consecutively. Temporal jitters will be placed between cues and outcomes and between trials toimprove both temporal and spatial resolution. Following encoding, participants will complete the threeretrieval phases described earlier: contestant recognition, and contestant-prize, contestant-door, anddoor-prize associative memory.Analyses. Behavior: In brief, item memory will be calculated for contestants using correctedrecognition, which accounts for false alarm rates. Associative memory metrics will be calculated as hitrates (percent correct in selecting the old item). I will compare memoryacrossagencyandforced-choiceconditions using paired t-tests. In line with my previous findings, I expect memory for items and itempairs to be enhanced for those that appear in agency compared to forced-choice trials.Neuroimaging: 1) In order to examine sustained mesolimbic activation, Iwillcomparesustainedbaseline VTA engagementusingpairedt-testsonparameterestimatesfromanatomicalROIs.Ipredictthesustained activation to be higher for agencycomparedtoforced-choicerunsandtrials.2)Toexplorehowthe event-evoked VTA-HPC coupling may drive memory, I will use a 2x2 within-subjectsANOVAwithcondition (agency, forced-choice) and memory (sequence intact, sequence disrupted) on parameterestimates from anatomical ROIs during cues and outcomes. I expect this couplingtopredictmemoryforagency but not forced-choice trials. 3) To examine the interaction betweenengagementofthesedifferenttime scales, I will relate measures of sustained VTA and VTA-HPC activations on the event-evokedmemory signals acrossrunsforbothconditions,separately,usingmulti-levelGLMmodelswitharandomeffect of participant. Post-hoc analysis willmakedirectcomparisonsacrossconditions.IexpectsustainedVTA activation and VTA-HPC coupling during agency runs to be associatedwithevent-evokedmemoryeffects in the VTA and the hippocampus on agency trials, across runs.Broader Impacts. Much of the literature exploring motivated learning comes from work that isparticularly interested in response to reward feedback. However, motivated learning in the absence ofreward may contribute toriskfordevelopmentofsubstanceabuse.Suchlearningmaypotentiatecuesandcontextual factors that strengthen drug associations independent of the anticipation or experience of thereward. I will use agency as a model to elucidate the neural underpinnings of motivated learning in theabsence of rewards. This model will allow for the isolation of the core mechanisms that underliesubstance abuse, which may depend on VTA-HPC interactions. The proposed research will dissect howVTA-HPC interactions contribute to the complexity of behaviors associated with substance abuse byelucidating how both state-dependent and event-evoked interactions drive memory encoding.Additionally, the results of the proposed research may provide valuable contributions toimproving pedagogical techniques. Understanding how agency might enhance learning at differenttimescales could support new teaching methods which incorporate agentic choices over both short andlong term goals. Even inmyanecdotalexperienceemployingactivetechniquestoengagementees,Ihavefound that giving individuals’ agency over minor choices, such as choosing what topic to read anddiscuss, and more substantial choices, such as choosing a research topic, leads to significantly moreengagement and long-term retention. The proposed research will directly test how agency can affectmotivation and learning at various timescales, which will contribute both to our understanding of howitcan support learning and how we may utilize it to broadly enhance teaching methods.References. 1Kuhn, 2012 2Haggard et al., 2002 3Haggard, 2009 4Wolpert et al., 1995 5Chambon et al,20146Hon, 20177Gureckis&Markant,20128Markantetal.,20169Murtyetal.,201510Murtyetal.,201911Shohamy & Adcock, 2010 12Lisman & Grace, 2005"
11.0,"language pairs and translation tasks. However, two of the most pressingopenproblems inNMTare domain adaptation and low-resource scenarios [1] (i.e., language pairs for which largeparallel corpora do not exist). This project will address both issues via cross-lingual domainadaptation in an extremely low-resource setting. The work is motivated by an urgenthumanitarian crisis: refugees seeking asylum in the US who speak only Central Americanindigenous languages like K’iche’, Mam, Kanjobal, and Mixtec [2]. The scarcity of interpretersin these languages makes itdifficult for speakers toaccess legalservices,andexistingnonprofitsand NGOs providing legal aidto refugeesdonothaveadequate resourcestoprovideinterpreters.Additionally, existing methods for low-resource NMT do not scale down to the extremelylow-resource situation for these Central American languages. Improving both extremelylow-resource applicability anddomain adaptationfor NMT will increasethenumberof scenariosin which NMT is a viable solution. Additionally, domain adaptation techniques that workinthechallenging low-resource setting will also improve NMT domain adaptation in higher-resourcesettings. Thus, this project addressesachallenging andwidelyapplicable scientific problemwithimmediate humanitarian impacts.Objectives and Hypothesis: The objective of my proposed research is atwo-prongedapproachto domain adaptation in low-resource NMT. I will address the domain issue by fine-tuning apretrained NMT model on synthetic parallel data generated via backtranslation [3] ofmonolingual in-domain data in the target language. I will first validate my approach in ahigh-resource setting by fine-tuning a baseline Spanish-English model using backtranslatedin-domain English data. Independent of the performance on low-resource languages, aSpanish-English translation system that is well-adapted for asylum testimonials will be of greatuse to organizations providing legal aid to refugees. Then, I will apply the fine-tuning viabacktranslation approach to the low-resource case. In order to successfully backtranslate theEnglish finetuning data, a decent English-LR model is needed. Thus, I plan to apply acombination of unsupervised NMT, transfer learning, multilingual translation models, andvarious data augmentation approaches totheproblem ofextremelylow-resource NMT.Althoughvarious methods to augment parallel corpora have been proposed [4-7], they typically augmentcorpus size from a few hundred thousand sentences to a few million . They do not adequatelyaddress a context in which only thousands or tens of thousands of sentences are available, as isthe case for K’iche’ and other indigenous languages. This project will develop alinguistically-informed method to generate enough plausible, syntactically correctsyntheticdatathat existing corpus augmentation techniques like backtranslation can be applied, bridging thechasm between researchers’ assumptionsabout theamount of availablemonolingual dataand thereality for many low-resource languages.Method and Research Plan: In this research, I will use K'iche as a case study language todevelop methods, then I will apply those methods to Kanjobal, Mam, and Mixtec. BLEU score,the standard evaluation metric for machine translation [8], will be the primary metric forevaluation of this proposed research. Possible BLEU scores range from 0 to 100, with higherscores indicating a closer match to the reference translation. In my exploratory work on theMagdalena Peñasco Mixtec dialect,achievinga maximumBLEU scoreof7with amodel trainedfrom scratch on 8000 Mixtec-English parallel sentences. After this preliminary work, I decidedto use K’iche’ as the test language in further study for two reasons: first, monolingual K’iche’speakers are more common among migrants to the US than monolingual Mixtec speakers;second, there are more than fiftydistinct dialectsof Mixtec,each withvaryingdegrees ofmutualintelligibility, which makes collection and cleaning of data intractably difficult.The first year of my work will focus on domain adaptation to asylum testimonialsin thehigh-resource Spanish-English setting. High-resource NMT models are primarily trained onparallel data extracted from news articles, while LR models are generally trained on whateverportions of the Bible are available for a givenlanguage.Because theultimate goal ofmyprojectis to translate first-person testimonials ofasylees aboutthe violencefromwhich theyarefleeing,I needto adapttheNMT modelsto fluentlytranslatefirst-person narrativesandto understandthespecific vocabulary relevant to asylees. One way to accomplish this adaptation is to fine-tune apretrained translation model on in-domain data. While parallel data is ideal,itis also possibletouse monolingual target-language data via backtranslation. I plan to use the transcripts from theMALACH dataset [9], compiled from the USC Shoah Foundation’s Video History Archive,which collects testimonials from Holocaust survivors. I hypothesize that these testimonials aresimilar enough to the stories of modern asylees to be useful as in-domain training data.In the subsequent years of the project, I will apply the domain adaptation techniquedevelopedin thefirstyear totheextremelylow-resource setting.Thefirst priority for Year2is togather and clean a K’iche’-English parallel corpus (at least the New Testament and someJehovah’s Witness literature is readily available). Next, I will establish a baseline machinetranslation result for K’iche’-Englishusing available parallelcorporawithnodata augmentation.Then, I will apply the same backtranslation for domain adaptation technique used forSpanish-English. However, I expect that the backtranslation results will be unsatisfactorybecause of the limitations of the English-K’iche’ model used for backtranslation. Thus,I plantodevelop a data augmentation scheme syntax-aware compositional data augmentation methodsfrom [10], withsome simplifications necessaryduetodata scarcity. Sentencesproducedwiththismethod are guaranteed to be grammatically correct, but may not make sense semantically;however, they are much easier to produce than semantically correct sentences, especially in alow-resource setting. This research will investigate whether NMT systems can learn usefulinformation from synthetic data that is syntactically, but not semantically, correct. In additiontodata augmentation, I plan to explore whether the K’iche’-English and English-K’iche’ modelscan be further improved by transfer learning and unsupervised NMT techniques.Intellectual Merit:Thiswork addressesacrucial gapincurrent methodsfor low-resource NMT:cases where there are fewer than 10,000 sentences of monolingual data available. My proposeddata augmentation method would allow researchers to generate enough synthetic monolingualdata to apply state-of-the-art methods in low-resource NMT to languages that would otherwisebe intractable due to lack of data. If this data augmentation is successful,it willshowthat NMTsystems can learn grammar and syntax from synthetic training data that is semantically verynoisy, opening up future research on how exactly NMT systems learn grammar and how thislearning differs from learning vocabulary.Broader Impacts: The human rights impact of my proposed research is immediate andtransformative. Interpreters for K’iche’ and related languages are hard to find and oftenprohibitively expensive. Translation systems, even imperfect ones, would allownon-governmental organizations and pro bono immigration lawyers to help asylees they werepreviously unable to serve. Widespread access to legal assistance would speed up backloggedimmigration courts and give thousands of asylum seekers per year a chance to enter the USlegally. This work could save lives, because many of these asylees are children and teenagersfleeing from horrific gang violence.References[1] Koehn, Philipp, and Rebecca Knowles. ""Six Challenges for Neural Machine Translation.""Proceedings of the First Workshop on Neural Machine Translation. 2017.[2] Medina, Jennifer. “Anyone Speak K'iche' or Mam? Immigration Courts Overwhelmed byIndigenous Languages” New York Times (2019).[3] Sennrich, R., Haddow, B., & Birch, A. (2015). Improving neuralmachinetranslationmodelswith monolingual data. arXiv preprint arXiv:1511.06709.[4] Zhang,Jinyi, andTadahiroMatsumoto. ""CorpusAugmentationbySentenceSegmentation forLow-Resource Neural Machine Translation."" arXiv preprint arXiv:1905.08945(2019).[5] Li, Hongheng, & Heyan Huang. “Evaluating Low-Resource Machine Translation betweenChinese and Vietnamese with Back-Translation”. arXivpreprint arXiv:2003.02197(2020).[6] Currey, Anna, Antonio Valerio Miceli-Barone, and Kenneth Heafield. ""Copied MonolingualData Improves Low-Resource Neural Machine Translation."" WMT 2017.[7] Fadaee, Marzieh et al. ""Data Augmentation for Low-ResourceNeuralMachine Translation.""ACL 2017.[8] Papineni, Kishore, et al. ""BLEU: a Method for Automatic Evaluation of MachineTranslation."" ACL 2002.[9] Ramabhadran, Bhuvana, et al. USC-SFI MALACH Interviews and Transcripts English –Speech Recognition Edition LDC2019S11.[10] Andreas, Jacob. ""Good-enough compositional data augmentation."" arXiv preprintarXiv:1904.09545(2019)."
12.0,"Introduction. The question of how a child learns her native language remains highly debated in linguisticresearch. Cross-linguistically, children learn much of the morphology (word structure) of their nativelanguage by age three, when their vocabulary is approximately 1000 words.1 Yet the frequencies of wordsin child-directed speech follow a skewed, roughly Zipfian distribution, with the frequency of a wordbeing proportional to the inverse of its rank.2 This means that a few words may occur hundreds of times inthe child's linguistic input, but most occur only a few times. Similarly, most words only appear in a few oftheir possible inflected forms (e.g. the child may hear fall and fell, but not falls or fallen).3 Such inputcontrasts with the larger, more saturated data used by most machine learning systems today. How, then,does a child learn the morphology of her native language from such a skewed, sparse input? During myPhD, I seek to answer this question via computational modeling of morphological acquisition.A plausible model of morphological acquisition should follow the developmental and behavioralpatterns of children, which may be studied experimentally and through analysis of errors in children'sspeech. Productivity of a linguistic process is marked by its ability to generalize to novel contexts, and isa foundational component of language. In the famous wug study, for example, most English-learningchildren generalized -ed, -ing and -s to novel verbs (e.g. gling) by age three, demonstrating that they hadlearned the productivity of these suffixes.4 Further, most errors in children's speech are caused by over-application of productive processes: English verb acquisition is known to follow a ""U-shaped"" curve,where a sudden dip in performance is caused by the overapplication of -ed to irregular verbs (e.g. goed,feeled) when the child discovers its productivity.5 Several promising computational models that accountfor these facts utilize the Tolerance Principle (TP), a threshold of productivity which posits that a childgeneralizes a process when it is more computationally efficient to do so under a Zipfian distribution.2Such distributional learning models have been the focus of my undergraduate research: working with Dr.Charles Yang, I developed a model that acquires meaning-form mappings (e.g. PAST = -ed) betweensuffixes and their corresponding semantic features, such as person (first, second or third), number (e.g.singular, plural), and tense (e.g. past, present, future).6 This model follows developmental patterns andcorrectly acquires morphological rules on small vocabularies of Spanish and English verbs. I also createda model that acquires such mappings for German plural nouns and English verbs, even displaying U-shaped regression in English, and contributed to a third model with comparable results.7 It is my goal tobuild on these models to create an integrated, incremental, and cognitively-plausible model ofmorphological acquisition that succeeds on a wide array of languages.Aim 1: To create a model of incremental morphological acquisition that succeeds on a typologicallydiverse set of languages. While the models outlined above provide promising results, no single model isable to account for all languages: the latter two succeed on concatenative, non-agglutinative languages(e.g. English, German), but fail to model non-concatenative (e.g. Hebrew, Arabic) and agglutinative (e.g.Spanish, Swahili, Japanese) languages. Segmenting a word into morphemes is more challenging in suchlanguages: in Spanish, for example, a verb may take multiple suffixes (e.g. ama-ba-s = love-past-2nd+singular = ""you loved""). These models are also not incremental learners, but extract morphological rulesfrom fixed-size vocabularies; this contrasts with the incremental nature of language acquisition. In myfirst aim, I thus plan to build on the models above to design a novel algorithm that incrementally acquiresmorphological rules across agglutinative, non-agglutinative, and non-concatenative languages. Whilecurrent models take in each item as a lemma, inflected form, and semantic feature set (e.g. walk, walked,{PAST}), the child may be able to group each of the inflected forms in which she has seen a word (e.g.get, gets, gotten). I hypothesize that doing so will allow for cleaner segmentation and identification ofmorphemes, helping the learner to succeed on a wider array of languages. To test this, I will create such amodel and compare it with experimental findings on the aforementioned languages and others. I will workclosely with linguists and cognitive scientists from differing subfields to ensure that this work benefitsfrom both theoretical and experimental insights and provides a plausible account of acquisition.Aim 2: To extend this model by incorporating models of other portions of language acquisition.Morphological acquisition does not happen in isolation, and morphology is known to interact with otherlevels of linguistic representation, particularly phonology (e.g. -s is pronounced /s/ in cats but /z/ in dogs).The nature of the input to the models discussed above assumes that the child has already learned much ofthe phonology of their native language and extracted the relevant semantic features such as person,number and tense onto which they will map the segmented input. The grouping of forms as discussed inAim 1 makes the additional assumption that the child is able to form these groups. To create a moreholistic account of acquisition, I will thus integrate models of morphological learning like the above withmodels of acquisition of other levels of linguistic representation, particularly those on which Aim 1 relies.It is highly plausible that similar learning algorithms are used for each level of representation, so I willbegin by testing the ability of the algorithm developed above to account for these other levels. I will alsotest integration of the model developed under Aim 1 with existing models in the literature. With the endgoal of an algorithmic hypothesis about how children acquire their native language, I will collaborate withexperts on each of the levels of representation I will consider. I will compare the integrated model'spredictions with experimental findings on a typologically diverse set of languages to ensure that thisalgorithmic account of learning is a plausible and generalizable one.Intellectual Merit. The end goal of this work, an input-to-grammar model of morphological acquisition,will provide insight into linguistic theory and the learning mechanisms employed by children. The modeldeveloped under Aim 1 will provide an algorithmic hypothesis regarding how children learn fromskewed, sparse data, and structural hypotheses regarding morphological knowledge in the mind as the endresult of acquisition. Both will be valuable in answering questions of learnability and the structure oflinguistic knowledge, and may also provide insight into atypical language development. The modelsdeveloped under Aim 2 will yield hypotheses about the interactions between linguistic levels ofrepresentation, which may be compared with theoretical hypotheses to provide new insight into linguisticstructure. Further, these models will give a bottom-up account of language acquisition, and thus yieldtestable hypotheses regarding the innate factors that may enable it, a highly-debated topic. This modelwill, to my knowledge, be the first to model morphological acquisition from phonological input to astructured grammar, and it will thus provide a basis for further integrated models of language acquisition.Broader Impacts. This work has applications to Natural Language Processing (NLP), which focuses onthe creation of language technologies. Models used in NLP are typically trained on data several orders ofmagnitude larger than that to which the child is exposed. This can lead to biases and makes modelsinaccessible for “low-resource” languages for which large corpora do not exist, such as Indigenouslanguages and languages of Africa and Asia.7 Cognitively-motivated approaches already show promisingresults,8 and since the algorithms I will develop are designed to succeed on small, sparse input, they willbe strong candidates for use with low-resource languages and for testing bias mitigation strategies. This,in turn, will allow for the creation of more accessible, equitable language technologies for all.References. [1] Brown, R. 1973. A first language: The early stages. Harvard University Press. [2] Yang,C. 2016. The price of linguistic productivity: How children learn to break the rules of language. MITPress. [3] Chan, E. 2008. Structures and distributions in morphological learning. UPenn Dissertation. [4]Berko, J. 1958. “The child’s learning of English morphology.” Word. [5] Pinker, S. and Prince, A. 1988.“On language and connectionism: Analysis of a parallel distributed processing model of languageacquisition.” Cognition. [6] Payne, S, et al. 2021. “Learning Morphological Productivity as Meaning-Form Mappings.” Proceedings of the Society for Computation in Linguistics. [7] Belth, C, Payne S, et al.2021. “The Greedy and Recursive Search for Morphological Productivity.” Proceedings of the CognitiveScience Society. [8] Bender, E, et al. 2021. “On the Dangers of Stochastic Parrots: Can Language ModelsBe Too Big?” Proceedings of the ACM Conference on Fairness, Accountability, and Transparency. [9]Xu, Chao et al. 2020. “A Cognitively Motivated Approach to Spatial Information Extraction.”Proceedings of the Third International Workshop on Spatial Language Understanding."
13.0,"Progress in neuroscience is limited by the lack of proper tools available to biologists and neuroscientists tostudy neural circuits with high spatial resolution and cell type specificity. One area of neuroscience that isparticularly affected by this absence is the study of somatosensory and motor control systems. Currentlyavailable tools used to study these systems and mimic their functions consist of electrode arrays, such asthe polymer cuff electrode, attached to the peripheral nervous system1 or the Utah electrode array implanteddirectly into the motor cortex2. These electrode arrays, however, lack the ability to induce or record neuralactivation with cell specificity.Herein, I propose the development of a novel class of miniaturized, battery-free, wireless, soft, implantableneural machine interfaces (NMIs) utilizing optogenetics to study the somatosensory system in non-humanprimate (NHP) models via the peripheral nervous system (PNS). This proposal considers the recentadvancements within the fields of optogenetics and photometry, advanced micro- and nano-fabricationmethods, and the necessary collaborations to bring this project to fruition within a three-year period.BackgroundOptogenetics is a growing neuroscience tool which utilizes viral injections to genetically modify neuronpopulations to express light-sensitive ion channels. The targeted neurons can be selectively stimulatedamong other tissues by selecting viral vectors and opsins with preferential tropisms. In NHPs, initialresearch in optogenetic stimulation of the peripheral nervous system shows channelrhodopsin-2 (ChR2)and Chronos delivered via adeno-associated virus and stimulating muscle injection to be successful3. Oncethe opsins are virally delivered, these neuron populations can be excited or silenced by targeting them withvarying wavelengths and stimulation frequencies from light-emitting diodes. Recent papers have shown thesuccess of optogenetics in stimulating the central nervous system via the brain and the spinal cord4,5.Similarly, genetically-encoded calcium indicators (GECI’s) and photometry can be used to visualize neuralactivation of defined cellular populations in-vivo6. These tools have significant advantages over electricalprobes which lack the stimulation and recording specificity required for high resolution research into lighttouch information propagation through the low-threshold mechanoreceptor afferent neurons in the dorsalroot ganglia (DRG). Neuron populations of particular interest for this study are the low-thresholdmechanoreceptor afferent neurons within the DRG located in cord segments C6, C7, and C8, which areresponsible for light touch information propagation from the lower forelimb and hand7.Aim 1: Optical Recording and Stimulation of Low-Threshold Mechanoreceptor Afferent NeuronsThe primary functions of the proposed device are to optically record the neural activity of low-thresholdmechanoreceptor afferent neurons in a healthy NHP’s DRG, and to stimulate those neurons to replicatelight touch information being transmitted through the neural circuit up A- and A- fibers. The cells willbe targeted following the methods outlined by Williams et al. using ChR2 and Chronos, and with GECI’s.To enable both recording and stimulation, the device will employ a colloc𝛽𝛽ated micr𝛿𝛿oscale inorganic light-emitting diode (μ-LED) and photodetector (μ-IPD), both interfaced with a microcontroller for stimulationcontrol and data processing respectively.Additional functional requirements to ensure device reliability are highly deformable mechanics and ausable lifetime of 10 years or longer. To ensure the device function for applications lateral the spinal columnserpentine geometries, polymer substrate and encapsulations, and thin annealed metal traces will beemployed to keep local strains under fatigue limits even under high bending and linear loads. To extend theusable lifetime of these devices, dielectric interlayers of thermally grown Silicon Dioxide and HafniumOxide will be used. These interlayers, employed in a total thickness up to 100µm, work to extend usablelifetime by retarding the ingress of ions and water vapor and disrupting pin-hole defects while remainingtranslucent8.Device encapsulation and fatigue mechanics will be tested using accelerated life testing (ALT) in an 87℃phosphate buffered saline bath with complex mechanical loading conditions for 4 months, simulating animplanted lifetime of 10 years and 8 months. While ALT is being performed, the long-term reliability ofthe stimulating and recording capabilities will be assessed by measuring the irradiance of the μ-LED andthe recorded signal of an external light source via the μ-IPD over time. Device electronics and wirelesspower harvesting will be assessed via continuous data logging.Aim 2: In-Vivo Testing in Non-Human PrimatesThe final aim of the proposed research is to implant the proposed device into NHPs to conduct research onlight touch propagation via low-threshold mechanoreceptor afferent neurons in the DRG. The anatomicalsimilarities of mechanoreception between NHPs and humans allows for the study of light touch perceptionin the forelimbs that could not be studied in small animal models. This work will be done with collaboratorswho conduct NHP behavioral studies, external to the Yoon Lab at the University of Michigan where Ipropose to do my PhD. One study of interest includes training the NHPs to perform two-alternative forcedchoice tasks involving the differentiation of textures on their fingertips or palms and studying the neuralactivity for each of the presented textures. After the behavior is learned with accuracy of 95% or greaterand the neural activity has been recorded and decoded, the NHP will perform the task again. However, thistime the NHP will receive light touch information from the device via optogenetic stimulation of themechanoreceptor afferent neurons in the C6 C7 and C8 DRG without any textures presented.Future DirectionsUpon completion of this work, the goal is to transition from the study of the peripheral nervous system’srole in light touch perception to implantation within a NHP amputee. Once implanted, this device willinterface with an upper-limb prosthetic via near field communication and used to replace the lost light touchperception abilities of the amputated limb. If shown to be successful, the next step is to use this device inconjunction with functional magnetic resonance imaging to study the long-range neural circuits of thesomatosensory system, a study never before possible.Intellectual MeritThe development of this device will utilize recent advances in materials science, fabrication, andoptogenetics to advance neuroscience tools. The design process and in-vivo testing will also requirecollaboration with the departments of Biology and Neuroscience as well as external collaborators to informthe selection of virus, opsin, μ-LED, and μ-IPD. Once developed, these devices would directly promote anadvancement in the understanding of the peripheral nervous system’s role in somatosensory processing andpropagation and introduce a platform of devices for the targeted study of the somatosensory system andother short- and long- range neural circuits in-vivo.Broader ImpactsThese devices would be applicable not only in the study of light touch information but any neuron type andthus have broad impacts in neuroscience, neurotherapies, and limb rehabilitation or replacement forparalyzed or amputated individuals. Outside of the medical field, the ability to transmit somatosensory froman external input to the peripheral nervous system could also be used to advance entertainment systems andvirtual reality to include touch perception.References1. Elyahoodayan, S., et al.. Acute in vivo testing of a polymer cuff electrode with integratedmicrofluidic channels for stimulation, recording, and drug delivery on rat sciatic nerve. J.Neurosci. Methods 336, 108634 (2020).2. Maynard, E., et al.. The Utah Intracortical Electrode Array: A recording structure for potentialbrain-computer interfaces. Electroencephalogr. Clin. Neurophysiol. 102, 228–239 (1997).3. Williams, J., et al.Viral-Mediated Optogenetic Stimulation of Peripheral Motor Nerves in Non-human Primates . Frontiers in Neuroscience vol. 13 759 (2019).4. Ausra, J. et al. Wireless, battery-free, subdermally implantable platforms for transcranial and long-range optogenetics in freely moving animals. Proc. Natl. Acad. Sci. 118, e2025775118 (2021).5. Kathe, C. et al. Wireless closed-loop optogenetics across the entire dorsoventral spinal cord inmice. Nat. Biotechnol. (2021)6. Burton, A. et al. Wireless, battery-free subdermally implantable photometry systems for chronicrecording of neural dynamics. Proc. Natl. Acad. Sci. 117, 2835 LP – 2845 (2020).7. Vanderah, T. W. & Gould, D. J. Nolte’s the Human Brain: An Introduction to its FunctionalAnatomy. (Elsevier, 2021).8. Jeong, J. et al. Conformal Hermetic Sealing of Wireless Microelectronic Implantable Chiplets byMultilayered Atomic Layer Deposition (ALD). Adv. Funct. Mater. 29, 1806440 (2019)."
14.0,"Background: The vestibular system, located in the inner ear, provides sensory information regardingspatial positioning and balance, enabling coordination of movement and orientation1. Additionally, thevestibular system plays a key role in enacting compensatory eye movements in response to body movementthrough the vestibulo-ocular reflex (VOR)1. This system can become unilaterally impaired in the presenceof vestibular schwannoma, a benign tumor that develops on the vestibulocochlear nerve connecting thesensory organs to the brain2. Due to the vestibular system’s active role in locomotion, impairment viavestibular schwannoma may lead to balance deficiency, vertigo, and oculomotor process changes2. Previouswork has demonstrated that in patients with vestibular schwannoma, head movements during locomotionand gaze stability exercises are less rapid and the VOR is impaired2,3. Encouragingly, resection, or removal,of vestibular schwannoma has been demonstrated to result in improvement of kinematic parameters andother head movements, with major changes occurring within the first six weeks post-operation4. Generally,normal vestibular activity is reflected in the parieto-insular and temporo-parietal junctions; however, littleis known about cortical changes that occur due to vestibular impairment, with most studies focusing onkinematic parameters and quantification of specific visual reflexes5. Therefore, there is a need to examinethe impact of vestibular schwannoma on neural activity at large. Obtaining this information about theways the human brain copes with vestibular system deficiency will elucidate both patterns of neuralplasticity in response to specific sensory input alteration, and how best to approach treatment of those whoare impacted by vestibular schwannoma. I propose to determine the patterns of neural activity and gazefocus in response to locomotor tasks in people with vestibular schwannoma, both pre- and post-resection.This work will be conducted across three Aims: first, a comparison of electroencephalography (EEG) andgaze-tracking patterns in healthy subjects and vestibular schwannoma patients; second, a longitudinal studyof EEG and gaze patterns in vestibular schwannoma patients pre- and post-resection; finally, generation ofa support vector machine (SVM), a machine-learning technique which will discriminate between gazepatterns of patients with vestibular schwannoma and healthy controls.Intellectual Merit: This project will contribute to the body of knowledge surrounding normal vestibularfunction, as well as provide insight into the specific pathological state inherent to vestibular schwannoma.The insight into the changes in gaze functionality will be especially important because the impact ofvestibular schwannoma on overall gaze patterns is not well-understood beyond interruption to the VOR.Changes to further parameters such as saccade frequency, fixation time, and primary areas of focus duringlocomotion are unknown. This project will elucidate changes in those patterns.Research Plan:Aim 1: Collection of baseline patterns in vestibular schwannoma patients vs. healthy subjectsHypothesis: Vestibular schwannoma patients will display greater primary motor cortex activation thanhealthy control subjects during gait, representing a more effortful process due to balance impairment.In this phase, we will focus on establishing a baseline of functionality in healthy subjects and patients withvestibular schwannoma, recruited from Johns Hopkins Acoustic Neuroma Center, a specialty clinic focusedon the treatment of vestibular schwannoma. Testing will consist of a modified functional gait assessment(FGA) battery. The ‘gait with eyes closed’ portion of the FGA will be excluded, due to the inability torecord gaze location while eyes are closed. During the FGA, subjects will have gross brain activity recordedvia a 58-channel EEG cap connected to a wearable Arduino Uno microcontroller to allow for continuousmobile data collection. Additionally, subjects will don wearable eye-tracking glasses to assess continuousgaze location. This phase will conclude with successful collection of gaze-tracking and EEG data for amatched number of healthy subjects and vestibular schwannoma patients executing the FGA tasks.Aim 2: Longitudinal study of vestibular schwannoma resectionHypothesis: Between six weeks and six months after vestibular schwannoma resection, patterns of neuralactivity in vestibular schwannoma patients will become more like that of healthy subjects during gait.This phase will enact a longitudinal study of patients with vestibular schwannoma pre-resection,approximately six weeks post-resection, and at least six months post-resection. This experimentation willconsist of the same paradigm as Aim 1, with subjects performing the modified FGA. Additionally, duringthis phase, the EEG signal collected in Aim 1 and Aim 2 will be processed and analyzed. Preprocessingwill consist of an independent component analysis, wherein the multivariate EEG signal for each subjectwill be decomposed into additive ‘components’, which combine at different weights to compose the overallEEG signal. These components will be assessed via visual inspection to remove extraneous signal, such aseye blinks and motion artifacts. Then, processed data will be examined for event-related potentials at severalkey points in the gait cycle. This phase will conclude with the accomplishment of two tasks: successfulcollection of gaze-tracking and EEG data for vestibular schwannoma patients pre- and post-resection, andanalysis of differences in patterns of neural activation between healthy controls, pre-resection vestibularschwannoma patients, and post-resection vestibular schwannoma patients.Aim 3: Generation of Support Vector Machine to categorize gaze patternsHypothesis: Individuals with vestibular schwannoma will display distinct gaze patterns, including greatergaze latency to area of interest, as compared to healthy control subjects.This phase will center around the generation and validation of asupport vector machine (SVM) that will enable automatic machineclassification of vestibular schwannoma patients versus healthycontrols. An SVM is a supervised machine learning approach that usesa hyperplane to split groups of variables, or support vectors, intodiscrete classes (shown in two dimensions in Figure 1); once trainedon the stereotypical values for each class, it compares new data tothose clusters to classify the state of the new input6. An SVM has beenused to accurately detect pathology based on gaze patterns; Figure 1: Design of an SVM 7specifically, individuals with dyslexia versus healthy controls while reading text6. Measured parameterswill include number of saccades, number of gaze fixations in key areas of interest (ground underfoot, groundahead of stride, wall), length of gaze fixations, and latency of gaze arriving at areas of interest. Within thismodel, some erroneous classification is inevitable. Because the primary purpose of the model is quantifyingimpacts of vestibular schwannoma to the ocular system, it is preferable to bias the model towards detectingvestibular schwannoma in order to find any and all gaze-pattern disruptions. Accordingly, the SVM will betuned to have higher sensitivity to prevent false negatives. This phase will conclude with the demonstrationthat the resultant SVM is able to discriminate between the gaze patterns of healthy individuals and thosewho suffer from vestibular schwannoma, with at least an 85% detection rate for positive cases.Alternative Approaches: An SVM can have limited efficacy if the training dataset is of insufficient size.If there is not be a significant dataset that will allow for training of the model and model accuracy is belowthe 85% threshold, an alternative approach may be the use of a convolutional neural network (CNN), whichconsists of a cluster of signal-transmitting kernels that loosely resemble neurons. A CNN has previouslybeen used to classify gaze-tracking data based on what website a user was viewing; however, thismethodology has not been extended to pathology detection8. If needed, this possibility could be explored.Facilities: This work will be conducted with Dr. Kathleen Cullen at the Cullen Laboratory at Johns HopkinsUniversity. This laboratory has previously conducted studies of vestibular schwannoma patients usingkinematic parameters and is equipped to continue this work with other methodologies.Broader Impact: While the primary purpose of this work is to learn about the unimpaired vestibular systemthrough a study of vestibular schwannoma as a disease state there is potential for secondary application asa diagnostic measure. The diagnostic process for vestibular schwannoma is two-stage – first-round testingconsists of a battery of hearing tests, and if those tests suggest the presence of a tumor, second-round testingconsists of an MRI with contrast. With the rising costs of healthcare in the United States, finances can be aprohibiting factor to patients pursuing this diagnostic testing. The creation of a lower-cost intermediatediagnostic would prevent unnecessary clinic visits for final diagnostic testing for patients whose hearingloss may have a different cause. If the SVM can recognize patients with vestibular schwannoma, then anintermediate gaze-tracking diagnostic tool could be developed and used to screen patients with hearing lossto determine whether vestibular schwannoma is a likely culprit for their symptoms.References: 1. K. Cullen, Nat Rev Neurosci, 2019; 2. A. Batuecas-Caletrio et al. Laryngoscope, 2015; 3.L. Wang et al. Sci Rep, 2021; 4. O. Zobeiri et al. Sci Rep, 2020; 5. E. Nakul et al. Front Neurol, 2021; 6. L.Rello et al. W4A ’15, 2015; 7. “Support Vector Machine”, javaTpoint; 8. Y. Yin et al. ICMLA ’18, 2018."
15.0,"in coral reef environments and providing ecosystem services that are intrinsic to the longevity of society.The diverse microhabitats provided by the elaborate morphologies of corals function as predation refugeand are essential for supporting the low trophic level (LTL) fish community.1 Specialist fish species willlive within one coral colony (or others of similar morphology) for much of their lives, whereas generalistfish can associate with a wider variety of microhabitats. Trophic cascading of the LTL fish communityresults in flourishing commercial fisheries, which are estimated to be globally valued at $5.7 billion USDannually.2 Yet, the existence of coral dominated tropical reefs is largely threatened by global scale,anthropogenic warming-induced coral bleaching events—which has in part contributed to a 50-75%decline in worldwide coral cover over the last ~35 years.3,4 The loss of microhabitat often leads to drasticdeclines in the reef fish community5 and can crash commercial fishery markets. To mitigate against thefurther decline of coral reefs and the fisheries they support, restoration strategists in-part rely on large-scale coral propagation and outplanting—involving the artificial fragmentation of reef-obtained donorcolonies and returning the clonal population back to the reef.6 Often, studies attempting to describe coralreef environments solely focus on percent coral cover and fail to capture the complex nature of coral reefecosystems.7 It remains unclear how reef fish community assemblages are directly affected by bleaching-induced changes in microhabitat availability. Understanding fish-microhabitat associations is essential fordevising targeted, efficient fisheries restoration efforts. The proposed research aims to elucidate theunique fish-microhabitat associations to better inform outplanting-based fisheries restoration efforts.Revealing fish-microhabitat associations would lead to the development of a comprehensive CoralOutplanting for Fisheries Guide (COFG) to be leveraged by coral restoration and fisheries managers. Thisresearch also aims to capture changes in the population levels and spatial distributions of commerciallyvaluable high trophic level (HTL) populations while under the presumed pressure of depleted LTL preypopulations following a bleaching event.Hypotheses: I hypothesize that (1) bleaching events will induce the largest decreases in specialist, LTLfish populations relative to generalist fish populations (H1), (2) bleaching events drive HTL predatorpopulations to relocate to less-affected regions of the reef where food sources are sufficient, or in moreextreme cases, recruit to nearby reefs owing to the reductions of LTL fishes associated with H1 (H2), and(3) outplanting of fisheries-specific coral taxa will facilitate the recovery of the fishery stock (H3).Experimental Approach: Timeseries Density Maps: The framework of one entire reef would be imagedbefore and after a single bleaching event, which would be scheduled according to existing local degreeheating week (DHW) data. DHW is a measure of accumulated thermal stress obtained by the 12-weektime-integration of sea surface temperature data exceeding the local bleaching threshold and is a reliablebleaching predictor.8 The onset of bleaching is expected when DHW values reach 4 °C-weeks, whereasmass bleaching and mortality is expected at 8 °C-weeks.9 The framework would be characterized bygenerating high taxonomic resolution photomosaics10 of the benthic coral community coupled withArcGIS-generated density maps of coral colony microhabitat volume approximated using structure-from-motion (SfM). SfM is a computationally intensive software that would allow me to digitally reconstructthe reef and extract microhabitat volume data from each colony. The movements of lower and highertrophic level fish populations would be continuously monitored utilizing size-specific acoustic telemetrytransmitters and receivers to create 3D population density maps in ArcGIS. It is imperative to implantsize-specific transmitters to minimize potential adverse health impacts to best isolate for tracking thenatural movements of the fish.11 Fish populations would be estimated for all implanted fish taxa usingwell-established tag-recapture techniques and the appropriate stock assessment model according to thespecies-specific life history traits.Statistics: To determine specific fish-microhabitat associations and build the COFG, colony location andtaxonomic classification will be tested against the time-based location density of the tagged LTL fish. Totest for potential reef-level population reduction differences in LTL fish species (specialists vs.generalists) (H1), I would linearly model the tag-recapture-obtained population abundance data andevaluate whether species, time, and the interaction of species and time are significant predictors of meanabundance. To test for potential significant changes in the movements of HTL predator populations (H2),I would model the time-based location density of tagged LTL fish populations paired with the time-basedlocation density of tagged HTL predators. I would encourage future studies to utilize the COFG producedby this research to answer H3. These studies would require quantifying the background recruitment ratesof LTL fish populations and the new recruitment rates following a large-scale outplanting effort. I wouldalso overlay the microhabitat volume density maps with the fish population density maps to allow forbetter visual interpretation of the data.Resources: I am applying to be advised by Dr. Sandin who is a leading expert in coral reef ecology at theScripps Institution of Oceanography (SIO). Dr. Sandin’s team is comprised of many individuals withyears of experience who would assist me in reliably imaging the reef and identifying coral colonies. Ihope to also receive guidance from Dr. Brice Semmens (SIO), who often uses telemetry techniques in hisresearch, to safely implant fish with acoustic transmitters and reliably track their movements. This studywould rely on the resources available to SIO, especially the use of custom-framed, study-optimizedcameras12 and SfM to digitally reconstruct the reef from imagery and perform the colony microhabitatvolume calculations. The spatial monitoring of LTL populations would require surgically implanting fishwith Juvenile Salmon Acoustic Telemetry System (JSATS)13 microacoustic transmitters and installingJSATS N201 receivers around the perimeter of the reef. The spatial monitoring of HTL populationswould follow the same methods but require Vemco™ V16p-4H transmitters and VR2 receivers.Intellectual Merit: This research would provide valuable insight to ecologist’s holistic understanding ofsuccessional reef fish communities. Although previous work has evaluated the role of decreased reefframework on fish community composition using transect based methods,14 it remains unknown howshifts in specific coral species alter the population level and distribution of specific fish species. Thisresearch aims to reveal these mysteries with the increased quantized framework resolution from SfM andthe paired monitoring of fish movements. This project would provide great predictive value to infer whatreef-associated fish communities may look like in the future if the current frequency of bleaching eventscontinues. Particularly, which fish species we might expect to decline at a given reef site if targetedconservation efforts, such as those that would be made possible by the COFG, are not enacted.Broader Impacts: The COFG developed from this research could guide the decisions of coral restorationand fisheries managers by detailing which coral species serve as primary microhabitat for a particularLTL fish population—enabling managers to easily identify which coral species to outplant to maximizethe available microhabitat for LTL prey species for a specific fishery. In theory, increased microhabitatavailability and trophic cascading would result in increased LTL prey population(s) and the HTL fisherystock. However, the world’s leading coral outplanting organization, the Coral Restoration Foundation(CRF), has only optimized the large-scale propagation and outplanting of 4 coral species. The fisheries-relevant coral species identified by this research that are not currently being outplanted would providereason to increase funding for the development of new programs working to optimize the large-scalepropagation of these species. The realization of this optimized restoration strategy would require acollaborative effort between SIO, CRF, and fisheries managers around the world to outplant the specificcoral taxa that provide the most relevant microhabitat for prey of target fisheries. This innovativerestoration optimization would be a valuable strategy to help work towards sustainable fisheries andmaintaining their incredible economic value for future generations. I intend to disseminate the findingsfrom this project via publications in peer-reviewed journals to drive similar studies that would build uponmy findings and broaden the geographic relevancy of the COFG. I will present my findings to students atnearby institutions to inform aspiring ecologists of the problems our worlds reefs are facing, hopefullyinspiring them to pursue related careers and research.References: [1] Bellwood et al (2004) Nature 429:827-833. [2] Cesar et al (2003) Cesar Environ EconConsul, NLD. [3] Goreau & Hayes (1994) Ambio 23:176-180. [4] Bruno et al (2019) Ann Rev Mar Sci11:307-334. [5] Jones et al (2004) PNAS 101:8251-8253. [6] Rinkevich (1995) Restor Ecol 3:241-251. [7]Brito-Millán et al (2019) Mar Ecol Prog Ser 630:55-68. [8] Liu et al (2014) Remote Sens 6:11579-11606.[9] Liu et al (2003) Eos 84:137-144. [10] Gracias et al (2003) IEEE J Ocean 28:609-624. [11] Lefrancoiset al (2001) Mar Biol 139:13-17. [12] Kodera et al (2020) Coral Reefs 39:1091-1105. [13] McMichael etal (2010) Fish Res 35:9-22. [14] Richardson et al (2018) Glob Change Biol 24:3117-3129."
16.0,"Introduction: Development of personalized medical treatments and diagnostics is limited by our ability toengineer tools that can keep up with demand. Exosomes are a type of nano-sized extracellular vesicle (EV)naturally produced by cells and released via endosomal fusion with the plasma membrane. Althoughprimarily utilized in the detection of diseases such as cancer, exosomes are increasingly being investigatedas a means of drug delivery due to their potential for multi-functional targeting and inherentbiocompatibility [1]. A longstanding setback in the study and application of exosomes for therapeuticpurposes is the lack of standardized methods with which to isolate, concentrate, and characterize them [2].Although scientists can functionalize exosomes with targeting molecules and drug payloads, scale-up islimited by the ability to quickly identify favorable processing conditions and thus good manufacturingpractices. In this project, I propose creating a system that will reduce the burden of the screeningprocess by developing a tool to rapidly assess exosome production and functionality. This study willhelp others engineer EVs as a tool for medical and non-medical applications.Traditional methods quantify exosomes via nanoparticle tracking analysis (NTA) which relies on lightscattering of particles to judge size and concentration via analysis of Brownian motion. However, NTA alsocounts non-exosome particles such as protein aggregates, leading to large discrepancies in actualdetermination of exosome quantity in solution. Moreover, NTA does not account for functionality ofengineered exosomes, which typically display targeting molecules on their surfaces. Current exosomepurification methods are labor intensive, requiring multiple days of centrifugation and gradient separationto remove the crude EV material prior to quantification and analysis of the exosome product [2]. Therefore,developing a method to rapidly quantify and assess exosome functionality with targeting molecules wouldenable scientists to focus their attention on scaling up and purifying only the most promising therapeuticexosome processes. This development would bypass the current time-consuming roadblocks associatedwith engineered exosome production, advancing the field.Objective: I will establish a high-throughput method to screen processing conditions for engineeredexosomes. Protein microarrays, which have previously been used in diagnostic exosome assays [3], havethe potential to quantify functionalized exosomes in an efficient and accurate manner. Therefore, Ihypothesize that protein microarrays can be utilized as a high-throughput method to screen processingconditions for engineered exosomes. I plan to (1) investigate optimal microarray conditions by engineeringspot formulations and process steps and (2) evaluate ideal processing specifications for exosome productionby running protein microarrays in tandem with cholesterol-based quantification standards.Aim 1: Investigate optimal microarray conditions to create a high-throughput screening tool forfunctionalized exosomes. Protein microarrays can be modified to include different antibodies in each spot.The ideal formulation would be one that binds only targeted exosomes. Exosomes express the surfaceligands CD9, CD63, and CD81 as unique identifiers from other EVs [2,3]. Antibody cocktails for theseligands, as well as the expressed targeting molecules, would enable binding of only the desired exosomes—even in the presence of a crude EV sample—while washing away all other materials in solution. I willdetermine the concentration of each antibody for optimal exosome binding kinetics, which is a function ofthe desired targeting molecule antibody. The formulation must also be modified to include a solventmaterial that is suitable for microarray printing but that does not interfere with the antibody-exosomeinteractions. Once I find the ideal antibody cocktail ratios, I will test the formulation to ensure that it printsappropriately on the microarray slides. The solution’s fluid properties must enable it to flow easily forspotting on the slides, dry quickly,and spread evenly. Differentadditives noted in literature wouldbe investigated to find those thatwork best with the solution. Iexpect that a 5% glycerol contentwill produce the desired results,as it was used in a previouspublication illustrating a methodfor exosome phenotyping using protein microarrays [3]. Various factors affect microarray accuracy,including sample application times, wash and blocking buffer identities and concentrations, number andduration of washes, and drying times. Each of these factors will be explored systematically using a factorialdesign of experiments (Fig.1a). If the microarray cannot be optimized for EVs, a column-based affinityseparation method could be used to purify and confirm functionalization of exosomes. Completion of thisaim will optimize the microarray as a tool for engineered exosome analysis.Aim 2: Evaluate ideal process conditions for engineered exosome production. Having identified themost favorable microarray conditions, I will compare processing conditions for engineered exosomes toprove the rigor of the quantification method. With this high-throughput method, dozens of differentprocessing specifications—such as days in culture, feed conditions, pH setpoints, and centrifugationsteps—could be analyzed simultaneously with a minimal demand on sample volume. The first step inengineering a suitable high-throughput method is to create a reliable standard for comparison. Whileworking at Codiak Biosciences, I pioneered a cholesterol assay that circumvented time-consuming NTAby quantifying exosomes based on their cholesterol content and presented a poster on this work at theInternational Society for Extracellular Vesicles (ISEV) 2020 Annual Meeting. The fluorescence plate-basedAmplex Red Cholesterol Assay was quick and accurate within a prescribed range of 0-8 µg/mL. Employingit also enabled me to quantify exosomes without the need for purification, reducing cost and time spent. Iwill utilize this preliminary work to develop a standard method for exosome quantification for this project.Once the standard is established, exosome samples conjugated with fluorescent tags will be applied to themicroarray spots. After washing, only the exosomes with the desired surface ligands will bind to the spotand be fluorometrically detected (Fig.1b). The relative fluorescence of each spot will be compared to astandard where only the exosome detecting antibodies, not the targeting molecule antibody, are present.This relative fluorescence describes the number of functionalized exosomes in the sample. These signalswill be compared to the cholesterol-based assay to obtain a quantitative readout of the number of engineeredexosomes and their relative protein loading. If fluorescent tags are not feasible, dyes, luminescentsubstrates, or other types of markers could be conjugated to the exosomes to create a measurable readout.The results of this aim will elucidate the ideal conditions for engineered exosome screening.Intellectual Merit: This work will generate a deeper understanding of protein microarrays as high-throughput screening tools and can be applied to development of new exosome or nanoparticle-basedtechnologies, which could be used in applications ranging from drug delivery to water treatment. It willexpand the foundational knowledge surrounding EVs and support future endeavors to optimize theproduction of different types of engineered exosomes, aiding in the discovery and development of EVtherapies. As a member of the Leonard Lab, which has expertise in exosome production and engineering, Iam well positioned to complete this project.Broader Impacts: This proposal integrates chemical engineering, bioengineering, and biochemistryprinciples to create an interdisciplinary project that advances dynamic drug delivery platforms throughhigh-throughput screening. Successful completion of this project will enable accurate exosomequantification, reducing labor and time investments. Rapid identification of improved processing conditionswill support efficient and sustainable production of therapeutic exosomes, increasing manufacturingfeasibility, and thus increasing their eventual accessibility on a global scale. I will leverage my connectionsat Codiak Biosciences to establish a collaboration to facilitate and support the project. I will disseminatethe knowledge gained from this research to the scientific community for feedback and further developmentthrough conferences and publications, such as the ISEV Annual Meeting. Providing students withopportunities to learn about STEM fields and to participate in projects directly will foster the nextgeneration of research scientists. I plan to direct my outreach toward programs encouragingunderrepresented students to consider graduate school, such as REUs and the Northwestern MorningMentors and Mentorship Opportunities for Research Engagement (MORE) programs, where I will mentorstudents and encourage their participation in STEM research.References: [1] Wang, J. et al. (2017). ACS Applied Materials & Interfaces, 9(33), 27441-27452. [2] Chia,B. S. et al. (2017). TrAC Trends in Analytical Chemistry, 86, 93-106. [3] Jørgensen, M. et al. (2013).Journal of Extracellular Vesicles, 2(1), 20920."
17.0,"Intellectual Merit How do you study something you cannot see? Dark matter (DM) is responsible forshaping the large-scale structure of the universe we see today, comprising 80% of all matter.Decadesofdirect and indirect searches for annihilation radiation have notyieldedanysignals. InordertostudyDMastrophysically, we must use the luminous parts of the universe — the galaxies and galaxy clusters thatreside inside DM halos — as tracers. The DM halo relationship is cleanest near the outskirts of thesestructures, where non-gravitational physics, such as AGN feedback, have the least effect. Whiledifficultto observe due to their low density, new and upcoming advancements in instrumentation are detectingobservational tracers in halo outskirts for the veryfirsttime.SubsequentmeasurementsofDMhalomassand dynamical quantities, such as accretion rate, will constrain not only large-scalestructurebutalsothenature of the DM particle itself.My projectexplorestheoutskirtsofbothindividualgalaxiesandgalaxyclusters:twostructuresatdifferent cosmological scales but subject to fundamentally similar dynamics. Starting on the scale ofgalaxies, the brightest cluster galaxies(orBCGs)haveextendedfaintstellarhalosthatarenowthoughttohave physically significant edges [1]. Moreover, in recent optical images from the Hyper Suprime-CamSubaru Strategic Program (HSC survey), it has been shown that the stellar mass in the outskirts(10-100kpcradii)oflow-redshiftBCGsisanexcellentproxyforDMhalomass[2].Theoutskirtsofthesemassive galaxies are dominated by stars accretedduringmergerswithprevioussatellitegalaxiesandthusprovide an estimate of “historical richness” for their DM halos. While promising for its potential ofmeasuring DM halo mass, this result leads to more questions: Why is the 10-100kpc region significant?Could an even tighter relationship to DM mass exist with stellar profiles past 100kpc?On a larger scale, the outskirts of galaxy clusters (>1Mpc) also provide a laboratory for theeffects of DM. At these distances, dark matter particles turn around at their apocenter, called thesplashback radius, the location of which depends on the halo’s mass accretion rate(MAR)[3,4].Recentsimulation work suggests this radius coincides with an analogous “stellar splashback” radius of thecluster’s stellar distribution (or intracluster light, ICL), which would also vary with MAR and revealintriguing DMhalodynamics[1].Thesmallsampleanduseofzoom-insimulationsinthisstudywarrantsafollowupinvestigationwithcosmologicalboxsimulationsandalargersample.HowobservablethisICLedge will be with future instruments is also unclear and requires further modeling.There are many other unexplored phenomena at theseclusteroutskirts,notonlyinDMbutinthehydrodynamics of gas. In particular, an accretion shock must beproducedwhencoldgasfallsintoahaloand experiences a drastic jump in temperature. According to the self-similar collapse model [5, 6], theradius at which this shock appears should be almost identical to the cluster’s splashback radius [7],making it another potential observational tracer of DM. However, the model has not held up insimulations, in which the shock radius has been found to be 20-100% larger than the splashback radius[8]. This disagreement is a fundamental theoretical gap that needs to be understood to interpret currenthigh-resolution observations of the Sunyaev-Zeldovich effect in clusters (from the South Pole Telescopeand Atacama Cosmology Telescope) as well as near-future radio observations of accretion shocks.Cosmological simulations offer the opportunity to understand the connections between baryonicphysics and underlying DM halos inordertobothinterpretobservationsandmakepredictionsouttohaloradii we cannot yet observe. While many processes in simulations are implemented via subgrid models,the processes in halo outskirts are mostly-first principle physics producible even with these limitedmodeling conditions. I will use cosmological simulations of dark matter and galaxies includingthelargevolumecosmologicalboxsimulationsuiteIllustris-TNG(TNG)[9].IproposetouseTNGtoinvestigatemultiple baryonic physics phenomena at theoutskirtsofgalaxiesandofgalaxyclustersaspotentialtracers of dark matter halo properties.My projectconsists of 3 scientific goals:1. Develop robust techniques to measure BCG light profiles out to large radii to find optimalestimators of DM halo mass2. Test the connection between ICL edges and splashback radius3. Analyze the relationship between the accretion shock radius and splashback radius1. Galaxy Stellar Outskirts andDMHaloMassTheHSCsurveyisamajorstepinobservers’abilitytooptically image BCG outskirts out to 100kpc given its high-quality seeing conditions and a depth 3-4magnitudes deeper than the SDSS. In collaboration with the HSCteam,Iwillusethesedatatodevelopanew technique for extrapolating stellar mass profiles of high-mass BCGs beyond their observable radiusby testing them against a sample of simulated, mock-observed TNG galaxies of similar mass.A crucial consideration in simulation-observation comparisons is how to recreate the conditionsused by observers. To achieve this science, we need to overcome technicalhurdles.Forexample,thefileordering of the TNG output data is by friends-of-friends groups, suchthatoverlappinggroupscouldleadto one group missing particles near its outskirts. Forgoingthisdataorganizationtoinsureallparticlesareaccounted for makes extracting complete profiles out to large radii in large simulations is a non-trivialtask. I recently implemented a function in a simulation-extraction software, Hydrotools, that overcomesthis challenge and allows users to extract all particles within a given radius. This function lays thefoundation for any project relying on large-scale radial profiles.2. ICL and Mass Accretion Rate Satellites fall into a halo with various velocities, orientations, andinternal energies, so it is not immediately clear that disrupted stars in the ICL are faithful tracers of theDM halo potential. I will use Hydrotools to extract complete stellar and DM profilesfromhalosinTNGto investigate this connection. The splashback radius is signified by a caustic in the DM density profilewhereparticlespileupaftertheirfirstorbit,sothe“stellarsplashbackradius”canbefoundanalogouslyinthestellardensityprofile.Iwillcalculatethedifferencebetweentheseradiiandtheirdifferentcorrelationswith MAR for a sample of various mass halos, predicting the use of the ICL as a DM halo tracer.3. Shock and Splashback Radii While TNG does not outputshocksurfacesspecifically,wecaninsteaddetect a sharp drop in gas entropy profiles to signify the accretion shock radius. I will analyze therelationship between the shock and splashback radii in massive halos over different stages of clustermerger events, using complete gas and DM profiles. These results will provide context to interpret therapidly growing amount of Sunyaev–Zeldovich, and soon radio, accretion shock observations.Future Directions The outskirts of galaxies and of galaxy clusters are a clear next place to look forpotential tracers of DM halos. Upcoming instruments and surveys will revolutionize our constraints onDM features such as the splashback radius. Moreover, the low surface brightness of BCG and ICLoutskirtsarealreadydetectablewiththeHSCsurveyandwillbeevenmoresowiththeupcomingVera.C.Rubin Observatory and the Nancy Grace Roman Space Telescope. In the radio regime, the SquareKilometer Array (SKA) will vastly increase the number and resolution of observed accretion shocks.However, to interpret any of these observations we need to improve our theoretical understanding. Myproject will build off of current observational work and provide the foundation for interpreting futuredata.Broader Impacts I will continue my work to limit barriers for underrepresented minorities (URM) inastronomy. Particularly, I will use the remainder of my term serving on the AAS SMGA (sexualorientation and gender minorities in astronomy) committee to develop a mentorship program forLGBTQ+ early career astronomers. This program will match graduate students and postdocs with moreexperienced mentors who share a LGBTQ+ identity. Mentors will guide mentees through navigating thefield, especially challenges specific to LBGTQ+ individuals in the workplace. I will also continue toco-lead the BANG! (Better Astronomy for the Next Generation) seminar series in my departmentwhichcovers alternate career paths and EDI issues in Astronomy. I will focus on planning seminar sessionscovering previously under addressed topics, such as accessible teaching strategies and equitableworkplace practices. Finally, I will continue to work with the GRADMAP (Graduate resources foradvancing diversity with Marylandastronomyandphysics)teamtoprovideexternalresearchexperiencesfor students at minority serving institutions by teaching workshops in career skills and serving as asummer research mentor.[1] Deasonetal(2020)[2]Huangetal(2021)[3]Diemer&Kravtsov(2014)[4]Adhikarietal(2014)[5]Bertshinger et al (1985) [6] Shi etal(2016a)[7]Shietal(2016b)[8]Aungetal(2021)[9]Pillepichetal(2018)"
18.0,"been political debated, although empirical work suggests that low-skill immigration has a small negativeeffect on average British wages. However, individual-level migration data suggests that Eastern Europeanmigrants tend to be highly educated and highly skilled [1], motivating my proposed investigation of theeffects of the emigration of high-human capital individuals from Eastern Europe following the collapse ofcommunism in the nineties. In particular, what are the macroeconomic impacts of a shock to thedistribution of human capital as a result of immigration?Empirical work suggests that there are large differences in output per worker across countries [2],and that differences in total factor productivity (TFP) account for the bulk of these differences in output[3]. The mass emigration of high human capital workers decreases aggregate productivity directly throughthe decrease in labor productivity. Research on the emigration of academic Jewish scientists in NaziGermany [4, 5] shows that there are persistent negative impacts on emigrants’ departments’ performancesbecause the replacements are of lower intellectual caliber and positive effects on receiving departments’patenting. This micro work validates macro-theoretical research on shocks to human capital.One aspect of Schumpeterian growth theory is that innovations are the result of firms’ investmentdecisions based on expectations of future profits. Previous work has made the connection betweennational knowledge stock and innovation at the scientific frontier. My work bridges the connectionbetween negative shocks to population and aggregate human capital with innovation in a technology-follower setting. I incorporate the firm’s trade-off between investment in R&D and production as afunction of the distributional shift in human capital, specifically firms’ decisions to invest in R&D as aresult of Eastern European emigration in the 1990s.Methods: I will construct a general equilibrium overlapping generations (OLG) model of a small openeconomy populated by heterogeneous agents that vary in their stock of human capital and age, and thenapply this framework to estimate the impact of the migration-induced productivity shock on Bulgaria’seconomic growth. Using the structural model, I am able to analyze the welfare effects on heterogeneousindividuals through equilibrium values of the model.Human capital accumulation is an endogenous process, meaning that agents choose their optimallevel of human capital. The agents allocate their time between labor, leisure, and human capitalaccumulation in each period. Firms split labor and capital inputs into production and research, where theresearch production function is governed by a Poisson process. Intuitively, investment in research yieldsinnovations or tangible improvements in technology at random and fairly rare points in time, and thisunderstanding of knowledge production closely fits within a Poisson process. The parameter that governsthis Poisson process is endogenously determined by the stock of human capital in the labor force availableto the firm and the existing stock of knowledge. In other words, within a discrete period of the model, thediscovery of one innovation does not have any bearing on the discovery of a second innovation(memoryless property of the Poisson distribution), but between periods the total innovations impact therate of knowledge production. The firm faces a tradeoff between production for profit in a given periodand investment in research for a potential payoff in a future period. This decision involves risk. Firms alsoconsider distortionary taxes and expectations about economic conditions in their decision.One metric of innovation is patent applications. Applications are a reasonable proxy forimmigration because patent applications are not dependent on a government agency’s determination ofthe worthiness of an innovation for being patented. In a revealed preference framework, applying for apatent indicates that the firm believes it has produced innovation worthy of patenting. This belief informstheir decision to invest in research and development. The preparation of a patent application is notwithout effort; therefore, applying for a patent represents the firm beliefs I am interested in capturing.I will calibrate my structural OLG model to Bulgarian data to conduct a policy experiment onemigration. Using individual-level data, the Mincerian earnings function for the returns to schooling andexperience can be calculated [6], which gives direct estimates of the parameters governing the humancapital accumulation decision by the agent. The weight of consumption in utility is adjusted to captureaggregate hours worked in the data (available from OECD), and this parameter characterizes the labor-leisure decision of the agents. The parameters governing the relationship between the rate of knowledgeproduction and human capital are estimated in the literature. These parameters combined with the first-order conditions of the structural model determine the firm’s behavior. Because I can fluently read andspeak Bulgarian and my personal experiences, I am uniquely able to obtain the necessary innovation datato calibrate the rest of model. Otherwise, standard data and computational resources are sufficient.Because the migration decision depends on a number of unobservable characteristics, it isimplausible to include this decision in a structural model. Information about the types of people whomigrate, include their ages, educational levels, and work experience are observable in individual-levelsurveys conducted in Bulgaria. Accordingly, emigration is captured by changes in the relative sizes ofhuman capital and age cohorts as well as level changes in population. Additionally, as a result ofbottlenecks related to work visas, documentation, and language barriers, this mass migration does nothappen immediately after the collapse of communism. This delay in the timing of the migration shockallows for a few years post-collapse to establish the baseline macroeconomic trends.Thus, there are two balanced growth paths of interest. One growth path is the case where nomigration occurs, and the model is calibrated to match the known pre-shock periods. The other growthpath includes the migration shock, and data on immigrants is used to adjust the measures of ages andhuman capital types. The comparison between the second balanced growth path and the data measures theperformance and predictive capacity of the model, while the comparison between the second growth pathand the first (the simulated counterfactual) represents the effect of the migration shock. A successfulmodel will replicate targeted moments of the data.Broader Impacts: Understanding the effects of high-skilled emigration is crucial to reconciling why theeconomies of Eastern and Western Europe have not converged since the 1990s and designing policies thatencourage talent to remain in Eastern Europe. The model I propose captures another dynamic ofmigration through shifts in the age distribution. Empirical work indicates that migrants tend to beyounger, and a large migration event such as in Eastern Europe following 1991 may shift the agedistribution upward. Previous work has analyzed the macroeconomic implications of an aging populationas well as changes to human capital separately, but the interaction between the two remains an openquestion. My model and associated calibration would partially fill this gap in the literature.Furthermore, the mass migration of young people negatively shocks the population growth rate.Because the growth rate of the population is determined by the proportion of young people, the one-period shock to the population growth rate propagates through R generations, where R is the cutoffbetween young and old. Combining this with the level decline in population as a result of the shock, mymodel also captures the absolute population declines observed in some Eastern European countries.Because population declines independent of wars, pandemics, and the like are rarely observed, there islittle empirical work on the macroeconomic implications of declining populations. My model andcomputational approach incorporates these effects and could isolate them via simulating an age-biased,human capital-neutral migration event, where migrants are young but equally skilled as the population.Several countries in Asia, Western Europe, and Latin America are expected to experience populationdecline in the near future, and the mass-migration events of the nineties started this process earlier inEastern Europe than the rest of the world. Accordingly, my findings on the effects of population declineare of significant interest and would contribute to an open and deeply relevant question.Moreover, the relationship between high-skilled emigration and innovation is not limited toEastern Europe. There are several US states, including my home state, Kansas, with net high school andcollege graduates leaving, resulting in a negative shock to human capital. During the Covid-19 pandemic,several Midwestern cities, including Topeka, Kansas, adopted policies that gave workers a lump-sumtransfer of money in exchange for moving and living in the city for at least a year. A natural extension ofmy work is to evaluate the prevalence and demographics of uptakers of such policies, and then analyzemy model with shocks to the human capital distribution as observed in the data.References: [1] IMF report “Emigration and its Economic Impact on Eastern Europe” [2] McGrattan andSchmitz, (1998) Federal Reserve Bank of Minneapolis [3] Hall and Jones (1999) QJE [4] Moesa et al(2014) American Economic Review [5] Waldinger (2016) Review of Economics and Statistics [6]Patrinos (2016) IZA World of Labor"
19.0,"free, first-principles theoretical treatment of core-to-core x-ray emission spectroscopy (ctc-XES). Successin this research program will have wide impact for refining analytical and fundamental study of the element-specific electronic structure in highly correlated materials, an extremely broad class with significantindustrial, technical, and environmental relevance. Furthermore, by having fully addressed the forwardproblem, i.e., prediction with no adjustable parameters of ctc-XES spectra from local structure, we will beable to use unsupervised and supervised machine learning (ML) methods to understand the informationcontent in core-to-core XES across systems already widely studied (e.g., 3d transition metals) and alsosystems that have seem comparatively little XES (e.g., materials with heavy d- and f-electron elements).This will likely lead to prediction of new diagnostic spectral signatures of magnetism in f-electron systems.Introduction and Background: X-ray emission spectroscopy (XES) is the very high-resolutionstudy of fluorescence given off by the radiative decay of a core-shell excited atom, inherently probing theoccupied electronic states. XES can carry information about the local chemical environment of thefluorescing atom, such as valence-level spin, oxidation state, ligand identity, local coordination geometryand bond covalency. While there are many truly first-principles theoretical tools for parameter-freecalculation of most other advanced x-ray spectroscopies (e.g., XAFS, RIXS, and valence-to-core XES), thesame is not true for ctc-XES, which is a deeply many-body problem where the treatment of highly correlatedmaterials with partially filled d- or f- shells is especially challenging [1]. DFT approaches fail to describethe local many-body correlation effects while more accurate configuration interaction (CI) methods arecomputationally expensive and often difficult to implement beyond simple systems [1, 2]. Multipletimplementations are therefore the preferred theoretical framework for ctc-XES with many theoretical codesand models being developed over the last 40 years [3]. Those multiplet models show adequate agreementwith experimental results after fits to screening and correlation parameters. Here, I will develop tools tocalculate those parameters, moving from a descriptive treatment to a predictive treatment of ctc-XES.Research Plan: My research plan has three main components: (1) A theoretical component thatbuilds off of my prior work (below) and the expertise available from my theory mentors, Profs. Rehr andKas; (2) Validation via measurement of ctc-XES across a wide range of materials, this capability is firmlyenabled by lab-based XES available in my Ph.D. advisor’s lab (Seidler group, UW); and (3) a MLcomponent that will build on emergent methods in the XAS community, such as recent Seidler group workon unsupervised ML [6]. Hence, I am strongly supported by local expertise and needed facilities.Beginning with theory, Figure 1 shows the distinction between common practice, the result of mywork over the past six months, and a large part of the proposed further improvements. First, the centralgreen column highlights the key-components that go into standard Multiplet Ligand Field Theory (MLFT).Note the need for many local environmental components such as the crystal field and charge transfer leadsto a large increase in free parameters, limiting predictive capability. Next, the leftmost column shows myprogress over the past 6 months building on work by Haverkort et al. [4] by applying a DFT + MLFTapproach to ctc-XES, using the full-potential local-orbital (FPLO) DFT code to determine many, but notall parameters needed by the multiplet engine (Quanty). Using ‘reasonable’ values for the remainingundetermined parameters, I find excellent agreement between my new calculations and experiment for theenvironmentally important speciation of Cr3+ with respect to the carcinogenic Cr6+, see Figure 2. Third, asshown in the right column of Figure 1, I will use the real-space Green’s function code FEFF to both replaceFPLO and also implement new calculation of the thus far undetermined parameters for the MLFT treatment[5]. The result will be the first truly parameter-free, first-principles MLFT treatment of ctc-XES.Moving to experimental validation: past, ongoing, and recently funded work in the Seidler groupincludes ctc-XES measurements of numerous elements in battery materials, oxygen evolution reactioncatalysis, cements used in long-term storage of toxic and radioactive wastes, and carcinogens occurring inconsumer products or industrial wastes (e.g., Cr toxicity, such as probed by Figure 2). This provides aplethora of testing grounds across numerous problems with high societal relevance. In this work, I will beable to collaborate with other students in the group to design reference standard studies, validate againstmy theory calculations, and then apply the resulting information to draw best inferences about the systemsof actual interest.Fig. 2. Agreement between current theory andexperiment for the K XES of Cr(III) and Cr(VI).Finally, the validated theoretical approachwill be distributed to the general x-ray spectroscopycommunity via the workflow management toolFig. 1. Theoretical MLFT workflow schema:Corvus and will also be used as the basis for MLstandard approach (center), current progressinvestigations of the information content of ctc-(left) and future plans (right).XES. This latter work will start with unsupervisedML, such as t-SNE, which the Seidler group recently introduced as an important way to determine thechemical information content in vtc-XES and XANES [6]. This will be the first ML study of this kind tobe applied to ctc-XES as traditional theory methods are either too inaccurate or too computationallyexpensive compared to the novel DFT+MLFT approach. This work will help determine which generalproblems are, or are not, well-encoded into ctc-XES across the periodic table and different chemical classes.Intellectual Merit: The interleaved characterization of local atomic and electronic structure posescentral challenges across numerous problems, including electrical energy storage, catalysis research, agingof construction materials, toxicity in consumer products, environmental consequences of industrial wastes,and low-diffusion matrices for long-term storage of toxic or radioactive wastes, all of which still have openquestions that require an MLFT approach to accurately describe. These questions come at a time of rapidgrowth in access to experimental ctc-XES capabilities via the development of lab-based instrumentationfor education and analytical study (a trend led by Seidler group), major upgrades of synchrotron facilitiesand specialized XES end-stations for applications in energy sciences, and the steadily increasing applicationof XES in ultrafast x-ray free element laser (XFEL) studies probing chemical and electronic dynamics.This project will have a uniquely outsized impact not only because of the importance of the socialand scientific problems being addressed but because of the synergy with the emerging experimentalaccess to core-to-core XES capabilities. Additionally, the open access model of the tools developed in thisresearch program will facilitate broad adoption within the x-ray community, bridging the gap betweenaccurate ab-initio theoretical methods and the experimental need for reliable first-principles theory.Broader Impacts: Much of my prior experience in outreach has centered around introducingpeople to a side of science which focuses on the curiosity and intrigue sparked by the natural world aroundus. I firmly believe that to accomplish this, access to intuitive introductory tools is a necessity. As addressedin my personal statement, I will continue to develop and refine x-ray specific educational material such asthe XAS-RW, addressing the acute need for qualitative and intuition based introductory material in mysubfield. I will compliment this with fun, science-based community engagement efforts through groupssuch as the UW Science Explorers and UW Stem Pals to bring hands-on physics directly into the classroom.This will provide the ideal environment to expand upon my Physics Field Day event, as I couple myorganizational experience with new community collaborations to deliver a unique, immersive program.Works Cited: [1]doi:10.1002/qua.24905;[2]doi:10.1002/cphc.201800038;[3]doi:10.1016/j.elspec.2021.147061;[4]doi:10.1103/PhysRevB.85.165113;[5]doi:10.1039/B926434E;[6]doi:10.1039/D1CP02903G"
20.0,"framework for a planet’s formation, evolution into its present state, and past and present geophysicalproperties such as magnetic fields and atmospheric conditions. In this era of prolific exoplanet discovery,the quest to investigate planetary interiors and surface conditions is more pressing than ever. With thegrowing number of exoplanets ranging in size from super-Earths to sub-Neptunes, and the omnipresentgoal of “finding a new Earth”, it is becoming evident we need to concentrate our studies on such planets.Currently, radial velocity and transit methods used to detect exoplanets give mass and radius data forexoplanets but offer no compositional information. Comparing the mass and radii of exoplanets with mass-radius relationships of pure materials such as iron, silicates, and ice, and stoichiometric mixture thereof,offer a glimpse into their plausible bulk compositions [2-4]. However, attempts to infer bulk compositionhave resulted in degeneracy with many interior composition combinations fitting mass and radius valuesfor a particular exoplanet. Many models assume planets are fully differentiated, yet previous works usingdensity functional molecular dynamics (DFT-MD) simulations at high pressures and temperatures havepredicted deviations from this model. For example, the miscibility of H O with H and He [5] and miscibility2of Fe and MgO [6]. Contrary to traditional models, we infer “fuzzy layering” if material is miscible atboundary conditions, which would result in the gradual mixing of heavier elements into the upper, lessdense layers. DFT-MD simulations are a powerful tool in predicting the equation of state (EOS) of a widevariety of planetary materials and mixtures at conditions that are difficult to achieve empirically.Hypothesis. Some super-Earth and sub-Neptune exoplanets, termed waterworlds, contain a significantamount of water (H O) ice overlaying a magnesium silicate interior [7,8]. The stability of such a stratified2internal structure depends on whether these simplified two-layer models reflect realistic water worldstructures. Instead of relying on static two-layered models, I am motivated to explore the dynamics ofmaterial mixing at the magnesium silicate-ice boundary layers under P-T conditions relevant to the interiorconditions of waterworlds. Understanding whether these two materials are miscible will help us betterresolve the internal composition and stratification of water worlds. I propose to study the miscibility of acommon high-pressure planetary magnesium silicate, enstatite (MgSiO ) [9], and high-pressure3water (H O) ice using DFT-MD. MgSiO will be referred to as rock and ice is assumed to be water ice for2 3the remainder of this proposal.Research Plan. (Research Goal 1: Building Rock-Ice Systems and running DFT Simulations) I will buildthe rock-ice systems and equilibrate each to a respective pressure in gigapascals (GPa) (Table 1).Table 1. Systems with MgSiO and H O crystal phases, space groups, and initial system pressure3 2System MgSiO phase Space group (MgSiO ) H O phase Space group (H O) P[GPa] x3 3 2 21 ppv Cmcm [63] ice X Pn-3m [224] 120 0.292 pv Pnma [62] ice X Pn-3m [224] 60 0.293 pv Pnma [62] ice VIII I41/amd [141] 30 0.26The crystal phases chosen for each material present at 0Kwere the most stable structures at each respective pressure (Table1.). I will run simulations on each system using a canonicalensemble (constant number of particles N, constant volume V,and constant temperature T) increasing the temperature of eachsystem from 500 K to 8000 K with the Nosé-Hoover thermostat[10]. I will perform simulations in 500 K increments in a “heat-until-mixes” approach, similar to the “heat-until-melts” method[11]. Although this approach is prone to overestimating meltingtemperatures, my goal is to calculate the upper bound P-Tconditions for rock-ice mixing which will be accomplished withmy MD. I describe each system by its ice to rock mass ratio[𝑚 /(𝑚 +𝑚 )], for example, System 1 has an ice to𝐻2𝑜 𝐻2𝑜 𝑀𝑔𝑆𝑖𝑂3rock mass ratio of 0.29. Then to investigate in which proportionsrock and ice will mix I will simulate additional systems with icemass ratios of 0.29 and 0.20. I will accomplish this by increasing the number of rock molecules whilekeeping the number of ice molecules constant. If the two materials spontaneously mix during thesimulations, I will know rock and ice are miscible at this temperature. Preliminary results from my MDsimulations of System 1, run at 8000 K (Fig 1.) show exciting, novel results of miscibility.(Research Goal 2: Determining Miscibility) An efficient way to detect mixing is to analyze howfar atomic species move from their original positions by calculating their mean squared displacement(MSD). When the diffusion coefficient for all species is above zero, I will consider the system fluid.However, I will not know based on MSD alone whether the atoms crossed the rock-ice interface. The systemcould contain molten rock and water which remain immiscible instead of forming a homogeneous mixture.Therefore, I will also visualize each trajectory and verify that diffusion occurs across the boundary. Myfinal method for confirming miscibility is to calculate the radial distribution functions (RDF) of magnesium(Mg) and silicon (Si) versus the oxygens (O) in MgSiO , termed rock oxygens, and oxygens associated3with ice, termed ice oxygens. My goal is to show that Mg and Si lose coordination with the rock oxygensand interact with the ice oxygens. For example, when I plot Mg−O and Mg−O , at the same temperature,rock iceif rock and ice are miscible, their radial distribution curves should overlap.Intellectual Merit. In addition to working with my Ph.D. advisor, Dr. Burkhard Militzer at U.C. Berkeley,I will collaborate with Dr. Sarah Stewart, a professor in the Earth and Planetary Science Department at U.C.Davis, to perform dynamic smoothed particle hydrodynamic (SPH) simulations of colliding planetarybodies. This will give insight into whether these large impact events produce the conditions necessary formaterial mixing. It is important to determine post impact conditions because giant impacts govern animportant stage of planet formation, mold their interiors, and drive geophysical properties [12].Multicomponent EOSs of material mixing will shift the planetary science community’s focus fromstatic planetary models, where fully differentiated layers are modeled, to dynamic ones which includechemistry deep within the planet. If we neglect the presence of “fuzzy layers” within planets, we may misskey planetary properties such as its thermal evolution and magnetic field generation, which influence otherproperties such as tectonics, outgassing, dynamics, and volcanism. I plan to continue investigating rock-icemiscibility by considering other rocky material, such as Mg SiO or MgO with H O, and exploring lower2 4 2pressure regimes [8]. Moreover, provided that the necessary conditions are reached, I will further myresearch to elucidate whether a homogeneously mixed rock-ice layer could persist over long periods of timeand even become stably stratified within water worlds. This will affect overall heat flow throughout theplanet which will help us better understand the evolution of water worlds.Broader Impacts. My proposal has applications in a diverse range of disciplines such as condensed matterphysics, high energy density physics, geochemistry, and geophysics. I will publish my work in journals(e.g. PNAS), present at conferences (e.g. AGU and APS), and most importantly continue outreach bypresenting my research at local, public seminars (e.g. BASIS, SLAM, and Compass Lectures). I spent myfirst summer as a graduate student volunteering at two workshops recruiting students to pursue graduateschool in planetary science. I also began tutoring environmental sciences at San Quentin Prison which helpskeep me informed on challenges facing our most at risk communities and how to aid in their success asaspiring geoscientists. After only one year in graduate school, I have already helped form the firstUnlearning Racism in the Geosciences (URGE) pod at Berkeley. We will present our pod’s work ofintegrating URGE deliverables into the Berkeley EPS department-level strategic plan for enhancingdiversity at AGU 2021. The NSF fellowship will allow me to produce and share my findings with my peersas well as a general audience, increase equity in my field, and recruit the next generation of geoscientists.References. [1] B. J. Fulton et al. The Astronomical Journal 154, 109 (2017). [2] A. Vazan et al. arXivpreprint arXiv:2011.00602 (2020). [3] O. Shah et al. Astronomy & Astrophysics 646, A162 (2021). [4] S.Seager et al. The Astrophysical Journal 669, 1279 (2007). [5] F. Soubiran et al. The Astrophysical Journal806, 228 (2015). [6] S.M. Wahl et al. Earth and Planetary Science Letters 410, 25 (2015). [7] M.S Marleyet al. Journal of Geophysical Research 100,348 (1995). [8] T. Kim et al. Nature Astronomy, 5, 815-821(2021). [9] T Duffy et al. Front. Earth Science 7, 23 (2019). [10] N. Shuichi Progress of Theoretical PhysicsSupplement 103, 1 (1991). [11] G. Robert et al. Physical Review B, 82, 104118 (2010). [12] P.J. Carter etal. Journal of Geophysical Research: Planets 125, 1 (2020)."
21.0,"Introduction: Externally actuated micro/nanorobots have generated considerable excitement over the lastdecade due to their potential to carry out controllable microbiological tasks.7 Specifically, microroboticswarms,containingtensofthousandstomillionsofindividualrobotsthesizeofbacteria,havethecapacityto perform diagnostics and directed drug transport within deep tissues and microvasculars hitherto inac­cessiblebyconventionalmeans.7 Unlikeindividualmicrorobots, swarmsleveragethecoupledinteractionsbetween constituents to form large­scale collective motions that far exceed the speed, strength, and func­tionalityofa singlemicrorobot. Because theseswarms areexternallyactuated bymagneticor opticfields,actuation schemes can be programmed to control a swarm’s collective motions and morphology. Specificexamplesincludeclustering,swirling, dispersion,orribbonformation, whichtogetherallowforhighenvi­ronmentaladaptivity.4 Unfortunately,experimentationaloneisinsufficienttoproperlydesignmicroroboticswarms for real­world applications; instead, efficient computational tools are needed to augment experi­ments by enabling rapid investigation into the effect of design parameters. However, modeling swarmingmicrorobotsisinherentlychallenging—owingtothetheoreticalandcomputationalcomplexityofresolvingthe many­body hydrodynamic effects and short­ranged collisions. No state­of­the­art method is currentlycapable of accurately simulating real­world microrobotic swarms. This issue is further exacerbated by thegenerallackoffundamentalunderstandingofhowtobestgenerateandcontrolaswarm’scollectivemotionsfor specific tasks, especially within confined microfluidic environments. I aim to overcome these chal­lengesby(1)creatingthefirsthigh­fidelity,scalablecomputationalframeworkabletosimulatedensesuspensions of complex­shaped, microrobots and (2) numerically investigating how key parameters,likerobotshape,actuationscheme,andgeometricconfinementaffectaswarm’scollectivemotions.IntellectualMerit: Accurate simulation of microrobotic swarms within real­world microfluidic environ­ments is essential if these systems are to be designed for practical biological applications. Previous worktoward modeling these systems has primarily focused on dilute suspensions, where the long­range hydro­dynamic interactions dominate the system dynamics, allowing the effect of near­body dynamics to be ap­proximatedbyrepresentingcomplex­shapedparticlesintermsofsimplegeometrieslikespheres,ellipsoids,or rods. However, as the number of particles per unit volume increases, near­body interactions becomeincreasinglyimportantcausingtheseapproximationstobreakdown. Withindensesuspensions,evenseem­ingly insignificant modifications to robot shape, like using spherical robots vs cubic robots, will result indrastically different close­to­contact dynamics, which directly impacts internal pattern formations. There­fore,fordensemicroroboticswarms,itisimperativethatthenear­bodydynamicsbetweencomplex­shapedparticlesbeaccuratelycapturedtocorrectlypredict/controltheircollectivemotions.Task1­Isogeometricanalysis: Toaddresstheseissues,Iproposetodevelopahighfidelitymodelcapableofaccuratelyresolvingthehydrodynamicinteractionsbetweencomplex­shapedmicrorobots. Guidedbythisgoal,IhavebeenworkingindirectcollaborationwithProf. B.Shanker(anelectromagnetsexpert)andProf.H.M.Aktulga(ahigh­performancecomputingexpert)todevelopanisogeometricboundaryintegralmodel,which I am implementing within Python. The fundamental principal of isogeometric analysis is to utilizesmoothbasisfunctionstorepresentparticlegeometriesandthephysicsontheirsurfaces,therebyprovidinghigherorderdescriptionoffieldsandenablingaccurateresolutionofnear­bodydynamics. Towardsthisend,Ireformulatedanexistingboundaryintegralsolver3 basedontheassumptionthatmicrorobotsaretypicallygenus­zeroshapes,allowingmetopullbacksurfacequantitiestotheunitsphereandthendiscretizeintermsof spherical and vector spherical harmonic basis functions. I then solve the governing boundary integralsthrough Galerkin’s method. One of the challenges when solving hydrodynamic boundary integrals is theevaluation of the nearly­singular integrals that arise when solving particle to self and particle to nearbyparticleinteraction. Toaddressthisdifficulty,Iderivedasingularity­freemethodforevaluatingparticleself­interaction through established techniques of singularity subtraction/isolation. My next step is to integrateadaptivequadraturetechniquestohandletheinteractionbetweenclose­to­contactparticles. Oncecomplete,Iwillresolvetheeffectofno­slipconfinementsbyaddedadditionalconstraintstomylinearsystembasedonwell­establishedmethods.6 Myfirstmilestonewillbetobenchmarkthismodelagainstanalyticalsolutionsfortheflowbetweensphericalparticlesbothwithandwithoutconfinement.Task 2 ­ High performance software development: High fidelity simulation of microrobot swarms iscomputationally intensive and requires fast, scalable numerical methods to make modeling real­world sys­temsfeasible. Thekeybottleneckistherapidcomputationofpairwisehydrodynamicinteractionsbetween𝑁 particles, which scales as 𝒪(𝑁2). To overcome this issue, I will integrate my hydrodynamic solver intotheparallelcomputingframeworkdevelopedbyProf. Shanker. ThisFORTRAN­basedframeworkcentersaroundtheAcceleratedCartesianExpansions(ACE)algorithm,whichreducesthecomputationalcomplexityofevaluatingthepairwiseinteractionsfrom𝒪(𝑁2)to𝒪(𝑁).1Toaccomplishthistask,IwillfirstconvertmycurrentPythonimplementationintoFORTRAN.IwillthendevelopsuitabledatastructuresfortheefficientcomputationandcommunicationofsphericalharmoniccoefficientsandcreatecustomMPIcommunicationschemes for the tonsorial kernels that arise in our calculations. These implementation details are vital forensuring that our framework remains computationally tractable and will be validated based on scalability.TheNSFGRFPwillsupplementourcomputationalresourcesbyprovidingaccesstoXSEDE,enablingmetofullyharnessthecapabilitiesofthishigh­performanceframework.Task 3 ­ Dense microrobotic swarms under rigid confinement: The simulation of dense microroboticswarms requires simultaneously resolving the hydrodynamic interactions and short­ranged collisions be­tween particles. Unfortunately, traditional collision resolution algorithms become numerically unstablewhen applied to dense assemblies. To overcome this limitation, my collaborator Dr. W. Yan (a compu­tationalbiologist)developedacollisionresolutionalgorithmusinggeometricallyconstrainedoptimization.5Throughthiscollaboration,IwillexpandDr. Yan’sexistingopen­sourcecode­basetoincludefastmethodsforevaluatingthedistancefunctionsandsurface­normalsbetweencollidingnon­convexparticlesbasedonadvances within the computer graphics community.2 I will then couple this code­base with the frameworkdeveloped in Task 2. Once complete, I intend to leverage the speed and flexibility of this computationaltool to analyze how key parameters like robot shape, robot actuation type, and confinement geometry af­fectaswarm’scollectivemotionsandpatternformations. Bysystematicallyperformingsimulationswithinthisexpansiveparameter­space,Iintendtoprovideexperimentalistswithacomprehensivepictureofhowtobestdesigntheirmicroroboticswarms. Toquantifytheeffectstheseparametershaveonaswarm’scollectivemotions,Iwillutilizemyexistingpost­processingtoolkit,whichIhaveappliedtoactivemattersystemsforextractingtheirlarge­scaletopologicalstructuresandensembleaveragedstatistics. Basedontheseresults,Iwilliterativelydesignloadingschemesandrobotshapestostreamlinethetransportandcapturingoflarge­scalecargowithinsimulatedmicrovascular­likeenvironments.BroaderImpacts:Controllablemicroroboticswarmshavethepotentialtoprofoundlyimprovepublichealthby facilitating novel treatment methods like the transport of chemotherapy drugs directly to cancer sites.7My work’s efficient computational framework will augment existing experimental techniques by enablingresearchers to virtually prototype their swarm designs within realistic environments. To facilitate the useof this framework by others, I will open­source and thoroughly document all software I develop. In doingso, Ihopetohaveafar­reachingimpactonthefieldsofsoftcondensedmatter, robotics, andmicrofabrica­tion, whichcouldbenefitsignificantlyfromamodelfordenseparticulatesuspensions. Furthermore, Iwilldisseminatethisresearchtonon­engineersbyparticipatingintheAlliancesforGraduateEducationandtheProfessoriate’sChalkTalks,whichseektodistillcomplexscientificworksforgeneralaudiences.References:[1]Baczewski,A.D.,etal. 2010,JournalofComputationalPhysics,229[2]Bender,J.,etal.2014,Comput. Graph. Forum,33,246–270[3]Corona,E.,etal. 2017,JournalofComputationalPhysics,332, 504 [4] Xie, H., et al. 2019, Science Robotics, 4, eaav8006 [5] Yan, W., et al. 2019, The Journal ofChemicalPhysics,150,064109[6]Zhao,H.,etal. 2010,JournalofComputationalPhysics,229,3726[7]Zhou,H.,etal. 2021,ChemicalReviews,121,4999"
22.0,"Background: Consistent individual differences in behavior – or “personalities” – are ubiquitous in animalsand have long captivated biologists1,2. Individual differences are a prerequisite for natural selection, andmany evolutionary biologists explore how variation in behavior predicts survival and reproduction.Meanwhile, neuroendocrinologists experimentally alter an animal’s internal state to change behavior. Onlyrecently have these disciplines been integrated to begin evaluating the mechanisms that maintain naturalindividual differences in adaptive behaviors in wild animals3. So far, this work has advanced ourunderstanding of the neural mechanisms of social behavior, but remarkably, has found few patterns linkingbrain gene expression to individual behavioral differences4,5. This suggests that top-down processes aremissing key determinants of individual variation in behavior. Certainly, behavior requires more thanmotivation; it also requires that both brain and body are properly fueled. Thus, I propose that peripheralmetabolic processes may be the fundamental force driving consistent individual differences in behavior.The liver is the primary driver of metabolism and is under high demands topower the brain and muscle to execute energetically expensive behaviors (Figure).In times of endurance, the liver undergoes ketogenesis to secrete ketone bodies intothe blood for organs to use as energy6. This metabolic pathway is associated withbehavioral variation: Migrating birds use ketogenesis to maintain energyhomeostasis7, and racehorses have higher ketone levels during long-distancecompared to short-distance races8. Given supplemental ketones, bees behave moreaggressively9, and human athletes improve exercise efficiency relative to controls10.These observations suggest that variation in ketogenesis is critical for performingenergetically expensive behaviors, but this has never been assessed at the individual level. Consequently,there is uncertainty about how natural selection maintains animal personalities. I hypothesize that naturalindividual differences in behavior stem from variation in the ability to mobilize energy.I will test my hypothesis with two specific aims, focusing on social aggression in free-living femalebirds. First, I will assess how natural differences in aggression correlate with: (a) hepatic HMGCS2, therate-limiting enzyme in ketogenesis, and (b) beta-hydroxybutyrate (BHB), the main ketone body produced6.Second, I will manipulate circulating BHB and test effects on individual aggressiveness in a repeated-measures design. Both aims build off preliminary data I generated in my first year as a PhD student.Study system: Tree swallows (Tachycineta bicolor; TRES) are obligate secondary cavity-nesters; theycannot excavate a nesting site and must fiercely compete for a pre-made cavity to reproduce. Femalesreadily take to artificial cavities (i.e. nestboxes), and they are more aggressive than males. Social aggressionrequires endurance, as females engage in extended aerial chases and intense physical attacks duringcompetition for nestboxes. High aggression individuals have better body condition11 and are more likely tobreed than low aggression females, showing that aggression is adaptive12. Such strong natural selectionshould erode this trait variation, and yet substantial individual differences in aggression persist.Preliminary data: Last spring, I conducted 5-minute simulated territorial intrusions (STIs) on free-livingfemale TRES. I measured aggression (e.g. time spent hovering, diving, pecking) towards a conspecificdecoy placed at the nestbox. Individual aggression was repeatable in consecutive STIs (R=0.90; p<0.001).2-7 days after the last STI, I collected 10 high and 10 low aggression females and conducted a genome-wide analysis of their brains (i.e. RNAseq in hypothalamus and amygdala). Despite a well-powered design,I found very few differentially expressed genes between high and low aggression birds, indicating thatsubstantial behavioral variation cannot be explained by differences in baseline neural gene activity. Theseresults further support my hypothesis that behavioral differences emerge beyond the brain.AIM 1: To what degree do individual differences reflect variation in ketogenesis? From these samehigh and low aggression birds, I will extract RNA from the liver, where I confirmed HMGCS2 is highlyexpressed based on TRES transcriptomic data13. Using established lab protocols, I will design HMGCS2primers and perform qPCR to measure HMGCS2 gene expression, running samples in triplicate forHMGCS2 plus two endogenous control genes. I will also quantify BHB concentration in 10μl of bloodfrom these individuals, using test strips read by a handheld ketone meter that is already validated in wildbirds14. I will employ linear models to examine the degree to which natural variation in aggression ispredicted by HMGCS2 expression, BHB concentration, or a combination of the two.AIM 2: How does experimentally manipulating BHB alter individual aggressiveness? I will exposeincubating females to a commercially available BHB cream (BPI Keto Cream) applied to a fake egg in thenest for 12 hours. As the female incubates overnight, BHB will be absorbed via the brood patch, afeatherless area of vascularized skin on the belly. Control females will receive a fake egg with a vehiclecream. Past work has used this noninvasive approach to manipulate hormones in TRES15. Here, it will allowmanipulation of ketone levels independent of handling-induced stress. In a within-subjects design, I willuse 30-minute (prolonged) STIs to measure intensity and duration of aggression the morning before andafter BHB (or control) treatment, analyzing results with a repeated-measures ANOVA. Blood BHB will bequantified in both groups after the second STI using the ketone meter described in Aim 1. I will alsoseparately validate that BHB treatment elevates BHB blood concentration in a subset of birds.Predictions, Alternatives, & Next Steps: I predict that individual variation in aggression will positivelycorrelate with both HMGCS2 and BHB, with greater levels in high vs. low aggression individuals (Aim 1).Likewise, I predict that individuals will increase aggressiveness in response to supplemental BHB (Aim 2).Support for my hypothesis in Aim 1, but not Aim 2, would suggest that birds must engage in prolongedcompetition to promote ketogenesis, considering that females in Aim 1 were unprovoked at the time ofcollection. In this case, a future step would be to assess ketogenesis during sustained competition bymanipulating nestbox availability, which is shown to increase aggression and metabolically challenge thebrain16. I am also well-positioned to explore additional tissues from these same birds, such as the pectoralmuscle where BHB is converted into usable fuel6. Nearly all TRES fighting occurs in flight, suggesting thepectoral muscle is a promising tissue to connect energetic constraints to social behavior (Figure).Intellectual Merit: Individual differences serve as the raw material for evolutionary change, leading to thediversity of behaviors seen in nature. However, we have limited insight into the origin of this variation. Mywork explores the potentially critical role of peripheral energetics in shaping natural individual differencesin the wild. Recent work reveals other routes by which the periphery influences brain and behavior (e.g.gut-brain axis, microbiome), a view that my research extends. In the long-term, my proposal will not onlybuild the foundation of my dissertation, but it will also serve as a springboard for applying energeticperspectives more broadly, to understand how metabolism accounts for diverse behavioral differenceswithin and among species. Ultimately, my work will examine how both evolutionary and proximatemechanisms work together to build an aggressive female, an overlooked perspective in a field that, sinceDarwin, often assumes that females do not compete or that their aggression is just like that of males.Broader Impacts: As a first-generation, biracial graduate student, I strive to diversify STEM by helpinghistorically underrepresented undergraduates overcome institutionalized barriers, thereby demystifyingacademia’s hidden curriculum. My efforts include the creation of a “how to” guide for applying to graduateschool, which I disseminated to local and national groups. As a co-facilitator of an anti-racism group atIndiana University (IU), I developed action plans to hire and support diverse undergraduate researchers inmy lab. These efforts set the foundation for my goals as a graduate student and future faculty member toimprove recruitment and retention in STEM. Mentorship is central to this plan. I honed these skills withmentorship training while working with an undergraduate mentee in IU’s Center for the Integrative Studyof Animal Behavior NSF REU summer program. Moving forward, I will work with the Jim HollandSummer Enrichment Program, which provides research experience for high-achieving minority high schoolstudents and helps them transition into an IU STEM major, extending my efforts to broaden inclusion.References: 1Koolhaas et al. Front Neuroendocrinol (2010). 2Sih et al. TREE (2004). 3Hofmann et al. TREE(2014). 4Bell et al. Behaviour (2016). 5Benowitz et al. Behav Ecol (2019). 6Grabacka et al. IJMS (2016).7Frias-Soler et al. Biol Lett (2021). 8Volek et al. Metabolism (2016). 9Rittschof et al. J Exp Biol (2018).10Dearlove et al. Med Sci Sports Exerc (2021). 11Rosvall. J Avian Bio (2011). 12Rosvall. An Behav (2008).13Bentz et al. Sci Rep (2019). 14Sommers et al. J Field Ornithol (2017). 15Vitousek et al. Proc B (2018).16Bentz et al. PNAS (2021)."
23.0,"Aunderstand how neural circuits, assembled through geneticprograms, can give rise to complex behavior. Throughevolution, species display a wide range of behaviors, someof which have been mapped to specific genetic variations1.Genes that mediate complex behavior must act via neuralcircuits, yet little is known about these intermediate changes.In this proposal, I will bridge this knowledge-gap byinvestigating neural circuit differences that determineBvocal communication behaviors in two closely-relatedrodent species.Background and Rationale: Using sounds to communicateis widespread in nature — from croaking frogs, duettingbirds, to us, humans, engaged in conversation. Our lab hasrecently discovered a rodent species (Alston’s singing mice)FIG. 1: (A) Phenotypic variation inthat engages in similar fast vocal interactions. Singing micevocalizations of the lab mouse and Alston’sbreed in captivity, can be maintained in a colony, and showsinging mouse. (Spectrograms from thestereotyped vocal behaviors even in laboratory settings. Phelps lab, U.T. Austin) (B) Divergence andAdditionally, we have already established the use of viral tools duplication model as observed in cerebellarfor mapping, manipulating, and measuring neural circuits2. nuclei8.Singing mice (Scotinomys teguina) and lab mice (Mus musculus) are separated by a few millionyears of evolution (Steve Phelps lab, unpublished), are roughly the same size, and brain slices are largelyindistinguishable between the species. Yet, there are key differences in their vocal repertoires; Lab miceproduce only short, variable ultrasonic vocalizations (USVs), while S. teguina produce both USVs andhuman-audible ‘songs’ (Figure 1A). Crucially, unlike singing mice, lab mice do not participate in vocalturn-taking. Thus, we ask: What are the neural circuit differences underlying this behavioral distinction?Though traditionally thought to be unique to the primate lineage, our lab recently demonstratedrobust motor cortical control of vocal behavior in the singing mice. Using four complementary lines ofevidence (intracortical micro-stimulation, stimulation induced vocal arrest, focal cooling andpharmacological silencing), we defined a region of orofacial motor cortex (OMC) that mediates flexiblevocal behaviors in the singing mice2. In contrast, lab mice born without the entire cortex (including OMC)can still produce USVs3. Therefore, we predict that differences in the motor cortical circuitry between thelab mice and singing mice underlie differences in their vocal behaviors. We hypothesize that motorcortical control over vocalization in the singing mice evolved from the ancestral orofacial control neuralcircuits via a duplication of OMC followed by cell-type divergence (Figure 1B). This duplication-divergence model predicts the existence of a dedicated group of song-specific neurons in the singing mouseOMC with specific projection patterns to downstream vocal pattern generators in the midbrain and thebrainstem. Using novel spatial transcriptomics and barcoded projection mapping methods developed inTony Zador’s (my co-advisor) lab, I will determine the diversity of cell-types in the motor cortex and theirdownstream projection patterns in both the singing mice and the lab mice.Aim 1: Do motor cortical cell types differ between lab mice and singing mice? The duplication anddivergence model suggests that neural cell types in the OMC of singing mice evolved in a spatiallysegregated manner. First, to determine differences in cell types, I will perform single cell RNA sequencing(scRNAseq) in the OMC of lab and singing mice. Analysis of scRNAseq data requires aligning sequencedreads to a genome, publicly available for the lab mouse and recently generated by our collaborators for thesinging mouse (unpublished, Steve Phelps). Cell types will be identified using known marker genes foundin the literature. We will identify potentially novel cell types as those which have no assigned identitiesbased on canonical marker genes.While scRNAseq will allow us to quantify differences in neural cell types through in-depthtranscriptomics, we lose spatial information. To determine spatial location of neuronal cell types, we willuse a spatial transcriptomic method, BARseq, developed in the Zador lab4. This technique uses hybridizedprobes and in situ sequencing to determine spatially resolved expression data for hundreds of genes inparallel4. I am confident that I can perform this experiment as the Zador lab has a dedicated pipeline tocomplete this experiment and regularly performs spatial transcriptomic experiments.Aim 2: Do projection patterns of motorcortical neurons differ between lab andsinging mice? To determine OMCprojection patterns, I will first perform viraltracing experiments. I will inject AAV vectorthat expresses GFP into the OMC of bothspecies and image the brains using confocal FIG. 2: MAPseq protocol involves injecting barcodes into targetmicroscopy. While viral tracing can detect area and sequencing barcodes expressed in neural projections inbulk anatomical differences, this methoddownstream areas5.lacks accurate quantification of projections and cannot distinguish changes that occur on the single celllevel. To address these inadequacies, we will also be performing MAPseq, a method for single cell tracingdeveloped by the Zador lab. MAPseq is a method that uses virus to infect neurons with a DNA barcode thatis expressed in the cell body and axon of the neuron5,6. Through dissection and sequencing, we can recoverthe projection patterns of thousands of individual cells (Figure 2). I am confident that I can perform theseexperiments as I have already generated preliminary results for the lab mouse OMC. Furthermore, we cancombine MAPseq with our spatial transcriptomic method to correlate projection patterns and cell types4,7.Anticipated results: If the duplication-divergence model of the singing mouse OMC holds true, I wouldexpect to observe the following results: (1) novel cell types in the OMC of singing mouse (2) the spatiallocation of these novel cell types to be located in a spatially distinct area, and (3) novel projection patterns,perhaps to brainstem pattern generators, correlated with these novel cell types. In summary, theduplication-divergence model predicts correlated changes in cell transcriptomes and their projectionpatterns. Of course, another possibility is that cell type and projection pattern differences occurindependently. Even so, I will be able to distinguish independent changes due to the resolution of theoutlined experimental design. Thus, we have designed experiments that will produce results whether ornot our expected model (duplication and divergence) is true.Intellectual Merit: I anticipate three major contributions to neurobiological methods, as well as ourunderstanding of the evolution of neural circuits. First, this study can identify distinct neural populationsbased on projection patterns and/or genetic markers. Identifying neural populations in this manner allowsscientists to target these neural population for further functional validation and experimentation. Second,our results could identify genes underlying neural circuits in vocal communication, findings which couldcontribute to the development of better molecular tools for manipulating vocal circuits. Lastly, this studywould provide insight into the evolutionary underpinnings and biological basis of vocal communication.Broader Impact: I plan to make code and data available on open-source websites including GitHub.During my time at NIH, I created a RNAseq tutorial and shared resources on my GitHub page in additionto uploading code I wrote for analyzing RNAseq data9. I plan to maintain my GitHub page and upload codedeveloped for analyzing data collected through this project for other scientists to consult and use. Inaddition, I plan to create an online tutorial geared toward high school and/or college students that have littlecoding experience or exposure to bioinformatics. I also plan to publish our results in open access journalsincluding uploading early drafts of the manuscript to bioRxiv to facilitate timely advancement of scientificknowledge.References: [1] Metz et al. 2017. Current Biology. [2] Okobi, Banerjee et al. 2019. Science. [3]Hammerschmidt et al., 2015. Scientific Reports. [4] Chen et al., 2019. Cell. [5] Kebschull et al. 2016.Neuron. [6] Han, Kebschull, Campbell et al., 2018. Nature. [7] Sun, Chen et al., 2021. Nature Neuroscience.[8] Kebschull et al. 2020. Science. [9] https://github.com/eisko/RNAseq/"
24.0,"Background: With the growing implementation of inquiry-based labs in physics, students are nolonger following rote procedures and are expected to utilize complex experimental skills likemeasurement uncertainty, experimental modeling, and computation, which require students toengage in sensemaking [1]. We view sensemaking as “a dynamic process of building or revisingan explanation in order to ‘figure something out’—to ascertain the mechanism underlying aphenomenon in order to resolve a gap or inconsistency in one’s understanding” [2]. Given thatexisting research on sensemaking has focused on textbook problem solving, research is needed tounderstand how sensemaking appears in inquiry-based labs, given their increasing prevalence[3]. Physics is often viewed by students as a confusing, unapproachable subject; understandingstudent sensemaking is an important step in developing labs that are more accessible to a rangeof students, beyond physics majors.Preliminary Results: In response to the call for inquiry-based physics labs, as well as labs thatserve as better preparation for pre-medical and other life science students, the PER group at theUniversity of Utah has implemented Introductory Physics for Life Sciences (IPLS) labs, in whichstudents investigate physical mechanisms in the context of biological systems. In my preliminarywork with the group, I explored student sensemaking in IPLS labs, and I found that the instancesof sensemaking led to students having a deeper understanding about the relationship betweentheir data and the relevant physical systems. Furthermore, this initial analysis was instrumental indetermining an appropriate theoretical framework for the proposed research. First, I noticed thatstudents didn’t often fully articulate their thinking and thus my data was limited. To amelioratethis problem, I aim to use a think-aloud protocol in which students are encouraged to share alltheir thoughts during the lab investigations, which will provide a more complete data set.However, a limitation of interviews is that they are not as authentic, so I find it is important totriangulate interview data with the lab observation data. Second, I observed that students werefrequently comparing their model that was generated as a result of collecting and analyzing datato their existing mental model of the relevant system.Theoretical Framework and Research Plan: For my research plan, I draw on twocomplementary theoretical frameworks. First, I will use the modeling framework forexperimental physics, which was first developed for upper-division physics labs and functions ona recursive interaction between a student’s physical system model and their measurement model[4]. Given that measurement models are less common in introductory physics, I focus on a data-based model, which captures the focus of these labs where students are predominately analyzingdata; this is a similar adjustment to that which has been implemented elsewhere [5]. Second, Iwill utilize epistemic games, which are defined as the rules and strategies that guide inquiry; inPER, this framework has been used to study structured problem solving and knowledgedevelopment tasks [6-8]. Defining an epistemic game includes specifying the target epistemicform, constraints, entry conditions, moves within the game, and transfers to other games [9].Based on my initial analysis, student sensemaking in labs has characteristics that parallel theform of an epistemic game, e.g., making moves toward an end goal of resolving inconsistencies.These two frameworks are complementary as the recursive elements of modeling translate tomoves in a game. Each framework on its own has certain limits, but together they are morecomprehensive and powerful; they will allow for a rigorous analysis of student sensemaking ininquiry-based labs. The steps of my research plan are as follows:Step 1: I will identify all instances of sensemaking in the existing lab observation data, focusingon the classroom environment factors that contribute to the sensemaking.Step 2: Based on identified classroom environment factors from Step 1, I will write a task-basedinterview protocol intended to prompt sensemaking, and I will conduct these interviews with adifferent population of undergraduate life-science students.Step 3: I will first identify instances of sensemaking in the interview data. Then, using instancesof sensemaking from both the observational data and interview data, I will code my data using acoding scheme developed for the different parts of the modeling process, based on the modelingframework (e.g., revision of the data-based model, comparison between models).Step 4: With the modeling framework analysis done in Step 3, I will define sensemakingepistemic games that occur in the inquiry-based lab environment by coding the data with keyfeatures of epistemic games. Depending on the prior analysis, I will either define one epistemicgame that describes general sensemaking or a number of games that each describe a differentform of sensemaking.The merger of these two datasets, as well as the combination of the modeling framework and theepistemic games framework will allow for an in-depth understanding of student sensemaking ininquiry-based physics labs.Intellectual Merit: The inquiry-based physics labs are designed to better replicate genuineresearch environments, as well as encourage students’ agency and scientific inquiry; as such,they are increasingly prevalent in undergraduate curricula. But yet, there is limited research onsensemaking in these labs. To address this hole in the literature, I will take a novel approach ofcombining two disparate theoretical frameworks, epistemic games and modeling, and twocomplementary data sets, to gain a holistic understanding of sensemaking in inquiry-based labs.Results will be new knowledge about sensemaking in these increasingly prevalent labs that canserve as a foundation for future lab research, along with an example of how to effectivelycombine disparate theoretical frameworks that can serve as a model for other scholars in thefield.Broader Impact: Understanding student sensemaking in inquiry-based labs through theepistemic game framework will have direct instructional implications. For instance, we canimprove training so that TAs and instructors can recognize sensemaking epistemic games in labsand utilize them in instruction, asking questions that prompt sensemaking, rather thanrecollection of facts. Such instructional changes will improve undergraduate physics education,and specifically better prepare pre-medical students for medical school and other life sciencestudents for postgraduate studies and careers. Life science students often view physics as aforeign subject; supporting pre-medical students in productive sensemaking in these labs mayencourage more students to continue in these labs and be successful in long term careers in themedical field [10]. While typically unintentional, physics can be a “weed-out” course for thesestudents, so these changes intended to increase retention have the potential to improve equity andcontribute toward diversifying the medical field. Beyond the effect on pre-medical students,encouraging sensemaking is a step toward teaching students that they have relevant experiencethat can be used in a physics setting, making physics a more accessible subject for all.References: [1] Holmes, N. G. et al. Proc. Natl. Acad. Sci. (2015). [2] Odden, T. O. B. and Russ,R. S. Sci. Ed. (2018). [3] Odden, T. O. B. and Russ, R. S. Phys. Rev. Phys. Educ. Res. (2019). [4]Zwickl, B. M. et al. Phys. Rev. Phys. Educ. Res. (2015). [5] Vonk, M. et al. Phys. Rev. Phys.Educ. Res. (2017). [6] Tuminaro, J. and Redish, E. F. Phys. Rev. Phys. Educ. Res. (2007). [7] Hu,D. et al. Phys. Rev. Phys. Educ. Res. (2019). [8] Chen, Y. et al. Phys. Rev. Phys. Educ. Res.(2013). [9] Collins, A. and Ferguson, W. Edu. Psych. (1993). [10] Moore, K. et al. Am. J. Phys.(2014)."
25.0,"Introduction: Applications that demand large cooling capacities, such as high-powerelectronics, rely on the elevated thermal energy density that is associated with the latent heat ofvaporization. Flow boiling of a liquid in a channel is one of the most effective approaches forensuring high heat transfer efficiencies. For example, single-phase convection can remove heatwith a rate up to 20 kW/m²K, whereas flow boiling easily reaches 100 kW/m²K.1 Flow boilingwithin microchannels increases the heat transfer potential even further, due to the enhancedsurface-to-volume ratio. Many electronics and microelectromechanical systems (MEMS) withhigh power densities thus rely on microchannel flow boiling as their primary cooling strategy.Consequently, an efficient and reliable operation and control of this chaotic multi-phaseflow process is hence indispensable. In both macro- and microchannels, the two-phase mixturecan be characterized by different flow regimes, which are defined by the relative amounts andconfiguration of the liquid and vapor phases, and ranges from bubbly flow (few vapor bubbles inthe center) to slug flow, annular flow, and eventually mist flow (few liquid droplets in thecenter).Despite the great promise that two-phase flow boiling poses, it presents many challenges.For example, dry-out, or the critical heat flux (CHF), occurs at the onset of mist flow and leadsto a drastic rise in wall superheat that can induce material thermal fatigue and ultimately failure.Unfortunately, its occurrence and location there-of is difficult to predict, leading to theimplementation of large safety margins during the initial design of the thermal managementsystem. A different flow instability arises from the rapid expansion of bubbles, which causespressure oscillations that can ultimately jeopardize the structural integrity of the channel andeven initiate flow reversal.2 These instabilities have presented major challenges toward flowboiling applications and serve as a focal point for recent studies. To overcome the existinglimitations, I propose to use machine learning (ML) to detect and ultimately control theonset and nature of these flow instabilities. Autonomous detection and self-stabilization of flowboiling instabilities would significantly enhance the reliability of two-phase cooling systems,while reducing costs and greenhouse emissions.Intellectual Merit: The use of ML in thermal management is a novel method for predicting heattransfer properties. A recent publication by Ravichandran et al.3 has shown that neural networkmodels can predict the margin to the boiling crisis in a pool setting. I propose that in flowboiling, the onset detection and solution to mitigating flow instabilities can similarly beaccomplished through the implementation of a deep learning algorithm.Aim 1: Collect images and videos of various flow instabilities for different flow conditions.ML requires the collection of large amounts of data to train an algorithm. I will construct anapparatus for recording various instabilities. Korniliou et al.4 describes a method formicrochannel fabrication onto a polydimethylsiloxane (PDMS) substrate with an infraredtransparent tin indium oxide back wall. I will implement IR imaging to record wall surfacetemperatures while simultaneously recording high speed optical imagery at identical frame rates.The setup will include a pressure transducer mounted at both ends of the channel to monitordifferential oscillations. A micropump will serve to circulate the fluid after it passes through avacuum degasser. The ratio of heat flux to mass flux has been used to quantify microscaleinstabilities by defining their oscillation periods.5 I plan to induce various instabilities within themicrochannel by adjusting flow rate and heat generation while recording the resultant effects ontemperature and pressure. Each instability will be categorized into an appropriate set of resultantconditions. In addition to recording my own footage, I will also use data available in openliterature and contact authors of recent publications to share their data.Aim 2: Train a machine learning algorithm to predict each type of instability based on pre-defined conditions. I propose to train an artificial neural network (ANN) model by inputtingmeasurements of pressure oscillations, nucleation site density, surface temperature changes, andbubble movement into each instability category. A feature ranking algorithm will beimplemented to define the key measurement parameters. I will incorporate Google’s machinelearning library TensorFlow to accomplish these tasks. The data will be split appropriatelyamong sets for training, validating, and testing the model. Model evaluation will be conductedthrough 10-fold cross validation to ensure proper fitting. I will utilize the model’s mean absolutepercentage error as the primary evaluation metric. The results of the model will be used todetermine the corresponding flow, thermal, and differential pressure conditions associated withspecific instability types.Broader Impacts: This project serves as a vital proof-of-concept for validating the use ofmachine learning to detect flow instabilities. Time permitting, I plan to program the model into aclosed-loop control algorithm. Upon the detection of a specific instability, the algorithm wouldcontain and implement a pre-programmed solution, such as the adjustment of the flow rate orpulsation of ultrasonic waves. This solution will serve as a pioneering step toward theimplementation of machine learning in thermal management and demonstrates a potentialmethod for revolutionizing the peak performance and safety of electronics coolingapplications. The development of a working control algorithm would allow for systemoptimization and self-stabilization, which would answer direct needs of the thermal fluidscommunity.During our STEM club sessions for EduMate NYC, as introduced in my personalstatement, we received strong positive feedback from the students during the computer sciencepresentation. I therefore intend to use this project to initiate an Introduction to ArtificialIntelligence virtual workshop for high school and middle school students in the St. Louisarea. The St. Louis metro region is one of the most segregated cities in the United States, and itslong history of racial disparity contributes to educational inequity to this day. I will discuss thefundamentals of artificial intelligence, including machine learning, natural language processing,and computer vision in weekly sessions throughout 5 weeks in the summer months. Anadditional 5 weeks will be spent discussing the applications of artificial intelligence, such asrobotics, transportation, and my proposed project. The goal behind these events is to create afoundational understanding of artificial intelligence while inspiring students to pursue thesetopics in their future studies and careers. Allowing young innovators to develop skills in oneof the most important technologies today is valuable for contributing to their futuresuccess. I will collaborate with the St. Louis Academy of Science to host these events andprovide outreach assistance.References: [1] Bergman, et al. (2011) Fundamentals of Heat and Mass Transfer [2] O’Neill, et al.(2020) International Journal of Heat and Mass Transfer [3] Ravichandran, et al. (2021) Appl. Phys. Lett.[4] Korniliou, et al. (2017) Applied Thermal Engineering [5] Prajapati, et al. (2017) ExperimentalThermal and Fluid Science"
26.0,"Hypothesis: The compositional evolution of crustal material (both felsic and mafic) following impactevents can be simulated by heating and partially vaporizing such materials at high temperatures incontainerless experiments. Varying the temperature (T), time (t), and oxygen fugacity (fO ) in the2experiments enables the creation of synthetic impact glass analogues. Comparing the textures andcompositions of these analogues to those of natural glasses will inform how these materials evolve duringthe tektite formation process. Incorporating the results of partial vaporization experiments into mixingmodels of tektite formation will result in more realistic predictions of tektite parent materials and potentiallyexplain why tektites do not form from mafic protoliths on Earth.Background: There are currently 190 confirmed craters on Earth’s surface1, only four of which are inbasaltic targets. Lonar crater, which formed in Deccan Trap basalt, is the only well-known and fullyaccessible of these. During impact events, shocked and melted material may be ejected from the site ofimpact. Material that is thrown more than 2.5 crater diameters away from the impact location is known asdistal ejecta2. Glassy, chemically homogenous, and aerodynamically shaped distal ejecta are known astektites. Only four of the 190 terrestrial craters have resulted in tektite distribution over wide geographicregions known as strewn fields, and all known tektites originate from target rock that is felsic,approximating rhyolitic compositions. This research will investigate why tektites have never formed frommafic (basaltic) target rock during a terrestrial impact event, and to seek to better understand the nature andevolution of the starting materials that formed tektites from the four major strewn fields. Existing mixingmodels3–6 assume compositions of tektites represent idealized end-member mixtures of the melted targetmaterial, and most attempts to identify the parent materials involve general comparison among chemicalcomponents of tektites and predicted parent materials5. These models are overly simplistic as they do notaccount for the loss of chemical constituents to volatilization at high temperatures in the impact plume.Research Plan: (Research Goal 1: Tektite experiments) I will synthesize glasses that replicate thegeochemistry and textures of tektites and Lonar crater impact glass in an aerodynamic levitation laserfurnace7 (ALLF). Melting/vaporization experiments will vary T, t, and fO to identify conditions that form2Lonar glass analogues (Table 1).Table 1. Experimental conditions based on previous experiments estimating tektite thermal histories7Series 1 2 3 4 5 6 7 8 9T(°C) 1800 1800 1800 2000 2000 2000 2200 2200 2200t (s) 10-120 10-120 10-120 10-120 10-120 10-120 10-120 10-120 10-120fO Ar O CO+CO Ar O CO+CO Ar O CO+CO2 2 2 2 2 2 2Starting materials are Deccan Trap basalt, USGS basalt (BCR-2), and USGS rhyolite (RGM-2).Experimental methods will follow previous ALLF tektite experiments7. Approximately 10 mg of thestarting material is heated with a CO laser at low power to fuse the sample and form spheres suitable for2levitation. The fused spheres are then levitated on a flow of gas (Ar, O , or CO+CO ) while being heated2 2with the laser for seconds to minutes. After heating to the desired T-t, laser power is cut, and the meltspheres cool quickly to form glass. The chemical, mineralogical, and textural characterization ofsynthesized and natural glasses will be completed via scanning electron microscopy with energy dispersivex-ray spectrometry (SEM/EDS) at Indiana University–Purdue University Indianapolis (IUPUI) and electronprobe microanalysis (EPMA) at Washington University in St. Louis (WUSTL). These analyses will becompared to electron microprobe analyses of Lonar glass8, and previous work on felsic material7.Exploratory experiments using the starting materials and methods described above resulted in darkcolored, glassy spheres with little to no vesicles or mineral growth. SEM/EDS analyses of these preliminaryexperiments show preferential loss of Na and K from all melt compositions relative to non-alkalicomponents. Further, heating basalts in an oxygen-rich environment results in more rapid and completeloss of K O (undetectable after 10 s at 2000 °C).2(Research Goal 2: Mixing model calculations) Tektites are not produced from a single, homogeneoussource rock – thus, any explanation of tektite generation must involve a multi-component mixing modelinvolving likely upper crustal target rocks (sediments) as the major components9. Previous studies haveestimated the contributions of possible target lithologies to final tektite geochemistry using mixing models.However, the assumptions and limitations of existing models may hinder their ability to realisticallyrepresent tektite formation. Previous mixing models assume that end-member compositions of likelyprotoliths are unchanged during tektite formation, i.e., they do not take into account the effects ofevaporative fractionation3 on the target materials as they are heated in the impact plume. I will address thisby executing computationally intensive mixing models that properly account for fractionation due tovaporization and assess the extent of agreement between assumed target lithologies and tektitegeochemistry. I will incorporate my experimentally derived volatilization rates into dynamic multi-component mixing calculations by modifying the GeoChemical Data toolkit (GCDkit) software to create amore realistic representation of the processes attending tektite formation, and to determine the relativecontributions of each protolith to different tektite compositions. I will also compare my experimentallyderived volatilization rates, and the results of my mixing models with predictions of compositionalevolution of tektite melts from MAGMA code.My ongoing research uses the R programming language to perform multivariate analyses andimplement machine learning algorithms to investigate tektite compositional trends. I am creating an open-source web-based application to classify unknown tektites into their strewn fields and subgroups withinstrewn fields. This NSF Graduate Research Fellowship will afford me the opportunity to expand myprogramming and modeling portfolio to more computationally intensive applications.Intellectual Merit: The modification of planetary surfaces through impact cratering is the most importantsurface modifying process on most rocky bodies in the Solar System10, yet it remains underrepresented asa field of study. My research will improve both the understanding of the evolution of Earth’s crust and themodeling of impact events. My mixing model calculations will be the first to properly account for thechemical modifications that occur in the impact plume. These calculations will produce a useful tool forthe scientific community to analyze (or reanalyze) terrestrial impact products and target lithologies. Myexperimental data will also provide valuable insight into the volatilization behavior of felsic and maficmelts at conditions relevant to impact plumes. The results may shed light on the absence of mafic tektites.Broader Impacts: The results of my research will have applications in a diverse set of fields such as high-temperature geochemistry, computational modeling of geochemical processes, and impact processes. I willdisseminate the results in journals (e.g., Computers & Geosciences), at conferences (e.g., AGU, GSA), andto GK-12 students and the general public at outreach events (e.g., Pacers STEM Fest, Celebrate ScienceIndiana). As an NSF Graduate Fellow, I will continue working towards a more equitable and inclusiveculture. Already as a new graduate student I became a founding member of IUPUI’s Geology COmmunityfor Racial Equity (GeoCORE). I also came up with the idea for the Mineral Eponym CrowdsourcingInitiative (MECI), an effort to facilitate an examination, analysis, and synthesis of the origins of mineralnames, and the messages this historic naming system has created. This initiative is currently being led bymembers of the Mineralogical Society of America (MSA) Diversity Task Force (my advisor is a member)to seek funding for its execution. Most recently, I have been invited to participate in a project to create newcurricular materials for the IUPUI Earth Sciences Department focusing on issues at the intersection of Earthsciences, ethics, and equity. My role in this project will be to analyze, quantitatively and qualitatively, theassessment results of teaching material effectiveness. The analytical and computational skills I will learnas an NSF Fellow will allow me to perform analyses that will directly impact the inclusivity, diversity,engagement, and retention of underrepresented groups within the geosciences.References: [1] Earth Impact Database, PASSC [2] Glass, B. P. et al. Elements 8, 43–48 (2012) [3]Ackerman, L. et al. Geochim et Cosmochim Ac 276, 135–150 (2020) [4] Ferrière, L. et al. Chem Geol 275,254–261 (2010) [5] Love, K. M. et al. 52, 2085–2090 (1988) [6] Meisel, T. et al. Meteorit Planet Sci 32,493–502 (1997) [7] Macris, C. A. et al. Geochim et Cosmochim Ac 241, 69–94 (2018) [8] Ray, D. et al.Earth Moon Planets 114, 59–86 (2014) [9] Koeberl, C. Tectonophysics 171, 405–422 (1990) [10] Koeberl,C. in Treatise on Geochemistry 73–118 (Elsevier, 2014)"
27.0,"Background: Martian ice likely holds the key to interpreting Mars’ past climate, but much is stillunknown regarding the distribution and properties of Mars’ icedeposits. It is well known that Mars has extensive polar ice capsthe size of Greenland. Included in these large polar caps are thenorth and south polar layered deposits (NPLD and SPLD,respectively), that are comprised of kilometers-thick deposits ofwater ice. In addition, surveys by Conway et al. (2012) and Soriet al. (2019) have identified craters in the surrounding terrainswhich contain “outlying” deposits of ice (Figure 1), which mayor may not have formed at the same time as the deposition ofthe polar caps. These, as well as other efforts to identify andcharacterize ice, have used radargrams from the SHARAD(SHAllow RADar) sounding radar onboard NASA’s MarsReconnaissance Orbiter (MRO) to analyze the subsurface inthese icy areas, as radar images can essentially act as large-scaleice cores. However, additional data such as roughness andFigure 1. From Sori et al. (2019)dielectric constant of the shallow (<5 m) subsurface can beLocations of icy crater deposits nearextracted from these radargrams by analyzing the reflectivity ofthe southern polar cap. All threethe surface echo (Campbell et al., 2013; Castaldo et al., 2017;colors of points indicate outlyingGrima et al., 2012).crater deposits that will be consideredResearch Objectives and Motivation: I propose toin this work.leverage this technique to analyze the physical properties ofnorthern and southern outlying polar ice deposits. From my analyses, I will be able to draw conclusionsabout the purity, composition, and surface roughness of these ice deposits. Comparing similarities anddifferences between the ice deposits in the two hemispheres, as well as how the outlier deposits compare totheir corresponding nearby polar ice caps, will allow me to assess how localized or global the climateprocesses were which led to the formation of polar ice deposits. In addition, I will analyze the subsurfaceradar echoes of the southern icy outliers and search for the existence of any buried CO deposits that may2have been sequestered from the atmosphere in past climate events (Figure 2).The main objective of this project is to identify differences in physical properties between southernand northern icy outlier deposits. Conway et al. (2012) found evidence that supports similarities incomposition between the NPLD and outlying ice deposits located in northern craters. While the NPLD hasbeen found to be made up of pure water ice (Grima et al., 2009), large deposits of CO ice have been found2sequestered in the SPLD (Phillips et al., 2011). Given the similarity in composition between the NPLD andnorthern outlying ice deposits, it stands to reason that such a similarity may exist between the SPLD andcorresponding southern outlying ice deposits. This motivates my search for sequestered deposits of CO ice2within the southern outlying ice deposits. If such CO deposits are found, it would highlight a major2difference between the northern and southern outlier ice deposits, and would also place new constraints onthe thickness of the atmosphere in Mars’ past. CO sequestered in the ice would have once been in the2atmosphere, which would have a significant impact on the Martian atmosphere and climate system. Forexample, the amount of CO that has been found in the SPLD alone is enough to have doubled the2atmospheric pressure on Mars pre-sequestration (Phillips et al., 2011). Even if sequestered CO deposits2are not found in these southern outliers, other differences in composition, purity, and/or surface roughnesscould provide exciting insights on the icy processes at work in each hemisphere.Methods: To determine if a difference in physical properties of ice in the northern and southernoutlying crater deposits exits, I will examine the ice at two different spatial and temporal scales. First, I willuse the reflectivity of the primary surface echo to infer the roughness and dielectric constant of the youngersurface ice. Similar techniques have been used to make global maps of SHARAD parameters (Campbell etal., 2013; Castaldo et al., 2017; Grima et al., 2012), but have not been used to study these outlying icy craterdeposits. This will allow me to compare the present-day conditions of ice in the northern and southernhemispheres of Mars. I will also leverage the subsurface capabilities of SHARAD to analyze older layersof ice below the surface in these outliers, with the additional goal of determining the existence ofsequestered CO similar to that which Phillips et al. (2011) found within the SPLD. By analyzing the2properties of both subsurface and surface reflectors, I will be able to make comparisons of physicalproperties of northern and southern icy outliers as well as the climate conditions that could have formedthem.Figure 2. Example SHARAD radargramfrom Phillips et al. (2011). Reflection-free subsurface zones (“RFZ”) werefound to be pure CO2 deposits.Intellectual Merit: Such a study of the outlying ice deposits has not yet been done. The Martianice acts as a record of the climate in which it formed, so understanding the ice in both hemispheres isnecessary in order to be able to answer the questions of what conditions were present in Mars’ past. I expectthat this study will provide new insights on the similarities and differences in composition, purity, andsurface conditions of ice in the northern and southern hemispheres. Analyzing these properties can tell ushow the ice was deposited; for example, very pure ice would be indicative of a deposition via snowfall,while low ice contents generally imply formation due to condensation of atmospheric water vapor withinpore spaces of the regolith. Finding layers of ice with different properties would also provide strongevidence for changes in the climate, which could be linked to orbital/rotational variations analogous toMilankovitch cycles on Earth. This work may also confirm the presence or absence of sequestered CO2deposits in the southern outlying ice deposits, which if found would place new constraints on the thicknessof Mars’ atmosphere pre-sequestration of the CO .2Purdue University is an ideal institution at which to carry out this research. Here, I am advised byProf. Ali Bramson, who has extensive experience working with SHARAD observations of Martian ice (e.g.,Bramson et al., 2015) and is a Co-I on the NASA-funded Subsurface Water Ice Mapping (SWIM) project,which uses multiple types of observations (including radar) to map subsurface ice through the mid-latitudesof Mars. I will also be working with Prof. Mike Sori (also at Purdue University), using his database ofsouthern outlying ice deposits (Sori et al., 2019) as the set of southern deposits I will be characterizing.Broader Impacts: Indiana is home to the newest of our national parks: Indiana Dunes NationalPark (INDU). Using my experience as an Astronomy Ranger at Bryce Canyon National Park, I will workwith the chief rangers of interpretation and education and INDU, Bruce Rowe and Kim Swift, to develop anight sky educational program that connects what we see in the sky to geology here on Earth. Though INDUis near the light pollution of Chicago, major planetary bodies are still visible. This night sky program willbe held outdoors on the park’s West Beach weekly during the main summer season (April–November) andfocus on water and sand dunes throughout the solar system that are also present in the park. This park isuniquely located near major metropolitan areas, and as such these programs will serve communities thatare traditionally underserved by science outreach. More details on this planned project can be found in myPersonal Statement.References:Bramson, A. M., Byrne, S., Putzig, N. E., et al. (2015). Geophys. Res. Lett., 42(16), 6566–6574.Campbell, B. A., Putzig, N. E., Carter, L. M., et al. (2013). JGR: Planets, 118, 436–450.Castaldo, L., Mège, D., Gurgurewicz, J., Orosei, R., & Alberti, G. (2017). EPSL, 462, 55–65.Conway, S. J., Hovius, N., Barnie, T., et al. (2012). Icarus, 220(1), 174–193.Grima, C., Kofman, W., Mouginot, J., et al. (2009). Geophys. Res. Lett., 36(3), 2–5.Grima, C., Kofman, W., Herique, A., et al. (2012). Icarus, 220(1), 84–99.Phillips, R. J., Davis, B. J., Tanaka, K. L., et al. (2011). Science, 332, 838–841.Sori, M. M., Bapst, J., Becerra, P., & Byrne, S. (2019). JGR: Planets, 1–21."
28.0,"The sparse and heterogeneous distribution of water in savanna landscapes largely determines herbivoredistributions. Thousands of animals gather at watering holes and riverbanks, which become foci for bothcompetition and predation. Species with relatively low water needs can take advantage of large, naturallyoccurring gaps (“refugia”) between persistent water sources to avoid competition and predation; in fact,these refugia can be critical to maintaining their populations1. Kruger National Park (“Kruger”), SouthAfrica, offers a unique long-term experiment of the effects of surface water augmentation on herbivoredistributions. Kruger was fenced in the early 1960s, obstructing historic migrations of grazers to dry-season water sources. In response, management installed hundreds of watering holes (“boreholes”), butthese fragmented the dry refugia, allowing the ranges of drought-intolerant species (i.e. Equus quagga,Connochaetes taurinus) to expand into those of drought-tolerant, locally rare antelope1,2 (RA) (i.e.Taurotragus oryx, Hippotragus equinus, H. niger). This attracted predator attention to refugia and createdmore competition for forage during the severe droughts of 1981, 1985, and 19923. Surface water alsoattracts elephants, which can drastically alter the surrounding ecosystem and impact forage biodiversity4.This increased predation, competition, and ecosystem engineering all contributed to the alarmingpopulation declines in RA in the 1980s3. Realizing this, in 1997 Kruger’s management scheme was re-examined, and two-thirds of boreholes were closed4. The current conditions for Kruger’s RA are stillfraught, however. Borehole closures have not returned refugia to their original extents4; many remainopen for their touristic value (e.g. guaranteed presence of diverse species). In addition, climate modelsproject that southern Africa will experience more frequent and intense droughts5, especially threateningKruger’s drier northern areas, where most of the park’s RA refugia are located6.Motivation: Water access drives complex savanna dynamics. Lingering hydro-homogeneity inKruger, along with projections indicating more frequent and intense droughts, make it imperative tounderstand how this community is structured, how species interactions shape responses to climate change,and how these might change in the future. I will determine (1) how surface water sources and resultinginterspecific interactions affect how large herbivores respond to climate variables, and (2) how thischanging community structure impacts resilience to extreme environmental events. I will also create atool for Kruger park managers to infuse sharper knowledge of ecosystem structure into management aims.Methods: I will analyze herbivores’ interactions and responses using GJAMtime, a Bayesian time-series ‘generalized joint attribute model’ developed by Dr. Jim Clark7. GJAMtime models speciesdistributions through environmental and interspecies interactions, improving upon traditional staticspecies distribution models that fail to address interactions between species and temporal dependence onprevious conditions. GJAMtime builds species migration, density-independent (DI) growth, and density-dependent (DD) interaction terms into a familiar Lotka-Volterra model and generates a species-interactioncoefficient matrix (“a-matrix”), enabling dynamic community structure analysis8. In preparation, I havedivided a map of the park into 710 30km2 grid squares, to which I either aggregated (borehole locations,fires, geology, climate variables) or interpolated (rainfall, grass abundance) annual environmentalcovariates. I determined Bayesian priors for species interactions and responses to covariates throughreview of savanna literature and personal communication with established savanna scientists. Aerialannual census data are rich; from 1977-98, park rangers counted all animals in parkwide airplane census;from 1998 onwards, 800-m wide transects replaced the expensive full-park census (providing 15-28%coverage instead)9. For transects, data on individual animals’ distance-from-aircraft were also collected.With knowledge of each species’ detection rate (a factor of coloring, preferred habitat, and size), I willderive species detection functions using the R package ‘DSim’ to estimate true herbivore abundances.Research Plan: This project extends my exploratory analyses, detailed in my personal statement, toinclude borehole and fire data, larger temporal extents, narrower hypotheses, and novel statistical tools.(1) How does surface water affect community structure, and how can this in turn limit RA populationdensity? Hypothesis: If greater hydro-homogenization permits species invasion of refugia, then negativeimpacts on RA will be seen through (1) decreased populations and (2) negative a-matrix speciesinteraction coefficients. Equilibrium population sizes are a function of migration, DI, and DD growth; Iwill compare equilibrium abundances and a-matrix coefficients from greater and lesser boreholeconcentration years to determine whether these boreholes negatively affect RA populations. I willcompare pre-1997 data (high borehole density) to post-1997 data, the latter proxying a time with fewboreholes. I will extract how much of each species’ climate responses are coming from migration, DI, andDD growth using GJAMtime estimates. If results do not support my hypothesis (negative effect ofboreholes on RA population density), then this indicates the influence of other sources on RA populationdecline, including an increase in elephant disturbance after culling was ended in 1992; anthrax outbreaksin roan antelope in the early 1980s; or changes in fire management, extent, and intensity over time.(2) How does changing community structure affect the savanna ecosystem’s resilience to extremeevents? Hypothesis: If these changing community structures reduce ecosystem resilience, then increasinghydro-homogeneity will (1) increase drought recovery time, (2) increase fire recovery time, and (3)increased community instability. Major droughts occurred in Kruger in 1981-82, 1985-86, 1991-93, 2014-16, and 2018-20. These droughts straddle the borehole closures of the 1990s. Natural and prescribed fires,with varying intensities and times between burns, also occur regularly across the savanna. For intenselocal fires and parkwide drought events, I will compare each species’ population the year before the eventto its population in subsequent years to determine population recovery time. I will compare these trends toa stability analysis of the a-matrix (with negative eigenvalues indicating stability) for these periods to seeif community structure and species interactions are impairing ecosystem recovery after extreme events. Ifwe do not see evidence for this, then the influence of management decisions (e.g. removal of fencing, endof elephant culling) or climatic interactions (e.g. fires during drought years) played a larger role thanspecies interactions, setting the groundwork for future studies in community stability and resilience.Resources: The Kruger GIS Lab provided all data, and South Africa National Parks registered myproject this fall. In March 2021, I will share my research at Kruger’s Savanna Science Network Meeting,discussing my assumptions and findings with other savanna scientists. I will also travel to Kruger in 2021and 2022 to participate in dry season censuses and attend courses on savanna ecosystems. My mathdegree and two years’ coding experience at IBM prepared me well for the analysis of the complex modelsproposed in my study. I am advised by statistical and climate-change ecology expert Dr. Jim Clark andKruger grassland ecologists Dr. Steve Higgins (University of Bayreuth) and Dr. Carla Staver (Yale).Intellectual Merit: This analysis is crucial to maintaining one of the world’s last Pleistocenemegafauna savanna ecosystems. My novel statistical analysis will illuminate the relationship betweenspecies interactions and community environmental responses, a connection understudied in modernecological literature. I also propose to determine how a changing community structure affects anecosystem’s resilience to climate change, a pressing issue to today’s ecologists. Finally, my analysis ofKruger’s extensive, long-term datasets with GJAMtime’s Bayesian Gibbs-sampling techniques certainlymeet the 2020 GRFP solicitation goal of supporting “computationally intensive research”.Broader Impacts: Adaptive park management requires use of near real-time animal census andclimate data. I will provide park managers with a data workflow to feed each year’s new census data intomy GJAMtime model. I will (1) convert GJAMtime’s outputs to be readable by non-statisticians (2) writechange detection functions to highlight how herbivore responses vary from previous years, (3) convert thecode from steps 1 and 2 into an open-source user interface in RShiny to allow park managers to run codewithout prior programming skills, (4) solicit regular feedback from park management, and (5) provideiterative, ongoing support and tool improvement through GitHub. These five steps provide a link betweenthe environment-species interactions model and a more user-friendly interface for management decision-making. Additionally, as described in my personal statement, I will use my African savanna conservationresearch to develop open-source RShiny modules, based on Kruger herbivore and environmental data, toengage high schoolers in Durham Public Schools to boost their skills and confidence in math and coding.[1] CC Grant et al. 2002. Koedoe 45. [2] IPJ Smit 2011. Ecog. 34. [3] MP Veldhuis et al 2019. Ecol. Lett.22. [4] IPJ Smit 2013. Pachyderm 15. [5] Working Group II, Fifth Assessment Report of the Inter-governmental Panel on Climate Change, 2014. In: Impacts, Adaptation, and Vulnerability, Part B:Regional Aspects. [6] JO Ogutu & N Owen-Smith 2003. Ecol. Lett. 6. [7] JS Clark et al. 2017. Ecol.Monogr. 87. [8] JS Clark, et al. 2020. PNAS, 117. [9] JM Kruger et al 2008. Wildl. Res. 35."
29.0,"I propose to develop and implement a new approach to quantum-light spectroscopy. I will benchmark thenew approach against established classical-light techniques, namely nonlinear coherent spectroscopy, anduse classical- and quantum-light spectroscopies to investigate the mechanism of circularly polarizedphotoluminescence from chiral perovskite thin films.Introduction: Quantum light has been the subject of research in many physics subdisciplines, but untilrecently has not been considered as a tool for molecular spectroscopy by physical chemists. Much of thecurrent research on spectroscopy using quantum light has focused on using entangled photon pairs (EPPs)for nonlinear spectroscopies involving two-photon absorption1, effectively using quantum light as an analogfor classical light in nonlinear applications. I propose to use an established technique in the field of quantumphotonics – quantum-state tomography – in a spectroscopic application that treats both the quantumlight and the light-matter interaction fully quantum mechanically.The advantage of the proposed quantum-light spectroscopy over known nonlinear ultrafastspectroscopy techniques is the ability to directly investigate the quantum mechanical nature of a materialsystem and its dynamics. By measuring changes in the state of quantum light due to light-matterinteractions, we can learn about phenomena such as entanglement of correlated species in matter and spin-orbit coupling. Chiral perovskite thin films are an exciting and important material system to investigatewith this spectroscopy. Recent studies have shown that chiral perovskites act as sources for circularlypolarized photoluminescence (CPL), a highly sought-after feature applicable to bioresponse imaging, 3D-LED displays, quantum computing, spintronics, and more.2 An investigation of chiral perovskite thin filmsby Di Nuzzo, et al. has shown that the unknown mechanism of CPL in Ruddlesden-Popper perovskite thinfilms does not agree with the model of Rashba spin-orbit coupling,3 the primary model used to describespin-orbit coupling in 2D-semiconductors. I propose to use quantum-light spectroscopy, as well as two-dimensional photoluminescence (2DPL) and photo-induced absorption (2DPIA) spectroscopies, to explorethe claim that the CPL of chiral perovskite thin films is due to the charge of photoexcited Wannierexcitons and study the chiral symmetry transfer from optically inactive cations to excitons.Background: Quantum-state tomography is a technique used to completely characterize a quantum stateby measuring its density matrix. Photonic-state tomography involves the measurement of an ensemble ofpolarization-entangled photon pairs to determine the full two-photon density matrix4. I propose to use thespectroscopic setup in Figure 1, in which EPPs aregenerated in a polarization basis via spontaneousparametric down conversion (SPDC) using a pairof BiBO crystals. The two photons, the signal andthe idler, are spatially separated along the edgesof the SPDC emission cone and sent down distinctoptical paths. The idler photon is sent directly to a Figure 1. Proposed quantum-light spectroscopy setuppolarimeter composed of a quarter wave plate (QWP), a half wave plate (HWP), and a polarizing beamsplitter. The signal photon interacts with a sample before being sent to an identical polarimeter. Thepolarimeters project both photons onto a polarization basis defined by horizontal (H), vertical (V), diagonal(D), and right (R) polarizations. Finally, the photons are detected by single photon detectors that drive acoincidence counter. By determining the change in the full density matrix of the biphoton state, we canidentify the transformation matrix associated with the light-matter interaction in the polarization basis.Previous Work: The experimental set-up in Figure 1 has been realized in the Silva lab and validated bytaking tomographic measurements of several prepared biphoton states of the form andHH VVc |HH〉+c |VV〉with a quarter waveplate (QWP) in the place of a sample. In the defined polarization basis, a QWP performsa unitary transformation defined by a 90° rotation of the Poincaré sphere about an axis dependent on thewaveplate angle. Measurements taken with our experimental setup of a QWP at 0°, 22.5°, and 45° indicatesuch a transformation, corresponding to a rotation without loss of purity in the biphoton state.The proposed experimental set up is also easily combined with other spectroscopic techniquesfamiliar to the Silva lab, such as pump-probe spectroscopy. Preliminary data taken with a similarexperimental set up in which the sample is excited by a pump laser has shown that the quantum state of theprobe biphoton pair is altered by scattering off of a triplet-triplet intermediate state involved in singletfission. Through experimental measurements of bis(triisopropylsilylethynyl)tetracene (TIPS-tetracene)samples obtained through collaboration with Prof. John Anthony at the University of Kentucky andtheoretical development done in collaboration with Prof. Eric Bittner at the University of Houston, we havestudied the nature of the long-lived correlated triplet pair of TIPS-tetracene, which has been assumed to beentangled in the spin basis without experimental evidence.5 The experimental data, along with simulationsof the TIPS-tetracene system and the many-body scattering theory of the biexciton probe have allcontributed to a manuscript to be submitted to the Journal of Physical Chemistry C.Aim I – Theoretical comparison of quantum-light and 2D spectroscopies: The implementation ofquantum-light spectroscopy is nontrivial; working in the photon counting regime and with ensemblemeasurements requires precise environmental control and careful error analysis. It is necessary to show theproposed quantum-light spectroscopy will yield information that is inaccessible with known spectroscopictechniques and worth the challenge. I intend to do this theoretically using Lindbladian models to predictthe data that can (and cannot) be obtained for chiral perovskite systems comparatively with 2D andquantum-light spectroscopies as shown in our paper published on ArXiv6 for TIPS-tetracene.Aim II – Experimental investigation of CPL of chiral perovskite thin films: In concert with theproposed theoretical development, I intend to interrogate the mechanism by which chiral Ruddlesden-Popper perovskite thin films impart circular polarization on unpolarized incident light using 2DPL, 2DPIA,as well as the proposed quantum-light spectroscopy. I plan to synthesize the thin films with the help ofEsteban Rojas-Gatjens, a groupmate co-advised by Prof. Seth Marder. To my knowledge, these thin filmshave not been characterized with any 2D-spectroscopy which could probe their exciton dynamics.Intellectual Merit: I am ideally positioned to do this research. As part of the Silva group, I have access toa full toolbox of 2D- and ultrafast spectroscopic techniques which are regularly used by our group toinvestigate perovskites. Moreover, the project will benefit from continued collaboration with Prof. EricBittner and future collaboration with Andrei Piryatinski at the Center for Nonlinear Studies at Los AlamosNational Laboratory. The proposed research has potential to establish the experimental and theoretical basisfor a new form of quantum-light spectroscopy while also learning about the fundamental mechanism ofCPL from chiral perovskites, which could improve understanding of spin-orbit coupling in chiral materials.Broader Impacts: Understanding the chiroptical properties of chiral perovskite thin films wouldcontribute to many possible applications, as cited above. However, chiral perovskite thin films are justone of many interesting material systems that the proposed quantum-light spectroscopy can be used tostudy. Development of a new quantum-light spectroscopy would expand the capacity of spectroscopists tostudy fundamental quantum mechanical phenomena. It could be used to characterize gates used inquantum computation, singlet fission materials, spin-based memory devices, and so on.[1] S. Mukamel, et al. J. Phys. B., 53, 072002 (2020). [2] G. Long, et al. Nat. Rev. Mat., 5, 423-439(2020). [3] D. Di Nuzzo, et al. ACS Nano, 14, 7610-7616, (2020). [4] J. Altepeter, et al. Adv. Atom. Mol.Opt. Phys, 52, 105-159 (2005). [5] C. Yong, et al. Nat. Comm., 8, 15953 (2017). [6] arXiv:1909.12869"
30.0,"conserved protein belonging to the ribonuclear binding protein family.[1]It is involved in diverse, essentialcellular functions including RNA transport and alternative splicing. TDP-43 is also the primary componentof cytoplasmic aggregates that are the hallmark of amyotrophic lateral sclerosis (ALS), aneurodegenerative disease with very limited treatments and an average survival-after-diagnosis of 2-3years.[2]Some of these aggregates are amyloid—aggregatescharacterized by a fibrillar morphology andβ-sheet-rich structure as well as a prion-like ability to propagate the amyloid structure.[3]Amyloid proteinsare closely linked tohuman disease, including many neurodegenerative diseases like Alzheimer’s and Parkinson’s. One roleTDP-43 performs in the cytoplasm is protecting RNA during cellular stress by forming membranelessorganelles known as stress granules (SGs).[4]SGsare concentrated droplets of RNA and RNA-bindingproteins that are formedvialiquid-liquid phase-separation(LLPS), the spontaneous de mixing of asolution into a metastable concentrated droplet phase and a dilute phase driven by transient, multivalentinteractions. Phase-separated membraneless organelles have recently been discovered to play critical rolesthroughout the cell, but the complete composition and internal protein conformations of these droplets arenot fully understood.[5]Long-lived SGs (the resultof prolonged cellular stress) have also been shown tolose fluidity and become proteinaceous aggregates, suggesting a connection between protein aggregationand phase-separation.[4]A link between LLPS and amyloid aggregation is likely, as many amyloidogenicproteins can phase-separate without the need of other proteins or RNA.[6]In fact, it has been shown thatLLPS droplets made from the C-terminal domain (CTD) of TDP-43 act as an intermediate for amyloidaggregation in some conditions (Figure 1).[7]The study of protein conformation inside any LLPSdroplet has been very limited. Better understanding of thebasic composition of droplets (i.e. water content) as well asthe secondary structure of protein in LLPS droplets will helpFigure characterize how it may serve as an aggregation1. intermediate. Investigating whether either of theseTDP-43factors change with droplet age will add further tostates. the story. For example, is droplet aging andTDP-43 in (a) soluble (b) phase-separated, and accompanying loss of droplet fluidity due toCTD(c) aggregated states as viewed by brightfield changes in protein conformation, waterconfocal microscopy. Scale bars are 10 µm.exclusion,oranotherfactor?Doessecondarystructureofconstituentproteinschangeasdropletsage,ordoproteins remain disordered throughout droplet lifetime? Do aggregates formed via a phase-separationintermediate differ substantially from those formed in non-phase-separating conditions? Being able toaddress these questions will help 1) better understand LLPS and how itcarriesoutitsmanycellularroles,and 2) identify possible drug targets if TDP-43 LLPS proves to be linked to pathological amyloidaggregation. This work aims to understand secondarystructureofTDP-43inphase-separateddropletsandtrack changes that may occur in droplet hydration and protein conformation during the transition fromLLPS to aggregate.Methods and Experimental Design. I propose to utilize Ramanmicro-spectroscopy—asensitivetypeofvibrational spectroscopy which utilizes Raman scattering—coupled to a microscope, which allows forspatial resolution (Figure 2a).[8] Water scatters strongly in the 3000–3600 cm−1 range, allowing for anapproximation of hydration in droplets based on the intensity of the water bands versus a standard. TheRaman fingerprint region (1000–2000 cm−1) informsonthesecondarystructureoftheprotein(Figure2b).Specifically, the formation of β-sheet-rich structures upon amyloid aggregation results in a characteristicpeak at ~1665 cm−1.[9]Sidechain packing can be analyzedviaC-H deformation modes at 1300-1500cm−1.[9]One of the strengths of Raman spectroscopyis that a single droplet can be observed over time withRaman spectra taken at multiple time points. Raman data from Murthyet al.of another phase-separatedamyloid protein similar to TDP-43 suggests that protein within the droplets resembles soluble protein atearly time points, but it is unknown if this structure is sustainable or if it naturally proceeds to a moreamyloid-like secondary structure.[10]By followingthe lifetime of TDP-43 droplets, I can determinechanges in hydration and protein structure as the droplets age and lose fluidity, filling the gaps left byMurthyet al.and expanding the work to a proteinmore relevant to ALS pathology.Aim One: Collect Raman spectra inside of LLPS can act as a mechanistic intermediate duringTDP-43 LLPS droplets TDP-43 amyloid formation.TDP-43 will be used to prototype the experiment Aim Two: Characterize Differences betweenCTDbecause it reliably forms β-sheet-rich amyloid, and I Aggregateshave previously characterized its aggregation Figure 2. Raman spectroscopykinetics in a variety of conditions. TDP 43 canCTDphase-separate on its own and has been reported todrive aggregation of the full-length protein.[11]Afterproof-of-concept, full-length TDP-43 will beexpressed and purified. The protein will be placed inphase-separating solution conditions (high salt,neutral pH), and phase-separation will be confirmedviabrightfieldmicroscopy (Figure 1b). I will create a catalog ofRaman data from each time point giving a detailedview of the structural changes during dropletsolidification. I hypothesize that: 1) the conformationof TDP-43 will differ between its soluble, LLPS, andaggregated forms, 2) water content in droplets will(a) Diagramdecrease as droplets age and water becomes excludedillustrating the set-up of a Raman microscope,from the stacked β-sheets formed by proteins in theadapted from [12] (b) Representative Ramanamyloid conformation, and 3) as the phase-separatedspectra of TDP-43 fibrils with evident C-Hdroplet ages, the secondary structure band around CTDdeformations an amide I band reporting on β-sheet1600-1700 cm-1will narrow and sharpen, indicatingacontent.transition to β-sheet that mimics amyloid aggregates.Taken together, this data would demonstrate thatTDP-43 can aggregate even when phase-separation is not present. I will use Raman spectroscopy toanalyze the secondary structure of aggregates formed with and without the phase-separation. This willreveal any polymorphism in structure as Raman spectra are sensitive to not just secondary structure, butalso side chain packing as seen in the C-H deformation bands. I will also use Raman to examine samplespropagated from the brains of ALS patients (patient samples are used to ‘seed’ recombinant protein and,due to the prion-like nature of TDP-43, structure is preserved). By comparing the Raman spectrum of thein vitrophase-separated and non-phase-separated aggregatesto the spectra of the patient-propagatedsamples, insight into whether disease-related aggregation stems from a phase-separated intermediate willbe gained.Resources and Suitability.I have significantexperience using Raman micro-spectroscopy inDr. Jennifer Lee’s lab at the NIH. Paired with my prior work with TDP-43 and strong record ofindependence and publication, this makes me uniquely well-suited to pursue this project. The work will beconducted utilizingthe Raman micro-spectrometer at the University of Wisconsin Centers for Nanoscale Technology.Intellectual Merit.By characterizing the generalhydration and composition of LLPS droplets, this projectwill provide foundational information on LLPS, a process of interest to fields ranging from polymerchemistry to cell biology. Additionally, if LLPS are shown to be intermediates in amyloid aggregation, thisopens new doorways for drug development targeting the proteins in droplets.Broader Impacts. This project illustrates the value of an interdisciplinary approach in studying humanhealth and disease processes. Specifically, it demonstrates how physical chemistry methods—the mostmicro scale—can offer foundational, useful information on emergent processes in complex biologicalsystems. I hope that demonstrating the utility of physical chemistry, which can often feel hopelessly farremoved and theoretical for students, will pique the interest of the next generation of chemists as I workwith them as a teaching assistant and research mentor.References. [1] Y. Sun, Biochemistry 2017. [2] M. Neumann, Science 2006. [3] J. L. Robinson, ActaNeuropathol 2013. [4] C. M. Dewey, Brain Res 2012. [5] S. Boeynaems, Trends Cell Biol 2018. [6] S.Elbaum-Garfinkle, J BiolChem2019.[7]W.M.Babinchak,JBiolChem2019.[8]R.R.Jones,NanoscaleRes Lett2019. [9] Z. Movasaghi,Appl Spectrosc Rev2007. [10] A. C. Murthy, Nat Struct Mol Biol2019.[11] A. E. Conicella, Structure2016. [12] S. Lohumi,Appl Sc.2018."
31.0,"(sRNA-seq) technologies have fueled the discovery of many new classes of biologically relevantnon-coding sRNAs. Accumulating evidence suggests that sRNAs are critical contributors to thepathogenesis of various diseases and play an essential role in regulating gene expression levels1.RNA-seq analysis has revealed diverse classes of sRNAs circulating on various lipid and proteincarriers, including high-density lipoproteins (HDL) 1. The most well characterized sRNAs aremicroRNAs (miRNA) and the sRNA-seq analysis tools currently available are designed to focusmostly on miRNA quantification. Due to the limitations of previous analysis tools, our labdeveloped a novel sRNA analysis pipeline (i.e. TIGER) which profiles many classes of host (e.g.mouse, human) and non-host (e.g. bacteria, archaea, fungal) sRNAs present on lipoproteins2.Using our new TIGER pipeline, we discovered that the overwhelming majority of circulatingsRNAs on HDL are classified as host and non-host ribosomal RNA-derived fragments (rDF)2.Motivation: Although the functional role(s) of rDFs are poorly understood, mountingevidence suggests that rDFs are not products of random degradation, but regulated by specificendonucleolytic cleavage processes, similar to that of transfer RNA-derived fragments (tDFs)3.Indeed, various stressors were shown to induce transfer RNA cleavage events, producing stabletDRs3. Angiogenin is an RNase A-family enzyme that is thought to be primarily responsible forstress-induced tRNA fragmentation within mammalian cells4. Angiogenin has also been shownto induce rRNA fragmentation, although to a lesser extent4 Interestingly, the cleavage of tRNAs.and rRNAs are inherently linked to their chemical modifications (i.e. m1A and m5C)5. BothtRNAs and rRNAs represent the most abundant sRNAs, and the two most heavily modifiedRNAs in the eukaryotic genome. Although the identification of chemical modifications on tDFshas been actively pursued, few studies address the modifications found on rDFs. However,preliminary data I have generated using 2D-thin layer chromatography (2D-TLC) identifiedabundant base modifications (e.g. m5C, m6A) on sRNAs isolated from human HDL samples.A major limitation when exploring the sRNA world is that many base modifications candisrupt Watson/Crick base pairing and impede first-strand synthesis by reverse transcriptase(RT) 4. These chemical modifications therefore affect the detection and quantification of sRNAs,limiting the power of discovery. Although recent improvements to our TIGER pipeline havegreatly enhanced our ability to assess sRNA content on HDL, base modifications wouldsignificantly impair efficient detection of these modified sRNAs. To circumvent these issues, arelatively new method was developed called AlkB-facilitated RNA methylation sequencing(ARM-seq) which exploits the RT roadblocks created by chemical base modifications (i.e. m1A,m3C and m1G) in tRNAs6. The E. coli AlkB homologs (ALKBH1 and ALKBH2) act as “eraser”proteins, catalyzing the demethylation of specific chemical base modifications6. This methodrepresents a large step forward in the quantification of tRNAs, however a very limited number ofstudies have used ARM-seq for rDFs. Similar to tDFs and miRNAs, which were once readilydiscarded from RNA-seq datasets, rDFs may play important roles in the regulation of geneexpression. As such, accurately quantifying rDFs and their modification status on HDL is key togaining a more complete understanding of the biological functions of the epitranscriptome.Based on our previous studies and preliminary results I hypothesize that: (1) Improved sRNA-seq methods will increase the inclusion and identification of rDRs in HDL-sRNA datasets. (2)Stress factors induce parent rRNA fragmentation leading to an increase in circulating rDRs. Iwill address these hypotheses through two central aims.Aim 1: Enhance HDL-sRNA identification by characterizing the landscape of chemicalbase modifications found on sRNAs. To achieve this goal, we must capture and identify all hostand non-host rDRs. This will include a.) Expanding bioinformatic analyses for rDRs, b.)Improving the identification of modified rDRs, and c.) Removing modifications on sRNAs forenhanced rDF inclusion in sequencing analyses. To address this aim I will first collect bloodfrom healthy individuals and isolate their sRNAs found circulating on HDL using fast proteinliquid chromatography. I will then pretreat HDL-sRNAs with the purified AlkB enzymes prior tocDNA synthesis (RT step) and library preparation. By comparing AlkB-treated and untreatedsamples, I will reveal the positional modification profile of HDL-sRNAs, including rDFs. TheTIGER pipeline will be used to identify the diverse classes of sRNAs on HDL particles. Thepower of ARM-seq will be maximized by taking advantage of RNA modification databases, suchas Modomics and RMBase. I expect ARM-seq to efficiently reveal chemical base modificationsin the sRNA samples and increase the repertoire of rDFs.Aim 2: Characterize changes in parent rRNA fragmentation and cellular rDF export toHDL in response to environmental stress. Overwhelming evidence supports the role for specificenvironmental stressors to induce tRNA cleavage; however, very few studies have looked atrRNA fragmentation during environmental stress7. To determine whether oxidative stress, heatand cold stress, or γ-irradiation promote rRNA cleavage events, and the export of rDFs to HDL, Iwill treat human hepatic and non-hepatic cell lines with various environmental stressors(hydrogen peroxide, cold or heat shock, or irradiation with UV). Afterwards, the cells will befractionated into nuclear and cytoplasmic extracts, and HDL will be isolated using a FPLC. Toexamine stress-induced rRNA fragments within these cellular fractions, I will use improvedsRNA-seq approaches and confirm candidate rDFs using northern blot techniques. Moreover, wewill quantify the export of hepatic rDFs to HDL in response to stresses to using HDL-sRNAexport assays. I fully expect that exposure of specific environmental stressors will induce distinctparent rRNA fragmentation patterns and alter hepatic rDF export to HDLBroader Impact: Circulating sRNAs have been shown to be differentially altered inseveral diseases and hold great potential for the discovery of novel biomarkers and highlypromising therapeutics. Given the value of potential biomarkers, the field of sRNA has led tocutting edge research. However, there are still gaps in our understanding of sRNA diversity oncirculating HDL. My proposal helps to address this gap and may lead to the identification of yetunknown RNAs. With novel classes or sRNAs being discovered, and the validation of modifiedsRNAs, it is paramount that RNA-modification and sRNA databases are updated. I willdisseminate my findings to web portals and servers dedicated to compiling databases for RNAmodifications.Intellectual Merit: It was not very long ago that many sRNAs were considered “junk”and often removed from RNA-sequencing data analysis. However, we now know that sRNAscan regulate several aspects of gene expression. The novel pipeline generated by ourbioinformatics team allows us to discern several classes of small RNAs found in both eukaryotesand prokaryotes. This interdisciplinary proposal applies techniques from bioinformatics,transcriptomics, microbiology, and biochemistry, and represents the first study aimed atidentifying modified small RNAs on HDL. Successful completion of this proposal will not onlyexpand the repertoire of sRNAs and rDRs but will also show how rDRs are important biologicalmolecules.References: [1] Vickers et al. 2011. Nature Cell Biology. [2] Allen et al. 2018. Journal of ExtracellularVesicles. [3] Lambert et al. 2018. Non-coding RNA Investigation. [4] Su et. Al. 2019. J Biol Chem. [5]Rashad et al. 2020. Neural Regeneration Research. [6] Cozen et al. 2015. Nature Methods. [7] Thompsonet al. 2009. Cell."
32.0,"Title:Investigating Maine’s Indigenous Fire Prehistoryto Inform Forest Management Under GlobalChangeWith climate change amplifying underlying environmental issues, modern wildfires have becomeenormous devastating forces, costing lives, our natural resources, and billions of dollars. Early US ForestService practices focused on fire suppression as a management tool, which increased the presence ofunderbrush, snags, and flammable material in forests1.Though these practices have changed, those initialmanagement plans coupled with drought, eco-tourism, and rising temperatures have led to the large-scale,uncontrollable, high-intensity fires in the West that, as of October 1, 2020, have burned nearly 7.7 millionacres this year2.In contrast, indigenous fire management has played an important role in the ecology of many NorthAmerican landscapes for thousands of years. Native peoples used fire to clear the land for cultivation,promote healthy and diverse food-rich forests, and facilitate diverse wildlife habitat3. These practiceshave largely been excluded in modern forest management plans. By the 1970’s the Forest Service beganutilizing indigenous knowledge to set controlled burns in the West, Southwest, and Southeast, but not inthe mixed hardwood forests of the Northeast1, wherefire is not widely considered to be an importantprocess4. However, there have been large-scale destructivefires in the last century, like the Great Fire of1947 in Maine. This drought-intensified fire consumed 17,188 acres, destroyed 240 buildings, and costover $23 million in property damages5. As climate change is causing warmer temperatures and droughtsin the Northeast6, there remains a critical need tounderstand the long-term history of fire (both naturaland anthropogenic) in this region.Most of our academic knowledge about indigenous fire use in New England is largely based on journalsand other written accounts by European settlers. However, those records lack the perspectives ofindigenous people, and only explain fire use post-contact7.Historical observations and Wabanaki oralknowledge indicate that, within the last 500 years, Native peoples used fire to clear land for agriculture,and to improve hunting grounds south of the Kennebec River in Maine. Penobscot place names describeareas that experienced regular burning. For example, Schoodic (skudek) Peninsula, a part of AcadiaNational Park, means “burnt-land”8.Long-term fire records from lake sediment cores and tree rings have provided another valuable source ofinformation about the relationships between fire, climate, vegetation, and people in the American West,Midwest, and Southeast, but are still lacking for the Northeast, including New England. A recent studysynthesizing charcoal patterns across New England found no evidence of pre-European anthropogenic fireuse, but this reflected a regional fire record that would not highlight the more localized scale at whichindigenous peoples would have been burning4. Charcoalrecords in the Northeast have primarily beentaken from large bodies of water9, which are biasedtowards large regional fires, instead of local, lowintensity fires, which would have been the types of fires Native peoples used for land management4.Therefore, while previous paleoecological studies have been important for understanding the large-scalefire prehistory of New England and its relationship to climate, they are poorly suited to the study ofanthropogenic fires. And, by failing to partner with Native scholars and incorporating oral knowledge ofpast land use, such studies mask indigenous peoples’ expertise and contributions to the health of thelandscape10.Intellectual MeritMy research goal is to reconstruct localized fire records in Maine to better understand fire as a prehistoricland management tool in New England. I will take a multi-pronged approach to this work: 1.) I willconduct an actualistic study to identify the signals of localized understory burns and small patch clearingsin the charcoal and pollen records of forest hollows and small ponds. 2.) In collaboration with members ofthe Penobscot Tribe, I will collect sediment cores from small ponds and forest hollows near settlementsand prehistoric hunting grounds to examine whether small-hollow cores can identify local, small-scaleburning. 3.) I will then synthesize these findings with existing geoarchaeological, climate, and pollenrecords to assess the relationships between population and cultural shifts, climate, vegetation, and firehistories across scales. All of the necessary equipment and facilities to carry out this project are availableat the University of Maine Climate Change Institute, and Dr. Gill is building tribal partnerships viacollaborations with Penobscot faculty at UMaine: Dr. Darren Ranco, director of the WaYS program, andDr. Bonnie Newsom, archaeologist. Partnership opportunities are also available at Acadia National Parkthrough the National Park Service.Many fire-use studies focus on written accounts, the charcoal record, and tree scaring to reconstruct pastfire regimes, but researchers have historically excluded indigenous communities when studying pasthuman land use. My project will contribute to a more accurate historical record of prehistoric land use bybetter matching the tools to the questions to characterize anthropogenic fire histories and impacts. Thisproject will also add to our understanding of charcoal records taken from small hollows. In contrast withthe pollen record, hollow-based fire records are lacking, which limits our ability to interpret stand-scalefire impacts11.Broader ImpactsThough fire is not considered to be an important process in the Northeast, with climate changeexacerbating existing environmental issues, it is becoming an increasingly dangerous threat. This pastsummer, drought conditions and increased eco-tourism due to COVID-19 resulted in a summer of over900 high-intensity, destructive fires in Maine6. Maine’seconomy depends on logging and tourism12anddrought-induced fires put both of those industries at great risk. This study seeks to understand lowintensity fires and will inform conservation and management practices. Such fires clear underbrush andsnags, reducing the fuel load for uncontrolled fires. This would make Maine’s forests safer while alsoreducing tick populations by burning shrub species that foster these disease vectors13. Cleared underbrushwould improve forest health by reducing canopy competition and eliminating weaker diseased trees. Allof these benefits could increase timber quality and forest health, boosting two of the state’s majorindustries during a time of economic uncertainty.The Wabanaki Confederacy is a collection of Eastern Algonquin tribes including the Penobscot,Passamaquoddy, Mi'kmaq, and Maliseet people. I plan to use my research to contribute to the NativeAmerican Graves Protection and Repatriation Act (NAGPRA) by providing supporting evidence of longterm tribal habitation. This project will contribute to a long-term collaborative relationship with localtribes and will provide critically needed information in support indigenous sovereignty claims. I alsointend to collaborate with the NSF-funded Wabanaki Youth in Science (WaYS) program at UMaine.WaYS trains Wabanaki youth in both tribal knowledge and scientific approaches through summer campsand internships. I intend to include Wabanaki students in my project by bringing groups of students outinto the field and mentoring students in the lab to learn sediment coring and paleoecological techniques.References.[1]Forest History Society.US Forest Service FireSuppression. [2]Congressional Research Service.2020. 43. [3]Ryan K.C. 2013.Frontiers in Ecology and the Environment.[4]Oswald, W.W. 2020.Nature3,241–246. [5]National Park Service, Acadia. 2020. [6]The Maine Monitor. 2020.Bangor Daily News.[7]Ruffner, C. M. 2005.USDA: Proceedings 16th CentralHardwood Forest Conference.[8]Francis, J.E.2008.Farms, Forest, and Fire44(1): 4-18.[9]Patterson.1988.Holocene Human Ecology in NortheasternNorth America.[10]Kimmerer, R.W. and Lake, F. 2001.Journal of Forestry99(11):36-41. [11]HigueraP.E. 2005.The Holocene15(2): 238-251.[12]US Newsand World Report. 2020.Best States: Maine.[13]Gleim E.R. 2019.Nature."
33.0,"Introduction and Preliminary Results: Discovered at Drexel University in 2011, MXenes are a novelclass of 2D materials that comprise metal carbides and nitrides. Due to their excellent electronic, optical,thermal, and mechanical properties, MXenes have great promise for applications in several technologiesincluding additives in solar cells and electronic contacts for semiconductors [1]. Compared to other 2Dmaterials, MXenes offer an optimal combination of high electronic conductivity, low cost, and facilesynthesis methods. Furthermore, their tunable optoelectronic properties, such as work function and opticalabsorption, enable MXenes to improve emerging photovoltaic materials such as perovskites and inorganicsemiconductors. To realize the full potential of MXene photodetectors, an improved fundamentalunderstanding of their electronic and optical properties is needed.During my master’s thesis, I refined methodsfor photodetection analysis of MXene thin films. Whileit laid the groundwork for the optoelectronic study ofMXenes, the underlying factors that drive MXeneresponse to light (photoresponse), such as the impactsof the electrode and substrate type, are not well-understood. While MXenes have proven successful asadditives and electrodes, I aimed to bring them to themainstream as active materials. My work focused on thephotoactive capabilities of Ti C , which has already3 2succeeded as a transparent photodetector electrode [2].Although Ti C is the most commonly studied MXene,3 2its response to chopped illumination with visible lightFigure 1: Ti C films exhibit consistent negativehad not yet been reported. 3 2photoconductivity when deposited on patternedFigure 1 compares the average change influorine-doped tin oxide (FTO) substrates withoutresistance (R) upon illumination for films withthe use of silver paste contacts (black). However,different initial resistance values (corresponding tothin films with high R switch from negative tothickness) and with different contact methods. Here, 0positive photoconductivity upon illumination withthe application of silver paste as an electrical contactthe application of metal contacts (red) andcauses Ti C to deviate from innate behavior upon3 2experience suppressed photoresponse whenillumination, while a change to a thinner substratedeposited on glass slides (green). Schematic(glass slides) suppresses the magnitude of theof MXenes shown in inset [2].photoresponse. Ti C is a well-known metallic and3 2photothermal material with innate negative photoconductivity, leading to an expected increase in R uponillumination or heating; these observations that contradict expected behavior call into question the role ofsilver-MXene interactions, carrier dynamics, and heat transfer in determining the material’s photoresponse.I hypothesize that the photovoltaic (PVE) and the photothermoelectric (PTE) effect each play into thephotocurrent generated by MXenes, giving them the power to serve multiple applications, from thermalimaging to photovoltaic electrodes.Research Plan: I aim to both experimentally and computationally study heat and charge transport inMXene photodetectors to guide their design in imaging and energy generation applications. I will beginmy examination with Mo-based MXenes, a lesser-studied subset of MXenes with potential as a photoactivematerial. Previous empirical studies question computational results showing Mo TiC has semiconductor-2 2like properties [3]. This work will seek to confirm these analyses by isolating contributions to light-matterinteractions for Mo-based MXenes. Furthermore, over 30 different MXenes have been reported to date [1].This proposal outlines just the beginning of our exploration into MXene photodetection capabilities inresponse to visible light, as the methods listed can be applied to other photoactive MXenes as well.Objective 1 – Understand the impacts of device architecture: In varying the deposited film thickness,contact geometry, and substrate type, I will evaluate their individual influences on photodetector properties(responsivity, noise, stability) in response to chopped illumination with visible light. Using thermallyconductive substrates, such as sapphire, the impact of thermal effects can be mitigated. I expect stronglyabsorbing films, non-metal electrical contacts, and thin, thermally conductive substrates to produce thestrongest photoresponse for Mo-based MXenes. Upon gaining this phenomenological data, I will then studycharge carrier dynamics to understand the light-matter interactions for each MXene device. Under theguidance and expertise of Prof. R.J. Holmes, I will probe exciton diffusion at the interface of thephotoabsorbing MXene and the electrical contact via an external quantum efficiency measurement methodcurated in his group [4]. This broadly applicable method provides additional understanding of carrierdynamics upon photoexcitation and will guide selection of device architecture for improved performance.Objective 2 – Model optical and thermal transport kinetics: Through computational efforts to modelcontributions from the PTE and the PVE, I will confirm the dominant effect that determines thephotoresponse. Should combined contributions dictate the photoresponse, I aim to create a secondary modelsystem specific to MXenes. By compiling a model from literature for both heat and carrier transport, I willdetermine optimal film thicknesses, contact geometry, and substrate types for devices that rely on either thePVE or the PTE, creating two reliable device architectures with improved responsivity. Moreover,simulating the photodetector architectures created in Objective 1 using COMSOL will push them to theirthermal and electronic limits, granting insight into widespread implementation of MXene photodetectors.Objective 3 – Create devices and optimize performance: Equipped with the knowledge of optimal devicearchitecture and film deposition, I will build Mo-based photodetectors and investigate industrially relevantissues, such as stability, lifetime, and performance of larger area devices. Given the inevitable obstacles inscaling up a device, I must tailor the device parameters found in Objective 1 to suit applications that wouldbenefit from either the PVE or the PTE, such as energy generation or thermal imaging, respectively. Ienvision my contributions will spur the development of MXene-based photodetectors that suit multiplepurposes simply by changing the device architecture.Intellectual Merit: My well-rounded background in chemical engineering and materials science andengineering allows me to understand not only why MXenes behave the way they do, but also how we canimplement these materials in devices. I will utilize the wealth of knowledge from multiple energy transportexperts, including Prof. Holmes, as well as state-of-the-art facilities for nanotechnology research at UMNto ensure the success of this project. The proposed research will provide an improved fundamentalunderstanding of the factors that influence MXene photodetection, an emerging field of interest with limitedliterature available. Upon gaining this understanding, we can continue to use MXenes in optoelectronicapplications, reducing the cost to produce photodetectors and allowing for widespread implementation ofmore conductive, easily synthesized materials.Broader Impacts: My work aims to inspire other researchers to consider implementing MXenes in theirdevices, bringing the field closer to a reliable, reproducible method for renewable energy generation.Through my research on nanomaterials in energy applications, I aspire to make clean energy commonplace,expanding on my dreams of a sustainable future arising from wanting to develop accessible biodegradableplastics in high school. I also aim to continue my impactful record of mentorship and community outreach.Leaning on my extensive outreach experience described in the accompanying personal statement, I plan tocreate an interactive lesson on current and novel photodetectors and sensors through Science for All, astudent-run group created to support and promote STEM fields to local, underserved middle schools in theurban Twin Cities. Prof. Holmes also has the laboratory facilities to package photodetectors, allowing meto bring samples to the classroom. Lastly, through the Undergraduate Research Mentorship Program(UROP), I will seek and recruit undergraduate students from underrepresented groups for this project,serving as a research and personal mentor to guide them through their technical careers and encourage themto continue their STEM education.References: [1] L. Zhao, et al. Tungsten, 2 (2020): 176 – 193 [2] K. Montazeri, et al. Adv. Mater., 31.43(2019): 1 – 9. [3] G. Li, et al. Proc. SPIE 11279, 112791U (2020): 66 – 84. [4] T. Zhang, et al. Nat.Commun., 10.1 (2019): 3489 – 3495."
34.0,"wetlands (UPOWs), are a practical, cost-effective, and highly scalable approach to managing environmentalwater quality.1 UPOWs utilize microbial growth in the benthic region within a photosynthetic biomat,hosting a stratified population in aerobic, anaerobic, and anoxic zones. Biomat microbe ecosystems havedemonstrated treatment of influent water for nutrients, trace organic compounds, and other contaminants atrates that match—and in many cases exceed—those of traditional vegetated or subsurface wetlands.1 Thesemicrobial populations develop independently over multiple months, using algae and other detritus as carbonand electron sources.The low implementation and maintenance costs of these systems have drawn attention to their usefor increasing water availability and decreasing risk in otherwise water-poor or unprotected communities,such as the Arequipa region in Peru. These communities utilize surface water from local rivers such as theTambo, which contains concentrations of arsenic (As) exceeding Autoridad Nacional de Agua regulationsfor both domestic and agricultural use.2 Elevated concentrations are likely a result of high background levelsof As in local geology and introduction from mining operations in the area. The surrounding communitycan no longer safely consume or export important commercial products such as rice or river shrimp becauseof this threat. The long-term effects of these economic limitations and human health impacts cannot beunderstated and will continue to persist so long as the region suffers from degraded surface water quality.Dr. Josh Sharp, professor of Civil and Environmental Engineering at the Colorado School of Mines,is currently working with the Universidad de San Augustín de Arequipa (UNSA) in Peru to study thepotential for UPOWs to treat surface waters for metal and metalloid contaminants. UPOWs with thecapability to remove these hazards would provide communities with a first step toward improvingenvironmental water quality.Challenges of degraded water quality do not adhere to national or state boundaries. Communitiesall over the world, including here in the United States, contend with a lack of access to safe water sources.My goal is to apply research on UPOWs to ensure no one has to suffer from contaminated water. I willbuild upon Dr. Sharp’s research on UPOWs by determining their capacity for arsenic removal.Hypothesis: UPOWs are able to remove trace organics and nutrients from surface waters and are potentiallycapable of immobilizing metal and metalloid contaminants.1 Arsenic (V) is the most prevalent form of Asin surface water, and poses a hazard to human and environmental health.3 I hypothesize that UPOWsincorporating sulfate-reducing bacteria have the capability to remove As (V) from surface water viaprecipitation with sulfide in a variety of geographic settings.Research Plan: Objective 1: Analyze the potential for As (V) precipitation as As S in the presence of2 3sulfate-reducing bacteria in UPOWs.Arsenic (V) removal by precipitation relies on the transformation of As (V) to As (III), as biologicalsystems have previously been observed to utilize sulfate-reducing bacteria to precipitate As (III) withsulfide.3 H AsO , the naturally occurring form of As (III), reacts with sulfide to produce As S (Equation3 3 2 31) which is insoluble in typical surface water conditions.42𝐻 𝐴𝑠𝑂 +3𝐻 𝑆 ⇌ 𝐴𝑆 𝑆 +6𝐻 𝑂 Equation 1! ! "" "" ! ""Some sulfate-reducing bacteria hold the potential to reduce As (V) to As (III).5 The presence ofthese bacteria could allow for the precipitation of As (V), proceeding as in Equation 1. Sulfate-reducingbacteria colonize and function well in biomat ecosystems,4 but they have yet to be tested for specificremoval of As.Objective 2: Evaluate As removal efficiency of UPOWs across largely differing geographical areas.The formation and function of biomats in UPOWs rely on both biotic and abiotic constituents ofsurface water. Successful arsenic removal depends on the colonization and function of sulfate reducingbacteria, algae, and other diatoms, all of which could differ by region. I will observe whether biomatecosystems will support microbe composition necessary to perform key contaminant removal functionsregardless of UPOW location.Differing water chemistry and abiotic constituents could alter the As removal pathway or block Asprecipitation altogether. In some cases, As may complex with other constituents to produce undesiredbyproducts, such as Thioarsenic.6 A particularly interesting factor in the success of As removal may alsobe sulfate concentration, though determining direct influence is outside the scope of this project.Approach/Methods: Testing will occur at both the bench and field scales. After the effectiveness of themesocosm-sized UPOW biomat has been determined, the biomat composition and design will be tested atthe field scale at the Prado Wetlands in California and then at the UNSA in Peru.Objective 1: Bench scale biomats will be harvested from existing UPOWs in the Prado Wetlands anddeployed in the lab at the Colorado School of Mines to be tested with spiked influent water. Test water willconsist of local Colorado water samples spiked with known levels of arsenic. Sulfate will be held constantin a similar concentration to that of the Tambo. Removal efficiency will be calculated using a mass balanceon As, with a known influent concentration, precipitation concentration determined using a modifiedTessier extraction of the biomat,7 and effluent concentration determined using ICP-MS chromatography.Selective inhibitors for sulfate reducing bacteria, such as molybdenum8, will be used to evaluate arsenicprecipitation in response to sulfate reducing capacity.Testing will move to the field scale as a validation step in large-scale performance after bench-scale testing has been completed. Field testing will first take place at the Prado Wetlands, where a colonizedUPOW will be allowed to run for an extended period of time. As concentrations will be measured using thesame methods as at the bench scale.The continual rise of precipitated arsenic in the biomat could lead to concern after long periods oftime in field operating conditions. To mitigate potential release of highly concentrated arsenic from abiomat, I recommend an operating system in which sections of biomat could be harvested prior to theaccumulation of dangerous levels of As or other harmful constituents. These sections could go on to beused as fertilizer to utilize their elevated levels of nitrogen, resulting from abundant nitrifying bacteria.Objective 2: Successful function of a UPOW system in Peru will first be estimated at the bench-scale usingreplicated Tambo River water at the Colorado School of Mines. Biomat colonization and As removal willbe monitored over the course of several months using these waters, with an emphasis on the perception ofsulfate reducing bacteria presence.Field-scale testing will occur at the UNSA and will mirror procedures in the United States, nowusing unmodified water from the Tambo River. Biomats will colonize and undergo testing in mesocosmand field-scale UPOWs and arsenic concentrations will be determined using a mass balance approach.Intellectual Merit: Metal and metalloid removal has not yet been tested in UPOW systems, but thesuccessful performance of sulfate-reducing bacteria in previous UPOWs could lead to an innovation in Asremoval methods. This study would quantify the effectiveness of this mechanism for arsenic removal fromsurface water via UPOWs.UPOWs have been tested in the United States but have not been observed abroad. This study willdemonstrate the effects of differing surface water characteristics of different regions on arsenicprecipitation. Additional application of UPOWs, including biomat harvesting, could have basis for furtherinvestigation to increase utility of these already compelling systems.Broader Impacts: The development of this system, and other natural treatment systems, would makeprogress on the priorities set forth by the UN Sustainable Development Goals and the Grand Challenges ofthe National Academy of Engineering. My work would directly contribute to these efforts aimed atincreasing global sustainability and standards of living. More immediately, an UPOW system has thepotential to mitigate the threat of arsenic for the Arequipa community and others like it. Depending onsequestered metal concentrations, the periodic harvesting of UPOW biomats, for fertilizer or otherwise,could also prove UPOW potential to serve a purpose beyond water treatment. Further research on naturaltreatment systems will continue to drive down cost, increase understanding, and foster education incommunities using natural water treatment.References: 1Jasper, et al. (2013). Environ. Eng. Sci. 30(8), 421-436. 2Autoridad Nacional de Agua. (2020)Ministerio de Agricultura y Riego. 3Lizama, et al. (2011). Chemosphere. 84(8), 1032-1043. 4Jones, et al.(2017). Appl. Environ. Microb. 83:e00782-17. 5Macy, et al. (2000). Arch. Microbiol. 179, 49-57. 6Stucker,et al. (2014) Environ. Sci. Technol. 48(22), 13367-13375. 7Tessier, et al. (1979). Anal. Chem. 51(7), 844-851. 8Blum, et al. (1998). Arch. Microbiol. 171, 19-30."
35.0,"Introduction:Glass-Ceramics (GC’s) are critically relevant materials for industry and scientific research, primarily dueto their outstanding mechanical, thermal, and optical properties. High-grade GC’s such as lithium-aluminosilicate (LAS) are commonly used as insulation materials for high-performance aircrafts andmissiles, optical bodies of precision optics, and biomaterials for medical equipment[1]. Although thematerial properties of GC’s are very attractive in many engineering fields, the cost of manufacturingcomplex geometries can be prohibitive, primarily due to the high cost of tooling and limited machiningcapabilities of present manufacturing methods. Therefore, it is the goal of the proposed work to implementa novel method of manufacturing GC’s with predictable, tailorable, and optimized material properties.GC’s are classified as two-phase materials; one being the glass matrix, the other being small volumefractions of nanocrystal inclusions. Typically, GC’s are manufactured through casting or forming methodsbased on glass-making techniques. In these methods, the glass matrix is heated to high temperatures usinga two-step process. In the first step, known as nucleation, the GC is heated just past its devitrificationtemperature to create a high density of nuclei throughout the interior of the glass. The second step involvesre-heating the GC to a highly controllable temperature which directly impacts the growth rate, crystal size,and region of crystallization[2].Vat Photopolymerization (VP) is an Additive Manufacturing (AM) process which utilizes UV light toselectively cure a polymer-based resin in a layer-by-layer fashion. VP can offer unparalleled resolution,complex internal and external features, and high-solid loadings of GC’s to further enhance theirapplications. Digital Light Processing (DLP) is a sub-category of VP which uses a UV projector instead ofa laser to expose cross-sections of the design geometry onto a resin vat. Therefore, the curing characteristicsof the polymer resin are controlled by the light intensity and the effective pixel size from the projector.Additionally, by carefully tailoring the monomer, oligomer, and photo-initiator concentrations in the resin,high solid-loading of GC’s within the resin could be achieved[3]. The polymer matrix is finally burnt offthrough a debinding step before the sintering process, resulting in a highly pure and fully dense GC part.To further improve the mechanical, thermal, and optical properties of the GC’s, an Ion-Exchange (IX)process will be implemented after the sintering step. During this process, cations of small atomic size fromwithin the glass matrix surface are replaced by larger cations from the molten salt bath through a diffusionmechanism driven by temperature difference, resulting in a permanent compressive force in the surface ofthe part, which suppresses the growth of surface flaws and reduces crack propagation within the glass[4].Proposed Research Activities:Objective 1: Understanding primary and secondary parameter influence on material properties ofGlass-Ceramics. From my previous and ongoing research, it has been noted that the concentrations ofmonomer, cross-linker, and photo-initiator in respect to the solid-loading of the matrix material greatlyimpact the printability and final properties of the GC. Therefore, it would be highly beneficial to understandthe primary and secondary parameters during the printing, debinding, and sintering processes and theirimpact on the final material properties.Primary parameters from the printing step could include UV exposure times, light intensity, and layerthickness; secondary printing parameters could include layer waiting time, lifting speed, and vattemperature. Degree of Conversion (DoC) is a common measurement tool that utilizes Fourier-TransformInfrared (FTIR) characterization data to rapidly quantify the progress of the photopolymerization reaction.Primary and secondary printing parameters will therefore be directly quantified and compared through DoCby use of FTIR. For debinding, primary parameters could include ramping rates and holding temperatures;secondary parameters could include crucible materials and furnace atmosphere. Initially,Thermogravimetric analysis (TGA) will be performed on printed samples to determine ideal holdingThe University of Texas at El Paso 1Sebastian Vargas NSF GRFP Research Statementtemperatures in efforts to ensure complete removal of organic compounds. Additionally, Energy DispersiveSpectroscopy (EDS) will be used as an elemental analysis tool to compare anticipated composition of theGC’s to actual results. In terms of sintering, primary parameters could include final sintering temperaturesand furnace atmosphere; secondary parameters could include ramping rates and cooling rates. X-RayDiffraction (XRD) analysis will be performed on sintered samples to broadly determine the degree ofcrystallization by comparing results to literature and standards. Finally, by identifying the relationships(linear or non-linear) between parameters and material properties, a novel, reliable, and well-understoodmanufacturing method for GC’s based on the DLP process could be achieved.Objective 2: Inclusion of alkali modifiers for chemical strengthening through Ion Exchange (IX).Based on the recent work of Gy, R.[4] and Macrelli, et al.[5], lithium, potassium, and sodium ion modifierswill be added to the GC resin formulation in preparation for chemical strengthening through the IX process,which will be carried out directly after Objective 1. The depth of penetration of the cationic exchange layer,also known as case depth, is a direct metric of the effectiveness of the IX process and will be evaluated atvarious depths using a refractometer. As mentioned before, the strengthening process intrinsically developsa residual compressive stress at the surface of the GC. Therefore, the effect of cation modifier concentrationon the final mechanical properties will be assessed by compressive, flexural, and hardness testing based onASTM standards and will be performed with equipment available at the Keck Center and SMP lab.Extended Objective: Supporting the development of a machine-learning-based model to predictmaterial properties of glasses from compositional data. Based on the recent work by Ward. et al[6], aframework capable of extracting predictive models from existing materials data is being developed by theKansas City National Security Campus (KCNSC). My research will serve to provide the model withcharacterization and testing data obtained from Objectives 1 and 2 in order to effectively populate themodel. More detailed information may not be suitable for public release at this time.Intellectual Merit:The proposed work represents a novel method for manufacturing two of the most relevant materials tosociety: ceramics and glasses. My research would directly advance the limited understanding of the intricaterelationships between input parameters and material properties at different steps of the DLP manufacturingprocess. This critical understanding could potentially overcome a common barrier towards furtherdevelopment and implementation of DLP-based AM as a prevalent manufacturing method.Broader Impact:The development of DLP-based AM could enable previously unachievable part geometry of GC’s andtherefore, become a highly tailored process to impact many industries including aerospace, defense, andmedical. Additionally, this work could drive further research in STEM, including fields such as AdditiveManufacturing, Machine Learning, and materials science. Finally, the proposed work would directlysupport ongoing research efforts at the KCSNC, and thereby, the National Nuclear Security Administration.References:[1] Elan Industries. https://www.elantechnology.com/glass/glass-materials/las-glass-ceramics/[2] Rawlings, R. D., J. P. Wu, and A. R. Boccaccini. ""Glass-ceramics: their production from wastes—areview."" Journal of Materials Science 41.3 (2006): 733-761. [3] Kotz, F, et al. ""Three-dimensional printingof transparent fused silica glass."" Nature 544.7650 (2017): 337-339. [4] Gy, René. ""Ion exchange for glassstrengthening."" Materials Science and Engineering: B 149.2 (2008): 159-165. [5] Macrelli, Guglielmo,Arun K. Varshneya, and John C. Mauro. ""Ion Exchange in Silicate Glasses: Physics of Ion Concentration,Residual Stress, and Refractive Index Profiles."" arXiv preprint arXiv:2002.08016 (2020). [6] Ward, Logan,et al. ""A general-purpose machine learning framework for predicting properties of inorganicmaterials."" Nature: npj Computational Materials 2.1 (2016): 1-7.The University of Texas at El Paso 2"
36.0,"Center reported a total of 19,105 new cases of spinal cordinjuries with some amount of paralysis in 2019. Brainmachine interfaces (BMIs) are a burgeoning technologicalsolution to restore quality of life to these people throughconnecting brain signals to prosthetics. Current brainmachine technology collects and transmits neural signalsfrom implantable electrode arrays to be decoded usingalgorithms which are implemented on cumbersome andpower inefficient computers. These systems also requiredaily calibrations by scientists and clinicians to maintainFigure 1: Proposed Brain Machine Interfacetheir usability. Herein lies an approach to address theseArchitecture.problems through implementing specially designedalgorithms that are memory and computationally efficient onto a low power application specific integratedcircuit (ASIC) to decode patient intent using a fully implantable device. The goal is to engineer hardwareto implement intelligent decoding algorithms that will increase the reliability of neural decodingsystems and decrease the physical size and power requirements by orders of magnitude throughremoving the need for transmission of unprocessed neural data. I am working with professor AzitaEmami at the Caltech Mixed-mode Integrated Circuits and Systems (MICS) laboratory to begin my workas an ASIC and algorithmic designer implementing these systems.Previous Work: The promise of this project to produce reliable power efficient BMIs relies on thedevelopment of power efficient algorithms. The MICS lab has developed efficient algorithms that utilizedeep multistate dynamic recurrent neural networks [1], as well as done an assay on decoding algorithms[2]. Furthermore, if energy efficient algorithms do not provide the power performance required, in memoryprocessing architectures could be utilized to reduce power lost from transporting weights between memorybanks and processing units [3].Intellectual Merit: Current BMI technology largely utilizes hardware implementations which transmitdigitized neural signals back to a compute station. There are few BMI ASICs that decode patient intentdirectly without off-the-shelf hardware implementations. Custom machine learning ASICs provide anopportunity to optimize for power and area efficient systems. BMI systems have unique signal processingconstraints due to relevant information being mixed across time, frequency, and space in highlydimensional, redundant datasets. These constraints often require complex computational algorithms withhigh internal state complexity, that generally decreases power efficiency [2]. This poses a particularlydifficult engineering dilemma which requires a uniquely multidisciplinary approach to leverageknowledge of neuroscience with engineering expertise in ASIC design. This dilemma is to fit therequired computationally complex task of decoding patient intent from neural signals into a power and areaefficient package.Hypothesis: BMI ASICs implementing computationally, and memory efficient algorithms will be able toefficaciously ascertain patient intent while maintaining robust performance whilst still meeting power andarea constraints requisite of fully implantable systems.Research Plan: Several crucial prerequisite steps for this proposal are already underway in Azita Emami’slab including the evaluation of neural features measured from patient data for stability over time.Aforementioned specialized algorithms have been developed and evaluated for performance. While theseprerequisite steps are crucial to the outcome of this project, the scope of this proposal is constrained to theimplementation of these algorithms in hardware, optimizing for low power. Therefore, this proposal willonly discuss the development, implementation, and testing of the algorithms into CMOS fabric. Stage I-Algorithm CAD Design and Testing: Digital and analog hardware will be described and laid out intoVirtuoso Computer Assisted Design (CAD) program. The circuits will be fabricated with general poweroptimization techniques in mind such as clock and power gating portions of the circuit when they are notin use. Other techniques include using intentional specificity of transistor thresholds throughout the designso that leakage current is minimized, as well as a minimization of supply voltage levels. Furthermore,specific tradeoffs will be made between the bit precision of the algorithms and the resources required to runat those precisions. Significant energy is also lost by moving the algorithm’s weights from memory to theprocessing hardware, so a layout will be designed such that the algorithm weights are physically closer towhere computations are done. While designing the circuits, each component will be characterized at a unitlevel so that the performance of the entire circuit can be ensured. Stage II-CAD Simulation and FPGATesting: After designing the circuit in Virtuoso, the entirety of the circuit will be tested with patient neuraldata collected over several weeks. The circuit simulation will give significant insight into the performanceof the decoder system once fabricated. The simulation will also allow for design bugs to be caught andcorrected before resources are spent fabrication of the design into silicon. The digital components of thecircuit will also be implemented on a commercially available Field Programable Gate Array (FPGA) whichwill not only give physical proof that the logic and algorithms designed will work, but will give an upperbound on the power and area usage for the ASIC implementation. Stage III-Hardware Fabrication: Severaltest chips will be fabricated to investigate the performance of the decoding algorithms. This allows forverification and debugging of the major components of the algorithm, with the final system chip producedafter all components are aptly constructed and verified. The circuits will be implemented into standard celllibraries for 65 nm CMOS technology using Design Vision. Stage IIII-Hardware Testing: The fabricatedchip will be designed for testability using techniques such as built in scan chains to give access to theinternal states of the circuit. Custom test systems will be built to feed neural signals to the device under testand validate the performance of the hardware. Stage IV-Going Forward: Once designed, fabricated, andverified, the integrated circuits could be tested in vivo using the same experimental therapy program fromwhich the neural data was harvested.Pitfalls and Alternatives: Due to variation in fabrication processes, the fabricated chips may exhibitcharacteristics and behavior that were not accounted for during simulation. If experienced, the test hardwaredesigned into the chip will be utilized to identify and debug the flaw.Timeline and Collaboration: The duration of the proposed project is three years and will be conductedunder the supervision of Azita Emami. Azita Emami’s MICS lab works in direct collaboration with RichardAnderson’s neuroscience lab. This is a collaboration between two leading scientists in their respectivefields. The MICS and Anderson lab collaboration has significant skill and expertise to confidently producethe anticipated results of this proposal. Emami’s lab has adequate facilities and resources to fund thesignificant capital required to design and develop custom ASIC hardware. This project also has direct accessto neural recordings of patients with implanted UTAH neural arrays which provides essential data fortraining the decoder algorithms.Anticipated Results: Using specially designed decoding algorithms, significant reductions in power andarea will be observed in the resultant neuro decoding system. It is important to note this project aims tomaintain decoding accuracy and stability, despite using orders of magnitude less power and area.Broader Impacts: Fully implantable neural intent decoders will not only greatly improve the quality of lifeof patients with paralysis, but also provide the basis for fully implantable ASIC chips designed for directstudy of neural activity without the need to be linked to heavy, memory and power inefficient recordingsystems. The miniaturization of hardware and computational effort can further be generalized to many IOTor wearable systems which requires robust signal processing algorithms with limited power and arearequirements. This will enable a variety of wearable devices to decode bio signals without the need toupload the signal data to an off-chip server, greatly improving security, power, and speed performance.References: (1) B. Haghi, S. Kellis, M. Ashok, S. Shah, L. Bashford, D. Kramer, B. Lee, C. Liu, R.Andersen, A. Emami, “ Deep multi-state dynamic recurrent neural networks for robust brain-machine interfaces”, Program No. 406.04. 2019 Neuroscience Meeting Planner. Chicago, IL: Society forNeuroscience, 2019. Online. (2) Mahsa Shoaran, Benyamin A. Haghi, Milad Taghavi, Masoud Farivar,Azita Emami, “Energy-Efficient Classification for Resource-Constrained BiomedicalApplications” IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS), 2018.(3) T.-J. Yang, V. Sze, “Design Considerations for Efficient Deep Neural Networks on Processing-in-Memory Accelerators,” IEEE International Electron Devices Meeting (IEDM), Invited Paper, December2019."
37.0,"extraordinarily challenging by the extreme starlight suppression required toimage faint planets around brilliant stars. The noise-limited performance ofcurrent high-contrast imaging instruments can resolve planets up to 10million times dimmer than their host star. In order to access Earth-likeplanets – the highest-priority planetary targets named by the Astronomy andAstrophysics Decadal Survey [1] – sensitivity must be expanded to planets10 billion times fainter than their star. The primary limitation on increasingcontrast is speckle noise, which is scattering of the stellar point spreadFig. 1: Raw image of thefunction (PSF) that can mimic or obscure planet signals [2]. As observationplanetary system HR8799.time increases, the total noise contributions of read noise and photon noiseFour Jupiter-size planetsattenuate, but speckle noise does not, establishing a high noise floor thatare obscured by quasi-cannot be reduced without removing the speckles themselves. I propose tostatic speckles. Image: Dr.develop a computational method of speckle subtraction for data takenC. Maroiswith the Gemini Planet Imager (GPI), which will improve the precisionof existing data and will enable future higher-contrast observations of exoplanets. In addition toimproving the sensitivity of legacy data, the proposed work will provide timely support for the fundedGPI upgrade beginning in 2020 and returning to science operations with commissioning of GPI 2.0 in2023.Study Design: Speckle noise is created as starlight passes through non-uniform atmosphere and optics,producing a stellar PSF that varies with time. Atmospheric speckles can only be corrected by improvedadaptive optics hardware. This leaves “quasi-static” speckles caused by non-common path aberrationswithin the instrument optics. Quasi-static speckles change slowly over timescales of minutes to hours [3],resulting in an effect that varies both chromatically and temporally.This project aims to model, and subsequently remove, these quasi-static speckles using thenon-parametric technique of principal component analysis (PCA). PCA identifies “principalcomponents” as linear combinations of input parameters, producing a dimensionally reduced result whichidentifies the strongest predictors of the features of that data. These results can be used to subtract thequasi-static speckles directly. Previous applications of PCA to high-contrast imaging (e.g. [4, 5]) havefocused on subtracting the entire stellar PSF, both atmospheric and quasi-static speckles, which is usefulfor recovering target signal but results in improvement for only the dataset it’s applied to. The techniqueof applying PCA to isolated quasi-static speckle noise has never been successfully applied toexoplanet high-contrast imaging, but will result in a more flexible, broad correction for this type ofnoise. By characterizing quasi-static speckle behavior and evolution over given epochs and wavelengths,corresponding corrections can be applied not only to the training data, but any data of matchinginstrument, epoch, and wavelength. This method may also reveal stable speckle behavior which is presentat all times and wavelengths, and can be universally subtracted. Once applied, the precision of legacy datais expected to improve by one order of magnitude, and this speckle subtraction will allow future GPIobservations to achieve greater contrast by approximately two orders of magnitude [6], making importantsteps towards accessing Earth-like planets. The phases of this project are outlined below.Phase I: First, I will build a training dataset of GPI science images containing isolated speckle noise,which can be achieved by subtracting a noiseless model of the stellar PSF from all training data. This willleave only unexplained noise behind, the primary component of which is quasi-static speckle noise.Phase II: Grouping the training dataset over discrete time increments and wavelength intervals, I willapply PCA to the training data. The results of this analysis can be used to subtract an estimate of thestable components of quasi-static speckle noise from the data. Phases I and II will take place during years1 and 2 of graduate school, which is well-timed to inform the concurrent GPI upgrade.Phase III: Then, I will quantify the effectiveness of this correction procedure by measuring theimprovement of the intrinsic noise present in each subtracted image, as well as by performing injection-recovery tests with simulated planet signals.Phase IV: Finally, once these results have been verified, I will develop and release an open-sourcecodebase for quasi-static speckle subtraction, intended for use by scientists working with GPI data. PhasesIII and IV will take place during years 3 and 4 of graduate school, which will align with thecommissioning of GPI2.0 allow for my results to be folded into its data reduction pipeline.One anticipated challenge associated with Phases I and II is that speckle noise is difficult toisolate in legacy GPI data, either because accurate reference PSFs cannot be generated, or because thedata contains systematics which may confuse the subsequent analysis. In this case, since GPI will bepresent at Notre Dame for upgrades, I will be able to collect data directly from the instrument using thetelescope simulator operated by the Chilcote group. By allowing more control over observing conditionsand precise knowledge of the input PSF, data taken using the telescope simulator will enable a cleanerfirst-step analysis, allowing more robust treatment of the legacy data when it is later re-introduced.Another anticipated problem is that GPI2.0 goes on-sky before Phase IV concludes. However,modifications to the processing pipeline can still be made after science operations commence since post-processing can be retroactively applied. Additionally, even if completion of Phase IV lags, the robustnessand impact of this technique will be well understood from Phase III, and observations can be planned inanticipation of the correction tools of Phase IV being completed in the future.Intellectual Merit: The importance of increased contrast for high-contrast imaging campaigns is crucialeven beyond exoplanets, with implications for the direct imaging of all astrophysical objects at smallangular separations from a comparatively bright source — including circumstellar disks, stellar winds, orjets emitted from neutron stars, pulsars, or black holes. This project is a high-impact, far-reaching, andlow-cost avenue to increasing the science yield of existing direct imaging instruments, increasing thesensitivity of extant and future data without the need for the expensive and prolonged development ofnew instrumentation. Additionally, while the proposed solution will be built specifically for GPI, it can beadapted to interface with other ground-based high-contrast imaging instruments, including SPHERE onVLT and CHARIS on the Subaru Telescope. A software-based speckle subtraction method also hasstrong implications for the development of space-based direct imaging missions, such as the Nancy GraceRoman Space Telescope (formerly WFIRST), as speckle noise is similarly dominant in space-based directimaging [7]. The open-source release of my work will facilitate broad advancement andcollaboration across astrophysics subdisciplines and different instrument teams.Years of previous research experience, including publishing papers, presenting at conferences,giving talks, and directing analysis, have prepared me to effectively execute the proposed work. I haveyears of experience with data analysis and visualization with Python, and as part of my work with CERNand GPI have worked with large datasets and distributed computing. I currently work on GPI withProfessor Jeffrey Chilcote at the University of Notre Dame, so I am familiar with the instrument and amprepared to hit the ground running. I will also benefit from the technical expertise and support of theinternational GPI collaboration as I develop this project. Given the large volume of legacy data which willbe analyzed during this project, as well as the computationally demanding nature of PCA, I will requireaccess to high-performance computing facilities such as the Notre Dame Center for Research Computing.Broader impact: The NSF GRFP will support me to pursue high-impact research alongsidecommunity engagement. Alongside this speckle suppression project, I will establish a peer mentorshipprogram at my graduate institution, as well as develop curricula and workshops to engage publicelementary students in space science, both of which I detail in my personal statement. I will integratethese engagement efforts with the GPI Outreach team to create science communication materialsconveying the excitement of squinting through stellar glare to find new worlds orbiting underneath.[1] National Research Council 2010, New Worlds, New Horizons in Astronomy and Astrophysics[2] Marois, C., Doyon, R., Nadeau, D. et al. 2003, EAS Publications Series, 8, 233-243[3] Hinkley, S., Oppenheimer, B., Soummer, R. et al. 2007, ApJ, 654, 1, 633-640[4] Wang, J., Ruffio, J.-B., De Rosa, R., et al. 2015, Astrophysics Source Code Library, ascl:1506.001[5] Soummer, R., Pueyo, L., Larkin, J. 2012, ApJL, 755, 2[6] Soummer, R., Ferrari, A., Aime, C. et al 2007, ApJ, 669, 1, 642-656[7] Brown R., Burrows C. 1990, Icarus, 87, 2, 484-497"
38.0,"Motivation: Since the mid-1970s, global natural gas production has steadily risen to its current all-timehigh, and is projected to continue until at least 2040.1 Although natural gas is a cleaner burning fuel thangasoline, its implementation in the transportation sector has been stymied by its significantly lowervolumetric energy density.2 Densification strategies, including liquefaction or high-pressure storage, haveinherent safety and cost issues that are widely viewed as prohibitive for passenger vehicles.3Introduction: Adsorbed natural gas systems that employ porous materials, such as metal-organicframeworks (MOFs) and covalent organic frameworks (COFs), have received considerable attention aspotential alternatives to higher pressure and/or cryogenic storage methods.4,5 In these systems, favorableinteractions between the gas and the porous solid increase the amount of gas that can be stored at a giventemperature and pressure as compared to an empty tank.4 However, the non-molecular nature of theseextended structures makes solubility non-existent, reducing compatibility with post-syntheticfunctionalization and modification that can be leveraged to increase gas adsorption capacity and bulkmaterial properties, such as density and thermal conductivity.Hybrid metal-organic or all-organic molecular analogs of these porous materials, coined porouscoordination cages (PCCs) and porous organic cages (POCs), respectively, directly address this issue whileretaining the highly sought after permanent porosity and tunability of their expanded, 3-D counterparts.6,7PCCs often contain open metal sites that provide favorable interactions for increased gas storage ascompared to all-organic structures.6 However, these molecules tend to display surface areas that pale incomparison to MOFs and POCs. On the other hand, POCs display surfaceareas that are on par with many MOFs, but lack the tunable metal cation-gas molecule interactions seen in hybrid metal-organic systems.8 Toaddress these issues, I propose the design, synthesis, and application ofnovel porous organic cages with integrated metal sites toward the selectiveadsorption and/or storage of small molecules.Preliminary Results: Although porous organic cages have been heavilyinvestigated over the past decade, the study of the high-pressure storage ofgases in these materials is still well in its infancy.7 As a result, relativelylittle is known about their utility as gas storage materials. Similarly, post-synthetic metalation of these systems to introduce sites with catalyticFigure 1: Known POCactivity or selective gas adsorption has been unexplored. The standardtargeted for preliminary workmetric for porous materials, gravimetric surface area, is a simplisticrepresentation of a material’s ability to store a gas. While the high surface areas that POCs have displayedprovide a basis for using such materials as gas storage media, investigations into these materials for thespecific task of gas storage is surprisingly limited. Incorporation of metal chelating sites within molecularcages will allow for the precise insertion of a specific metal post-synthetically. Metal cations can play animportant role in tuning metal-gas interactions, which is necessary for creating a material for selective gasadsorption or high-pressure storage. I targeted a known POC based on triformylphenol and 1,2-diaminocyclohexane, where a half-salen unit is formed when the cage is constructed (Figure 1).9 Confirmedvia SEM-EDX and XPS, initial results show coordination of a Ni2+ center into the cage’s structure, whileretaining permanent porosity (Figure 2). Further gas adsorption studies to better understand the selectivityof the metal-integrated POC are currently underway.Aim 1: Create a library of ligands and cage topologies that are conducive to metal integration. Inspirationfor this approach can be drawn from recent literature on the reported topologies of cages, where the moststraightforward methods involve imine or boronic ester formation to create the covalently linked cage.Although specific angles must be considered within the ligands in order to access these desired topologies,functional groups and sizes of the ligands are typically tunable. Understanding this, cages will befunctionalized and built around traditional multi-dentate ligands, such assalen, catechol, and 2,2’-bipyridine to form metal complexes after cageisolation.Aim 2: Isolate porous organic cages and introduce metals to theirchelating sites. Typically, solution-based syntheses produce cages. Thepurity of isolated cages can be confirmed through several techniques dueto their molecular nature, most readily being high-resolution massspectrometry, NMR, and IR spectroscopy. For more complex cages, suchas cages containing chiral centers, additional efforts will be put forth toobtain diffraction quality single crystals, utilizing techniques such asvapor diffusion and liquid/liquid diffusion, to further confirm cage Figure 2: Model representingformation. After successful isolation, the porous organic cages will then the integration of Ni2+be introduced to metal salts to obtain their metalated counterparts via solvothermal methods and solid-statemetalations. A plethora of techniques are available at the University of Delaware for the characterizationof the metalated POCs, including SEM-EDX, XPS, EPR, and the previously mentioned techniques.Aim 3: Perform gas adsorption studies and identify the roles that both cages and metal-sites play inselective gas uptake and storage. Gas adsorption studies will be performed on both the base cage and themetalated cage to decipher the interplay of porosity and selective gas uptake. Surface area analyses will beperformed in the Bloch Lab, using both CO and N as probe molecules, along with systematic high-2 2pressure gas storage studies (hydrocarbons, H , etc.) and enthalpy of adsorption calculations.2Intellectual Merit: While small molecule storage has been heavily studied in extended frameworks likeMOFs and COFs, much less is known for porous molecular materials. These materials retain the soughtafter permanent porosity of expanded frameworks, as well as impart solubility that can lead to advantageouspost-synthetic modification. This project will elucidate how POCs can be utilized as gas storage media anddevelop the novel field of metalated POCs, including their design, synthesis, and utilization as adsorbednatural gas materials.Broader Impacts: My proposed project has opportunities for collaborations that I will pursue heavily tobetter understand these systems. Collaboration with computational groups will help predict gas storagecapacities and suggest more favorable interactions based on metals and ligands, and work with catalysis-focused groups can utilize my metalated cages as a homogenous setting for catalytic reactions. Just asimportantly, I will continue to mentor undergraduate researchers and first year graduate students to teachthem essential and advanced laboratory skills. I will share my findings at local, national, and internationalmeetings when they are deemed safe, and until then, I will present my work virtually and continue to publishresults.References1. EIA, ""World Energy Outlook 2019"", 2019, https://www.iea.org/reports/world-energy-outlook-2019/gas2. EIA, “How much carbon dioxide is produced when different fuels are burned?”, 2019,https://www.eia.gov/tools/faqs/faq.php?id=73&t=113. DOE, “Natural Gas Fuel Safety”, https://afdc.energy.gov/vehicles/natural_gas_safety.html4. Mason, J. A.; Veenstra, M.; Long, J. R. Chem. Sci. 2014, 5, 32-51.5. Das, S.; Heasman, P.; Ben, T.; Qiu, S. Chem. Rev. 2017, 117, 1515–1563.6. Gosselin, A. J.; Rowland, C. A.; Bloch, E. D. Chem. Rev. 2020, 120, 8987–9014.7. Hasell, T.; Cooper, A. I. Nat. Rev. Mater. 2016, 1, 16053.8. Zhang, G.; Presly, O.; White, F.; Oppel, I. M.; Mastalerz, M. Angew. Chem. Int. Ed. 2014, 53, 1516-1520.9. Petryk, M.; Szymkowiak, J.; Gierczyk, B.; Spólnik, G.; Popenda, Ł. Janiak, A.; Kwit, M. Org.Biomol. Chem. 2016, 14, 7495-7499."
39.0,"Introduction: Over the past 130 million years, flowering plants have evolved a variety of visual andchemical cues that mediate species’ interactions. The diversity of color phenotypes in flowers has been thesubject of many ecological studies and the biosynthesis and regulation of the main compounds responsiblefor pigmentation is well understood1,2. These compounds are well-known for their role in pollinatorattraction and additionally have many important biological functions that have been described (e.g.,allelopathy, lignification, protection from UV radiation)3. However, despite the significance ofpigmentation in plant growth, development, and reproduction, the role of pollen color remains unclear.About 75% of flowering plant species have yellow or white colored pollen, though pollen may alsobe pigmented red, blue, purple, or black4,5. The composition of specialized metabolites that occur in pollenacross several taxa (e.g., phenolic compounds, alkaloids, terpenoids) have also been described6. Several ofthese compounds are known to be important for pollen development, pollen germination, pollen tubegrowth, and protection from abiotic stress (i.e., temperature, UV)7.ARecent work from Dr. Shu-Mei Chang’s lab at theUniversity of Georgia, Athens (UGA), has discovered pollen colorpolymorphism in wild geranium, Geranium maculatum. Fieldobservations along the Appalachian Mountain region show thatB C Dpurple and yellow pollen color morphs persist in different ratiosalong an elevational cline8. Though pollen color polymorphismConfocal LC-MS/MShas been observed in other plant species, the ecological functionFigure 1. Experimental Design. (A) Purple &and adaptive value of this trait is still unknown9-12. Therefore, Iyellow pollen color morphs. (B) iNaturalistpropose to examine the geographic distribution patterns of pollen distribution data by community scientists. (C)Confocal microscopy & LC-MS/MScolor morphs, characterize their phenotypes, and evaluate thecharacterization of pollen phenotypes. (D)functional role of this trait in an ecological context (Fig. 1). Reproductive trait evaluation under abiotic stress.Aim 1: Determine the distribution of pollen color by integrating field surveys and community scienceIn native G. maculatum populations, dark pollen color morphs have been observed at higherelevations8; a trend that has not been observed in other plant species9-12. iNaturalist is an online platformand smartphone application that allows anyone to record observations in nature. To date, there are over13,000 records of G. maculatum and over 1,600 individuals that have made observations across the US. Todetermine if there is a correlation between elevation and pollen color, I will use this data to analyze theoccurrence of purple and yellow pollen color morphs in a logistic generalized linear model with latitudeand elevation predictors, as described by Austen et al12. I expect to see a positive correlation betweenelevation and pollen color intensity along an elevation gradient based on previous field observations.Furthermore, I will geo-reference 20 populations of G. maculatum along an elevation gradient toserve as collection sites for my study. To supplement my own collection material, I will advertise this studyon the Ecological Society of America listserv and on social media pages of native plant societies to recruitcommunity scientists. I will then create collection kits with an overview of the project and a guide topropagule collection to mail to each individual, who will harvest from their respective site and mail theircompleted kits to UGA. Clear instructions will be provided to each collector on how to image the populationand gather one representative specimen to allow confirmation.Aim 2: Characterize reproductive traits and pollen phenotypes of G. maculatum populationsPollen features that vary among pollen color morphs can have a significant impact on male fitnessof a plant13. Thus, I will examine the morphological and biochemical characteristics associated with pollencolor. I will propagate G. maculatum from rhizomes from Aim 1 in the greenhouse at UGA and generatean F2 population that segregates in pollen color. Upon flowering, reproductive traits (e.g., flower number,flower size, flower color, pollen color, seed set) for each plant will be described. I will collect and imagepollen by confocal microscopy at the UGA Biomedical Microscopy Core (BMC) for characterization ofpollen features (e.g., size, surface ornamentation). Then, I will utilize the Proteomics and MassSpectrometry facility at UGA to quantify specialized metabolite accumulation in anther tissue and pollenby liquid chromatography tandem mass spectrometry (LC-MS/MS). Specialized metabolites havepreviously been characterized for G. maculatum pollen and it was found that the metabolite profile differedsignificantly among four collection sites6. I will further characterize the correlation between pollen colorintensity and metabolite profile to identify compounds that are important for pollen performance in Aim 3.Aim 3: Evaluate trait-correlated tolerance to abiotic stressTo evaluate the pollen performance of the F2 individuals, I will measure pollen viability,germination, and siring success under different conditions. I will expose different colored pollen to agradient of temperature and light intensity treatments (mimicking field conditions), and assay for pollenviability and germination in vitro, where pollen germination rates and pollen tube length will be recorded.Additionally, I will observe the siring success of each pollen color morph in vivo.I will then conduct common garden experiments to evaluate fitness in field conditions. ReplicateF2 populations that contain identical genotypes (by splitting rhizomes) will be planted in common gardenplots at the Highlands Biological Station in North Carolina (high elevation) and the UGA State BotanicalGarden (low elevation). Floral/reproductive traits (as described in Aim 2) and transplant survival will berecorded for each population over the span of 2 years. I hypothesize that dark pollen individuals will havegreater reproductive success in high elevation due to specialized metabolites that confer abiotic stresstolerance. I expect that light pollen individuals will have decreased transplant survival but compensate tosome extent by producing more flowers with higher seed set; a maternal reproductive strategy described byKoski et al. in Campanula americana13. I will also collect the same subset of individuals from each gardenfor metabolite analysis (as described in Aim 2) to determine whether there is a genetic-by-environment(GxE) effect on their profiles.Intellectual Merit: My previous research experience in floral development, pollen biology, biochemistry,and molecular biology makes me uniquely positioned to lead this interdisciplinary and community-sciencesupported endeavor. Under the guidance of Dr. Chang, an expert in plant ecology and plant mating systems,I will expand our limited understanding of the role of pollen color polymorphism as a strategy forreproductive success. Additionally, previous graduate students in the Chang group have led community-science research endeavors using iNaturalist, making this an ideal environment to build upon thisinfrastructure. In taking advantage of the biological research stations available to graduate students at UGA,this study will be the first of its kind to evaluate pollen color morph-dependent fitness in an ecologicalcontext. Moreover, coupling biochemistry and ecology approaches, I will generate a comprehensiveunderstanding of the role that specialized metabolites play in pollen germination and reproductive success– information that has implications in both agricultural production and native plant conservation.Broader impacts: I foresee my graduate research as a vehicle for mobilizing community scientists,providing educational opportunities to underserved communities, and improving diversity in academia. Iwill work closely with K-12 instructors to develop plant biology lessons with field work and familyengagement components to create community awareness of these relevant topics. Students and theirfamilies will be given demos on how to use the iNaturalist platform to encourage outdoor activity andparticipation in community science. In collaboration with Max Barnhart, a graduate student in the IntegratedPlant Sciences program who is the lead PI on an American Society of Plant Biology (ASPB) sciencecommunication grant, I will create and distribute a zine to provide an overview of my work to the generalpublic and the community scientists involved. I will also share my experience with the scientific communityby developing a workshop entitled “Incorporating Community Science Into Your Research Program” forthe annual ASPB conference. During this workshop, I will discuss the various platforms available forbuilding community science projects, demonstrate how to navigate and utilize these platforms, and lead anexercise on brainstorming ways to engage community scientists in your research.References: 1Rausher MD. Int. J. Plant Sci. 2008. 2Grotewold E. Annu. Rev. Plant Biol. 2006. 3Jiang et al.Plants. 2016. 4Lunau K. Plant Syst. Evol. 1995. 5 Miller et al. Optics & Laser Tech. 2011. 6Palmer-Younget al. Ecol. Monogr. 2018. 7Muhlemann et al. PNAS. 2018. 8Udell-Perez R. Field Obs. 9Jorgensen &Andersson. New Phytol. 2005. 10 Koski & Galloway. New Phytol. 2018. 11Wang et al. Evolution. 2018.12Austen et al. Ecology. 2019. 13Koski et al. J Evol. Biol. 2020."
40.0,"Novice programmers who are writing code encounter frequent challenges, which they often attempt toaddress by searching online to learn new concepts or debug their code [3]. However, novice programmersrarely receive explicit instruction on how to effectively resolve programming challenges with onlinesearch. Seeking help through online search is a critical self-regulatory skill for students, both in class andafter graduation, but professional programmers agree that it can be difficult to learn [8]. Some prior workhas investigated the challenges that professional developers face when searching and how to supportthem. However, there is little research about how programmers learn to use online search, how to teachsearch effectively, or how adaptive tools can support this process. Further, prior work has been limited tocollecting user search queries and developer surveys, without using the programmer's code and errors tounderstand their context and the reason for their search.My research goal is to 1) understand the strategies that undergraduate novice programmersuse to search for help online when writing and debugging code, and 2) explore how to design adaptivelearning environments that support students in learning effective online search strategies.Building on Prior Work: Collecting data from novices’ programming environments has become popularin Computer Science (CS) Education due to their ability to provide granular views into the programmingprocess via incremental code snapshots. These code snapshots have been leveraged to extract featuressuch as compiler error encounter rate and error resolution time, which are used to understand novicedebugging behaviors. In addition, code snapshots can be used in the design of interventions thatdynamically provide feedback during the programming process based on current and past code contexts.It has been shown that intelligent tutoring systems (such as augmented programming environments) thatprovide timely automatic feedback can positively affect learning [5]. Work has been done on augmentinglearning environments with tools such as web browsers that filter search results to be moreunderstandable by novices [2,7]. However, many of these tools focus on improving search results: notools have been developed with a focus on improving code search behavior and impartinggeneralizable long-term search skills.RQ1: What barriers do students face and what strategies do they employ when using web-based searchsystems to write and debug code? In Year 1, I will conduct exploratory lab and classroom studies toidentify common barriers that novices encounter and strategies that they use for online search duringprogramming problems. My goal is to discover what unique problems novices encounter when usingonline search, inform pedagogy on how online search should be taught, and learn what features toconsider when designing a tool to support better search behavior. In small scale lab studies during thefirst semester, novice programming students will be asked to solve programming problems appropriate totheir skill level while having access to online search. I will collect think-aloud protocols as students work,pre- and post-surveys, logs of students' search behavior, and fine-grained code snapshot logs. Classroomstudies of NCSU’s introductory computing (CS1) course during the second semester utilize augmentedprogramming environments and provide a large volume of search and code snapshot data, which willallow us to test if the findings from the lab studies are generalizable to the classroom. Using this data, wecan evaluate how search behavior from beginner programmers may differ from or align with prior workon professional developer behavior, such as in query style, query reformulation rate, and search sessionlength. Building on prior work on using code snapshots to track compiler error resolution [1], we willexplore how to track students’ success in using online search to resolve errors. This will allow us toidentify which students are struggling, where they are struggling, and what search strategies are effective.RQ2: How can we give students accurate and timely feedback on how to improve the effectiveness oftheir search queries? In Year 2, I will perform design-based research of a learning environment thatwill provide automated support to students to improve their search skills. While the ultimate designof the system will be shaped by student and teacher needs identified in Year 1, the system will supportdeveloping students’ skills in the key phases of the search process. The key phases of search are derivedfrom theories of help-seeking (e.g. [4]): recognizing the need for help, gathering relevant information,formulating an effective query, identifying an effective source of help, and integrating the help into code.For example, consider a student who is stuck on an error and whose previous search attempts were notsuccessful. The system could then suggest an effective help-seeking strategy that was identified in RQ1,such as query reformulation. The system will offer adaptive help, responding to a student’s current codeand error message. I will achieve this by building on foundational work by the HINTS lab at NCSU onusing program code to create adaptive feedback systems [6]. My current membership in the HINTSlab puts me in a unique position to design this system.RQ3: What effect does timely automated feedback on search queries have on novice programmingbehaviors and their ability to use web-based search to resolve future problems? After multiple rounds ofdevelopment in Year 2, in Year 3, I will deploy the system and perform classroom studies to measurethe impact this tool has on novices’ search and programming behavior. These classroom studies willinitially take place over the course of a semester in NCSU’s CS1 course. The effectiveness of theintervention will be measured by the change in student programming performance and retention of help-seeking behaviors over time (as evidenced by features of successful help-seeking identified in RQ1, suchas query styles and error resolution rate). In addition to quantitative metrics, qualitative feedback bystudents at the end of the semester will also inform the system’s overall impact.Intellectual Merit: This project will provide novel insight on the strategies that undergraduate noviceprogrammers use to search for help online. By extending prior work on programmer search behaviorsthrough the novel methods of observing program and error state, we can gain granular information aboutthe contexts in which novice programmers seek help. In exploring effective search strategies (RQ1), wecan inform the design of better pedagogy for teaching online search. The feedback methods designed inRQ2 and evaluated in RQ3 will inform future designs of intelligent tutors for help-seeking. This projectwill provide a base for future work on understanding programming help-seeking behavior, and themethods discovered could be extended to learner populations beyond novices.Broader Impact: In many CS programs, searching for code help online is a skill that is not explicitlytaught, yet is expected of students in upper-division courses and after graduation. Explicit pedagogyaround web search will normalize help-seeking behavior and reduce impostor syndrome. Automatedfeedback systems allow for the refinement of these skills even if an instructor is unavailable. This projectwill inform future online search pedagogy and the design of learning environments that reinforce theseskills. By addressing these two issues, this project will help make CS more accessible.[1] Jadud. “Methods and Tools for Exploring Novice Compilation Behaviour” ICER‘06 [2] Lu et al. “ ”Information Retrieval Journal ‘17 [3] Muller, et al. Exploring Novice Programmers' HomeworkPractices: Initial Observations of Information Seeking Behaviors SIGCSE ‘20 [4] Nelson-LeGall. “Help-Seeking: An Understudied Problem-Solving Skill in Children” Developmental Review ‘81 [5] Nesbit, etal. “Intelligent Tutoring Systems and Learning Outcomes: A Meta-Analysis” Journal of EducationalPsychology ‘14 [6] Price, et al. “iSnap: Towards Intelligent Tutoring in Novice ProgrammingEnvironments” SIGCSE ’17 [7] Venigalla et al. “StackDoc - A Stack Overflow Plug-in for NoviceProgrammers that Integrates Q&A with API Examples” ICALT’19 [8] Xia, et al “What do developerssearch for on the web?” Empir Software Eng ‘17"
41.0,"Introduction: Traditional stair construction, in which stair flights are rigidly connected to thestructure at both ends (“fixed-fixed” connections), has been shown to cause damage to stairs andsurrounding structural members during earthquakes due to stairs being stretched and compressed dueto the relative displacement between a building’s floors1. (See Figure 1.) In response to this, alternativedesigns (“fixed-free” connections) have been developed2 that permit stairs to accommodate the relativedeformation between stories by detaching the stair at one of the floors. Recent tests2 have confirmedthat fixed-free connections have the potential to eliminate damage due to seismic forces beingdistributed to stairs, but further testing is needed to understand these novel designs. Scissor stairs, acommon configuration in which the stair turns back on itself at a mid-story landing, have not yet beentested in conjunction with fixed-free connections.Stairs have been shown to affect a structure’s seismicresponse3, so it is essential to investigate scissor stairs withfixed-free connections not only in isolation but also interactingdynamically with a building. One concern is that releasingdegrees of freedom for fixed-free connections may cause awhiplash effect due to the stair’s mass being less constrained,damaging nearby building components. Additionally, thiswhiplash effect may cause undesirable torsion in the building.Therefore, a key challenge in designing fixed-free stairs is toremove enough restraints to permit some movement whilepreventing completely free oscillation.I propose to observe and quantify the dynamiccharacteristics fixed-free scissor stairs and their effect on thelateral response of buildings. Three variations of fixed-freeconnections will be considered: 1) removing all connectionsfrom the lower end of the stairs so that it may slide freely on itslanding, 2) use slotted connections to allow some movementwhile restraining most movement, and 3) using a sliding hangerconnection to allow translation in all three dimensions withoutleaving the stair completely unattached.Hypothesis: Fixed-free stairs will prevent damage that traditionally constructed stairs would otherwisesuffer by allowing relative movement between stairs and floors and dissipating energy through friction.Research Goal 1: Develop Models for Stairs with Fixed and Free ConnectionsI will model four scissor stair configurations: three with fixed-free connections and a control casewith traditional, rigid connections. Because building prototypes and performing dynamic testing istypically cost-prohibitive, I will develop finite element models of the stairs and their connections usingthe finite element program LS-DYNA. An essential feature of LS-DYNA is its ability to model frictionand the interaction between components that come into contact with one another4. The models will besubjected to cyclic and dynamic loading protocols to identify the force-deformation behavior of thestair system under earthquake loading.Research Goal 2: Model Scissor Stairs in StructuresUsing the methodology described by Wang et al. (2015)5, I will represent fixed-free stairs instructures by developing a system of nonlinear springs using the force-deformation relationships foundin Goal 1, which will be then integrated into full-building structural models to observe the effect offixed-free stairs on the seismic response of buildings. For this task, I will use the structural finiteelement framework OpenSeesPy, which is more appropriate for modelling an entire building. In orderto evaluate the effects of fixed-free stair connections, I will subject the models to dynamic loadingusing a variety of ground motions, then compare member forces and nodal displacements betweenmodels without stairs, with traditional stair connections, and with fixed-free stair connections.Research Goal 3: Improve Full-Building Models by Comparing to Shake Table TestWorking under Professor Keri Ryan at the University of Nevada, Reno, I will participate in theshake table testing of a full-size, 10-story timber building at the NSF’s National Hazards EngineeringResearch Infrastructure (NHERI) facility at UC San Diego in 20216. Because this structure will includescissor stairs with various types of fixed-free connections, this will be an unprecedented opportunityto gather physical data showing the effects of stair-structure interaction. Using this data, I will developa model of the building including the stairs using OpenSeesPy, which I will validate and calibrate withdata from the shake table test. Discoveries from this test will allow me to improve the stair-structuremodels developed for Goal 2.Intellectual Merit: Although prior research efforts have investigated the performance of fixed-freeconnections2 and the interaction between scissor stairs and structural systems5, no study has yetaddressed coupling the two. Previous tests have demonstrated the potential of fixed-free connectionsto mitigate damage in stairs2 sufficiently to warrant further investigation. Accurately characterizing thenonlinear force-deformation relationship of scissor stairs with fixed-free connections will facilitatefuture research into the seismic response of buildings. Furthermore, developing a nonlinear springmodel in Goal 2 will be an important step in helping practicing engineers integrate the results of thisresearch into design practice.This research will also advance the quality of computational analysis in structural engineering. Tomy knowledge, I will be the first student at the UNR to extensively use OpenSeesPy, an adaptation ofthe finite element framework OpenSees for Python. Python has many data science libraries that willimprove analysis of data from shake table tests and computational models. Linking Python andOpenSees will improve the quality of structural research by facilitating pre- and post-processing ofdata from tests and simulations.Broader Impacts: Failure to account for differential movement between floors has led to stairscollapsing in the recent Wenchuan and Christchurch earthquakes2. Fixed-free connections can preventsimilar collapses in the future, but first their effects on building response need to be considered beforethis life-saving technology can be fully implemented. Since stairs are the primary means of egress froma building during a catastrophic event, protecting stairs from collapse during seismic events is anessential task for preventing loss of life. Furthermore, better modelling of stair-structure interactionwill ensure safer design and reduce damage, which will in turn facilitate rapid recovery after disastersby reducing building downtime and preventing economic losses.In order to promote the use of safer stair systems, I will disseminate my findings throughpublications in structural engineering journals, the NHERI TallWood outreach webpage, and throughseminars hosted by UNR’s Earthquake Engineering Research Institute chapter. The NHERI TallWoodproject will give me opportunities to work closely with industry collaborators to further develop andpromote fixed-free connections. I will also present a simplified version of my research to K-12classrooms to foster youth interest in earthquake research.References:[1] D. Bull, (2011). Canterbury Earthquakes Royal Commission.[2] C. Black, et al., (2020). 17th World Conference on Earthquake Engineering.[3] J. Zhu, et al., (2011). Applied Mechanics and Materials.[4] https://www.lstc.com/products/ls-dyna[5] X. Wang, et al., (2015). Earthquake Engineering & Structural Dynamics.[6] K. Ryan, et al., (2020). Colorado School of Mines."
42.0,"which humans can run experiments and the enormous size of the search space.1 Recently, the materials-design loop has been accelerated through several different mechanisms, including robotic high-throughputexperimentation (HTE), atomistic simulations, and machine learning (ML).1 While each of these techniqueshas independently shown promise for accelerating discovery, an approach that applies all threeharmoniously would revolutionize chemical research with wide-ranging implications, enabling faster andcheaper discovery of new pharmaceuticals, catalysts, and photovoltaic devices. I aim to advance this effortin my own research by coordinating the interplay between these three methods for the discovery and designof new dye molecules. Dyes are a suitable class of molecules for testing an autonomous, integrated designplatform because they have several readily measurable properties that must be optimized simultaneouslyfor use cases ranging from solar cells to medical imaging. Specifically, I am focusing on the followingobjectives: (1) developing ML models to predict UV-Vis absorption and emission spectra accuratelygiven a dye molecule and solvent pair, (2) creating a generalizable, automated active machinelearning framework to improve the prediction models, and (3) utilizing this framework to design anovel near-infrared (NIR) dye for biomedical sensing and diagnostics.Objective 1 - Model Development for UV-Vis Spectra Predictions: Accurate prediction of UV-Visoptical properties is essential to dye design for any application. Previous work toward predicting UV-Visspectra with ML has mostly consisted of the simpler task of predicting two scalars, the wavelengths of thepeaks of maximum absorption and emission (𝜆 and 𝜆 )2, and has been limited by data sparsity. Sinceabs emstarting my work with Prof. Rafael Gómez-Bombarelli in January, I have addressed this limitation bycollecting all openly accessible UV-Vis data from seven online repositories (29,811 measurements in total)and standardizing it into a consistent format. I then used a combination of a directed message-passing neuralnetwork (DMPNN)3 and a feed-forward neural network to predict a value for 𝜆 given an input molecule-abssolvent pair and an analog of 𝜆 computed with time-dependent density functional theory (TD-DFT).absUsing this method, my model has achieved a test-set mean absolute error (MAE) of 8.68 nm (a 17%reduction in error over the previous best model) on the largest dataset for which ML predictions havebeen published.2 My first step toward extending this method to predict full spectra will be to train mymodel to predict the peak widths and intensities for each 𝜆 using the data I assembled. I foresee the limitedabsquantity of available data presenting a challenge for predicting full spectra since the majority of openlyaccessible data contains only 𝜆 values for each molecule-solvent pair. I will address this issue with aabspretraining strategy in which I train my model with lower-fidelity data and use the resulting neural networkweights as the initial weights when training my final model (as opposed to a random initialization). I willthen estimate the epistemic and aleatoric uncertainty in my model’s predictions using a deep ensemblingapproach.4 Finally, I will replicate the previous steps for predicting emission spectra. My accurate modelsfor predicting absorption and emission spectra will aid experimentalists in choosing which moleculesto test, even before I further automate this process in the following objective.Objective 2 - Active Learning: My models’ abilities to make predictions with corresponding uncertaintieswill fulfill an important prerequisite for implementing active learning (AL), which improves models byfocusing the sampling of new data on molecules with high uncertainties in their predictions. The additionalcomponents needed for active learning are (1) a set of new molecules from which to sample, and (2) amethod of measurement for each sample. My experimental collaborators in Prof. Klavs Jensen’s group havecreated (1) by extracting a list of 7 million purchasable compounds from chemical vendor websites. Further,they have created a method for (2) by building an HTE apparatus for measuring 96 UV-Vis spectrasimultaneously. Since TD-DFT calculations are faster and cheaper than experiments, I propose using theseto augment the strategy for (2) by reducing the number of necessary experiments. I will design acomputational framework to automatically deploy calculations for molecules with a high epistemicuncertainty and retrain my models using this new data. From molecules that still have high uncertainty afterthe calculations, my system will use the uncertainty values along with molecular similarity to choose 96molecules to recommend for measurement in the HTE apparatus. My models will automatically receivedata from the experiments and repeat the previous steps iteratively until their predictive performances reachasymptotes of aleatoric uncertainty. The proposed AL framework integrates TD-DFT and HTE in anautomatic fashion, which will enable significant time and cost savings and will be readilygeneralizable to many areas of chemistry and materials science research.Objective 3 - Design of a Novel Dye for Biomedical Imaging: Once I am able to demonstrate that myML models are sufficiently accurate over a large region of chemical space, I will adapt my AL frameworkto design novel molecules with optimized properties for biomedical imaging. Specifically, it is favorablefor dyes to have absorption and emission peaks in the NIR-II range (1000-1700 nm) because this range hasdeeper tissue penetration compared to visible or shorter-wavelength NIR-I light.5 High Stokes shift (𝜆 -em𝜆 ) and high quantum yield are also desirable.5 Additionally, I will leverage the ongoing work of myabscollaborators in Prof. Bill Green’s group who are predicting solubility, toxicity, and photodegradation, asthese are also important properties for this application.5 I will employ the generative models of Jin et al.6 tocreate new molecules out of substructures that are likely responsible for desired properties of interest inknown molecules. Next, I will make predictions on these new molecules with my ML models. I will thenmodify my AL framework to explore the new chemistries proposed by the generative models; it will deployTD-DFT calculations as necessary for molecules with uncertain predictions and ultimately recommendnovel molecules with predicted properties in the target ranges to my experimental collaborators in theJensen group. They will use automatic retrosynthesis methods7 to synthesize the novel compounds and willthen use their HTE apparatus to test which proposed molecules indeed have the desired properties. Finally,I will propose the best-performing molecules to Prof. Angela Belcher’s group for further study and in vivotesting. If successful, this strategy could serve as a blueprint for combining experiments, theory, andML for multi-objective molecular design across the field of chemistry.Intellectual Merit: Design problems in chemistry and materials science often suffer from a combinatorialexplosion of configurations to explore, which makes solution of these problems intractable by brute force,or with HTE, physics-based calculations, or ML alone. By using all three methods simultaneously andautomating the interactions between them, my work will be an advancement toward a “closed-loop” systemthat can explore massive chemical spaces with minimal need for human intervention beyond thespecification of design objectives. Conducting my work at MIT gives me the opportunity to collaboratewith experts who have proven records of integrating chemistry and computer science methods, and it givesme access to computing resources to run atomistic calculations and train ML models. An NSF fellowshipwould supplement my current computing resources with access to XSEDE and would ensure thenecessary funding for my completion of this project.Broader Impacts: I will design a novel dye that could be applied to guide surgery or to detect cancer atearlier stages. My work’s flexible multi-objective optimization will also be able to design new dyes foradditional applications such as dye-sensitized solar cells. Furthermore, the AL framework I develop couldbe widely adopted to design other types of molecules and materials, such as those in batteries and catalysts.I plan to make all code and datasets I develop openly available online with detailed documentation, whichwill enable other researchers to replicate and build upon my work more easily. Additionally, I will host aworkshop to demonstrate my framework, with the goal that even experimentalists with littlecomputational experience would learn to utilize the AL component of my framework to accelerate theirprogress in molecular or materials design. My work ultimately aims to encourage greater collaborationbetween experimental, theoretical, and computational researchers by automating the connections betweentheir work in pursuit of design challenges that would otherwise be intractable.References: [1] Angew. Chemie Int. Ed., 2019, doi:10.1002/anie.201909987. [2] ChemRxiv, 2020,doi:10.26434/chemrxiv.12111060.v1. [3] J. Chem. Inf. Model., 2019, 59 (8), 3370–3388. [4] J. Chem. Inf.Model., 2020, 60 (6), 2697–2717. [5] J. Mater. Sci., 2020, 55 (23), 9918–9947. [6] ICLR, 2020, arXiv:2002.03244. [7] Science, 2019, 365 (6453), eaax1566."
43.0,"Introduction/Intellectual Merit: Advanced metabolic engineering allows scientists to use geneticengineering techniques in lower organisms, such as Escherichia coli and Saccharomyces cerevisiae, toproduce molecules of interest through recombinant pathways. Metabolic pathways are groups of genes thatencode enzymes that work together to produce the molecule of interest. Scientists have identified the mostefficient genetic modification methodologies to create optimal production strains, including promoterlibraries. A promoter library consists of a variation in the DNA sequence which varies the transcriptioninitiation rate of the associated gene. This variation can come from promoters in the native organism or innon-native organisms, which are identified using RNAseq data. These libraries have been extensivelydeveloped for model organisms such as E. coli and S. cerevisiae. However, these tools are currently limitedfor non-conventional organisms. Leaders in the field of biochemical engineering have identified thedevelopment of genetic tools for use with non-conventional organisms as a foremost goal because modelorganisms lack the complexity that non-conventional ones can provide1. Leveraging unique properties ofnon-conventional organisms allows scientists to build upon promising results that push the boundaries ofthe pharmaceutical, environmental, and cosmetic industries.Using the knowledge that I’ve gained from working in two metabolic engineering labs over the lastthree years, I intend to further explore the non-conventional oleaginous yeast, Yarrowia lipolytica. Thisyeast is particularly interesting because of its ability to naturally prevent bacterial contamination3, itsutilization of various hydrophobic and hydrophilic carbon sources4, and its ability to efficiently producelarge amounts of lipid-based products5. Y. lipolytica was originally identified in environments containinghydrophobic substrates and studied for its ability to biosynthetically produce enzymes and citric acids2.Recently, metabolic engineers have become more interested in this yeast as they explore non-conventionalorganisms for production of high-value compounds.Research Plan: Although genetic engineers have made significant strides in the toolkits available for Y.lipolytica, they currently lack the diversity necessary to fully exploit its potential. One of the newest toolkitswas developed by a group in France and is called the Golden Gate toolkit for Y. lipolytica6. The GoldenGate toolkit allows researchers to efficiently transform and integrate heterologous pathways in theorganism. This toolkit includes a validated promoter library and has been successfully used to express afunctional xylose utilization pathway. A major goal of my research plan is utilizing this toolkit and othersto integrate several metabolic pathways into the organism. Additionally, scientists have discovered morepromoters and have begun to look into computational models for the organism. These promoters includeTATA box promoters7 and one repressible and one bidirectional promoter2. Repressible promoters providenegative feedback to down regulate transcription pathways, which is useful for products that are inhibitoryto growth. A bidirectional promoter can be initiated in both directions for transcription; this becomesimportant for efficient gene co-expression. However, in order to further enable the metabolic engineeringgoals for this non-conventional yeast, more native and non-native promoters need to be identified so thatresearchers can better optimize the genetic conditions forefficient pathway expression.One way I plan to find new native promoters isthrough the use of RNAseq, described in Figure 1. RNAseq isa technique that can identify transcriptionally active regions ofthe genome and provide data on the expression levels of genesunder various growth conditions or metabolic states. Thisleads to the identification of relevant promoter regions anddata that can then be compared between specified growthconditions and normal growth conditions in which thepromoter region should not be active. This technique will alsobe applied to other organisms in order to create non-nativepromoters, allowing for the identification of exciting newFigure 1. Simplified steps of RNAseq promoters that behave differently than native ones in the hostprocess for identification of promoters. organism. The identified promoter sequences will then beobtained for both sets of promoters and will be expressed recombinantly and characterized in the hostorganism. The identification of both native and non-native promoters will allow for a more robust toolkitthan what is currently available.These new promoters will then be applied to pathways of high interest in the scientific community.One such pathway is the pathway for production of plant oils. Jojoba oil was identified as a leadingingredient for anti-aging formulas for skincare in the 2000s and is still widely used today with increasingdemand8. Harnessing the ability of Y. lipolytica’s high metabolic flux towards fatty acids enables a newroute of production for the long chain fatty acids that comprise Jojoba oil. The enzymes in this pathwayhave been identified9 and can be recombinantly expressed to produce Jojoba oil. This pathway will be atesting ground and motivator for identifying and characterizing novel promoters. After successfulintegration of the recombinant pathway, scale-up production studies can begin.The experiments for genetic optimization will take place in small volumes within 48-well plates(2mL). After creating multiple genetic libraries through use of promoters discovered and discussedpreviously, fermentation growth conditions can be studied in shake flasks (250mL). Studying the organismin shake flasks informs decisions about growth conditions in large scale studies, such as those done in abioreactor. After determining high quality growth characteristics, studies will be moved into a bioreactor(typically 1.5L+) to study the industrial feasibility of the process. Few studies have been done using Y.lipolytica in bioreactors, so this process will require permutations of multiple parameters to determine thebest operating conditions for growth and oil production. This objective will be assessed by the ability of thebioreactor process to be scaled and replicated and for Y. lipolytica to produce high oil titers at scale underthe optimal conditions.Throughout the completion of genetic cloning and scale-up studies, enhancement of currentcomputational models will be occurring in parallel. Genome-scale-metabolic models (GEMs) have beencreated for Y. lipolytica. GEMs provide a kinetic model of cellular metabolism, meaning that a GEM canpredict the metabolic activity of an organism based on user-defined parameters. The Y. lipolytica modelsneed to be improved so that a wider range of researchers can use them. The best models achieve around80% accuracy to experimental findings, however for people outside the field of metabolic engineering, themodels are hard to understand and use7. In order to enhance the model, experimental data will be collectedon new promoters, new growth conditions, and scale-up studies. Confirmation of already existing geneediting tools will also be collected. Utilizing GEMs is radically different than metabolic engineering’srandomized approach for identification of optimal conditions. The GEMs allow researchers activelyworking with Y. lipolytica to first test their hypotheses in silico so that they do not spend excess time andresources attempting to screen every genetic and/or fermentation parameter.Broader Impacts: Currently, Jojoba oil is produced via extraction of the oil from the seeds of the Jojobaplant. This time-intensive and costly process could be made easier through the creation of a heterologousproduction host. An efficient recombinant host organism allows for scientists to produce the same qualityoil with a significant decrease in the environmental stresses associated with harvesting from the naturalJojoba plant. Metabolic engineering enables Y. lipolytica to sustainably convert widely available carbonsources into high-value natural products. However, until further promoters are developed, and scale-upstudies are done on the organism, its full potential cannot be harnessed. Furthermore, the in silico model ofY. lipolytica will allow a large community of scientists to work together to better understand the growthand production capabilities of the organism. Taken together, the work proposed above leads to a continuedadvancement of metabolic engineering technologies to benefit society through development of advancedmethodologies for sustainable chemical production.References:1. Whitehead, T. et al. Biotechnol. Bioeng. (2020). Journal (2014).2. Hussain, M. TigerPrints (2017). 6. Larroude, M. et al. Microb. Biotechnol. (2019).3. Michely, S. et al. PLoS One (2013). 7. Ma, J., et al. J. Ind. Microbiol. Biotechnol. (2020)4. Ledesma, R. et al. Trends Biotechnol. (2016). 8. Ahmad, A. et al. Biomed. Dermatology (2020).5. Gonçalves, F. et al. The Scientific World 9. Miklaszewska, M. et al. Plant Sci. (2016)."
44.0,"Introduction: Understanding exactly how femalesmake matechoices,andhow thesemapontomale fitness and quality, has important ramifications for understanding trait evolution, and forconservation in species where female choice plays a large role in male success. Leks, wheremany males display at once hoping to attract a mate make an ideal model because femaledecision-making pathways should depend only on male display or other on-lek factors, and noton extrinsic factors such as territory quality or parental care. I propose to apply eye-trackingtechnology to female Greater Sage-Grouse (Centrocercus urophasianus) to determine how theyacquire information from visual displays produced by males when selecting mates. Eyemovements can be used to understand cognitive processes as animals must focusonaparticularsubject to process the information given by it. Limitations on sensory processing determine theamount of information a female can take in about a male and thus limits what she considerswhen choosing a mate.1 I hypothesize that female choice is drivenby integrationof multiplesignal elements.Aim 1: I will track female eye movements to determine which parts of the display femalesconsider as they focus on one element at a time before moving on to another. Femalescannot take in the whole of the display at once and should reduce their focus to only what theyperceive to be the most important signal elements.Aim 2:Once females’preferredtraits areknown, Iwill assessthe differencesbetween successfuland unsuccessful males to determine if there is likely selection pressure onthose traits. Findingthose differences will demonstrate that females are intaking sensory information,processing it, and using it to make decisions about mating.Methods: I will travel to lek sites across Montana to conduct this research. Using cameras andGPS units2 placed on males, I will track general positions of males on the lek and estimateterritory sizes for each male. I will also track mating success for each male by counting thenumber of matings attempted and presumably successful matings. For assessing females’preferred traits, I will use eye-tracking technology3 to determine how females are acquiringinformation about important traits. Analysis of eye movements will show the specific featuresfemales target and use as criteria for mate choice. Using data from eye-tracking, I will measurethe male ornaments females focused on, comparing between males with many matings, malesthat received some female attention butfewmatings, andmalesthat didnotreceive anymatings.I will analyze the differences between each group for each trait to assess whether the trait isunder significant selection pressure driven by female choice.1Dukas R. 2002. Behavioural and ecological consequences of limited attention. Philos. Trans. R. Soc. B Biol. Sci.357, 1539-1547.2Wann, GT, Coates, PS, Prochazka, BG, Severson, JP,Monroe, AP, Aldridge, CL. Assessing lek attendance of malegreater sage‐grouse using fine‐resolution GPS data: Implications for population monitoring of lek mating grouse.Popul. Ecol. 2019; 61: 183– 197.3Yorzinski JL, Patricelli GL, Babcock JS, Pearson JM, and Platt ML. 2013. Through their eyes: selective attentionin peahens during courtship.J. Exp. Biol 216:3035-3046Intellectual Merit: Eye-tracking is currently an under-utilized4 but insightful method inunderstanding visual cognition, especially in sexual selection. My research contributes toexpanding use of this technology to understand how sight can affect trait evolution in anorganism where visual traits are dramatic and emphasized. It provides other scientists with theunderstanding that these types of studies are feasible. It can be translated to see how cognitionvaries across the animal kingdom and increaseknowledgeabout visualcognition’sroleinsexualselection. Further, my research addresses the NSF Big Idea “Understanding the Rules of Life”because the process of decision making in sexual selection and female choice is still notclearlydefined. Not only will I address the intraspecific interactions between males and females in apopulation, but I will assess cognition on an organismal level, to figure out how females intakeinformation and perform a highly complex behavior, choosing a mate, in response.My experience working with autonomous recording units and GPS mapping with myundergraduate advisor and in my honors thesis has prepared me to work with various types oftechnology in the field. I am comfortable handling and managing equipment and data collectedfrom the units. I am able to translate these experiences to this project to create an efficientworkflow tohandle massamounts ofdata. I alsohave experienceanalyzinghighvolumesof datafrom audio recordings, which is easily transferable to processing video.Broader Impacts: With my proposed research, I will create opportunities for undergraduateresearch and citizen science. I will have undergraduate research assistants help me withcapture, measurements, and placement of GPS receivers. These students will gain valuabletraining in fieldwork techniques andexperimental design,preparing them toalso goontoexpandscientific knowledgein ecology, animalbehavior, andorganismal biology.Asadisabled andfirstgeneration student, I will reach out to increase research participation for underrepresentedstudents alongside my plans to expand access to research opportunities for underservedstudents. I will also recruit citizens from the surrounding areatohelp collectdata about thelek,thereby allowing more citizens to appreciate the habitat and gain an appreciation of science.Special effort will be made to extend this opportunity to local high school students who maynot otherwise have the opportunity to conduct research with scientists. Leks areparticularlycharismatic and tend to engage a wideaudienceof citizensof all agesinterested inbirds,whichgives me the opportunity to teach them about behavior andhow tomake scientificobservations.At the same time as citizen outreach, I will also educate thepublic about thesagebrush habitat,Sage Grouse, and emphasize the importance of conservation science inmaintaining ahealthyenvironment. I also plan to collaborate with the U.S. Fish and Wildlife Service, localconservation organizations and Native American tribes to preserve habitat and reduce theamount of frackingin thearea,leadingtoimproved environmentalconditions, preservationofculturally important land, and more greenspaces, improvingmentaland physicalwellbeingfor those living around sagebrush. Increased contact between the groups will also facilitatebetter working relationships and improve society for people who use these lands.4Billington J, Webster RJ, Sherratt TN, Wilkie RM, and Hassall C. 2020. The (Under)Use of Eye-Tracking inEvolutionary Ecology. Trends in Ecology & Evolution.Trends Ecol. Evol."
45.0,"mate. The majority of research regarding the signal function of visual traits has concerned color or patchesof color. This focus on charismatic coloration has left unexplored the potential signaling function ofachromatic patterns. Some patterns, such as the white check patch of great tits (Parus major) or the blackfacial spots of female paper wasps (Polistes dominula), have been suggested to function as assessmentsignals1. Yet, there is little empirical evidence that females consider patterns when selecting a mate. In thesongbird family Estrildidae, patterns or certain features of patterns may be dimorphic, which suggests thatthese may be sexually selected traits. This idea is only beginning to be explored, with some studies linkingestrildid patterns to individual quality2,3. Little is known, however, about how females perceive thesesignals, including the extent to which features that stand out to human observers draw the attention of thebirds themselves.The zebra finch (Taeniopygia guttata) is an ideal species in which to study the role of achromaticpatterns in estrildids. This species exhibits four notable sexually dichromatic traits, two being related tocolor and two being related to pattern. These include red beak color, which is more pronounced in malesthan females, and an orange cheek patch, barred bib, and white flank spots that are only exhibited by males.Although the function of male beak color has been well studied, little attention has been paid to the otherthree dimorphisms, aside from one study showing that female zebra finches prefer males with moresymmetrical barred bibs5. There also is little known about how these dichromatic patterns have evolved,and to what extent these patterns vary in wild populations. Furthermore, zebra finches come from thesubfamily Poephilinae6, which exhibit a wide range of chromatic and achromatic patterns, allowing for atractable phylogenetic comparison of perceptual abilities. I hypothesize that dimorphic achromaticpatterns function as signals of quality independent of color, and that species with these patterns areable to perceive and assess variation in this signal better than those without.Aim 1a: Quantify the range of natural variation of zebra finch patterns. Using museum specimensfrom the University of Michigan’s ornithology collection, I propose to measure the degree of variation ofplumage patterns within zebra finches. After photographing the ventral and lateral sides of specimens, Iwill use ImageJ software to determine the degree of variation in male bar and spot pattern. This is animportant analysis because in order for sexual selection to operate, there must be existing variation for it toact upon. After determining the range of this variation, I will categorize the types of plumage variation thatmay exist among males. Likely, these will include differences in regularity, contrast level, and density ofpatterns, though other features may also differ. Finally, I will test if certain aspects of these patterns covarywith body size, beak color, or other indicators of individual quality.Aim 1b: Determine the extent to which females can discriminate natural pattern variation. Usingmethods developed to test for categorical color perception in this species (i.e. training birds to flip discs ofdifferent colors for a food reward)7-9, I propose to ask how female zebra finches perceive variation inpatterns of bars and spots. To do this, I will make discs using paper printed with images of male chest orflank plumage as determined in Aim 1a. I will place these discs on wells in which a food reward is foundand train birds to select the pattern that varies the most from the others. Using the range of variationdetermined in Aim 1a, I will test for the extent to which two varied patterns are indistinguishable to zebrafinches. Given that zebra finches court at close range, this approach matches how close females would beto these patterns when assessing males. Additionally, I will test for sex differences in contrast perceptionto ask whether dichromatic bar/spot patterns could function in female choice, male competition, or both. Ifdichromatic patterns serve as signals of quality, I predict that female zebra finches will have thevisual acuity necessary to discern variation in this signal.Aim 2: Test female preference for pattern quality. I will test for female preference of male patternvariants using digitally manipulated images of male conspecifics. Using a television screen separated by apartition (hereafter “Bird TV”), I will observe how long females spend near one of two videos of male zebrafinches as evidence of their mate preference. I will first record videos of male zebra finches and thendigitally alter certain aspects of their achromatic patterning that have been positively correlated with bodycondition and dominance hierarchy in other estrildids: regularity of barred plumage2 and number of whitespots10, respectively. Bird TV, a novel method for mate choice experiments, is currently being developedby my advisor, Dr. Steve Nowicki, to test female preference for male beak color, which is a well-supportedpreference. After this approach is validated, Bird TV will be used in my proposed experiment. I predictthat females will prefer the aforementioned pattern variants that have been previously suggested toserve as signals of quality (regularity of barred plumage and large spot size), but have not beenexplicitly tested.Aim 3: Compare the perceptual abilities of closely related estrildids: The zebra finch is a uniqueestrildid in that it simultaneously exhibits dimorphic carotenoid coloration, barred plumage, and spottedplumage. As a result, it is possible that zebra finches are adept at both color and contrast perception. Otherclosely related species, such as the long-tailed finch (Poephila acuticauda) or double-barred finch(Taeniopygia bichenovii), exhibit fewer dimorphic traits. Therefore, I will compare the visual abilities ofzebra finches to its closest relatives to see if there are trade-offs associated with color and contrastperception. For example, the double-barred finch does not exhibit carotenoid coloration, but both males andfemales have bars and spots, and so may have been selected to specialize in perceiving patterns. In contrast,the long-tailed finch exhibits carotenoid coloration in both male and female beaks, but lacks bars or spots,suggesting that it may have been selected to specialize in color perception. I hypothesize that there is atrade-off between color and contrast perception—specifically, that exclusively colorful birds arebetter at color perception, and that exclusively patterned birds are better at contrast perception. Ipropose to test the color and contrast perception of the long-tailed finch and double-barred finch using themethods from Aim 1, then test female choice using methods from Aim 2, and compare these results to thoseof the zebra finch. An inconspicuously colored and more distant relative of the zebra finch, the Bengalesefinch (Lonchura striata domestica), was recently shown to discriminate colors differently than zebrafinches11. My work will follow up on this investigation to test the effect of phylogenetic relatedness oncolor and pattern perceptive abilities in Estrildids.Intellectual Merit: In studying the potential signaling function of certain visual traits, it is criticallyimportant to first understand how these signals are perceived. If such signals are not being perceived ordifferentiated at all, then we need to re-evaluate what other function they could possibly serve apart fromintraspecific communication. While patterned plumage has been previously proposed to function inintraspecific signaling, no study has actually examined how adept animals are at perceiving variation inthese patterns. Therefore, my work will be a necessary first step in determining if patterned plumage shouldcontinue to be explored as a signal of quality.Broader Impacts: As an NSF fellow, I will ensure that my research, at every stage, contributes to mypersonal goal of retaining as many students as possible in the sciences. At Duke, I will invite undergraduatesto not only assist in collecting data, but also in contributing intellectually so that they may share authorshipon future publications. During the summer, I will train undergraduates through Duke’s paid BiologicalSciences Undergraduate Research Fellowship. This program not only introduces students to biologicalresearch, but also provides professional development opportunities and a campus-wide showcase to presenttheir research. I benefited greatly from summer research programs like this, particularly the NSF REU, soI know firsthand the impact they can have. I want to help Duke undergraduates feel comfortable in aresearch environment through close mentorship and reassurance that questions and mistakes are part of thelearning process. Beyond Duke, I plan to share my research with scientists and science enthusiasts in thebroader “Triangle” (Durham. Raleigh, and Cary, NC) area. The North Carolina Museum of NaturalSciences in nearby Raleigh hosts opportunities for science communication and outreach that I plan to fullyengage in. For one, they host the Scientific Research and Education Network (SciREN) Networking Event,which gives an opportunity for scientists to adapt their research into K-12 lesson plans. They also host aDarwin Day event, in which people of all ages are invited to learn about Darwin and his legacy. I will sharemy findings at these events, and adapt them so that they are appropriate and accessible to all ages andbackgrounds. Finally, I will participate in Skype-a-Scientist to connect with students globally.1Pérez-Rodríguez et al. 2017. Proc. Royal Soc. B. 2Marques et al. 2016. Roy. Soc. Open Sci. 3Soma &Garamszegi 2018. Behav. Ecol. 5Swaddle & Cuthill 1994. Proc. R. Soc. Lond. B. 6Olsson & Alström 2020.Mol. Phylogenet. Evol. 7Caves et al. 2018. Nature 8Zipple et al. 2019. Proc. Royal Soc. B. 9Caves et al.2020. Behav. Ecol. Sociobiol. 10Crowhurst et al. 2012. Ethology 11Caves et al. (in press) Am. Nat."
46.0,"Introduction: Recent advancements in technology have enabled new ways of constructing metamaterialsthat possess desirable mechanical properties. Materials that have high elastic stiffness and low density areconsidered some of the strongest, stiffest, lightest materials available today [1]. By controlling theirmicrostructures, we can tune mechanical properties of metamaterials that endure extreme conditions.A trait of a metamaterial microstructure that is much ignored to date is randomness (aperiodicity),owing to the limitation of current design approaches based on unit cells. Aperiodic structures are likely toresult from natural self-assembly and self-organization processes and may be more robust againstuncertainty. Examples of robust microstructures can be seen in natural formations such as wood andnacre, or in parts of the human body such as bone [2]. Materials that are robust against uncertaintyperform well under various forces and stresses they may encounter.Currently, state-of-the-art approaches for designing metamaterial microstructures for desirableproperties can be categorized either as parameterized design or topology optimization methods. Bothapproaches build their foundation on the assumption that material microstructure consists of periodicallyrepeated motifs (or unit cells). Parametrized design is a simpler design method to design structures likelattices; it allows for a few design parameters to map directly to specific properties, but it needs an ad-hocdesign to start with. Since the design space is quite narrow, there is a narrow range of achievable materialproperty ranges. Meanwhile, topology optimization is a design method that allows for freeform design ofa structure with almost any geometry; it is mathematically well defined, but computationally expensive,leading to unpredictable geometries that are hard to manufacture [3]. Both approaches are difficult to useas tools to efficiently and effectively explore the vast design space of material microstructures.By combining optimal features of each method, this research aims to expand the microstructuredesign space to maintain local parametric behavior while enabling global freeform design. Throughnumerical approaches, one can program aperiodic material microstructures towards desirable propertiesusing a “growth”-like process that is encoded by “DNA”-like pairwise combination rules. With this“growth” process, a method for physical self-assembly is desired as it allows for rapid production ofprogrammable metamaterials. The mechanical self-assembly process has been explored in severalapplications across different scales [4, 5], but none have been able to achieve mechanical self-assembly ofan aperiodic microstructure. This design approach allows us to efficiently generate new random yetordered microstructures. It enables effective exploration of the material design space and pushes theboundaries of applying new stronger materials to applications such as shock absorption and acoustics.Research Plan: I aim to develop self-assembly methods to investigate how the shape and design ofcellular automata base cells affect mechanical properties of programmable metamaterials. I willthen develop specific tunable metamaterials for applications that desire the particular properties.Numerical, experimental (Fig. 1), and application phases can be achieved with FEA software, 3Dprinting, and mechanical testing equipment commonly used in any mechanical engineering lab. Myprevious research experience doing this with square-shaped tiles demonstrates my technical capability.Specific Aim I. Develop Baseline Samples: I will first develop a wave function collapse algorithm [6] forvarious polygons, such as pentagons and hexagons. The versatile algorithms will be developed in Pythonfor both 2D and 3D self-assembly with flexibility for varying polygons. To do this, I will use a so-calledwave function collapse algorithm, which performs a “growth” process similar to cellular automata. Wedefine fundamental building blocks and connectivity rules over a cellular space. Once the first cell is set,connectivityrules areenforced todeterminesurroundingcell states;this processthen repeatsinpropagating cycles. This will generate 2D and 3D samples for multiple shapes to be transformed into 3Dobjects in Rhino with Grasshopper C# that will be tested using FEA software. This forms a baselinemapping of the programmable microstructure design space as a success assessment.Specific Aim II. Perform Physical Experiments: From my numerical analysis of these structures, I will3D print specific samples generated by the algorithms and physically perform the same mechanical testsas in the numerical phase using mechanical testing equipment, such as an Instron machine. The results ofthese tests can be compared to the numerical results to evaluate their degree of error.Physical 2D and 3D self-assembly methods for multiple shapes will be developed to gain furtherinsight into this self-assembly method. Inspired by the mechanics of DNA self-assembly processes [7], Iwill design tiles for each shape with their respective channel types dictated by the polygon’s interiorangle. This tile design and experimental setup must ensure some randomness and adhere to the algorithm-determined connectivity rules. The self-assembled 3D printed tiles will be used as a carrier casting moldin which to pour a plastic material to generate the metamaterials to be tested. Using the same mechanicaltesting methods as the previous samples, I will then compile and analyze data for information about thedesign space and microstructure properties with a focus on how base cell shape and correspondinginterior angles affect mechanical properties of the metamaterial microstructures. At this point, a newphysical method for developing aperiodic programmable metamaterials will have been created. Thedesign space can be studied by constructing a material database. If the physical self-assembly method isnot experimentally reliable, the initial algorithm-based numerical study and mechanical tests still providea wealth of data to be used in the applications phase.Specific Aim III. Develop Metamaterials for Applications: Based on my findings from the studiesperformed in the previous phases, I will then study special properties of certain metamaterials that thisself-assembly method yielded, such as shock absorbency and acoustic capacity. My experimental resultswill yield a desired mechanical property by tuning the specific base cell shape and quantity of channeltype. With these settings, the physical self-assembly method can be used to create an array of samplesused for additional testing for specific properties, such as impact testing in the shock absorbency case.These studies will demonstrate how the metamaterials yielded from the developed self-assembly methodcan be applied as lighter, stronger, and more flexible alternatives to materials currently used in aerospaceand medicine.Intellectual Merit: This project will develop a mechanical self-assembly method of aperiodicprogrammable metamaterials, contributing new 2D and 3D self-assembly methods to metamaterials andmechanics research; this will improve the fundamental understanding of material microstructures andtheir properties. This project will also enhance understanding of the mechanics of self-assembly such asattractive forces and interlocking as seen in DNA self-assembly [7]. Working on this project at a researchinstitution with a strong mechanical engineering program will provide the proper resources to researchand publicize my findings related to metamaterials, bio-inspired processes, and their applications tomedical and aerospace fields.Broader Impacts: This project will contribute new 2D and 3D self-assembly methods to metamaterialsand mechanics research while developing new aperiodic programmable metamaterials with large degreesof tunability. Scale-independent metamaterials will be created that can be applied to aerospace materials,soft materials, and medical devices for its capabilities of enhanced shock absorbency, acoustic properties,elasticity, and strength. Additionally, through its biologically linked process of self-assembly, themicrostructures developed have the potential to be applied to sustainable, environmentally friendlystructures and devices. This project also has a parallel educational impact to introduce high school andundergraduate students to numerical and experimental methods in mechanical engineering and STEM. Iplan to mentor students in researching metamaterials.[1] J. B. Berger et al., “Mechanical metamaterials,” pp. 533–537. [2] H. Wagner et al., “Bonemicrostructure,” pp. 1311–1320. [3] O. Sigmund et. al, “Topology optimization,” pp. 1031–1055. [4] E.Klavins, “Programmable Self-Assembly,” pp. 43–56. [5] G. M. Whitesides, “Self-Assembly,” pp. 2418–2421. [6] Heaton, R. (2018). Wavefunction Collapse Algorithm. [7] S.-S. Jester et al., “DNAnanostructures,” pp. 1700–1709."
47.0,"Americans spend approximately 90% of their time indoors1, and by 2050 over two-thirds of theglobal population will live in urban environments2. Studies have shown that human exposure to pollutantsindoors is orders of magnitude higher than the exposure experienced outdoors3. Focusing air qualityresearch on the places occupied by the most people and where pollutant exposure is highest is importantin preventing negative health outcomes. It is paramount to understand the chemical, physical, and societalprocesses of the interface between urban and indoor air quality.Volatile organic compounds (VOCs) are emitted into the atmosphere from both anthropogenicand biogenic sources, including industrial and transportation activity, biomass burning, and vegetation.Indoor VOCs accumulate both from outdoor sources permeating into indoor spaces and from indooractivities such as cooking, cleaning, and off-gassing of furniture. Besides these gaseous VOCs, othersemi-volatile organic compounds (SVOCs) exist both in the gaseous and condensed phase due to theirlower vapor pressure and higher boiling points. When VOCs and SVOCs react with sunlight and oxidantssuch as ozone and nitrogen oxides (NO ), secondary organic aerosols (SOA) are produced, particles withxknown hazards to human health.While there have been mobile, real-time measurements of air pollutants such as PM 4 (particles2.5less than 2.5 microns in diameter), there have been limitations to similar VOC measurement campaigns:the measurement’s location was stationary5, the hour-long sampling time made it difficult to locatepollution “hot spots”6, or the instruments used could not measure compounds made possible by currenttechnology7,8. Prior to recent advances in VOC mass spectrometry, it had been difficult to measure VOCsand SVOCs at (a) sensitivities that allow identification of compounds and measurement of accurateconcentrations and (b) mobile, real-time temporal scales to link concentrations to specific sources.Recent technological advances have made it possible to overcome such limitations. The newestgeneration proton transfer reaction time-of-flight mass spectrometer, the Vocus 2R PTR-ToF-MS fromAerodyne, Inc., has world-leading real-time mass resolution and sensitivity. The new mass spectrometermeasures concentrations of more than 1600 chemical compounds every second, including many high-molecular-weight molecules considered semi-volatile. It can detect concentrations at part per trillionlevels with 1-second measurements and even part per quadrillion scales at 1-minute averaging for certaincompounds, allowing us to explore the frontier of trace – and very toxic – air pollutants. Using the mostadvanced mass spectrometer for this project will lead to the reporting of both SVOC molecules and traceVOCs that past mass spectrometers did not have the sensitivity to detect. The new Vocus Inlet forAerosol (VIA, Aerodyne, Inc.) gives us the novel ability to measure the time-resolved chemicalcomposition of SOA and other particles found indoors and outdoors, which is important for quantifyinghuman exposures.Research ObjectiveAs the first study to investigate the interface in VOC and SOA concentration between urbanoutdoor and indoor environments using world-leading measurement capabilities, this project aims to:(1) Quantify concentrations and human exposure to air pollutants that have been previouslyunidentifiable due to technological limitations.(2) Identify and apportion the major sources of VOCs and SOA found indoors and in urban areas.Task 1: In controlled experiments, we will characterize indoor concentrations of VOCs and SVOCs suchas benzenoids, siloxanes, and hydrocarbons. In UT-Austin’s environmental chambers and UTest House –a full-scale house on UT-Austin’s research campus outfitted with an array of sensors – experiments willsimulate common household events such as cooking and cleaning; VOC, SVOC, and particleconcentrations will be measured. To supplement our work, we will also use past datasets of theHOMEChem measurement campaign, which used the UTest House to characterize typical householdactivities9. Due to the increased sensitivity and precision of our novel mass spectrometer, we hypothesizethat we will make novel characterizations of SVOCs that previous instruments were insensitive to andthat past research campaigns did not report.Task 2: Using citizen science pathways already established from past research campaigns10, we willrecruit 20 volunteer homes of diverse economic and geographical locations around Austin, Texas to take2 air samples of ambient air twice per day for 2 weeks, one inside their home and one directly outside ofit. The air samples will be taken off-site for evaluation by our Vocus mass spectrometer to find VOC andSVOC concentrations across both temporal and spatial scales in urban and suburban environments. Weexpect that VOC concentrations will vary widely from home to home due to individual differences inventilation, personal care product usage, and cooking routines; however, we hypothesize that homeswithin a neighborhood will have similarities due to proximity to vehicle and industrial emissions.Task 3: Concurrently with the citizen science measurement, we will drive the Vocus mass spectrometeraround Austin to measure VOC concentrations found on Austin’s roads at 1 Hz time scales. Pairing theVOC data with GPS data will be vital in finding point sources and pollution “hotspots”. Every other daywe will measure with and without the VIA aerosol inlet, collecting data on both the gaseousconcentrations and chemical composition of particles. We hypothesize that many pollution “hotspots”will be from sources usually overlooked as key polluters. Due to the complex chemical and physicalprocesses that impact air quality, it is important to measure other air pollutants, not just VOCs andSVOCs. We will also measure mobile concentrations of ozone, NO , and PM – all interdependent onx 2.5the presence of VOCs – as well as meteorological data such as wind speed, temperature, and humidity. Toadd to our data set, we will use the Texas Commission on Environmental Quality’s (TCEQ) 6 Austin-areamonitoring stations as supporting data for wind speed, temperature, PM , ozone, NO , and sulfur oxides.2.5 xTask 4: Combining the mobile air pollutant data with citizen science air samples and environmentalparameters will produce a rich dataset across temporal and spatial scales. Using source apportionment andmultivariate analysis methods – for example, positive matrix factorization (PMF) and principalcomponent analysis (PCA) – we will identify major sources of urban and indoor VOCs. We hypothesizethat our analysis will reveal unknown, as well as verify known emissions from indoor sources – suchas cleaning or cooking – but will also quantify indoor exposure to pollutants penetrated from outdoors.Broader ImpactDue to the breadth of samples we will collect during Tasks 2 and 3, I will recruit at least 2undergraduate students from both UT-Austin and Huston-Tillotson University (an H.B.C.U. located inEast Austin) to help with the collection of the citizen air samples. To introduce them to air qualityresearch, I will train them in off-line VOC analysis and data processing.By conducting this campaign in actual homes and roads in addition to a laboratory setting, ourfindings can be used quickly for recommendations in homes and residential developments in Austin andother cities. By utilizing citizen science, this research will increase public awareness of the pollutantsthat residents are inhaling. The papers published from this campaign will be used as recommendations forurban planners, environmental regulators, and, perhaps most importantly, the general public. Bymeasuring around Austin, we will also report on the accuracy of the TCEQ stations and how eachstation’s neighborhood-scale measurements differ from precise ground-level measurements.Intellectual MeritWhile several studies have measured air quality across urban areas, this will be the first study todo so by measuring a wide range of VOCs (with atomic mass units from 30 to 500) in real-time usingnew mass spectrometry technology. It will also be the first campaign to use such a wide-ranging dataset –the controlled studies in the UTest House, the citizen science household air samples, TCEQ stationmeasurements, and the mobile dataset – to better understand the urban and indoor air quality system. Byfocusing on air pollutants that are not well-understood due to past technological limitations, we will alsoreport on exposures that have been overlooked.Works Cited[1] Klepeis et al. (2001), J Expo Anal Environ Epidem., 11(3), 231-252; [2] UN World Urban. Proj.: The2018 Revision (2018), pg. 10; [3] Nazaroff (2008), Build. and Env., 43(3), 269-277; [4] Apte et al. (2017)Environ. Sci. Tech., 51(12), 6999–7008; [5] Deng et al. (2018), Aero. Air Qual. Research, 18, 3025-3034;[6] Zheng et al. (2020), Sci. Total Environ, 703, #135505; [7] Maji, Beig and Yadav (2020), Environ.Pollution, 258, #113651; [8] Crippa et al. (2013), Atmos. Chem. Phys., 13, 8411–8426; [9] Farmer et al.(2019) Environ. Sci: Proc. Imp., 21, 1280-1300; [10] Bi et al. (2018), Environ. Inter., 121(1), 916-930"
48.0,"Tour Orbiter to the Ice Giant PlanetsIntroduction and BackgroundThe ice giant planets, Uranus and Neptune, have only been observed directly during flybys of the Voyager2 probe in 1986 and 1989. As it stands, the ice giants are two of the most under explored objects in oursolar system, and raise many of the most important questions about planetary and solar system evolution.In order to complete a thorough survey of the outer planets, the Ice Giants Pre-Decadal Survey (IGPDS)decided on two areas of interest to pursue in the first flagship missions: the atmospheric composition ofthe ice giants, including the tropospheric 3-D flow, heat balance, and meteorology, and the composition,structure, and evolution of the ice giant satellites1. To accomplish both objectives, a flagship missiontoeither system would require a complex trajectory that takes into account both an atmospheric entry probeand a satellite tour of the system. Due to the lengthy flight time involved in such a mission, it would bebeneficial to consider a pair of twin atmospheric entry probes in the interest of ensuring that the scientificobjectives are met. Two atmospheric probes would allow for a greater spatial resolution as well as asecond sampling of the atmosphere, which decreases the chances of the atmospheric probe landing in anunrepresentative region, as the Jovian Galileo atmospheric entry probe experienced in 19952. A flagshipmission with twin atmospheric entry probes and a satellite tour brings forth a complex trajectory designproblem when factoring in the vehicle weights, atmospheric entry locations, and launch windows of eachplanet.ProposalTo further investigate the feasibility of a flagship mission to either Uranus or Neptune including twoatmospheric entry probes and a satellite tour, I propose using two NASA trajectory design andoptimization softwares, the Copernicus Trajectory Design and Optimization System and the Program toOptimize Simulated Trajectories II (POST2), in order to systematically test combinations of spacecraftweights, atmospheric probe weights, and separate atmospheric probe latitudinal/longitudinal entrancesalongside traditional ballistic (chemical) trajectories and solar electric propulsion (SEP) trajectories.MethodsIn order to design and evaluate the various mission combinations with end-to-end optimization, I willutilize POST2 alongside the Copernicus software. Copernicus serves as the primary trajectoryoptimization tool for mission design at NASA, and as such, has many degrees of customizability in termsof low and high thrust trajectories.3This will allowfor the testing of various combinations of SEP in theinner solar system flight with a later transition to chemical propulsion. POST2 gives the ability tointroduce multiple vehicles at any point in the simulation- these “child” vehicles inherit the state of their“parent” vehicle, and will be vital in further analyzing the trajectories of the two atmospheric entry probesonce they begin separation from the parent spacecraft.4I plan to utilize the Copernicus API in order torapidly test various configurations for the launch stage up to the entry probe separation stage of themission, including the transition from SEP to chemical propulsion. I will then push this output into thePOST2 software, which will complete the trajectory design by simulating the separation of theatmospheric entry probes, the atmospheric entrances, and the satellite tour of the remaining orbiter. I plan1Ice Giants Pre-Decadal Survey Mission Study Report.,2017. (JPLD-100520) https://www.lpi.usra.edu/icegiants/mission_study/2Irwin, P. G. J.,2009. Giant Planets of Our Solar System. Giant Planets of Our Solar System: Atmospheres, Composition, andStructure, Springer Praxis Books. ISBN 978-3-540-85157-8. Springer Berlin Heidelberg, 2009.3Williams, J. et al. “Overview and Software Architecture of the Copernicus Trajectory Design and Optimization System.”(2010).4NASA Langley Research Center, “Overview of the Program to Optimize Simulated Trajectories II (POST2)”,https://post2.larc.nasa.gov/overview/to write the data interchange software in either Python or Julia, a new programming software used fortrajectory design, depending on the requirements of the two trajectory design softwares.The outputs of these combinations can then be compared in terms of the fastest route, the mostcost-efficient route, and the lightest route. The combinations explored will be constrained by thefollowing scientific considerations outlined in the IGPDS report5.UranusThe flight length and severe axial tilt of the Uranus brings forth many constraints to the mission:A combination of SEP and chemical trajectory could deliver a basic probe and orbiter combination toUranus in ~11 years of interplanetary time, when considering an average vehicle, such as the Atlas V 551.Due to the axial tilt, the planet’s seasons last ~21 years. In order to observe a different season than that ofVoyager 2’s observations, we must launch before the end of the 2030 decadal window so that the missionis completed before the 2049 equinox. Due to the alignment of the planets during this window, a gas giantflyby and/or gravity assist can only be considered with Jupiter. As such, the best gravity-assist sequencefor this window is Venus-Earth-Earth-Jupiter (VEEJ).NeptuneThe flight length of a Neptune flagship mission would require covering a much largerinter-planetary distance than that of a Uranus flagship mission, and would in turn require a strongerlaunch vehicle. The Delta-IV Heavy or the SLS Block 1-B would lend itself better to a Neptunian systemmission than the Atlas V 551, and would allow the spacecraft to complete only an Earth-Jupiter (EJ)gravity assist in order to reach Neptune. Neptune’s largest satellite, Triton, raises many questions aboutsatellite evolution due to its retrograde orbit, and thus it may also be beneficial to explore multiple flybysof this moon during the satellite tour.Intellectual MeritThough the Uranian and Neptunian systems were brieflyanalyzed in the Voyager 2 flybys, therehave been no further missions to these systems. Furthermore, the only scientific data regarding thesesystems are from remote telescopic observations and the limited Voyager 2 observations.6As the IGPDSreport stressed the importance of a flagship mission with an optimal launch window in the 2030 decade,we must prepare a mission as soon as possible. Due to the extreme inter-planetary distances, we mustoptimize the mission to ensure that all scientific goals are met in both a timely and efficient manner. Theinclusion of twin atmospheric entry probes would ensure that the in-situ atmospheric readings are preciseand representative of the planet, while the satellite tour would allow for further exploration of the systemand its evolutionary path. This proposed study will explore the trajectory design and optimization of amission with twin atmospheric entry probes and a satellite tour, ensuring that the decades of preparationand execution involved in an ice giants mission will be as fruitful as possible.Broader ImpactsA flagship mission to the ice giants would constitute a new age in space exploration, particularlyif the mission included a set of twin atmospheric entry probes. As a twin probe setup has never beenexecuted, this would revolutionize the future of in-situ atmospheric measurements, while also providingprecise results for a relatively unknown part of our solar system. The trajectory considerations of asatellite tour could also result in the discovery of new satellites, as well as previously unknown chemicalsignatures and geographical landscapes. This study would allow for the exploration of a brand-newspacecraft and probe configuration as well as the potential for a flagship mission to one of the mostunder-explored areas of our solar system, allowing for technological and scientific research for decades tocome.5Ice Giants Pre-Decadal Survey Mission Study Report.,2017. (JPLD-100520)https://www.lpi.usra.edu/icegiants/mission_study/6Irwin, P. G. J.,2009. Giant Planets of Our SolarSystem. Giant Planets of Our Solar System: Atmospheres, Composition, andStructure, Springer Praxis Books. ISBN 978-3-540-85157-8. Springer Berlin Heidelberg, 2009."
49.0,"BACKGROUNDRNA modifications, also known as epitranscriptomics, are emerging as a novel layer of dynamicgene regulation [1]. RNA modifications alter existing RNAs’ structure and function to influence variouscellular pathways via RNA processes such as transcription and translation [2, 3]. Pseudouridine (Ψ) wasthe first RNA modification discovered [1]. Despite being the most abundant and widespread RNAmodification in living organisms, little is known about its function. In this proposal, I will establish anapproach to systematically investigate the biological roles of pseudouridine by focusing on theunique activities that this modification imparts to RNA.The isomerization of uridine to pseudouridine by pseudouridine synthases (PUS enzymes)structurally stabilizes RNAs via the formation of an extra hydrogen bond donor [3]. Ψ was previouslythought to primarily stabilize tRNA and rRNA; however, new developments in modern-sequencingtechniques reveal the more interesting downstream effects of pseudouridylation. For example, there isevidence that H/ACA RNPs, RNA-dependent PUS enzymes convert stop codons into sense codons inyeast [2]. Interestingly, PUS7-mediated pseudouridylation has been found to regulate stem cell growthand fate determination partly through tRNA modification, which activates tRNA-derived fragments toinhibit protein synthesis [3]. This provides evidence that Ψ modifications may be critical in cell lineagecommitment.Specifically, mutations associated with several pseudouridylating enzymes, namely PUS1, PUS3,and PUS7, are associated with neuronal disorders and intellectual disability [4]. PUS1 mutations areassociated with cognitive impairment [5]. Additionally, PUS1 acts on the steroid RNA activator, a co-activator of the nuclear estrogen receptor α regulating neuronal survival [5]. Truncated PUS3 and reducedlevels of ψ U39 in tRNA were detected in patients with intellectual disability [5]. Lastly, mutations inPUS7 can cause intellectual disability and microcephaly in humans [4]. These studies suggest that Ψ cansubstantially impact neuronal differentiation and function; however, the mechanisms regardingpseudouridylation in neurogenesis are poorly understood. My results will contribute to ourunderstanding of the importance of pseudouridylation in neurogenesis and neuron function.PROPOSED RESEARCHMy overarching goal is to elucidate the molecular and functional roles of Ψ in neuronal cells. Based onthe reported links to human brain function, I hypothesize that Ψ is a critical modification for neuronaldifferentiation and function. I further propose that the Ψ landscape between stem cells and neuronsis unique and specific. I will test the above hypotheses with the following aims:Aim 1—To identify pseudouridylating enzymes for investigation. There are 13 known PUS enzymesin human cells; however, only a few predicted human PUS enzymes have been studied to date. In additionto PUS1, PUS3, and PUS7, I will predict other PUS enzymes associated with neuron function by firstperforming weighted gene co-expression network analysis (WGCNA). This analysis will reveal sets, ormodules, of highly correlated genes. To perform WGCNA, I will use multiple human tissue RNAsequencing datasets from the GTEx project. I expect PUS1, PUS3, and PUS7 to appear in one modulebecause these genes share connections to neuronal function. Other PUS enzymes that are expected tocorrelate with neuronal genes will fall in this module. I will then conduct gene ontology enrichmentanalysis to verify that the PUS candidates in that module are associated with neuron processes. Theseexperiments will predict key pseudouridylating enzymes in addition to PUS1, PUS3, and PUS7 that willbe investigated in neurons.Aim 2—To investigate the effects of PUS enzymes on neurogenesis. To define the impact of PUSenzymes on early neurogenesis, I will use CRISPR/Cas9 to knock out each candidate gene identified inaim 1 in iNGN cells. iNGN cells are a human induced pluripotent stem cell line which has beenengineered to be readily induced into neurons within four days by doxycycline [6]. I will design a gRNAthat targets the N-terminal coding exon of each gene to induce nonsense-mediated mRNA decay andvalidate that the enzyme is no longer expressed via Western Blot.I will explore two different methods of inducing edited-iNGN cells into neurons to gather morecomprehensive results. For the first method, I will supplement cell media with doxycycline to induce theformation of neurons. Since this procedure only takes four days, it will allow for efficient generation ofeasily reproducible data. However, the rapid induction of robust neuronal morphology by doxycyclinemay limit the resolution of detectable phenotypic changes in these cells. Thus, the second, slowermethod enables me to mark differences at each stage during differentiation. I will differentiatesuccessfully edited colonies of iNGN cells using a slow differentiation method following a previouslypublished protocol [7]. I will use an inducible Cas9 system to knock out the PUS enzyme at different timepoints to determine the most critical points of pseudouridylation during neurogenesis [8]. To detectpotential morphological alterations, I will use a neurite outgrowth assay and monitor the expression ofneuronal markers via immunofluorescence. Rescue of the phenotypes by ectopic expression of the wild-type protein will control for the specificity of the effects. I will select the cell lines with the strongestphenotypes for additional study in aim 3. These studies will reveal how PUS enzymes impact neuronalmaturation.Aim 3—To determine neuron-specific RNA targeting by PUS enzymes. To determine the positions ofRNA binding by PUS enzymes at single-nucleotide resolution, I will perform UV cross-linking andimmunoprecipitation, followed by high-throughput sequencing (iCLIP-seq) in iNGN-derived neuronsexpressing Flag-tagged PUS enzymes. I will corroborate these results by using specific antibodies to pulldown endogenous PUS complexes. Mock-infected cells will be used as a control to exclude non-specificRNA binding. I will complement these results with Ψ-seq to confirm that the PUS-bound sites arecatalyzed. A comparison of the sites bound in stem cells and neurons will reveal neuron-specific Ψ sites.Because PUS enzymes may have multiple substrates, it may be unclear how to assign mutant phenotypesto loss of modification in specific RNA species. I will control for this variable via rescue experiments bytransfecting specific synthetic pseudouridylated RNA substrates identified by Ψ-seq experiments. Theseexperiments will reveal the Ψ landscape in neurons and identify RNA targets for future study.Summary: Successful completion of these aims will shed new light on the poorly understood but criticalroles that pseudouridylation plays in neuronal development. A potential follow-up study to investigate thefunction of Ψ in the identified RNA targets is an RNA pull-down assay. To determine the role of Ψ on thecomplex composition of RNA species, Ψ and non-Ψ RNA probes tagged with biotin can be pulled down,and the interactome can be analyzed by mass spectrometry. Future research into the biological roles ofRNA modifications will reveal novel causes of neurological disorders.Intellectual Merit: In Dr. Murn’s lab, I’ve gained the necessary training in molecular biology techniquesand RNA biochemistry to carry out this project. I am currently optimizing pseudouridine-seq and will usemy experience in this technique for this proposal. I will receive the bioinformatic training necessary tocarry out my proposal through future mentoring from Dr. Chaolin Zhang. Dr. Zhang’s lab at ColumbiaUniversity is an ideal fit because of his focus on RNA-protein interactions, RNA regulatory networks inneural development, and expertise in high-throughput transcriptomic data analysis.Broader Impacts: As a graduate student and later as a professor, I will mentor undergraduate women ofcolor and encourage them to pursue research careers. I will continue to empower young high schoolwomen by establishing additional chapters of Queens of STEAM. The support of the NSF GRFP willenable me to carry out my research while continuing STEM outreach.[1] I. A. Roundtree, M. E. Evans, T. Pan, and C. He, “Dynamic RNA Modifications in Gene ExpressionRegulation,” Cell. 2017. [2] J. Karijolich and Y. T. Yu, “Converting nonsense codons into sense codonsby targeted pseudouridylation,” Nature, 2011. [3] N. Guzzi et al., “Pseudouridylation of tRNA-DerivedFragments Steers Translational Control in Stem Cells,” Cell, 2018. [4] H. Darvish et al., “A novel PUS7mutation causes intellectual disability with autistic and aggressive behaviors,” Neurology: Genetics. 2019.[5] M. T. Angelova et al., “The emerging field of epitranscriptomics in neurodevelopmental and neuronaldisorders,” Frontiers in Bioengineering and Biotechnology. 201. [6] V. Busskamp et al., “Rapidneurogenesis through transcriptional activation in human stem cells.,” Mol. Syst. Biol., Nov. 2014. [7] Y.Shi, P. Kirwan, and F. J. Livesey, “Directed differentiation of human pluripotent stem cells to cerebralcortex neurons and neural networks,” Nat. Protoc., Oct. 2012. [8] K. I. Liu et al., “A chemical-inducibleCRISPR-Cas9 system for rapid control of genome editing,” Nat. Chem. Biol., 2016."
50.0,"topologically relevant materials. Successful execution of this approach will lead to new materials discoveryand generate new methods for electronic band structure engineering.Introduction: Linking desired physical properties to structural motifs is a fundamental goal in solid-statechemistry. As we will see, topological semimetals are exemplars to this goal: ultrahigh mobility electronsarising from linear band crossings can be derived from specific structural motifs, further predicted frombasic electron counting rules1. Previous work regarding topological materials has largely focused onsystematically scanning the thermodynamic stability of compositions in particular space groups, followedby selected synthesis of the most promising candidates. More recently, scientists have explored how slightperturbations of topological systems can induce charge density waves through structural modulationsrationalized via electronic considerations.2 That is, seemingly simple chemical substitutions used as n- orp-type dopants, aimed to adjust the Fermi energy to lie in a precise electron state.Changing the composition of a structure may induce a structural change, such as a Peierls distortion,which would open a up bandgap at the Fermi energy. Further, exactly how a given structural transitionnavigates its potential energy surface to form such specific distortions is largely unexplored in the literature.Finally, even if such chemical substitution does not significantly alter a structure, development of predictivestrategies for diversifying the possibilities of elemental substitution should be developed. Exploringsynthetic control over such distortions in topological materials can provide one such prescription to theseissues. Thus, understanding how such distortions lead to thermodynamic favorability, paired with how suchdistortions alter physical properties, may then open a playbook for “band engineering” where finely tuningcomposition can gap out unwanted bands from the Fermi surface.Background: Topological materials can be well understood through the Zintl-Klemm concept: an electroncounting method where transfer of electrons is assumed from the most electropositive element to the mostelectronegative element in a crystal structure. The remaining atoms then form covalent networks to reducethe total thermodynamic energy of the system. In Figure 1a, a Walsh diagram shows the thermodynamicstability of a Te chain as a function of bond angle. The linear geometry is found to be stabilized at 223electrons, the third level, due to less overlap of anti-bonding interactions. In extended solids, it would thenbe predicted that linear chains form when there are 7 electrons per chain atom. Such is the case in UTe ,2seen in Figure 1b, a promising candidate host for the sought-after Majorana quasi-particle.1 Each uraniumis found to be in the 3+ oxidation state and transfers two electrons to the nearest tellurium sites, formingwhat can be thought of as [UTe]+ slabs and a linear Te- chain. As a result, the geometry enforces a linearcrossing at the Fermi energy, shown in Figure 1c. Doubling of the unit cell, when considering 2 atoms perunit cell, folds the band structure back on itself creating a linear band crossing located at the Fermi energy.Figure 1. a) A Walsh diagram of a Te molecular unit. b) The unit cell of UTe , a topological3 2superconductor containing linear chains. c) The resulting band structure for an isolated linear chain of Te-ions. A linear crossing at the Fermi energy occurs halfway between the Γ and Ζ points. d) A folded bandstructure, arrived at by doubling the unit cell, forming a Dirac node at the Fermi energy.Proposal: Derivation from ideal electron counts in such covalent networks can lead to a structuraldistortion. In the classical example of a linear chain of atomic orbitals, a half-filled band will favordimerization, leading to differing bond lengths, opening a band gap at the Fermi energy. However, recentinvestigations have found, counterintuitively, that such distortions can lead to improved topological bandstructures or high-mobility electrons. Despite a structural modulation in the square-net layer, which can beviewed as a two-dimensional linear chain, NdTe exhibits ultra-high mobility electrons and anomalous3quantum oscillation behavior1. Moreover, the distorted square nets in GdSb Te was recently shown to0.46 1.48gap out trivial band crossings, “cleaning” the band structure by retaining the screw axis associated withits structural distortion.2For my analysis of similar systems, I will utilize recent adaptations of Density Functional Theory (DFT)outputs that establish direct links between local features in solid state compounds and their contribution toelectronic and steric favorability. Specifically, the reversed applied Molecular Orbital (raMO) analysis aimsto fit tight-binding parameters from DFT band structure calculations. As a result, DFT-calibrated molecularorbital diagrams are visualized to explain formation of closed-shell configurations (see Figures 1a) and c)).Parallel with this, DFT-Chemical Pressure (DFT-CP) resolves local packing frustrations that can arisewithin dense atomic packings, from which the role of atomic size can be assessed. Utilization of both thesecomputational analyses can explain why a structure may obey or derivate from predictive bonding schemessuch as the Zintl–Klemm concept.3 Both raMO and DFT-CP software packages are freely available via theGNU Public License and will be used in this work. I will also synthesize and structurally as well aselectronically characterize the targeted compounds.Therefore, I will employ an iterative approach of experiment and theory to implement a frame ofunderstanding structural distortions in topological motifs, targeting the following research objectives:1. Determination of topological systems in which unexpected electron counts or steric packingfrustrations may favor charge density wave formation.2. Synthesis of candidates, as well as detailed structural and physical characterization of materials.3. Demonstration of band engineering through the tuning of new superstructures guided bytheoretical analysis.Intellectual Merit: Princeton University offers many unique multidisciplinary approaches needed to studytopological materials. In the Schoop laboratory, I have access to core instruments needed for thisinvestigation, including furnaces for solid-state synthesis, a single-crystal X-ray diffractometer formaterials characterization, a magnetic properties measurement system, and a physical propertymeasurement system with a dilution fridge. Through NSF supported opportunities, such as the PrincetonCenter for Complex Materials (PCCM), frequent collaborations will be drawn through an interdisciplinaryresearch group focusing on topological quantum matter. Specifically, the Department of Physics offersmany avenues for collaborative work in physical properties characterization with Dr. Nai Phuan Ong, Dr.Ali Yazdani, and Dr. Sanfeng Wu. There are also many opportunities through the NSF-funded Imaging andAnalysis Center which provides access to instruments such as the scanning electron microscope.Frequent collaborations outside of Princeton University will also be utilized. So far, single-crystaldiffraction data obtained for a linear chain system indicate missed satellite peaks with a modulation vectorq = 0.06a*, indicating a massive 50/3 supercell periodicity. Despite prediction of such a distortion ruiningthe metallicity of the structure, electrical transport data suggests that the system remains metallic. To thisend, a proposal has been written and sent to Dr. Yusheng Chen at Argonne National Laboratory, hopefullyto be accepted for next cycle in March of 2022. Additionally, the Schoop laboratory frequently collaborateswith scientists at Helmholtz-Zentrum Berlin such as Dr. Andrei Varykhalov at the BESSY II beamline forangle-resolved photoemission spectroscopy. Looking ahead, probing the band structure of a crystal relatedto this project may significantly improve the impact of my investigations.Broader Impacts: The predictive framework I develop will open paths to tailoring band structure throughcomposition, allowing for more control of physical properties used in technological applications. In thefuture I will expand to other structural motifs such as the square- and Kagome-nets. Finally, visualizationtools, like raMO, have chemical education implications for use in NSF-funded operations, such as PCCM’sPrinceton University Materials Academy.References:1. J.F. Khoury and L.M. Schoop. 10.1016/j.trechm.2021.04.011 Trends in Chemistry, (2021).2. Lei S.; Theicher, S.M.L.; Topp, A. et al. 10.1002/adma.202101591 Adv. Mater. (2021).3. Warden H.E.M.; Lee S.B.; Fredrickson D.C. 10.1021/acs.inorgchem.0c01347 Inorg. Chem. (2020)."
51.0,"fluorescence from satellitesAcross the globe, the terrestrial biosphere is responding to growing climate extremes, including morefrequent heatwaves, droughts, and high-impact weather events1. For North American forests, this meansincreasing physiological stress on one of the continent’s most important carbon sinks, along with the vastquantity of biodiversity that these ecosystems support. A fundamental way to understand and potentiallymitigate the ecological impacts of extreme heat and drought events is by tracking carbon uptake throughphotosynthesis (i.e., gross primary productivity, GPP), across seasons at the forest-scale2. However, large-scale monitoring of GPP is challenging when considering the highly dynamic and remote nature ofmountain biomes, which make up a substantial portion of North American biomass3. Montane landscapesexhibit vast gaps in spatial coverage of surface-level GPP measurements, along with complex topographythat makes land surface models and atmospheric tracer approaches prone to significant uncertainty4. Thus,remotely-sensed data from space present a promising tool to fill in these spatial gaps to better understandforests’ response to stress and the implications for the terrestrial carbon cycle.Traditionally, forest-level GPP has been derived from satellites using reflectance-based indices thatquantify the “greenness” of a land surface. However, the temperate and boreal forests that comprise manyNorth American mountain biomes consist mainly of evergreen conifer trees which retain their needles, andtherefore, greenness, even in photosynthetically dormant seasons (e.g., drought or winter). Thus, studiesthat use reflectance-based indices as metrics of conifer GPP face significant challenges in capturingseasonal to decadal changes of photosynthetic activity5,6. In contrast, solar-induced chlorophyllfluorescence (SIF), which is emitted by chlorophyll pigments as a byproduct of the photosynthetic processand can be measured via satellite instruments, has been shown to closely follow the seasonal cycle ofphotosynthetic production in evergreen forests7. The combination of newly available remotely-sensed high-resolution SIF data, in conjunction with measures of complex terrain characteristics (e.g., slope angle,aspect and elevation), represents a uniqueopportunity for understanding GPP overmountain biomes. In my proposed work, I willanalyze GPP derived over the Sierra Nevadamountain range in California using ground-based flux tower data, biogeochemical models,and remotely-sensed SIF and reflectance-baseddata in order to test the hypotheses describedbelow and in Figure 1.Hypothesis 1 (H1) - SIF is an improved wayto measure GPP over montane coniferecosystems compared to traditionalreflectance-based remote sensing indices.High-resolution SIF from satellites providesextensive spatial data coverage, but satellite- Fig. 1: Satellite-based SIF will be analyzed againstbased SIF has yet to be analyzed for fine-scale reflectance-based indices and modeled GPP in mountainsspatial and elevation gradients in complex to assess drought-induced stress.terrain. To test H1, I will analyze SIF data fromthe TROPOMI and OCO-2/3 satellite instruments over the Sierra Nevada range, comparing to GPPmeasured at eddy-covariance flux towers from the NSF-funded National Ecological Observatory Network(NEON) and Southern Sierra Critical Zone Observatory sites in the Sierra Nevada range. I will thencompare SIF and traditional reflectance-based indices (NDVI, EVI, CCI) to quantify differences in theirability to predict seasonal and interannual GPP as a function of elevation and terrain characteristics.Hypothesis 2 (H2) - Characterization of mountain conifer forests response to drought can be improvedwith the aid of remotely-sensed SIF. The Sierra Nevada range has consistently experienced extreme droughtconditions over the past decade; these forests’ productivity in response to drought can be estimated usingbiogeochemical models, but does the spatial resolution of a model limit its ability to resolve such dynamicprocesses over complex terrain? Based on insights from H1 and using SIF as a means of constrainingmodeled GPP, I will test H2 by comparing remotely-sensed SIF to GPP modeled using the CommunityLand Model version 5 (CLM5) over the Sierra Nevada region for timeframes with available high-resolutionSIF data, detecting mismatch between satellite- and model-derived GPP over seasonal and interannualcycles during observed drought periods.Hypothesis 3 (H3) - SIF can provide information towards early warning capabilities for forest health inresponse to drought conditions. Assuming a direct linkage between forest productivity and physiologicalstress, remotely-sensed measures of GPP could act as highly localized indicators of forest health in drought-stricken regions. Using information gleaned from H1 and H2, I will analyze high-resolution SIF data withrecent records of forest drought disturbances to quantify trends in SIF as they are correlated with droughtevents. Through this analysis I will uncover statistical relationships between SIF and drought stress in thecontext of variables such as elevation, aspect, and snow cover to determine the extent to which SIF can actas an early warning system for forest health over land-use management scales.Collaborations and Computing Resources: To complete this work, I will build on an existing NSF-funded collaboration between the University of Utah and researchers at California Institute of Technology(led by Prof. Christian Frankenberg) to access high-resolution, pre-processed topography-corrected SIFdata products over the Sierra Nevada range. I will also work with the University of California–IrvineInnovation Center for Advancing Ecosystem Climate Solutions to engage regional Sierra Nevadastakeholders in scientific discussion. To accommodate the computational needs associated with my work,I will utilize dedicated group-access nodes (purchased by advisor Prof. John Lin) on two supercomputersmaintained by the University of Utah’s Center for High-Performance Computing.Intellectual Merit – Global Carbon Budget: Remotely-sensed SIF has the potential to track forestproductivity over global scales and is a promising tool for rectifying uncertainties in the carbon budget ofmountains. My work will be among the first to examine high resolution SIF over complex terrain in orderto address uncertainties in forest drought response. As heat and drought stress grow increasingly prevalentdue to climate change, understanding the highly dynamic response of montane carbon stocks will be criticalfor improving terrestrial biosphere models that inform global climate policy. Forest Management: Highresolution SIF can help diagnose features of forest wellbeing and tree mortality at scales meaningful to landuse management. Through assessment of H3, I will examine how SIF can be used to provide early warningcapabilities for near-term forest disturbances and allow land managers to adapt to rapid changes in foresthealth from critical stress events. These capabilities ultimately aid in emergency preparedness capabilitiesto mitigate damage from wildfires and bark beetle die-off.Broader Impacts – Stakeholder Engagement: The U.S.D.A. Forest Service Region 5 and SierraNevada Conservancy have a vested interest in central California’s forest health and wildfire management,motivated by public protection, forest resource care, and conservation advocacy. In addition, the CaliforniaAir Resources Board is seeking accurate methods to estimate forest carbon stocks for implementation ofcarbon accounting policies. Through collaboration with UC–Irvine (see above), I will engage stakeholderrepresentatives from these institutions through ongoing virtual correspondence and regularly heldstakeholder meetings to disseminate my findings on forest drought response and early warning capabilities.Code Sharing: As I have already done in my previous publications, I plan to provide open access to R andpython scripts to the entire research community produced through my personal Github webpage(github.com/lkunik).References: [1] IPCC 6th Assessment Report, (2021) [2] E. Tomppo, et al. (2021) Remote Sens. 13, 597.[3] D. Schimel, et al. (2002) Eos Trans. AGU. 83(40), 445–449. [4] M. Rotach, et al. (2014) Bull. Amer.Meteor. 95(7), 1021-1028. [5] K. Springer, et al. (2017) Remote Sens. 9, 1–18. [6] D. Sims, et al. (2006) J.Geophys. Res. 111, G04015. [7] T. Magney, et al. (2019) Proc. Natl. Acad. Sci. 116(24), 11640-11645."
52.0,"Background: The Bushveld Complex is located in South Africa and was emplaced approximately 2.056Ga. It is the largest layered mafic intrusion in the world, covering an area of 65,000 km2 with a thicknessranging from 7-9 km1. The Bushveld is an important resource for the world, hosting major quantities ofplatinum and platinum group elements (PGE), titanium, iron, vanadium, tin, and chromium2. The mafic toultramafic cumulate sequence of the complex is called the Rustenburg Layered Suite (RLS) and is dividedinto five zones: Marginal, Lower, Critical, Main, Upper, and Roof Zones. The Upper and Upper MainZones (UUMZ) are genetically related to each other and are separated from the lower zones by a layerknown as the Pyroxenite Marker (PM)3. The UUMZ hosts the most significant Fe, V, Ti, and P deposits2.The UUMZ is dominated by gabbro, anorthosite, and Fe-Ti-oxide rich rocks, which includemagnetitite (magnetite and ilmenite) and nelsonite (magnetite, ilmenite, and apatite) mineral assemblages.Fe-Ti-oxide rich rocks make up the smallest proportion in the UUMZ but host the majority of economicallysignificant minerals. The Fe-Ti-rich rocks are typified in 26 magnetite and 6 nelsonite layers documentedin the western limb4; the same number of magnetite layers have also been mapped in the eastern limb3.The gabbro layers in the UUMZ are thought to have been formed by cooling and differentiation ofthe residual magma that produced the Pyroxenite Marker. However, the evolution and relationship betweenthe anorthosite and magnetite/nelsonite layers is still poorly understood. Several models have beenproposed to explain the genesis of these layers, including: immiscibility, fractional crystallization/mineralaccumulation, disequilibrium crystallization, chamber rejuvenation and magma mixing, and hydrothermalenrichment2-6, 9. However, no consensus has been reached as to which model may most accurately describesthe petrogenesis of the Fe-Ti-oxide rich layers because each authors’ interpretation of the data has in turnbeen challenged by other workers.Objective: To apply new techniques and ideas developing in magma chamber research to the magnetitelayers in the Upper Zone of the Bushveld Complex and use the new datasets to test models for developmentand differentiation of UUMZ layers. The results will be applied to magnetite pipes that have similar mineralassemblages to the magnetite layers, but have contested origins.Hypothesis: Testing different previously proposed models for the origin of Fe-Ti-oxide rich magnetite andnelsonite layers in the Bushveld Complex will result in a new model that combines certain aspects of theseend-member processes to more accurately define the petrogenesis of the oxide-rich layers.Methods: The western limb will be studied via samples from the Bierkraal cores and the magnetite layersin the eastern limb will be sampled during field-mapping. Bulk rock geochemistry analyzed with x-rayfluorescence (major elements) and LA-ICPMS (trace elements). Mineral compositions will be analyzed byEPMA and LA-ICPMS; the data will be integrated with photomicrographs collected using optical andscanning electron microscopy.Testing models: Immiscibility. Silicate liquid immiscibility occurs when a homogenous silicate meltseparates into two compositionally distinct liquids with identical mineralogy but in differing proportions.In mafic layered intrusions, this process begins after considerable crystal fractionation, resulting in a crystalmush. If the permeability of the crystal mush is high, liquid may separate by gravity, producing nearlymonomineralic layers5,6. The presence of Fe-rich silicate inclusions in minerals such as plagioclase andapatite is evidence for liquid immiscibility. If the Fe-Ti-oxide rich layers formed through immiscibleprocesses, then the crystallized Fe-rich melt inclusions will have major and trace element content similarto the magnetite layers. Additionally, the host mineral that crystallized from the Si-rich melt will have lowerREE, HFSE, P, Ti, and FeO contents than the conjugate Fe-rich inclusions7.Fractional crystallization/mineral accumulation. Fractional crystallization of magma will lead to adense residual magma that will begin to crystallize magnetite and accumulate into magnetite-rich layers.The resulting magma after magnetite crystallization will have a lower density and rise buoyantly continuingthis process. If fractional crystallization occurred, a fractionation trend will be recorded in minerals withincreasing height in the section. Consequently, stratigraphically higher magnetite and anorthosite layerswill have increased iron enrichment, lower plagioclase An%, lower Mg# in pyroxene and olivine, lower Vcontent in magnetite, and higher whole rock SiO wt%4. Additionally, in a closed system with continuous2fractionation, incompatible elements become more enriched and compatible elements depleted withincreasing stratigraphic height. In magnetite, this will be recorded as decreased Ti, V, and Cr (compatibleelements) and increased Si and Ca (highly incompatible)8.Disequilibrium crystallization. Another idea proposed for the genesis of magnetite-rich layers israpid crystallization in disequilibrium conditions in response to increased oxygen fugacity (ƒO ) towards2the base of a magma chamber. As magnetite crystallization progresses, ƒO is lowered. Vanadium (V)2partitioning between magnetite (mt) and clinopyroxene (cpx) can be used as a proxy for oxygen fugacity,as it is sensitive to changes in ƒO . If V /V increases, this corresponds to decreasing ƒO 2. Additionally,2 mt cpx 2if the Fe-Ti-oxide layers crystallized instantaneously in high ƒO , Mg, Al, and Si contents will be relatively2enriched in magnetite from these layers compared to disseminated magnetite in anorthosite/gabbro layers9.Chamber rejuvenation and magma mixing. If new magma was injected periodically during theemplacement of the UUMZ to form the distinct Fe-Ti-oxide rich layers, step-like changes in mineralcomposition through a vertical unit, rather than a smooth fractionation trend, will be observed. Also, if thenew magma is more primitive than the final fractionation stages of the previous injection then compositionalreversals will be observed moving up-section. Compositional reversals in magnetite will be seen as higherCr and V contents followed by an abrupt change to lower content4.Hydrothermal enrichment. This model is particularly applicable to the magnetite pipes that have avertical structure. If the Fe-Ti-oxide rich layers formed by hydrothermal enrichment rather than having amagmatic origin, magnetite will be depleted in Ti, Al, and HFSE, as hydrothermal fluids have generallylow concentrations of these minerals due to their relatively low solubility. In contrast, magmatic magnetitewill be relatively enriched in compatible elements. Silicon and Ca are two elements that are highlyincompatible with magnetite, so if these are enriched in the samples, it suggests hydrothermal activity.Another characteristic of magnetite that is indicative of hydrothermal enrichment is if the Ni/Cr ratio is >1(in silicate magmas this ratio is always less than one)9. In addition, photomicrographs of magmaticmagnetite commonly show concentric compositional zoning in contrast to patchy textures common ofhydrothermal minerals9.Intellectual Merit: The UUMZ contains world class deposits of important strategic elements includingvanadium, iron, and titanium. There are limited global resources of these minerals and global demand isgrowing exponentially2. Understanding how these ore deposits form is critical both globally and to U.S.interests, as the country is dependent on foreign sources for many of these important commodities. Thereis still much debate about the formation of magnetite layers in the Bushveld Complex, even though theyhave been identified and studied for many years. Results from this research may be applied to other partsof the extensive Bushveld Complex, as well as to other layered mafic intrusions around the world.Broader Impacts: Impacts of this research of magnetite layers of the Bushveld Complex extend beyondEarth. One of the most exciting endeavors of the last century, sending humans to outer space, has propelledscientific curiosity perhaps more dramatically than any other scientific activity. As interest in deep-spaceexploration and establishing a human presence on Mars and other planets increases, so has the need tounderstand the processes of ore-deposit formation and exploitation. Lessons learned about thedifferentiation of magnetite layers in the Bushveld Complex may provide insight and strategies for sourcingiron and titanium (and other metals) that are crucial to permanent infrastructures, but are not cost effectiveto send from Earth.Results from my research will be shared with colleagues through publications and presentations atconferences, including the national GSA and AGU conferences. Additionally, one of my long-term goalsis to provide educators with resources they can implement in their classrooms to encourage curiosity intheir students and create lifelong learners who can contribute to scientific advancement. Applying ore-forming processes to ideas of colonization on other planets offers the potential for stimulating STEMactivities that can be leveraged into K-12 educations to increase engagement in science. Potential projectstopics could include what would be required to mine resources on another planet, requiring students toengage strategy and research, and in the process hopefully garner a lifetime of scientific curiosity.[1] Zeh et al. (2015) Earth Planet Sci Lett, v. 418, p. 103-114 [2] Fischer (2018) PhD Dissert., 129 p.[3] Scoon and Mitchell (2012) S Afr J Geol, v. 115.4, p. 515-534 [4] Tegner et al. (2006) J Petrol, v. 47, p.2257-2279 [5] Cawthorn (2015) in Layered Intrusions, p. 515-587 [6] VanTongeren and Mathez (2012)Geology, v. 40, p. 491-494 [7] Veksler et al. (2006) Contrib Mineral Petrol, v. 152, p. 685-702[8] Dare et al. (2014) Miner Depos, v. 49, p. 785-796 [9] Klemm et al. (1985) Econ Geol, v. 80, p. 1075-1088"
53.0,"Intellectual Merit: Inspired by the adaptability of biological organisms, soft robots haveemerged to address some of the technical limitations of conventional rigid robots. Although rigidrobots are remarkably capable at high-precision and load-bearing tasks, their stiff materialproperties, with a Young’s modulus in the range of 109 – 1012 Pa, inherently limit their ability tophysically interact with their environment. In contrast, soft robots are composed from gels,fluids, and elastomers with a Young’s modulus of 106 – 109 Pa. These soft materials mimic themechanical properties of biological tissues and can bend, stretch, and compress.This ability to conform is key for applications in human-robot interaction, biomedicaldevices, and space-restricted environments. For example, a rigid wearable accessory has limitedplacement locations that are both comfortable for the user and informative for the device. Incomparison, a soft wearable device, that can compress and stretch, greatly increases compatiblelocations and therefore potential applications, such as biometric monitoring or activity tracking.These advantages have also been shown to enable successful applications in medicine, such asorgan-assist sleeves, drug delivery, prostheses, and surgical tools [1].However, soft robot applications still face prevalent barriers to improved functionalityover rigid robot solutions. The same material properties that give soft robots their versatility alsocreate challenges in actuation, control, sensing, and modelling. In contrast to rigid systems with asmall number of joints, soft robots possess many more degrees of freedom due to theircontinuous elastic bodies. In order to fully understand their environment, soft robots requireeffective sensing tools in a stretchable format. However, soft and stretchable sensing solutionshave only been recently developed for strain and pressure sensing. The lack of sensoryinformation has resulted in the absence of sensor-based control and higher-level decision makingthat would be customary for a rigid robot [2].The Soft Machines Lab at Carnegie Mellon University led by Professor Carmel Majidimade a breakthrough in soft robotic sensing capabilities by demonstrating a hybrid soft sensorskin with orientation, pressure, temperature, and proximity sensing processed on-board [3].Finally armed with multimodal sensing to determine the soft robot’s environmental and internalstate, a unique opportunity has arisen for the development of sensor-based control for soft robots.Proposal: As part of the team that developed the soft sensor skin in [3], Iwill build on our previous work and implement sensor-based control of arobotic arm and gripper using the sensor skin. This will serve as the firstdemonstration of the feasibility of sensor-based feedback control on thissoft robot system. The approach can then be extended to a wider range ofsoft robotic systems, which I will further explore in my graduate research.I. Physical System: The sensor-based control system will be implementedon a robotic arm and elastic sensor skin adhered to a two-finger softFig. 1: Hybrid soft sensorgripper. The sensor skin will have two soft strain sensors made of liquidskin described in [3].metal traces, a time-of-flight (TOF) sensor to measure distance, an IMU, and an on-boardmicroprocessor to process sensor data. The strain sensors will be located on each of the innerfingertips to detect the presence of the object. The TOF sensor will be placed on the palm of thegripper, parallel to the scanning surface. The IMU will be placed next to the on-board processor,which will be located at the top of the gripper, to sense the gripper’s orientation. The sensor skinwill be connected to a computer hosting a finite state machine to sequence behaviors. Thecomputer will be connected to the robotic arm, so that the finite state machine can generate itsmovement.1II. Control System: The sensor-based control system will consist of two main components.i. TOF data processing: The gripper will scan the table in uniform rows, collecting TOF data tocreate a 2D array of distance measurements. This 2D array can then be analyzed as an imageusing OpenCV to identify the size, orientation, and location of each object on the table. Thedevelopment of this TOF algorithm is crucial to the development of soft robot feedback controlby allowing for rudimentary image processing when camera sensors are inaccessible.ii. Finite state machine: The robot arm and gripper will be controlled by a finite state machine(FSM) that uses sensor input to govern the state. The size, orientation, and location of the objectwill be determined by the TOF algorithm in the first state. After the object has been identified,the strain, IMU, and TOF sensor data will be used to sense the presence of the object in the graspof the gripper. The presence or absence of the object would inform the system of a successfulgrasp, lift, transport, or release of the object. This information will be used to traverse the statesof the system. The FSM will demonstrate basic autonomy with feedback control of a soft system.III. Testing: The performance of the sensor-based control of the robotic arm and gripper will betested against an open-loop robotic arm and gripper in a grasping and placement task. Objects ofvarying size will be placed on a flat surface in front of the gripper. The gripper and robot armwill be programmed to have four potential actions: grasp, lift, move, and release. The open-loopsystem will use provided object locations and complete each of the actions strictly in sequence.The sensor-based control system will not have prior information about object locations; instead,it will use sensor data to determine the object locations after scanning the workspace. Thesensor-based control system will also have the opportunity to decide whether to move onto thenext action or repeat previous actions based on sensor data. The success rate of grasping, lifting,placing, and overall task completion for each system will be compared over multiple trials.One potential challenge with this system is noise from ambient light conditions affectingthe accuracy of the TOF data. Because the system relies on the TOF data to locate and interactwith the object, the accuracy of this data is crucial. A potential solution would be calibrating theTOF sensor to the specific environment that it will be operating in.Broader Impacts: This proposal addresses a key challenge in the control of soft robot systemsby demonstrating a novel implementation of sensor-based control using a multimodal sensingskin. A successful demonstration will further push the boundaries of control and autonomy in thefield of soft robotics, as well as provide a platform for more complex control architectures in thefuture. These advancements are necessary for ubiquitous soft robots in everyday life; forexample, soft robot applications in the care and improved quality-of-life for the elderly.One practical application for this specific soft robot system consisting of a sensing skin,soft gripper, and robot arm would be to sort and grasp food objects in a food processing facility.The soft gripper would be better-suited to handling delicate food objects than a rigid gripper,which would reduce the amount of damaged food from handling errors. The sensor-based controlsystem would allow for unsupervised operation, as expected in a modern facility.In addition, this project could be demonstrated in STEM outreach to inspire interest inSTEM from K-12 students. Because the differences in performance of this system could beeasily seen and understood with little technical background, and the task of grasping and placingis familiar, this demonstration would be particularly well-suited to a young audience.[1] M. Cianchetti, et al.,""Biomedical Applications of Soft Robotics."" Nature Reviews. Materials 3.6 (2018): 143-53.[2] T. Thuruthel, et al.,“Control Strategies for Soft Robotic Manipulators: A Survey,” Soft Robotics, Vol. 5, No. 2(2018).[3] T. Hellebrekers, K. B. Ozutemiz, J. Yin and C. Majidi, ""Liquid Metal-Microelectronics Integration for aSensorized Soft Robot Skin,"" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).2"
54.0,"for the Multiple Mirror TelescopeThere are planets where it rains rubies. Specifically, some planets orbiting alien suns showevidence of clouds made of corundum, which is the basis on earth for rubies and sapphires(Wakeford, 2016). This is a romantic discovery from observing exoplanet atmospheres, but morepractical research looks for possible signs of life on other planets, be it bacteria or dog in an aliensuit, in the form of combinations of molecules that indicate non-equilibrium chemistry (e.g.water, oxygen, methane, ozone). In ground based observatories, research of this nature is onlymade possible with adaptive optics (a system that corrects observations in real time by usingwavefront sensors to observe the shape of incoming light). Infrared (IR) observatories, like theMultiple Mirror Telescope (MMT) have the potential to observe these signs of life, but at presentlack the instrument sensitivity or wavelength range to achieve these goals. In collaboration withthe NSF funded project MAPS (the MMT Adaptive optics exoPlanet characterization System) Iwill test, model, integrate, and perform on sky commissioning with two IR and visiblepyramid wavefront sensors on the Multiple Mirror Telescope. This proposed upgrade to theMultiple Mirror Telescope provides observations of fainter targets yet to be observed, increaseswavelength range, and tests new wavefront sensing techniques that inform the next generation oftelescopes.Intellectual Merit :In the field of exoplanet astronomy, ground-based observations are limited by the Earth'sturbulent atmosphere, which causes incoming light to a telescope to spread irregularly over alarger area. When the object is a faint point-like source, better adaptive optics systems enableobservations of star-planet-systems that we could not have seen before. With adaptive optics(AO), observations are corrected in real time by applying a correction (calculated by wavefrontsensors) with mirrors that deform to correct the atmospheric perturbation. Figure 1 demonstratesan example of how light appears after corrections by adaptive optics at Lick Observatory, as wellas the predicted improvement to the AO system at MMT with the MAPS upgrade.Figure 1. Left : Lick Observatory image with and without applied correction to the wavefront. (Max, 2019) Right :Using Strehl (the ratio of peak intensity for an image with and without aberrations) as a metric, we can see thepredicted performance upgrade due to AO on MAPS through J and H bands (the IR regime) (Morzinski, 2018).My proposal is unique because I plan to incorporate the two wavefront sensors in asingle system with a dynamic choice of IR or visible wavefront sensing, drastically improvingimage quality for the final IR science images. The observer will choose which wavefront sensorwill provide better correction for their observation; for a source that is significantly brighter inthe IR or visible, collecting more photons means faster and more accurate corrections, which isespecially vital when the atmosphere is moving in real time. To that end, the switch to pyramidwavefront sensors will provide an additional improvement to photon collection over the presentShack-Hartman wavefront sensor installed on MMT. Additionally, two new detectors will beused for the wavefront sensors: a CCID-75 visible detector (which is new to astronomyapplications), and a SAPHIRA IR avalanche photo diode detector; both have reduced readnoiseover the current detector used for wavefront sensing with MMT (Morzinski, 2018). As shown inFigure 1, the proposed improvements to the AO system will provide a 40-50% improvementin the observed intensity in the IR as compared to the existing MMT system.Plan of Work :(1) Run an initial analysis to select our exact infrared regime. Specifically I will predictif the J band (1.1-1.4 microns), H band (1.5 – 1.8 microns), or an overlap, will be better for ourproposed targets. This will be informed by my work with the Exoplanet Characterization ToolKit, with an emphasis on the atmospheric retrieval tools contributed by Mike Line (Fowler,2018.) I will evaluate to what extent water, carbon monoxide, and carbon dioxide (the moleculeswe expect to find in our initial round of Jupiter-like targets) are observable in these proposedbands given varying host stars shining through varying planetary atmospheres. (2) Performinitial experiments to setup and test the two wavefront sensors in the optics lab facilities atthe University of Arizona. Specifically I will integrate the two wavefront sensors in the Arizonalab facilities and find ways to programmatically take images and control hardware on the testbed.This work will be streamlined by my work on GLARE (the Generalized Lab Architecture forRestructured optical Experiments), a Python suite of automated experiment software and testbed-agnostic controllers for hardware common in optical testbeds (Fowler, 2020). (3) Performfurther testing to reduce and correct for alternate noise factors including dark current,readnoise, optical ghosts, etc. Specifically, I will generate multiple images, isolate signs of thesealternate noise factors, and test alternative hardware configurations and modes and/or calibrationsoftware to optimize final image quality. My work on the Wide Field Camera 3 Quicklookproject (a codebase including a dark current and readnoise monitor) will inform the detection andremoval of noise from these experiments. (4) Integrate the wavefront sensors into MMTalongside on-sky commissioning of the full adaptive optics system.Unique Resources :Many of the investigators of MAPS are at the University of Arizona, including Dr. KatieMorzinski (the principal investigator and an Assistant Astronomer) who will advise me for thisproject. The University of Arizona has two lab spaces that will support this project, as well as avibrant instrumentation group to support and facilitate this work. The University of Arizona hasunfettered access to MMT as well as 50% of its telescope time for calibration observations andexperiments, and as it is local to the university at Mount Hopkins, we can actively iterate withMMT and the lab to test and improve new components.Broader Impact :The original NSF MAPS proposal includes a Winter School, a brief winter workshopaimed at graduate students, postdocs, and professionals to teach the science of exoplanetinstrumentation. The Winter School is based on previous NSF Professional DevelopmentProgram funded programs like Adaptive Optics Summer School led by the Center for AdaptiveOptics. As part of this work, I will design a lab demonstration exploring the distinction betweenan IR and visible wavefront sensor. My experience as a teaching assistant and SoftwareCarpentry instructor will facilitate creating and leading a lab for my colleagues, under theadvisement of Dr. Morzinski who is leading the Winter School.Fowler, J. et al. “G.L.A.R.E..”, AAS Meeting #235, 2020 --- Fowler, J. et al. “ExoCTK”, AAS Meeting #231,2018 --- M., Claire “Introduction to AO and the CfAO.” AO Summer School. 2019. --- Morzinksi, K. “MAPS:The MMT AO ExoPlanet Characterization System .” Cf AAO Retreat. 2018. --- Wakeford, H. R. et al. “High-Temperature Condensate Clouds in Super-Hot Jupiter Atmospheres.” MNRAS, (2016)"
55.0,"The continued reliance on agricultural and petrochemical-based methods of production for manyhigh-value compounds threatens future generations with shortages of essential manufacturingmaterials, organic solvents, biofuels, and pharmaceuticals. The application of synthetic biologyto metabolic engineering has worked to address this growing concern by transferring requisiteenzymatic pathways from native organisms to standardized chasses and manipulating them toboth decrease dependence on non-renewable inputs and increase overall production yield.Despite continued efforts to improve the tunability and consistency of reaction progress withinlarge-scale bioreactors, traditional controller-based methods of optimization remain stymied byexcessive variability characteristic of biological systems.Intellectual Merit: Recent developments in the application of optogenetic tools to metabolicpathways have demonstrated potential in addressing the lack of tunability and irreversibility oftraditional chemical-inducer based control schemes. Leveraging the implicit reversibility ofoptogenetic induction, Milias-Argeitis et al. (2016) developed an automated transcriptionalcontrol system to maintain a constant concentration of fluorescent protein as a proof of concept.While the proposed systems can be used to rapidly increase protein production, the tunability ofthe system is limited by the slow rate at which the proteins degrade. In the context of metabolicapplications this delay could contribute to the non-optimal accumulation of toxic intermediatesand decrease the applicability of feedback structures, reducing overall fermentation yield. Thedevelopment of a reversible post-transcriptional control mechanism presents a novel,generalizable solution to this meaningful challenge.Hypothesis: A reversible, post-translational system for the control of selective proteindegradation can be created using existing optogenetic toolkits to rapidly and precisely decreaseprotein concentration.Approach: A selective protein degradation tag is conjugated to the coding sequence of a targetprotein. The tag marks the target protein for degradation via ClpXP, a selective proteasecomprised of ClpX and ClpP subunits2. The reconstitution of these subunits is facilitated throughheterodimerization of the cryptochrome Cry2 to the protein CIB13. Sufficiently orthogonal to thegreen light (535nm) and red light (672nm) utilized for transcriptional control, instances of Cry2and CIB1 reconstitute in the presence of blue light (470nm) and spontaneously disassociate in itsabsence4.A BFig. 1 Genetic circuit for proposed selective degradation scheme. (A) Representation of “slowresponse” transcriptional control used in Milias-Argeitis et al. (2016) updated with a proteindegradation tag. (B) Representation of “fast response” post-transcriptional control featuring thereconstitution of the ClpXP protease in the presence of blue light to degrade tagged proteins.1Research Statement Kevin FitzgeraldAim 1: Ensure conjugation of CIB1 tag to ClpX subunits has negligible impact on hexamerformation.The ClpX subunit is itself composed of six ClpX subunits. In the context of this project, eachaClpX subunit would be conjugated to a CIB1 dimerization domain and constitutively produced.aAlthough a small protein, it is critical to ensure that the conjugation of CIB1 to ClpX does notainterfere with the formation of hexameric ClpX. To achieve this aim, a library of mutant ClpXasubunits with CIB1 conjugated at different locations will be generated via rational design andscreened for their ability to recombine via western blot. Utilizing a non-conjugated ClpP subunit,the functionality of structurally-promising CIB1-ClpX mutants will then be evaluated by theiraenzymatic capacity to degrade tagged fluorescent proteins.Aim 2: Ensure ClpX/ClpP fusion is negligible in the absence of blue light and reversiblefollowing exposure to blue light.Wild type ClpX and ClpP subunits independently recombine via the formation of hydrogenbonds between key looping peptides on their exteriors. For the system to selectively degradetagged proteins of interest, it is necessary to minimize any spontaneous recombination andsubsequent functionality of conjugated ClpXP in the absence of blue light. Simultaneously,the utility of this light-based system is contingent on the reversible nature of the optogeneticreactions. Once background ClpXP functionality is minimized, it is also possible that the ClpXPcomplex formed upon initial Cry2/CIB1-mediated binding interactions will remain cohesive andfunctional despite the dissociation of conjugated light domains. It is therefore necessary torationally generate and screen mutations within the ClpX/ClpP binding domains capable of bothminimizing subunit binding affinities and maintaining enzyme functionality in the presence ofblue light.Broader Impacts and Future Directions: Properly tuned to compatibly function withexisting transcriptional control systems, a rapid, light-controlled protein degradation systemwould serve as a valuable tool in improving the sensitivity of optogenetic feedback systems inindustrial fermentation processes. By effectively decreasing system lag, target concentrations ofpotentially toxic enzymes can be maintained more consistently despite excessive backgroundnoise. In the context of metabolic engineering this increase in control has the potential toimprove both the speed and yield of fermentations. For those individuals relying onfermentation-based pharmaceuticals for the treatment of disease or fermentation-based biofuelsfor energy, even minor improvements in fermentation yield could decrease costs and improveaccessibility to such essential compounds. In the future, the implementation of optogeneticprotein controllers at each step in a metabolic process could serve to drastically improve thetunability and engineering capacity of large-scale fermentation processes.References: 1. Milias-Argeitis, A., Rullan, M., Aoki, S., Buchmann, P., & Khammash, M. (2016). Automatedoptogenetic feedback control for precise and robust regulation of gene expression and cell growth. NatureCommunications, 7(1). doi: 10.1038/ncomms12546 2. Baker, T., & Sauer, R. (2012). ClpXP, an ATP-poweredunfolding and protein-degradation machine. Biochimica Et Biophysica Acta (BBA) - Molecular CellResearch, 1823(1), 15-28. doi: 10.1016/j.bbamcr.2011.06.007 3. Park, H., Kim, N., Lee, S., Kim, N., Kim, J., &Heo, W. (2017). Optogenetic protein clustering through fluorescent protein tagging and extension of CRY2. NatureCommunications, 8(1). doi: 10.1038/s41467-017-00060 4. Kennedy, M., Hughes, R., Peteya, L., Schwartz, J.,Ehlers, M., & Tucker, C. (2010). Rapid blue-light–mediated induction of protein interactions in living cells. NatureMethods, 7(12), 973-975. doi: 10.1038/nmeth.15242"
56.0,"projects, and written reports, as well as quizzes and exams. In large enrollment courses,instructors often use multiple-choice questions as an assessment method because of the ease andperceived objectivity in grading.1 Although multiple-choice exams are useful for providing fastfeedback about student performance, the multiple-choice item format has been criticized forprimarily assessing low levels of cognition.2 Biology assessments that fail to target higher-orderthinking can be detrimental to the student learning process because these low-level assessmentslimit the development of critical reasoning and problem-solving skills, do not promote the long-term retention of course material,3 negatively affect study habits,4 and hinder scientific inquiry.5Bloom’s taxonomy is widely used in biology education researchas a tool for evaluating student performance and guiding thedevelopment of instructional strategies.6,7 Bloom’s taxonomyconsists of a hierarchy of cognitive skills: remember, understand,apply, analyze, evaluate, and create.8,9 This framework can beused to categorize the cognitive levels assessed by multiple-choice and constructed-response items on biology exams.The division of biology courses into introductory andadvanced courses implies that the higher-level coursesprovide opportunities for students to gain a greater depth ofconceptual knowledge and to practice the higher-ordercognitive skills that are necessary for STEM careers. Although previous research has identifiedthat introductory biology courses primarily assess the two lowest levels of Bloom’s taxonomy,10there are few studies that analyze if assessments in 300- and 400-level courses target the higher-order thinking that is presumed in advanced biology courses.The advantages and disadvantages of multiple-choice and constructed-response items arewell-studied,1 but there is little research on the extent to which the different item formats areused when assessing content knowledge and cognitive skills in introductory and upper-levelbiology courses. Multiple-choice items are traditionally associated with assessing the lowestlevels of Bloom’s taxonomy and constructed-response items are often thought to target higher-level thinking, but there has not been extensive research in biology courses to determine if thereis evidence to support these stereotypes about item format.There is a gap in the literature regarding the frequency with which multiple-choice andconstructed response items are used in introductory and upper-level biology courses to assesshigher-order cognitive skills. My research will fill this gap, highlight strengths of the currentmethods of biology assessment, and identify the areas where assessment can be improved tobetter reflect the knowledge and skills that are required for success in STEM careers.Research Questions 1) Is there a difference in the cognitive levels assessed in introductory andupper-level biology courses? 2) What is the relationship between item format and cognitive levelassessed on undergraduate biology exams? 3) What decisions, processes, and methods areinstructors using to design undergraduate biology exams?Methods To answer these research questions, I will survey exams from biology instructors at arange of undergraduate institutions. Biology instructors will be recruited for participation in theresearch through professional networks such as the Ecological Research as Education Networkand the Society for Advancement of Biology Education Research. The collected examdocuments will be reviewed using a directed qualitative content analysis, a process in which eachquestion on the exam will be categorized by a Bloom’s cognitive level as well as by item type.I will mentor undergraduate research assistants and teach them how to use Bloom’staxonomy to review exam items. I will use a Cohen’s kappa analysis to determine interraterreliability for consensus of the classifications of Bloom’s level and item type between thereviewers. To determine which factors predict the Bloom’s level of exam items, I will runordinal regressions with item type and course level as predictor variables and instructor as arandom effect. In the analysis of Bloom’s levels on individual exams, I will calculate a weightedaverage because items designed to assess higher-order thinking may tend to have a higher pointvalue than items assessing lower-level cognitive skills.I will conduct semi-structured interviews with instructors to clarify the decisions, processesand methods used to design biology exams. The interview protocol will consist of three sections:1) questions about possible constraints, such as large class size, that might limit the type ofassessments administered in their courses, 2) participant familiarity with Bloom’s taxonomy orother frameworks for assessing cognitive skills, and 3) goals for exam design.This research focuses on exams because this form of assessment tends to reflect the types ofknowledge and skills that students are expected to master in a course, but I acknowledge thatthere are assessment methods other than exams. There are some limitations to Bloom’staxonomy as a framework because of its design for broadscale application in education research.These limitations will be addressed by using the Blooming Biology Tool, which is amodification of the Bloom’s framework tailored for the analysis of questions on biology topics.7Intellectual Merit My experiences as a high school science teacher and as an AssessmentSpecialist at the Educational Testing Service provided the skillset that I will use to conduct theproposed research. Previous studies have identified that introductory biology courses primarilyconsist of items assessing low-level cognitive skills,10 but there are few studies that haveexamined either the assessment methods in upper-level biology courses or the relationshipbetween item type and cognitive level assessed on biology exams.Broader Impacts One goal for this research is to strengthen the quality of the undergraduatebiology education experience through identifying areas of assessment that can be improved.Students who are administered high-level items throughout their science courses are more likelyto acquire deep conceptual understanding of the course material,2 so determining whereassessments can be modified to target higher levels of Bloom’s taxonomy is a step in the processof promoting intellectual development in biology students. This research also addresses thedisparity between the cognitive skills assessed on introductory biology exams and the cognitiveskills required for solving real-world scientific problems. Although this research will beconducted primarily on assessments from American undergraduate institutions, biology examsare not unique to the United States, and the implications of this research will have internationalreach. A second goal of this research is to promote the advancement of biology educationresearch, which will be accomplished through training undergraduate students in educationresearch methodology, involving students in the process of qualitative and quantitative dataanalysis, and collaborating with students to present the research at science conferences.(1) Stanger-Hall, K. F. CBE Life Sci. Educ. 2012, 11(3), 294-306. (2) Martinez, M .E. Educational Psychologist.1999, 34(4), 207-218. (3) Jensen, J. L. et al. Educ. Psychol. Rev. 2014, 26(2), 307-329. (4) Entwistle, A.; Entwistle,N. 1992, 2(1), 1-22. (5) Momsen, J. et al. CBE Life Sci. Educ.2013, 12(2), 239-249. (6) Bissell, A. N.; Lemons, P. P.BioScience. 2006, 56(1), 66-72. (7) Crowe, A. et al. CBE Life Sci. Educ.2008, 7(4), 368-381. (8) Bloom, B. S. et al.McKay: New York, NY, 1964. (9) Anderson, L. W.; Krathwohl, D. R. Allyn & Bacon: Boston, 2001. (10) Momsen,J. et al. CBE Life Sci. Educ. 2010, 9(4), 435-440."
57.0,"LeeAnnM.SagerMazziottiGroup,DepartmentofChemistry,TheUniversityofChicagoGoal: Iaimtodevelopamethodologytoexplorethedegreeofexcitoncondensationonaquan-tumcomputer,determinethepreparation(s)thatobtainmaximumexcitoncondensationforagivennumberofqubits,andprobethepropertiesofsaidexcitoncondensatestates.Introduction: Condensation phenomena has been an active area of research since 1924 whenEinsteinandBosefirstintroducedtheirideal“Bose-Einstein”gas.1Theidenticalparticlescompris-ingthisgas(bosons)wereproposedtobeabletoaggregateintoasinglequantumgroundstatewhensufficiently cooled.1 Later, London and Tisza attributed Bose-Einstein condensation (BEC) to bethesourceofsuperfluidity—thefrictionlessflowofzero-viscosityfluids—thathadbeenobservedin low-temperature liquid helium.2 In 1940, Pauli established the relationship between spin andstatistics, demonstrating that particles with integral spin values obey Bose-Einstein statistics—arebosons—and hence may form a condensate.2 Extrapolating further, pairs of fermions—particleswith half-integer spins—may interact such that the overall two-particle system has integral spinand is hence bosonic. In fact, recent experimental and theoretical investigation has particularlycenteredaroundthecondensationsofonesuchclassofbosons: excitoncondensates.3 Excitoncon-densation is defined by the condensation of particle-hole pairs (excitons) into a single quantumstate to create a superfluid. The superfluidity of electron-hole pairs involves the non-dissipativetransferofenergy,whichhasapplicationsinenergytransportandelectronics.3Motivation: While excitons form spontaneously in semiconductors and insulators and whilethe binding energy of the excitons can greatly exceed their thermal energy at room temperature,theyrecombinetooquicklytoallowforformationofacondensateinasimplemanner. Tocombatrecombination,thecouplingofexcitonstopolaritons,whichrequiresthecontinuousinputoflight,4andthephysicalseparationofelectronsandholesintobilayers,whichinvolvesimpracticallyhighmagneticfieldsand/orlowtemperatures,5areemployed. Thus,anew,more-practicalavenueforthecreationandstudyofexcitoncondensationisdesired;quantumcomputingofferssuchanavenue.A qubit is the basic unit of quantum computing (analogous to the classical bit); the qubit itselfis a quantum system whose most-general state is a linear combination of its two basis states ( 0| ⟩and 1 ,theclassicalbitstates)withanappropriatephasefactor(eiφ)givenby6| ⟩θ θ cos θΨ = cos 0 +eiφsin 1 = 2 .| ⟩ 2 | ⟩ 2 | ⟩ eiφsin θ! "" ! "" ! # $2 ""Ifaqubitisconsideredtobeaone-fermion,two-levelsysteminwhichth#er$eisaprobabilitypofthefermionbeinginthelower-levelstate( 0 )andaprobability1 pofthefermionbeingintheupper-| ⟩ −level state ( 1 ) where p = cos θ 2, then a single qubit can represent two particle-hole paired| ⟩ | 2 |orbitals. Assuch,asystemofN qubitscanbeviewedasN fermionsinanN-folddegenerate,two-# $level, particle-hole paired system. As explored in Ref. 7, such a model can demonstrate excitoncondensationinsystemswithasfewas3fermionsin6orbitals(i.e.,athreequbitsystem).Resources: InthisworktheIBMQuantumExperiencedevices—whichareavailableonline—willbeused. Qiskitopen-sourcequantumcomputingsoftwarewillbeemployedforanalysis.Aim-1, Probe the Extent of Exciton Condensation of an Arbitrary Qubit State: In or-der to computationally probe the presence and extent of condensation behavior, I aim to mea-sure the largest eigenvalue of the 2G˜ matrix—a calculable, characteristic property of excitoncondensation—on the quantum computer.8 The elements of the 2G˜ matrix are given by 2G˜i,j =k˜,˜lLeeAnnM.Sager Page1⟨Ψ |ψˆ i†ψˆ jψˆ ˜l†ψˆk˜|Ψ⟩ −⟨Ψ |ψˆ i†ψˆj|Ψ ⟩⟨Ψ |ψˆ k˜†ψˆ ˜l|Ψ⟩where |Ψ⟩is the N-fermion wavefunction; i,j,˜ l,k˜representspinorbitals;andψˆ andψˆarethecreationandannihilationoperators. Duetotheparticle-†hole pairing of each qubit, the spin orbitals denoted by i and j and the spin orbitals denoted by k˜and˜ l must correspond to the same qubit to be non-zero, simplifying the matrix. In order to obtainthe 2G˜ matrix on a quantum computer, these elements must be translated into the basis of Paulimatrices. The expectation values of the Pauli matrices can be obtained through direct measure-mentfromaquantumcomputer,andthematrixelementscanthenbecalculatedthroughuseoftheappropriateconversion. Thelargesteigenvaluecanbecomputedfromthematrixobtained.Aim-2, Determine Preparation(s) for State(s) with Maximum Exciton Condensation: Aquantum state of qubits can be prepared on a quantum computer by the application of a unitarytransformation,Uˆ ,suchthat Ψ (1,2,...,N) = Uˆ Ψ (1,2,...,N) describesthepreparationofi i i 0| ⟩ | ⟩anN-qubitstatefromtheinitialstate Ψ = 00 0 . Iaimtodeterminetheappropriateunitary0| ⟩ | ··· ⟩transformation for a given number of qubits that corresponds with the maximum condensationof excitons—the largest eigenvalue. One particular N-qubit state that may be of interest on thissearchistheGHZstate—thestateinwhichallqubitsareinthe 0 stateorthe 1 statewithequal| ⟩ | ⟩probability (i.e., Ψ = 1 0 N + 1 N );6 this state is highly entangled and is hence an| GHZ ⟩ √2 | ⟩⊗ | ⟩⊗idealcandidateforexcitoncondensation.# $Aim-3, Probe Properties of Exciton Condensates: Any physical, measurable property of asystem corresponds to a Hermitian matrix (Aˆ) such that the eigenvalues of Aˆare the possible out-comes of measurement of said property. The elements of these Hermitian matrices can be writtenintermsoftheexpectationvaluesofPaulimatricesandcanthereforebeobtainedforagivenqubitpreparation. Fromthismatrix,theprobabilityofagivenmeasurement( Ψ a )andtheexpectationi n⟨ | ⟩valueofthatproperty( Ψ Aˆ Ψ )canbeobtainedwhere a istheeigenstatecorrespondingtoai i n⟨ | | ⟩ | ⟩givenmeasurementa and Ψ isanN-qubitstatepreparedbytheunitarymatrixUˆ . Forexample,n i i| ⟩the energetics of a prepared qubit state can be probed by obtaining the eigenvalues/eigenstates ofthetwo-fermionreducedHamiltonianmatrixgivenby2K = 1 1 2 Zj + 1 1 .N −1 −2∇1 − j r1j 2r12Intellectual Merit: This project aims to expand our understa%nding of exciton’condensation&phenomena. Shouldtheseapproachesprovesuccessful,areliableandfacilepreparationforexcitoncondensatestateswillbeachieved,andpropertiesofsuchcondensateswillbeabletobeprobedinastraightforwardmanner.Broader Impacts: The superfluidity of excitons in a condensate allows for the frictionlesstransportoftheexcitationenergy,releaseduponrecombinationoftheparticleandhole. Addition-ally, such superfluidity in a bilayer—with electrons in one layer and holes in another—allows forthe frictionless transfer of charge as long as current is directed in opposite directions in the twolayers—a phenomenon known as counterflow superconductivity.3 Understanding and exploitingthe superfluid properties of exciton condensates may hence be instrumental in the effort to designwiresandelectronicdeviceswithminimallossofenergy,decreasingoverallenergyconsumption.1 Einstein,A.KöniglichePreußischeAkademiederWissenschaften1924,261––267.2 Vilchynskyy,S.I.;Yakimenko,A.I.;Isaieva,K.O.;Chumachenko,A.V.LowTemp.Phys.2013,39,724–740.3 Fil,D.V.;Shevchenko,S.I.LowTemp.Phys.2018,44,867–909.4 Fuhrer,M.S.;Hamilton,A.R.Physics2016,9.5 Kellogg,M.;Eisenstein,J.P.;Pfeiffer,L.N.;West,K.W.Phys.Rev.Lett.2004,93,036801.6 Kaye,P.;Laflamme,R.;Mosca,M.Anintroductiontoquantumcomputing;OxfordUniversityPress,2010.7 Lipkin,H.J.;Meshkov,N.;Glick,A.J.Nucl.Phys.A1965,62,188–198.8 Garrod,C.;Rosina,M.J.Math.Phys.1969,10,1855–1861.LeeAnnM.Sager Page2"
58.0,"Introduction: Grain growth is a critical process to both metals and ceramics processing, as grainsize plays a major role in bulk material properties, such as fracture toughness. Abnormal graingrowth (AGG) is a process by which the growth of a small fraction of grains is incentivized andthey grow faster than their neighbors, resulting in a bimodal grain size distribution andheterogeneous bulk properties. Though work has been conducted in this field for decades1, thecause and mechanism behind AGG are still poorly understood.The process is particularly import to ceramic materials, as the superior thermal resistanceof ceramics lends itself to extreme environment applications. At these elevated temperatureskinetics are accelerated, expediting grain growth and AGG – this is a particular challenge inalumina, which is very susceptible to AGG2. Processing of these materials also raises concern, ashigh sintering temperatures can have the same deleterious effect. As such, it is vital to understandhow to control grain growth in ceramics processing and applications.Textured microstructures with enhanced mechanical properties in materials can bedesigned by controlling their crystallographic orientation during processing. One technique thathas been explored is the application of a magnetic field during processing3,4. This project willinvestigate how texturing by applied magnetic field inalumina ceramics impacts grain growth through the useof electron and synchrotron X-ray based techniques, whichallow us to track individual grains and grain boundaries. Ihypothesize that a strong applied magnetic field duringprocessing will reduce grain growth and mitigate AGGdue the formation of low-angle and, thus, low-energygrain boundaries that have a low driving force to move.This will be validated by 3D microstructuralcharacterization, allowing the measurement of grainboundary orientation and character for textured samples.These results will be relevant to industrial applications ofalumina – including the manufacture of automotive partsand ballistic armor – as microstructure engineering can Figure 1. Preferential crystallographic orientation(ratio of 006 signal intensity to total signal intensity)improve the mechanical properties of ceramic materialsas a function of applied magnetic field strength inand improve stability and lifetime. alumina with different slip solid loading4.Objective 1: Preparation of textured Al O samples by thermomagnetic slip-casting process:2 3Alumina samples for this experiment will be prepared via slip casting with high-purity α-Al O2 3powder. Alumina is chosen as a test material due to its impressive mechanical properties andapplicability as a structural ceramic, as well as its susceptibility to texturing by magneticfield. A dispersant will be added to the slip to prevent the agglomeration of particles. The sampleswill be subjected to an applied magnetic field during casting – this texturing technique has beenshown to induce the growth of preferentially oriented grains during annealing3,4. Figure 1illustrates this effect in alumina. Samples will be cast under applied magnetic fields of 0-8 T with0 T being a control sample. Once cast, alumina green bodies will be sintered to near theoreticaldensity and annealed at temperatures above 1400 ºC for various times.Objective 2: Electron microscopy characterization of grain growth: From the bulk samples,centimeter-sized sections will be cut and polished for observation under a scanning electronmicroscope (SEM) to determine the initial average grain size by image analysis softwareemploying the linear intercept method5. The samples will then be further annealed under identicaltime and temperature conditions, and the same method will be repeated to determine the graingrowth in each sample. These results will provide a quantitative measure of grain growth asa function of applied magnetic field during processing and annealing time.Objective 3: 3D characterization of crystallinity and grain size distribution: The novelty ofthis work lies in the use of high-energy X-ray diffraction microscopy (HEDM) to characterize thesamples. In this technique, a sample is placed in the path of anincident X-ray beam and rotated while diffraction patterns arecollected. In post-processing, these can be indexed to generate acrystallographic map of the measured volume. From the bulk,millimeter-sized samples will be prepared. From 3Dcrystallographic maps measured by HEDM, true grain sizes (at aresolution of 1 µm) can be determined and a grain size distributioncreated. The non-destructive nature of this technique is extremelyadvantageous, as it will allow tracking of individual grains andboundaries across heat treatments. Thus, the slower movement ofindividual textured low-angle grain boundaries can be observedand quantified. Via 3D characterization of individual grains andboundaries, these results will verify a) the character andFigure 2. Set-up for HEDM synchrotronmotion of the boundaries as a function of applied magneticmeasurements, beamline 1-ID atAdvanced Photon Source, Argonne field, and b) whether observed low-angle grain boundariesNational Lab. Incident X-ray beaminduced by texturing reduce AGG.travels along positive z-direction.Research Plan: The timeline for this project is one year. Slip casting will be done at Oak RidgeNational Lab, which houses a commercially available thermomagnetic system offering up to 8 Tmagnetic field. The timeline for this step is one week, accounting for travel time, as slip casting isa well-known process that can be modified as needed. SEM will be conducted at the University ofFlorida, whose Research Service Centers house a TESCAN SEM. The polishing, samplepreparation, and data analysis will take 9-10 months. Lastly, HEDM experiments will be run at theNSF-sponsored CHESS synchrotron’s Structural Materials beamline, at which HEDM will beavailable beginning in December 2019. The experimental design considers that each HEDMmeasurement takes hours, and beamtime slots are limited. As such, high-priority samplesexhibiting strongly textured microstructure will be selected for synchrotron measurement.Broader Impacts: Beyond structural materials, microstructure engineering is essential tofunctional materials like oxide fuel cells and laser materials. The results of this project will offerquantitative insight into the use of an applied magnetic field to texture alumina ceramics – thisfundamental study will serve as a bridge for future industry-specific studies. As such, I wouldenjoy sharing these results at conferences and with industrial collaborators directly.1. Journal of the American Ceramic Society, 80(5), 1149–1156. 2. Scientific Reports, 6(37946),2–11. 3. Scripta Materialia, 54(6), 977–981. 4. Science and Technology of Advanced Materials,7(4), 356–364. 5. Journal of the American Ceramic Society, 91(7), 2304–2313."
59.0,"Background and Rationale: Cerebral Arteriovenous Malformations (cAVMs) are congenitalvascular lesions that affect 0.01-0.50% of the population. The annual risk of hemorrhage in thecAVM nidus is on average 3%, but the risk can be as low as 1% to as high as 33% depending onpatient-specific anatomies. Since the cAVM nidus has low resistance, it allows blood to shuntdirectly from arterial feeders (AFs) to draining veins (DVs) at high flow rates. As a result, if thehemodynamic stresses exceed the elastic modulus of the vessel wall, then a rupture may form andcause a hemorrhage. Embolization – the intravascular injection of embolic materials to AFs, is oneof the most common interventional therapies to prevent hemorrhages by diverting blood flow awayfrom the nidus. However, current methods to visualize blood flow patterns for embolizationtreatment planning are limited. Using the standard-of-care – 2D superselective angiogramsequences (2D+τ) and 3D rotational angiography (3DRA) – is challenging because the contrastagent reaches multiple regions of the nidus simultaneously, which prevents the identification ofcAVM compartments and fistulae. Therefore, there is an urgent need to elucidate blood flowpatterns in the cAVM to improve on treatment planning for embolization.One method to address this problem involves computational fluid dynamics (CFD) to simulatecAVM hemodynamics on patient-derived models. However, the length-scale of the nidus preventsCFD from being applied because the spatial resolution of 0.6mm in 3DRA is insufficient to capturethe 0.1mm or smaller diameters of intranidal vessels. As an alternative, some studies have resortedto using electric network models, but these models are typically not based on clinical data. Insteadof modeling every intranidal vessel in the nidus, I could model all the intranidal vessels collectivelythrough a porous volume [1], which has been shown to be a good approximation of the nidus [2].Simulating the density of intranidal vessels beyond the spatial resolution limitations of 3DRAis possible because voxel intensity is proportional to the amount of contrast agent in a vessel [1],which provides information on the internal geometry of the nidus. Based on Darcy’s law and massconservation, there are seven parameters that characterize blood flow through the porous volume:porosity, permeability, fluid viscosity, fluid density, quadratic drag factor, and the velocity andpressure boundary conditions [1]. These parameters can be inferred from 3DRA images. However,existing models [1], [3] lack validation with other paradigms, such as in vitro testing, and themodels are limited to Types IIa, IIb, and IV in the Yakes classification of cAVMs, which are notrepresentative of the population since there are a total of six types. My research objective is toinvestigate cAVM hemodynamics for embolization planning through generalized patient-specificCFD models for each angioarchitecture type in the Yakes classification, with a porous volumerepresenting the nidus, and validate in silico results using an in vitro flow loop.Specific Aim 1: Develop simplified cAVM digital phantoms for validation with varying spatialporosity distributions. I will design digital phantoms on SolidWorks (Dassault Systèmes, Vélizy-Villacoublay, FR) with one tubular AF and DV connected to a rectangular porous volume. Thephantom will be meshed using a triangular grid of approximately 104-105 elements for a celldensity of around 16 cells per voxel [1] using Gambit (Ansys, Canonsburg, PA). The velocityboundary condition at the AF inlet will be set according to phase-contrast magnetic resonanceangiography (PC-MRA) images, while a zero-pressure boundary condition will be used at the DVoutlet. Blood viscosity will be set to 4.00cP, density to 1060 kg/m3, and Re to 265. Fluent (Ansys,Canonsburg, PA) will be used to run the CFD simulation. To validate, I will confirm that the fluidbehaves according to expectations where flow paths will mostly circulate in nonporous regionsand that flow through porous regions will obey theoretical expectations from Darcy’s law.Specific Aim 2: Construct generalized patient-specific cAVM models. I will construct theboundaries of the cAVM and the AFs and DVs from 3DRA images. This will be done throughsegmentation (3D Slicer, Boston, MA), and I will obtain morphometric measurements usingAnalyze 12.0 (AnalyzeDirect, Overland Park, KS). Based on the segmented anatomies and patient-specific measurements, I will create generalized models for each cAVM angioarchitecture type onSolidWorks. Afterwards, I will mesh the models using a tetrahedral grid on Gambit. The porousvolume parameters will be inferred from 3DRA images and then averaged. As before, I will setthe fluid properties of blood based on nominal values. I will validate mesh convergence througha mesh independence study on Fluent by comparing coarse vs. medium, medium vs. fine, andcoarse vs. fine meshes. Moreover, I will compare porosity distributions through CFD between thegeneralized models and five different patient-specific anatomies for each angioarchitecture type.Specific Aim 3: Develop a flow loop for in vitro validation. I plan to modify an existing flowloop in Prof. Ajit Yoganathan’s lab to simulate cAVM hemodynamics. For example, the flowloop has two pressure measurement probes, which is insufficient for cAVM simulation. I will makemodifications to incorporate more probes to accommodate for all the AFs and DVs. From themodels used in the in silico study, I will manufacture transparent rigid physical phantoms using anMR-compatible resin (Watershed 11122, DSM Somos, Elgin, IL). I will then implement the sameinflow conditions from the computational study to the flow loop and compare flow field datathrough PC-MRA and digital particle image velocimetry for validation. Furthermore, I will alsomanufacture the five patient-specific anatomies for each angioarchitecture type in the Yakesclassification from Specific Aim 2 and compare in vitro flow field results to in silico results.Timeline and proposed laboratory: I would like to work with Prof. Ajit Yoganathan(Georgia Tech) because of the close alignment of research interests. I anticipate that this study willtake five years: one for Specific Aim 1, and two each for Specific Aim 2 and Specific Aim 3.Intellectual Merit: Currently, only Orlowski et al. [1], [3] have used a porous volume tosimulate the cAVM nidus. My project expands on this model, in that there have been nopublished papers that uses a generalized CFD model based on patient-averaged data, an invitro flow loop to validate and compare with in silico findings, and a porous model to simulatethe cAVM nidus. As post-embolization complications are a major concern, my computationalmodel can serve as the first step to a surgical planning software that meets this need.Broader Impact: With further exploration, the computational model proposed can beexpanded to a two-fluid model for simulating the propagation and solidification of embolictherapies, which can provide pre-operative outcome prediction, potential increase in embolizationsession efficiency, and optimize interventional strategies. My project may also be scaled-up forinterventional planning in other AVMs situated in the lung, muscle or bone, prognosis evaluation,and optimization of therapies. Finally, I will collaborate with clinicians at Emory University andUniversity College London to ensure clinical utility and present my work at conferences.Feasibility: From my master’s thesis, I will be supported by University College London,University College Hospital, and Kings College London to obtain 3DRA and PC-MRA patientdata. I will seek Institutional Review Board approval under Prof. Yoganathan’s support.[1] P. Orlowski, F. Al-Senani, P. Summers, J. Byrne, J. A. Noble, and Y. Ventikos, “Towards Treatment Planning forthe Embolization of Arteriovenous Malformations of the Brain: Intranidal Hemodynamics Modeling,” IEEE Trans.Biomed. Eng., vol. 58, no. 7, pp. 1994–2001, Jul. 2011. [2] C. W. Kerber, S. T. Hecht, and K. Knox, “Arteriovenousmalformation model for training and research,” AJNR Am. J. Neuroradiol., vol. 18, no. 7, pp. 1229–1232, Aug. 1997.[3] P. Orlowski, P. Summers, J. A. Noble, J. Byrne, and Y. Ventikos, “Computational modelling for the embolizationof brain arteriovenous malformations,” Med. Eng. Phys., vol. 34, no. 7, pp. 873–881, Sep. 2012."
60.0,"IntroductionOligodendrocyte precursor cells (OPCs) are the progenitor cells responsible for formingmature, myelinating oligodendrocytes (OLs) in the central nervous system during development.While the majority of OPCs differentiate early in life, there is a small pool that generate OLs overthe life span and can differentiate into OLs following white matter injury (WMI). Previous workhas shown that this differentiation is impaired in aging, reducing the ability to recover from WMI.1Significant upregulation of senescence markers in old vs young OPCs suggests senescence, a stressinduced state in which cells no longer proliferate, contributes to the impaired ability of OPCs todifferentiate2. Studies of senescence in other cell types has shown that it leads to reduced functionalcapacity and mediates the physiological consequences of aging. Thus, better characterizing OPCsenescence and the mechanisms involved would greatly improve our understanding of the role ofOPCs in brain aging.Although studies have identified some canonical senescence genes in OPCs, OPCsenescence genes have not been completely characterized, making accurate identification ofsenescent OPCs more difficult. Furthermore, due to our lack of understanding of OPC senescencemechanisms, there is a need to identify novel regulators of senescence in OPCs. Understandingthese mechanisms would greatly enhance both our understanding of OPC development, and ourability to promote CNS myelination in injury. Given this, I propose to identify an OPC-specificsenescence signature, and to identify novel regulators of senescence via a genome-wide CRISPR-Cas9 knockout screen. I intend to carry out this project with two faculty experts in glial cell biologyand genome-wide CRISPR screens.Research Plan:Aim 1: Define canonical and OPC-specific senescence markers in vivo by single-cell RNA-seqRationale: A comprehensive OPC-specific senescence signature has yet to be established. Such asignature would allow for a more accurate identification of senescent phenotypes in different OPCpopulations and may provide insight into the mechanisms underlying senescence in OPCs.Experimental Design: OPCs will be taken from 20-24 month old mice and senescent cell clustersidentified by flow cytometry using fluorescent antibodies to established senescence markers suchas NOTCH3, B2MG and DEP1, which have been shown to have high levels of expression insenescent cells3. Following identification of senescent OPCs, RNA from both proliferating (non-senescent) and senescent OPCs will be extracted and single-cell RNA-seq performed, with non-senescent OPCs serving as a control to identify genes implicated in aging, but not senescence. Datawill then be analyzed to identify differentially expressed transcripts in senescent OPCs, which willcomprise our OPC senescence signature. This experiment will yield genes both up- anddownstream of senescence and will allow for more accurate identification of senescent OPCs. Inthe future, candidates may be validated through in vivo knockout studies.Possible Outcomes: It is possible that with multiple senescence markers, we may miss OPCs whichdisplay lower levels of markers or do not display some at all. We can account for this in adjustingthe sensitivity of our gating and analysis, or utilizing fewer antibodies for selection.Aim 2: Identify regulators of OPC senescence via a CRISPR-Cas9 knockout screenRationale: Discovery of OPC-specific regulators of senescence will establish mechanismsunderlying OPC senescence and can be used to inform the development of mechanistic in vivostudies.Experimental Design: To identify regulatorsof OPC senescence, I will conduct a genome-scale CRISPR-Cas9 knockout screen with apooled lentiCRISPR library. Complex pooledDNA libraries will be combined, anddelivered in lentiviral constructs in 4-8 weekold OPCs, which should not display highlevels of senescence. OPCs can be culturedand transduced at a large scale which allowsfor genome-wide screens. After transductionand selection, I will select senescent cellsusing the flow-cytometry approach describedFig 1. Experimental outline for a senescence marker-in Aim 1 with multiple antibodies againstbased pooled CRISPR screen.senescence markers. Sorted cells will then beanalyzed to identify genes whose knockout increases or decreases senescence, yielding insight intomechanisms of OPC senescence.Possible Outcomes: If Aim 1 yields OPC-specific markers for which there are robust reporters orantibodies, these can be used in a parallel screen to support our initial findings. Future studies mayalso use the signature from Aim 1 to validate hits from Aim 2 in vivo.Intellectual MeritWhile senescence has been characterized in microglia and astrocytes, significantly less work hasbeen done in understanding OPC senescence and its relevance in aging. Given that the efficiencyof myelination has been shown to decline over time, this proposal, which aims to bettercharacterize OPC senescence, will greatly contribute to our understanding of the effects of agingon oligodendrocyte development and myelination. Furthermore, characterization of OPCsenescence opens up the possibility of mechanistic and in vivo studies, which will broadly increaseour knowledge of the roles of OPCs, and how they change over time.Broader ImpactThis work has implications for the understanding and treatment of neurological diseases, as agingis linked to an increase in disorders such as dementia and Alzheimer’s. Dementia is linked to lossof white matter and decreased myelination, and myelin breakdown is implicated in Alzheimer’s,pointing to a critical role for OPCs. This work could eventually allow for the reversal of senescentphenotypes in OPCs, potentially leading to curative treatments. Additionally, understanding OPCsenescence has important implications for demyelinating diseases such as MS, for which there areno remyelinating therapies. Recovery from such diseases is impaired by lack of understanding ofOPC differentiation; thus, understanding blockades to differentiation such as senescence mayinform future therapeutic development.FeasibilityMy previous work with oligodendrocyte development will significantly contribute to the successof the project. Furthermore, I am rotating in the lab of Dr. Ophir Shalem, whose lab has a strongand successful history of genome-wide CRISPR-Cas9 knockout screens, and who will provide theresources and mentorship need. I am also rotating with Dr. Chris Bennett, who has expertise in theisolation, culture, and sequencing of glia, and can offer the resources and mentorship needed forthis project. Having access to a wide breadth of experts in my fields, as well as Penn’s world-classfacilities and resources, also ensures the feasibility of this project.References: (1) Swenson et al. Translational Medicine of Aging 2019; (2) Neumann et al. Cell Stem Cell 2019; 3)Althubiti and Macip, Methods in Molecular Biology 2019."
61.0,"Introduction: Honey bees (Apis mellifera) are cornerstone pollinators and contribute nearly $20billion to the U.S. agricultural economy each year1. Honey bee populations have drasticallydeclined by an estimated 30-40% in the past three decades, and 2019 marks the largest winter hiveloss ever recorded1. Bee decline threatens the U.S. economy and food supply, which has drivenagricultural stakeholders and the scientific community to investigate reasons for honey bee deaths.A number of factors have already been identified, including habitat and foraging space loss,pesticide exposure, and infection by parasites, fungi and viruses. Bees have a commensalcommunity of microbes aside from pathogens that includes bacterial, viral, and eukaryotic species,collectively known as the microbiome. Like in most organisms, the microbiome in bees plays animportant role in nutrition and shaping host health through immunity and disease susceptibility.The extent to which the honey bee gut microbiome influences health outcomes remains unclearand for these reasons the scientific community is diligently working toward the characterizationof the honey bee microbiota.Intellectual Merit/Background: 16S sequencing has revealed that remarkably simple andspatially organized microbial communities of about 8-10 bacterial phylotypes occupy the honeybee gut, consistent across geographic distributions of bees2. In-depth metagenomic sequencing andsingle-cell characterization at the strain level revealed that each phylotype spans considerablemicrobial genomic diversity, leading to substantial polymorphism within and between hives. Suchvariation between bacterial strains belonging to the same phylotype could result from functionaldiversification (due to niche partitioning) but also suggests co-divergence and adaptation with hostlineages. Bidirectionally, the bee microbiome has been shown to play an important role inmodulating the host physiology. Germ-free studies highlighted that native gut microbiota is ableto stimulate the immune system in adult worker bees3 and the use of probiotics to effectivelymitigate parasite effects shows promise4. However, the contributions of the gut microbiota to hostimmune pathways and the mechanisms by which the host responds to gut variation has yet to beinvestigated. Lastly, the genetic architecture of a honeybee colony makes any two daughter workerbees of sister queens mated with a single drone share ¾ of their genes on average5. Takenaltogether with the consistent microbial phylotypes observed across colonies, this makes honeybees an ideal model for studying host-microbiome interactions. Using a combination ofcomparative genomics and field experiments, I aim to identify possible routes of honey-bee/microbiota co-diversification. I hypothesize 1) that the host genotype, diet, and environmentallandscape shape the functional capabilities of the honey bee microbial community, and 2) thismicrobial community can acutely impact the innate immune system of adult bees, and that thiscommunity can be modulated by the addition of probiotics. I plan to test these hypotheses with thefollowing aims:Aim 1: Determining the contribution of host genetic and environmental landscape in shapingthe functional microbiome of the honey bee microbiota. I will rear several hives derived fromsingle drone-mated sister queens from three different subspecies of A. mellifera: ligustica, carnica,and mellifera. These will be replicated in two geographically separated apiaries (collections ofhives). Bees in each location will have access to the same respective landscape of flora to foragefrom and will also be fed identical nutritional supplements (protein supplementation, sugar syrup).Samples will be collected at the initial hive set-up, then once every 3 months for a year. They willbe collected from nurse bees (who stay within the hive) and foraging bees (who leave the hive).Use of apiaries, acquisition of bees, and subspecies identification is enabled by collaboration withDr. Ramesh Sagili of the Oregon State Honey Bee Lab. High-throughput shotgun metagenomicsequencing will be used to assess and compare the bees’ microbial structure at the phylotype andstrain level between and within a) subspecies, b) nurses/foragers of each subspecies and at eachlocation, and c) longitudinally, to assess patterns of functional microbiome congruence betweenthe genetically and geographically distinct subgroups of bees. Additional data including wintersurvival rates per subspecies will be collected. Microbial samples from the flora the bees foragefrom in the different apiary locations will be collected in an attempt to ascertain the microbes theyare exposed to outside of the hive.Aim 2: Comparative analysis of functional and spatial diversity in the A. mellifera gutmicrobiome following immune system challenge. Germ-free bee studies have foundupregulation of antimicrobial peptides in hemolymph (blood analog in bees) to be associated withinoculation by specific gut microbiota members3. I will determine if altering the microbialcommunity with probiotics can modulate the immune response of the bees and reduce fatalitiesdue to Nosema ceranae (a parasite associated with bee depopulation⁶). To do so I previouslycollaborated with the Honey Bee Lab and performed a three-week in vivo experiment on theaddition of probiotics during Nosema infection. Microbiome samples were harvested from themidgut and hindgut of single bees (Nosema localizes in the midgut). My pilot 16S sequencingconfirmed that our methods are sensitive enough to detect distinct spatial microbial compositions(consistent with the literature). Shotgun metagenomics will be performed to compare strain-levelfunctional diversity between experimental groups and gut regions, as well as determine impact ofprobiotic strains on composition and functional diversity. Functional pathways will be identifiedin each group and quantitatively compared to ascertain up- or downregulation of antimicrobialpeptides and other immune-related genes in the context of infection or probiotic addition. mRNA-seq will be completed to quantify and compare host response to those pathways identified bymetagenomics. Pathway predictions will also be confirmed via LC-MS/MS of antimicrobialpeptides present in the bee hemolymph.Broader Impacts – Research Dissemination: The sequences and metadata generated by thisproject will be made publicly available through the online BeeBiome consortium2, where there isa pressing need for comprehensive honey bee microbiome data. Results regarding probiotictreatment of honey bee hives will be communicated to bee keepers through local and nation-widebeekeeping meetings and publications. I anticipate submitting a first author paper detailing myfindings in spring 2020, as well as presenting my results at the International Society for MicrobialEcology 2020 meeting in Cape Town, South Africa. At OSU, I will utilize opportunities to sharemy research, such as the bi-annual Center for Genomic Research & Biocomputing conferences,where I have presented research previously.Broader Impacts – Science Communication: I will share my research with audiences from K-12 children, community members, and faculty. I plan to involve the community in my research,by expanding this project to include bee gut and local flora samples donated by regionalbeekeepers. This community-driven, crowd-sourced approach is necessary to characterizing honeybee health outside of the lab setting. I also plan to involve high school AP Biology students duringthe collection and characterization of apiary flora (Aim 1). I currently serve as the OutreachCoordinator for the Micro Grad Student Assoc. and am in the process of developing severalopportunities for local K-12 children to learn about honey bees, utilizing hands-on activities at theweekly Saturday farmer’s market. This combination provides an optimal platform for me to talkabout my research and engage with community members on the themes of how humans rely onbees for our food production, and how the health of honey bees impacts us.1. Pollinator Health Task Force. 2016. Report to Congress. 2. Engel et al. 2016. MBio. 3. Kwong et al. 2017. R. Soc.Open Sci 4. Khoury et al. 2018. Frontiers. 5. Johnstone et al. 2012. R. Soc. Bio Sci 6. Rubanov et al. 2019. Sci. Rep"
62.0,"Stomata are small pores located on the epidermis of aerial plant tissues necessary for CO2assimilation and O release. Stomata are also the primary sites of transpiration, accounting for2nearly 90% of all water loss in rice. Plant species can modulate their stomatal density andconductance on newly emerging leaves in response to environmental stimuli such as CO , water2status, and temperature1–3. Despite many recent studies in dicots, limited attention has been paidto characterizing the genetic underpinnings of stomatal development and physiology inmonocots, namely grasses. Grass stomata exhibit specialized anatomical and physiologicalattributes, such as subsidiary cells that flank the guard cells allowing for faster stomatal aperturerates4. Concerted efforts could provide insights into environmental adaptation mechanismsexclusive to monocots. The few studies that have characterized aspects of monocot stomatalbiology have relied on mutant screens to identify key genes or have attempted to characterizehomologs from the model plant organism Arabidopsis thaliana4–6. The use of quantitativegenetic approaches as an alternative might reveal quantitative trait loci (QTLs) associated withthis important trait. I will complete a genome-wide association study on a rice (Oryza sativa)diversity panel to characterize stomata-mediated drought response in rice. Currently availablehigh density single nucleotide polymorphism (SNP) data allows for the resolution of discreteQTLs that are relevant in extant rice variation7. Further investigation of the most significantSNPs will enable the characterization of genetic variation involved in rice stomatal physiologyand development in response to water deficit.Aim 1: Genome wide association study of stomatal traits in drought simulationA hydroponic platform incorporating polyethylene glycol 6000 (PEG)-induced drought stresswill be used to yield a high-throughput and uniform assay of plant stomatal density andphysiology in response to drought. The stomatal density differences will be measured in normalwatering conditions and drought stress lines for each accession. A Li-6400 XT will be used tomeasure stomatal conductance of individual replicates. The optimized phenotyping platform willbe applied to a rice diversity panelof 300 accessions. I will useprincipal component analysis tomaximize coverage of totalgenetic diversity in the selectedlines. Stomatal density andconductance differences betweenthe two treatments will be used asthe phenotypic parameters in theGWAS alongside a high-densityrice array containing nearly 700kSNPs. Association mapping willbe conducted using a custompython script that can account forsubpopulation structure as apotential covariate. All loci abovethe significant p-value thresholdwill be further analyzed to identifylikely candidate genes associated1with stomatal density and conductance modulations responsive to drought. I will then usehaplotype analysis to determine the haplotypes and frequencies at the most significant QTL.Aim 2: Characterize candidate genes using CRISPR/Cas9 mediated knock-outs and ectopicoverexpressionThe candidate genes most closely associated with the highest significance SNPs will be furthercharacterized using CRISPR/Cas9 to induce mutations 8.Targeted mutagenesis will be executedin the Kitaake genetic background, in which I have already successfully produced knockouts.The short generation time of this accession makes it ideal for high-throughput research.Additionally, I will ectopically overexpress candidate genes with a strong promoter in theKitaake background. The stomatal density and conductance of mutant overexpression lines willbe measured to determine if these candidate genes play a role in stomatal development andphysiology.Aim 3: Multiplexing knock-ins of drought adaptation allelesCRISPR/Cas9 and geminiviruses will be used to produce knock-ins of advantageous alleles inthe homologous native location in the Kitaake background. Alleles selected will belong to thehaplotype associated with the highest significance SNPs from the most drought tolerantaccessions. Useful variation may exist in promoters, genes, or non-coding sequences. Thisvariation will be leveraged using the high replication rates of geminivirus replicons to increaserates of homology-directed repair, with precise positioning enabled by CRISPR/Cas9 mediateddouble-stranded breaks9. Quantitative traits are governed by numerous QTL that contributecollectively to a phenotype10. Simultaneous knock-ins of alleles from drought adapted accessionsinto the Kitaake background will be confirmed and assayed for performance in drought. Thisapproach highlights an avenue to leverage natural variation using targeted genome editing forallele swapping.Intellectual Merits: Grass stomatal adaptations are currently understudied. Rice can serve as amodel for other monocot species in investigating novel environmental adaptations that haveevolved relative to dicots. Access to high density marker sets coupled with transformationfacilities can enable biological investigations that go beyond the scope of model plant organisms.Results may be translated to species such as hexaploid wheat, where GWAS and transformationis more challenging, to explore the conservation of adaptive mechanisms among grasses.Furthermore, multiplexing adaptive allele knock-ins could bypass the high time investment andlinkage drag inherent to traditional breeding approaches, and be broadly applied to a range oftraits for which there is existing GWAS data11.Broader Impacts: Improved understanding of drought tolerance mechanisms in monocots canenable eventual crop improvements. These advancements will be necessary to improve plantperformance in the face of impending global climate changes. International collaborators willassist with field testing of the most promising edited lines, integrating a broad community ofplant scientists. I will leverage my connections in NPR Scicommers and local science museumsto share findings of this study with the public and discuss data at conferences, thereby engagingwith all individuals about plant-environmental interactions in the context of climate change.References: [1]Hamanishi, E. T., Thomas, B. R. & Campbell, M. M. J. Exp. Bot. (2012). [2] Gray, J. E. et al.Nature (2000). [3].Zhu, J. et al. Forests (2018). [4].Raissig, M. T. et al. Science (2017). [5] Raissig, M. T. et al.Proc. Natl. Acad. Sci. (2016). [6] Hughes, J. et al. Plant Physiol. (2017). [7] McCouch, S. R. et al. Nat.Commun.(2016). [8]. Doudna, J.A. & Charpentier, E. Science (2014). 9.Wang, M. et al. Molecular Plant (2017).[10] Crowell, S. et al. Nat. Commun. (2016). [11]. Jacobsen, E. & Schouten, H.J. Trends Biotechnol. (2007).2"
63.0,"25% of all marine life.1 They provide invaluable services by protecting shorelines from stormsurge, supplying food sources, and promoting eco-tourism.2 However, coral populations aroundthe world are exhibiting an alarming decline due to climate change, specifically from oceanwarming (OW). OW has severely diminished coral health through increased diseasesusceptibility and bleaching.3 To fully understand how corals might fare under future OWconditions, the potential for coral acclimatization over generations and life history stages must beassessed. Previous research on coral acclimatization has focused primarily on intra-generationalacclimatization (IGA), which investigates if corals can adjust to new conditions within theirlifetime. For example, Brown et al. (2002) found that when G. aspera were exposed to higherlevels of solar radiation, they were less susceptible to bleaching.4 While IGA is crucial inelucidating coral resilience, the study of trans-generational acclimatization (TGA) in corals isessential to understanding the persistence of coral reefs in a warmer future. TGA occurs whenthe phenotype of the offspring is influenced by the environmental conditions experienced by theparents and/or previous generations.5 Epigenetic modifications, or heritable alterations in geneexpression and cellular functions that do not involve changes to the original DNA sequence, arethought to play a role in TGA.6 Most studies on epigenetic modifications and TGA have focusedon exclusively DNA methylation; for example, Strader et al. (2019) found that parentalenvironments of S. purpuratus affected patterns of DNA methylation in offspring.7 However,other epigenetic markers, such as histone modification and chromatin remodeling, may berelevant to TGA in marine invertebrates, but have been seldom studied. I will address thisknowledge gap by investigating multiple epigenetic mechanisms and outcomes of TGA in corals,over multiple generations and life history stages, in the context of OW. Additionally, I willexamine coral physiological processes to better understand all aspects of coral TGA. Discerningthe influence of epigenetics on TGA will contribute to the knowledge of coral resilience andsusceptibility in an evolutionary and ecologically relevant context. The coral Pocilloporadamicornis was chosen for this study, as it is an important reef-building coral in the Indo-Pacificregion and is commonly used in laboratory experiments as a model coral.Aims and Hypotheses: Aim 1: Assess physiological effects of TGA in offspring at severaldevelopmental stages (larval, juvenile). Hypothesis 1: Both larval and juvenile offspring whoseparents were exposed to OW conditions will have a higher tolerance to OW conditions thanlarval and juvenile offspring whose parents experienced ambient conditions. Aim 2: Compare theepigenetic modifications, specifically DNA methylation patterns and histone modifications, ofcoral parents and offspring during several developmental stages (larval, juvenile) to evaluate theacquisition and stability of TGA. Hypothesis 2: Parents exposed to OW conditions will produceoffspring with differentially methylated genes and modified histones compared to offspringwhose parents experienced ambient conditions.Research Methods: I will collect reproductively viable adult P. damicornis from the fringingreefs of Kaneohe Bay, Hawaii and experimentally expose them to OW conditions. To simulatecurrent and future environmental parameters, experimental ambient/high temperatures will bedefined as 26/30°C.8 Exposures will take place in mesocosm tanks with flow-throughexperimental treatment water in the Hawaiian Institute of Marine Biology’s seawater system forone month. Following the adult exposure, I will evaluate the photosynthetic efficiency (F/F ) ofv madult corals to assess their capacity to photosynthesize. Additionally, I will preserve adult tissuesamples for later epigenetic analysis (see below). After the exposure and physiologicalassessment, I will induce adults from all treatments to spawn. I will collect larvae post-spawn1and experimentally expose them to ambient/OW conditions for one week. At the beginning andend of the larval exposure, I will measure lipid content and oxygen consumption from a subset oflarvae in each treatment to determine energy reserves needed for metamorphosis and metabolicrate, respectively. Following the exposure, I will preserve another subset of larvae from eachtreatment for epigenetic analysis (see below). I will transfer the remaining larvae from eachtreatment into 10 L tanks with ambient flow-through seawater and plugs for settlement. After 6months, I will experimentally expose the now-juvenile offspring to ambient/OW conditions forone month. At the end of the exposure, I will measure juvenile F/F ; following photosyntheticv manalysis, I will preserve juvenile tissue for epigenetic analysis (see below). Epigenetics: At theend of all exposures, I will collect tissue from juveniles/adults and a subset of larvae for DNAmethylation and histone modification analyses. Using extracted genomic host DNA, I will assesswhole genome DNA methylation using the MeDIP-seq approach. This method utilizes DNAimmunoprecipitation and next-generation sequencing to estimate methylation levels of specificDNA regions. I will also use genomic host DNA for histone modification analysis. Histonemodifications will be analyzed through the ChIP-seq method, which combines chromatinimmunoprecipitation and next-generation sequencing to identify regions of the genomeassociated with these modifications.Intellectual Merit: Not only will this research considerably enhance knowledge of physiologicaland epigenetic processes in coral biology, but it will be one of the first studies to provide adeeper understanding of coral resilience over multiple generations and life history stages. Theutilization of cutting-edge epigenetic analyses will help to define the contribution of DNAmethylation and histone modifications to TGA, which is currently understudied in corals.Additionally, the cognizance of coral TGA potential in the face of anthropogenic stressors willallow scientists and reef managers to make more informed predictions about future reef healthand population evolution. An increased knowledge of epigenetic mechanisms in corals will alsosupply a starting point to investigate TGA potential in other marine invertebrates who may besusceptible to climate change.Broader Impacts: The Graduate Research Fellowship will enable me to pursue importantresearch opportunities and will equip me with the knowledge and abilities needed to succeed as afuture governmental or non-profit research scientist. Moreover, the GRF will enhance my skillsas a scientific educator and mentor to younger students. I intend to partner with local highschools in the greater University of Hawaii area to connect students with marine science andresearch. Through this partnership, I will provide opportunities for students to undertakeindependent projects within the context of my research. I will guide students through anintegrated overview on how to conduct research projects from the initial proposal to the finalwritten product. More specifically, I want to include low-income high school students during theresearch partnership. Low-income students can often be excluded from fully pursuing theirinterests in science, due to lack of financial and academic support. I hope to provide thosestudents with research opportunities and support, so that they can receive an enrichingexperience.References: 1Reaka-Kudla (1997) Biod. II. 2Constanza et al. (2014) Glob. Env. Chan. 3Hoegh-Guldberg et al. (2007) Sci. 4Brown et al. (2002) C. Reefs. 5Torda et al. (2017) Nat. Clim. Chan.6Eirin-Lopez and Putnam (2019) Ann. Rev. Mar. Scie. 7Strader et al. (2019) J. Exp. Mar. Bio.Eco. 8IPCC (2013) AR5.2"
64.0,"Background: Cancer is among the leading causes of deaths worldwide with approximately 38%of men and women diagnosed with cancer at some point in their lifetime. With the rising cancer1epidemic and the need for cheaper and more accessible drugs for people in developing countries,it is crucial to find new ways to develop pharmaceuticals. One sustainable method is engineeringmetabolic pathways of microorganisms such as yeast (Saccharomyces cerevisiae) or Escherichiacoli to produce the precursor of a drug. One of most successful applications of this technique wasachieved in Dr. Jay Keasling’s lab at the Joint BioEnergy Institute by producing the precursor ofthe antimalarial drug artemisinin, reducing the cost 30-60% and ensuring a continuous supply.2,3Engineering microorganisms to produce pharmaceutical products more efficiently can beapplied to cancer drug development. Production of terpenoid and polyketide by engineeredmicroorganisms would be particularly beneficial as small amounts of the molecules are producedvia natural pathways and yields vary widely based on environmental and epigenetic factors.4Paclitaxel (brand name Taxol) is one of the most successful cancer drugs, and was first listed onthe World Health Organization Model List of Essential Medicines in 2013. However, there wasconcern regarding the high cost of the drug , as it initially required the bark of six 100-year-old5Pacific yew trees to treat a single patient with breast cancer. Although alternate methods have6been developed to extract paclitaxel from needles of the European yew, synthetic biology toolscan be used as a more sustainable alternative. Since paclitaxel is a highly decorated diterpenoid(Fig 1), its complicated structure makes it a good fit to be engineered from yeast due to thehighly versatile DNA transformation system and well-defined genetic system of yeast.The objective of this project isto engineer a yeast strain capable ofsynthesizing paclitaxel which can belater optimized for commercialproduction. This will have an essentialimpact by reducing the cost of a crucialanticancer drug and providing valuableinsight into the pathways required for theproduction of terpenoid and polyketidenatural products from yeast. This projectwill focus on engineering taxadienebiosynthesis in yeast by utilizing glucoseas the hydrocarbon source, studying andidentifying cytochrome P450oxygenation reactions in the pathway,and integrating these components toproduce paclitaxel (Fig 1).Aim 1: Engineering of Taxadiene Biosynthesis in YeastTaxadiene (taxa-4(5),11(12)-diene) biosynthesis in yeast is crucial to the production ofpaclitaxel but the levels of various precursors these organisms produce naturally are insufficientto make the process feasible. The diterpenoid precursor for taxadiene, geranylgeranyldiphosphate (GGDP), is necessary for a heterologous taxoid biosynthetic pathway but isproduced in insufficient quantities in yeast due to competition for steroid synthesis with farnesyldiphosphate. I will introduce heterologous genes from the Sulfolobus acidocaldarius GGDPsynthase instead of the native GGDP synthase from Taxus for improved production of GGDPAT axa-4(5),11(12)-d ien e (TP 450O xygen ationR eaction sP aclitaxelaxad ien e)BA im 1En g in eerin g o f T ax ad ien eBio sy n th esis in Y eastA im 2Stu d y C y to ch ro m e P 4 5 0 -Dep en d en t O x y g en atio nR eactio n s in P ath w ayA im 3Integ ratio n o f T ax ad ien ePath w ay an d O x y g en atio nR eactio n s to P ro d u ceP aclitax elFigure 1. A: Taxadiene intermediate structure catalyzed viavarious cytochrome P450 oxygenation reactions to producepaclitaxel.7 B: Identified route for production of paclitaxel.and taxadiene as there is no competition for steroid synthesis. Introduction of genes from S.4acidocaldarius also results in substantial production of geranylgeraniol, further increasingtaxadiene yields. Yeast will be transformed by the lithium acetate method on SC minimal8medium agar plates via CRISPR/Cas9 and select plasmids (pVV200 (tryptophane), pVV214(uracil), pRS313 (histidine) and pRS315 (leucine)). Yeast will be cultivated for 48 hours withglucose as the carbon source, lyophilized, extracted with pentane, and analyzed by GC-MS.Aim 2: Study Cytochrome P450-Dependent Oxygenation Reactions in PathwayThe oxygenation steps in the biosynthesis of paclitaxel have yet to be fully studied andidentified. After taxadiene biosynthesis, oxidative modification of the olefin and the elaborationof side chains are needed to transform taxadiene into paclitaxel. After the taxadiene skeleton isformed, the olefin must be modified by several P450-mediated oxygenations and coenzyme Adependent acylations. Candidate genes for all but one of the seven steps after taxadiene synthesisare postulated, but the entire pathway has yet to be confirmed. Uracil-specific-excision-reagent9(USER) cloning will be utilized for site-directed mutagenesis of the identified genes andcytochrome P450s and USER primers will be designed using the online AMUSER tool. Allintermediates will be characterized by GC/LC-MS.Aim 3: Integration of Taxadiene Pathway and Oxygenation Reactions to Produce PaclitaxelOnce the taxadiene pathway and oxygenation reactions are identified and characterized,the pathways will be integrated via the lithium acetate method, CRISPR/Cas9, and site-directedmutagenesis to produce paclitaxel. Glucose will be used as the starter carbon source in yeast andwill follow the native mevalonate pathway. The established genes to produce GGDP synthasewill be introduced and the previously identified taxadiene synthase gene that has been codonoptimized for improved yeast expression will be incorporated to produce taxadiene. Then, the4pathway developed with the cytochrome P450 oxygenation reactions will be introduced toproduce paclitaxel. All intermediates will be evaluated using GC-MS. It is possible that someproteins synthesized in the complete pathway are insoluble or inactive in yeast, thus similarproteins will be determined or engineered to be active.Intellectual Merit: My knowledge from chemical engineering coursework and research withdeveloping cell cultures, DNA analysis, and molecular modification of chemical structures givesme an essential, well-rounded training for fulfilling this project. This project will be the first timea polyketide synthase (PKS)-terpene hybrid has been produced in yeast and will mark animperative step in the industrial production of PKSs and thus, in the field of synthetic biology. Iwill collaborate with members of the Joint BioEnergy Institute to learn the genetic technique ofintegrating genes using CRISPR/Cas9 and use my knowledge of DNA sequencing and GC toconfirm the genes responsible for paclitaxel and determine the developed molecules at each step.Broader Impacts: Developing pharmaceuticals from microorganisms is an efficient and cost-effective way to produce the same high-quality compounds obtained from natural products. Byengineering yeast to produce paclitaxel, a lower-cost, more sustainable drug could be developedfor people suffering from lung, breast, or ovarian cancers who would otherwise not have accessto the medicine. If successful, this project will provide a framework for synthesizing other PKS-terpene hybrid chemicals and pharmaceuticals from microorganisms.References: 1. NIH, NCI. 2018. https://www.cancer.gov/about-cancer/understanding/statistics 2. Hale et al., Am SocTrop Med. 2007. DOI: 10.4269/ajtmh.2007.77.198. 3. Ro et al., Nature. 2006. DOI: 10.1038/nature04640. 4. Engelset al., Met Eng. 2008. DOI: 10.1016/j.ymben.2008.03.001. 5. Mendis. WHO Model List of Essential Medicines.2011. https://www.who.int/selection_medicines/committees/expert/18/applications/Mendis.pdf?ua=1 6. Horwitz,SB. Nature. 1994. DOI: 10.1038/367593a0 7. Chang et al., Nature. 2006. DOI: 10.1038/nchembio836. 8. Kaiser etal., Cold Spring Harbor Laboratory Press. 1994. ISBN: 0-87969-451-9. 9. Jennewein et al., PNAS. 2004. DOI:10.1073/pnas.0403009101."
65.0,"Motivation: Virus purification and concentration is critical for the production of vaccines andgene therapy vectors. Today, vaccines exist for 26 viral diseases1, but low yields and high costsof production prevent access to many vaccines. Similarly, production limitations will reduceaccess to viral gene therapy, which may provide cures to single-mutation genetic diseases.Although many technologies exist for virus particle purification, they each face uniqueroadblocks to developing rapid, high-yielding, and ultimately continuous processes.Chromatography is widely-used for virus purification, but suffers from low binding capacities,frequently inactivates viruses, and cannot operate continuously. Ultrafiltration is readily scalableand provides high throughput and recoveries, but fouling reduces flux and removal ofcontaminating proteins is often ineffective. Ultracentrifugation is difficult to scale up and haslow yield, even though purities are high.Aqueous two-phase systems (ATPS), most commonly constructed with two polymers ora polymer and a salt, are proposed as a replacement to simultaneously purify and concentratevirus. Biologically-gentle purification is achieved by choosing the identity and concentration(described by tie-line length, TLL) of phase-forming components, ionic strength, polymermolecular weight, and pH so that the desired bioparticle partitions to one phase and contaminantsto the other. Many viral particles have been purified in ATPS, with yields as high as 79%2, asignificant result in an industry that often accepts overall downstream yields of 30%. Still, thevaccine industry is reluctant to adopt ATPS in part because each bioparticle requires a uniquesystem for optimal recovery and predictive models are lacking to fill this critical gap.So far, attempts to develop a predictive model for bioparticle partitioning in ATPS can begrouped into three approaches. A first approach, the Collander equation, uses known partitioningof bioparticles in ATPSs to predict partition coefficient (K) in new systems3. Because of itsreliance on data, this model cannot predict K for new bioparticles. A more robust methodcombines density gradient theory with thermodynamic and association models. It successfullypredicted the mass transfer of an amino acid in ATPSs4, but cannot model partitioning of largebioparticles like viruses. A better solution by Chow, et al. uses surface and interfacial propertiesto predict partitioning5. Particle diameter, net charge, and contact angle with the ATPScomponents can predict K.No one has yet successfully combined a theoretical model with measurements ofATPS and particle surface properties to predict bioparticle partitioning. Chow, et al.verified their model empirically by successfully comparing K to TLL and pH, but did not attemptpredictions with surface measurements. I propose combining novel methods developed in my labto measure virus surface characteristics with Chow’s theoretical model to together predict viruspartitioning in ATPS, a task which was not previously possible.Hypothesis: Partitioning of virus particles in aqueous two-phase systems can be predicted usingsurface chemistry properties measured at the single-particle level.Experimental Plan: I propose extending Chow, et al.’s models to predict partitioning of virusparticles in polymer-salt ATPS. Chow’s model requires phase properties of ATPS and surfaceproperties of the viral particle to predict K. Methods for characterizing ATPS are rapid and well-known. Turbidity measurements will be used to determine the binodal points and tie lines.Characterization of viral particles is much more difficult. While virus diameters and isoelectricpoints (pI) (related to surface charge of viral particles) may be found in the literature, no methodto measure the contact angle between ATPS and viral particles currently exists.1Historically, virus characterization has depended on bulk solutionmeasurements or amino acid sequencing. Instead, my lab has developed anovel single-particle method to measure virus surface chemistry usingchemical force microscopy (CFM), shown in Fig. 1. In a recently submittedarticle6, we determined the pI of two model viruses using atomic forcemicroscopy probes chemically functionalized to carry positive or negativecharges. Adhesion to the viral particle was measured in varying solution pHand the pI determined6, giving a direct characterization of the viral surfaceand avoiding error from contaminants. A similar single-particle Fig. 1. CFM withcharacterization will be used to determine the parameters of Chow’s virus particlespartition model.The first stage of this work will focus on how virus surface chemistry changes in thepresence of ATPS components individually before combining them. First, virus-sized goldnanoparticles (AuNPs) coated in BSA or lysozyme will be immobilized on a gold surface. Thenadhesion force between the AuNPs and probes modified with charged or hydrophobic ligandswill be measured by CFM. These proteins are known to partition differently in ATPS, and sincecontact angle between these proteins and ATPS may also be determined by traditional sessiledrop methods, comparison will confirm that CFM can be related to surface tension for well-defined systems before extending the method to viruses. Similar determinations of surfacetension by CFM for non-biological surfaces have already been reported7. Then, two enveloped(surrounded by a lipid bilayer) and two non-enveloped viruses will be used to explore varyingviral surface chemistry.Once a relationship between CFM and contact angle is established, the second stage ofthis work will characterize multiple ATPS and adapt Chow’s model to predict virus partitioning.To verify the model as a function of component concentration, three TLLs for three commonpolymer (PEG) and salt (citrate, phosphate, and sulfate) ATPS will be evaluated to show themodel is robust for varying chemistries. Similarly, the pH of ATPS will be varied over the rangeof stability for each virus, typically 4.5<pH<7.5. The model will be complete when it predicts theK of a virus as a function of TLL or pH.Once this work establishes a reliable model for virus partitioning between the bulk phasesin ATPS, partitioning to the interface, which frequently results in the irreversible aggregationand inactivation of virus particles, can be explored.Intellectual Merit: Developing predictive models for virus partitioning in ATPS will add to theunderstanding of viral surfaces and the driving forces behind ATPS, reducing the experimentalcost of developing ATPS to purify new bioparticles in the future.Broader Impacts: By filling a critical gap in the literature, making ATPS research faster, andmaking ATPS more accessible to industry, this work will speed the development and productionof vaccines and gene therapy, ultimately reducing outbreaks of viral and genetic disease andsaving lives. In addition, I will mentor undergraduate researchers who will have the opportunityto develop contact angle measurements with protein-coated AuNPs.References:1. WHO. Vaccines and diseases. 2019 2. Joshi, P.U., et al. Journal of Chromatography B, 2019. 1126. 3. Madeira,P.P., et al. Journal of Chromatography A, 2008. 1190(1-2): p. 39-43. 4. Chicaroux, A.K. and T. Zeiner. Fluid PhaseEquilibria, 2019. 479: p. 106-113. 5. Chow, Y.H., et al.. Journal of Bioscience and Bioengineering, 2015. 120(1): p.85-90. 7(38): p. 21305-21314. 6. Mi, X., et al.. Under review. 7. Drelich, J., et al. (2004). Journal of Colloid andInterface Science 280(2): 484-497.2"
66.0,"Graduate Research Plan StatementTitle: Exploring critical zone structure and function in a tropical urban watershed throughconcentration-discharge relationshipsIntroduction: Streams are recognized as integrators of the surrounding landscape. Stream waterchemistry is thus an excellent indicator of broader critical zone (CZ) processes. The CZ is thespace from the top of the vegetative canopy down to bedrock and lowest extent of freely circulatinggroundwater.1 The CZ framework provides a holistic approach to develop predictive models thatdescribe processes at the earth’s surface, including the constraints on material export from thecontinents to fluvial networks.2 Concentration-discharge (C-Q) relationships in streams provide anintegrated signal of sources and transport processes to examine how solutes and sediment respondto changing patterns of runoff.3 Studies of the CZ are mostly limited to pristine systems; however,C-Q relationships in urban streams may be more complex due to altered hydrology, impaired waterquality and heterogenous subcatchments.4 With increasing pressures on urban landscapes, there isan urgency to understand hydro-biogeochemical processes of the urban CZ that govern waterquality and quantity. Research in urban systems will be highly valuable to cities and communitiesand can better inform management practices and help improve urban infrastructure.4I propose to characterize and compare C-Q relationships across two stream networks withdiffering levels of urban development through a series of whole-network sampling effortscapturing baseflow to storm events. I also propose to study the impacts of hurricanedisturbance on C-Q trends through analysis of long-term water chemistry records. I will testthree hypotheses on variability in C-Q relationships across stream networks associated withwatershed urbanization (H1-H3):H1: Watershed urbanization drives greater variability in C-Q relationships across stream networksassociated with increased impervious surface area.H2: C-Q relationships during storm events are more variable in the urban network due toheterogenous hydrologic signals, and ultimately depend on storm intensity.H3: Both urban and pristine stream networks show increased variability in C-Q relationships afterhurricane disturbance, but the magnitude of variability is higher in urban watersheds.Study sites: This work will be conducted in two watersheds: the urban Río Piedras in metropolitanSan Juan, Puerto Rico and the mostly undeveloped Río Espíritu Santo in El Yunque NationalForest in Río Grande, Puerto Rico, as a reference site. The Río Piedras flows through themetropolitan area with highly modified channels and significant impacts from failing sanitaryinfrastructure.5 In contrast, the Río Espiritu Santo originates in the Luquillo Mountains and ismostly undeveloped except on its coastal plain with significant changes in water chemistry evidentonly after hurricane impacts.6 The University of New Hampshire´s (UNH) Water Quality AnalysisLab (WQAL) group, including myself, has extensive experience working in these two streamnetworks. These watersheds are also a part of the Luquillo Long-Term Ecological Research(LTER) site and Luquillo Critical Zone Observatory (LCZO), which have generated multi-decadalrecords of stream chemistry and discharge. The lab also has ongoing collaborations with researchgroups at the University of Puerto Rico (UPR), who have worked in the Río Piedras: Jorge Ortiz-Zayas’s Tropical Limnology Lab and Alonso Ramírez’s Aquatic Ecology Lab.Proposed approach: (H1) I will establish a synoptic sampling regime of 20 sites in each streamnetwork, ranging from small headwater sites to larger mainstem and coastal sites. I will collectwater samples as well as measure physicochemical parameters with a handheld multiparameterinstrument at least 15 times in the span of 2 years. Samples will be analyzed for nitrate, ammonium,phosphate, dissolved organic carbon, anions, cations, dissolved greenhouse gases, and totalsuspended solids (TSS) at the UNH WQAL Lab. I will also take discharge measurements at each1Carla López-Lloreda NSF Graduate Research FellowshipGraduate Research Plan Statementsite and date either using dilution gauging or acoustic velocity measurements, depending on streamsize. I will target sampling dates that capture a range of flow conditions by monitoring the USGeological Survey (USGS) gauging stations at two sites within each stream network. Thissampling will allow me to characterize spatial and temporal variability in C-Q relationships acrossthe stream network. (H2) I will conduct targeted storm sampling at one USGS gauged site in eachstream network with ISCO automated samplers. These samples will be analyzed for the samesolutes as in H1, with a focus on TSS to calculate sediment flux during storm events. Real-timedischarge data for these events will be obtained online through the USGS´s National WaterInformation System Interface. I will capture events of different magnitude to explore effects ofstorm magnitude and intensity on C-Q relationships and solute and sediment transport. I will usethese results to evaluate specific patterns of hysteresis, which characterize the rising and fallinglimbs of a storm event, providing additional insights into material reservoirs within watersheds.(H3) I will use long-term chemistry data from sites in the Río Piedras and the Río Espíritu Santo,which have been sampled weekly for approximately 10 years for multiple solutes and are USGSgauged sites. I will quantify C-Q relationships before and after Hurricane María in September of2017 which will allow me to examine the resilience of these watersheds to a major perturbation.Intellectual Merit: This work will provide insight into the interactions and mechanisms ofsediment and nutrient production, pathways, and transport in a tropical urban watershed. Studyingstorm events and hurricanes is particularly important because they are “hot moments” of increasedhydrological activity, which transport disproportionate amounts of solutes and sediments tostreams and oceans compared to baseflow conditions.7 Understanding high flow processes iscrucial, as major storm events and hurricanes are expected to increase in intensity and frequencywith climate change.8 And though much C-Q work has been done on temperate systems, tropicalstreams supply disproportionate amount of sediments and solutes to the ocean and studying thesesystems in a global context is becoming increasingly important.9 This work also leverages researchdone by the Luquillo LTER and LCZO research networks.Broader Impacts: During this work, I will continue developing collaborations with local researchgroups in ways that will engage local underrepresented undergraduate students in fieldworkthrough my network and storm sampling efforts. I will work with UPR professors to offerindependent research opportunities to students. This research will provide a useful framework forlocal government agencies to use in their nutrient and sediment management plans across theisland, including restoration projects aimed to help reduce pollution and sedimentation and regaincritical zone services in urban ecosystems.7 For this, I will engage local researchers examiningsediment and nutrient loading and the local Puerto Rico offices of the Department of NaturalResources. I will also collaborate with local organizations working in the Río Piedras such as theSan Juan Bay Estuary and the ENLACE project of the Caño Martín Peña. These projects havelongstanding efforts in the Río Piedras watershed and extensive connections with localcommunities. In collaboration with these organizations, I will use this opportunity to teachcommunities about water quality in their watershed through a series of roundtables and byproviding educational resources to disadvantaged groups that have greater exposure to poor waterquality.References: [1] National Research Council (2001). Basic Research Opportunities in Earth Science. Natl. Acad.Press. [2] Brantley, S. L. et al. (2017). Earth Surface Dynamics, 5(4), 841–860. [3] Chorover, J. et al. (2017). WaterResources Research, 53, 8654–8659. [4] Kaushal, S. S. et al. (2014). Biogeochemistry, 121, 1–21. [5] Potter, J. D. etal. (2014). Biogeochemistry, 121, 271–286. [6] McDowell, W. H. et al. (2013). Biogeochemistry, 116, 175–186. [7]Kaushal, S. S. et al. (2018). Biogeochemistry, 141(3), 273–279. [8] Zimmerman, J. K. et al. (2018). Ecology, 99,1402–1410. [9] Schlesinger, W.H. & Bernhardt, E. (2013). Biogeochemistry (3rd. ed.). Academic Press.2"
67.0,"Overarching career goal: Push the frontier of multiphase flows in extreme conditions that aresubjected to high speed, strong turbulence, and large mass loading. For my graduate study, myaim is to unveil the underlying physical mechanisms of complex couplings between solid and gasphases in compressible particle-laden turbulence, which remains elusive for both industrial andastrophysical applications. The major challenge in this area is the lack of high-quality time-resolved experimental datasets that can illustrate the particle-gas and particle-particle interactionin extreme conditions. To address this issue, I plan to leverage a recently-developed ultra-high-speed diagnostic system to embark on understanding multiphase flows in this exciting new regime.Introduction and Background (Intellectual Merit):From the atomization in internal combustion to collisions and growth of dust particles inprotoplanetary disks (Fig. 1(left)), particles in high-speed compressible turbulent environments isubiquitous in nature and engineering applications. This two-phase interaction produces some keyissues: (i) Multi-scale physics: particles interact with compressible turbulence with many lengthscales and coherent structures. (ii) The coupled interaction can lead to clusters of particles anddroplets, resulting in enhanced collision rates and fast growth1. This is important for rain dropletgrowth in turbulent clouds, a type of incompressible turbulence. In this well-known regime,particles preferentially cluster in regions of low vorticity and high strain rate2, promoting collisionbut inhibiting mixing. This finding may have severe consequences in combustors, where mixingis desired for perfect combustion2. In compressible turbulence, there is a new structure – shocklet3.Rather than high vorticity or strain, it is a region of high pressure. Particles interacting with thisnew structure may alter the mean fields and turbulent characteristics of the flow4. A numericalstudy revealed that light particles cluster in very thin and long filaments on the shocklet surfaces5(Fig. 1(right)), while larger inertial particles form dense clouds downstream of the shocklet.Hypothesis: A new particle clusteringmechanism will become important incompressible turbulence due to the uniquecoherent structure – shocklet. Since shockletsare intermittent spatially and temporally,particles will interact with other low-speededdies as they would in incompressibleturbulence. The dynamic competition between Fig. 1. (left) Artistic illustration of protoplanetary dust.these two clustering mechanisms will pose an (right) Numerical simulation iso-surface displayinginteresting new regime where the particle light particles (green) and eddy shocklets (orange)5.collision rate may be sensitive to small changes of the dimensionless groups, such as theturbulence Mach number, Stokes number, and Reynolds number.Aim 1: Upgrade current 2D experimental facility to a supersonic turbulent environmentTo generate a turbulent and supersonic environment, I will construct a facility to produce a 2Dsupersonic mixing layer using two opposing parallel supersonic streams, a method that has beenused previously to visualize shocklets3. The two jets are separated by a vertical distance, enablingus to capture the interaction between the jets through the shear layer. Particles will be tracked usingan in-house particle tracking code. I have access to different algorithms developed in the lab, whichincludes both 2D tracking and the 3D Shake-The-Box Method6. We have access to the ShimadzuHPV-X2 camera, which captures images at 10 million frames per second at exposure times1Graduate Research Plan Statement Juan Sebastian Rubioof 200 ns. With this camera I will take time-resolved measurements. We also own several Phantomhigh-speed cameras, which can take 40,000 images during one run. With this camera I can performmulti-scale measurements and conduct extensive statistical analysis of the flow. To obtain gasphase velocities, I will seed the flow with sub-micron sized particles and illuminate them with anin-house burst-mode laser (20 mJ per pulse at 100 kHz) and analyze the results using particleimage velocimetry, a method I used previously both during my undergraduate research and at LosAlamos National Laboratory under the supervision of Dr. John J. Charonko. With this innovativeexperimental setup, I will obtain high-quality experimental images for further analysis.Aim 2: Investigate particle physics and its effect on the flow propertiesBecause eddy shocklets are structures with strong compressionParticle bow shocksregions, they can be visualized with either shadowgraph orSchlieren imaging methods. Thanks to the quasi-2D configurationof the proposed setup, shocklets will be visualized by the Schlierenmethod. I have already begun preliminary work to study particlemotion using the underexpanded particle-laden jet facility in ourlab. With the current experimental setup, I already visualizedoblique and normal shocks using the Schlieren method. I alsovisualized the bow shocks that form around individual particlesFig. 2. Shadowgraph experiments(like eddy shocklets) at high frame rates (Fig. 2). Finally, theusing the compressible particle-statistics of particle clustering will be evaluated using the Voronoiladen jet facility at Johns Hopkinsanalysis and the correlation between each of the particle’s location.depicting particle bow shocks.Aim 3: Study the turbulence by characterizing coherent structures - shockletsThe complex coupling covers a multi-dimensional space. Compressible turbulence can bequantified by both the Reynolds and Mach number, which includes both the free stream andfluctuation velocity. Small particles can be characterized by the Stokes number, particle Reynoldsnumber, and particle Mach number. I hypothesize that the particle Stokes number and particleMach number are key parameters that may control the couplings between the two phases. Alarge Stokes number results in a large particle inertia, which may induce a strong particle-basedshocklet. In addition, the particle mass loading is important. As in the incompressible case, the lowmass loading may be key in particle-turbulence interaction, but the particle-particle interaction orparticle-turbulence two-way couplings will not be important when the mass loading becomes large.I will spend part of my thesis to understand the behavior of particles in different parameter regimes.Broader Impacts:Improving our understanding of particle-laden compressible turbulence will have significantimpact in numerous areas of science and engineering. It may provide new insights as to how theuniverse formed. To aid researchers around the world, I will store our experimental data in an opensource repository. Outreach: I will mentor underrepresented undergraduate students on a semesterbasis, provide them with the opportunity of assisting me with experiments, and enable them to co-author in research papers. Through the Johns Hopkins SABES program, I will perform simpleexperiments to show students the importance of particle behavior in the environment.1. Pan, Liubin, et al. The Astrophysical Journal (2011). 2. Squires, Kyle D., & John K. Eaton. Physics of Fluids (1991).3. Papamoschou, Dimitri. Physics of Fluids (1995). 4. Chen, Chang H., & Diego Donzis. Journal of Fluid Mechanics(2019). 5. Yang, Yantao, et al. Physics of fluids (2014). 6. Schanz, Daniel, et al. Experiments in fluids (2016).2"
68.0,"Life Sciences: Systematics & BiodiversityBackground and proposal: Complex body plans evolve through the acquisition ofheterogeneous material properties. In the late 18th century, architects began to combine rigid andflexible materials in order to achieve new levels of structural complexity in response to thelimited space within urban areas. An analogous process happened in animals throughout theCambrian explosion. During this time, complex animal morphologies diversified with theappearance of both rigid and flexible materials, such as skeletons, muscles, and fluid-filledcavities. The relationship between rigid and flexible materials and the production of diversecomplex morphologies in skeletonized animals, such as arthropods and vertebrates, has beenextensively studied. However, little is known about this relationship in gelatinous animals. Injellyfish (Cnidaria: Medusozoa), the underlying material properties are not well understood butstill thought to play a major role in the evolution of diverse morphologies. Jellyfish have a free-living medusa stage (medusoid) with a bell that is responsible for locomotion1. Much of themedusa bell shape appears to be constrained to a concave ellipsoid, conserved across >2,000species of jellyfish1. In stark contrast to this relatively simple medusa morphology, a uniquesubclade, the siphonophores (Cnidaria: Hydrozoa), exhibits morphologies well beyond thisnorm. Siphonophores are colonial animals that use asexual reproduction to producephysiologically integrated chains of individuals, called zooids. Siphonophores swim usingretained, specialized medusoid zooids (nectophores), which propel the colony. Nectophoresdisplay astonishing diversity of extremely complex morphologies2,3 (Fig. 1). In order to achievethis diversity, siphonophores have something typical medusae do not: ridges and facets. In othersystems, the development of ridges and facets is dependent on the distinct underlying materialproperties4. Like free living medusae, nectophores are composed primarily of mesoglea, anexpanded extracellular matrix, which is a mesh network comprised mostly of collagen fiberslocated between epidermal and gastrodermal epithelia5,6. Anecdotal evidence of physicallyhandling siphonophores indicates that the mesoglea has a wide variety of elasticity and stiffnessboth within a nectophore and across species depending on the presence of ridges and facets. It isunknown to what extent the ridges and facets of nectophores depend on the material properties ofthe mesoglea, such as the density, elastic modulus, and viscous modulus. Typical medusae,without ridges and facets, have homogenous mesogleal material properties. I hypothesize thatcomplex nectophore morphologies with ridges and facets require heterogeneous mesoglealdeposition within the nectophore. If supported, the nectophores with complex ridges and facetswill have both regions with elastic properties and regions with viscous material properties. Incontrast, the nectophores that lack ridges and facets will have homogenous mesoglea withmaterial properties comparable to typical medusae. If the core hypothesis is not supported, itwould mean that material properties and nectophore morphology can vary independently. If thisis the case, I propose two alternative hypotheses: (1) heterogenous mesoglea evolved firstfacilitating the evolution of ridges and facets, and (2) ridges and facets evolved first, and weresecondarily reinforced byA: Phylogeny & Morphologyheterogeneous mesoglea. To Ridges/Facets N N Y N Y N Y(Y/N)test these hypotheses, I willcharacterize nectophoremorphology and materialOther Rosacea Vogtia Sphaeronectes Lensia Stephalia Agalmaproperties to test for Medusozoansphylogenetic correlations. Iwill also use the phylogeny tomodel trait evolution of the SiphonophoraeFigure 1: Phylogeny and medusa morphology. Adapted from Totton (1932).Research Proposal Lauren Mellenthinmorphology and material properties. This study will help understand not only how diversity inmorphology arises, but how it evolves. Are mesogleal ultrastructure and correlated materialproperties phenotypically integrated with nectophore morphology? Answering this questionrecognizes the role material properties have in achieving morphological diversity in gelatinousanimals. This work will contribute to the growing interest in the scientific community that realizediverse material properties and their critical role in the evolution of complex body plans.Methods: I will use museum specimens from the Yale Peabody Museum (YPM), which has~180 species with ~2-3 specimens per species to describe the ridges and facets of nectophoremorphology. Using literature and YPM specimens, I will delineate the presence and absence ofridges and facets across siphonophore species. Freshly collected specimens will be used formesoglea and material property analysis. I will use scanning electron and differentialinterference contrast microscopy to image the mesoglea. To analyze material properties, I willuse small angle x-ray scattering and rheology. These techniques characterize the structure of themesogleal mesh at sub-micron lengthscales, density, and viscous and elastic moduli. All tools areavailable at Yale. Using a well resolved phylogeny7, I will test if morphology changes incongruence with mesoglea and material properties using phylogenetic mixed models in Rstatistical software8 with the pglm package. I will also test the order at which morphology andmaterial properties arose via ancestral state estimation with the ape package.Feasibility: A risk of this project is the unpredictability of fieldwork to obtain fresh material formeasurements of material properties. However, working with experts in siphonophore biology inthe Dunn lab and collaborators will alleviate this risk. My previous research experience with Dr.Dean Adams in comparative morphometrics is directly applicable for questions of trait evolution.Former work at the Field Museum and current involvement in YPM has prepared me forhandling museum specimens and contributing to collections for future generations. Collaboratingwith Dr. Alison Sweeney of Yale will contribute to the biophysical aspects of this project and hermentorship reduces radical differences between fields of physics and organismal biology.Intellectual Merit: Using and adding to YPM specimens leverage an important collection for anovel interdisciplinary project. Nectophores are a diagnostic trait of siphonophores and are usedto characterize species, therefore this work will enhance what we know about siphonophoreidentification using gross morphology. Nectophore biology has broader implications forunderstanding how different zooid types contribute to colony integration. This work will alsoinform how extracellular matrix development has influenced metazoan evolution at early nodesin the metazoan tree, potentially with implications for body plan evolution. The emerging field ofsoft robotics currently use animal biomaterial properties to understand efficient fluid mechanics.A major breakthrough was a medusa-like robot. However, nectophores utilize heterogeneous softmaterials to integrate a variety of functions beyond self-propulsion, allowing engineers to designrobots exclusively of soft materials with extensive functional repertoires.Broader Impacts: I plan to bridge the disparate fields of physics and evolutionary biology, bysharing this work. Additionally, by being a graduate affiliate of the Yale Peabody Museum, I amable to share my research and educate about natural history to a broader audience. I will takeadvantage of this unique platform that encourages global curiosity about ocean exploration andoverall scientific curiosity to put my work in perspective. Importantly, these opportunities exciteboth the scientific and public communities about current interdisciplinary research.References: Megill, W.M., PhD diss., McGill University (1991)1, Carré, C., Carré, D. Ordre des siphonophores(1995)2, Totton, A.K. Siphonophora. (1932)3, Gibson, L.J. The Royal Society (2012) 4, Bergheim, B.G. Essays inBiochemistry (2019)5, Gambini, C. Biophys. J. (2012)6, Munro, C. Molecular Phylogenetics and Evolution. (2018)7,R Development Team. R Foundation for Statistical Computing (2008)8"
69.0,"NSF GRFP Application – Research ProposalResearch Question and Intellectual Merit: How does internal migration influence thegeographic diversity of intergenerational income mobility (IIM1) in the U.S.? The IIM literaturehas seen a surge in activity, in part thanks to Chetty et al. (2014) (henceforth CHKS), who linkseveral years of IRS tax data to investigate IIM in the U.S. on an unprecedented scale. Amongtheir key findings is that the expected economic outcomes of a child vary drastically based ontheir commuting zone (CZ) of origin.Both CHKS and much of the literature that has followed it have focused on theimportance of the characteristics of where an individual is from in influencing their expectedincome mobility as opposed to where (or whether) they go. This may be in part because CHKSthemselves appear to put the issue to rest: they find that their IIM estimates do not changemeaningfully after limiting their sample to individuals who stay in their original commutingzone, nor do they appear to be strongly correlated with CZ-level net migration rates.However, limiting the sample to stayers is insufficient to fully investigate the role of self-selected migration in forming the landscape of IIM in the U.S. if this sample is endogenouslydetermined2, and focusing on more narrow migration patterns than net rates uncovers a moresuggestive relationship. Figure 1 juxtaposes state-level IIM estimates with the college graduateoutflow rate3 in each state. With few exceptions, the most income-mobile states in the country(namely, those in the rural Midwest and the Mountain States) also exhibit some of the highestrates of out-migration. Table 1 reveals this visual association to be statistically robust on a basiclevel after controlling for the most important correlates of IIM that CHKS identify.This project will more meticulously consider the importance of internal migration ingenerating spatial variation in IIM through the development and estimation of a structural model.In doing this, I will provide new insight on an oddity that has not been thoroughly probedhitherto: the fact that children from underprivileged backgrounds seem to fare the best whencoming from some of the most remote and forgotten-about places in the country. Creating aformal model will also allow me to add to the relatively much smaller recent literature thatcarefully evaluates policy counterfactuals regarding IIM.Methodology: I intend to construct and solve a lifecycle model that follows the migration andchild-rearing decisions of agents from high school graduation into early adulthood. The modelwill expand the classic Becker and Tomes (1979) framework to incorporate local labor marketconditions and moving opportunities. Agents are born in a home CZ to parents of a certainincome level, who also endow them with a set of inherited attributes and human capitalinvestments. The children then choose whether to stay or move to a new location, after whichthey select how many children to have of their own and how much to invest in them. Investmentsin children are differentially costly across locations to reflect heterogeneity in public schoolquality. In this framework, local labor market quality will induce dual effects on IIM: strongerlabor markets will improve the outcomes of stayers but will also depress incentives for agents toleave and find a better match. This may provide motivation for recent empirical findings thatconventional measures of local labor market quality have little predictive power for IIM.1 Measured as the expected national income percentile in 2011-2012 of a child born in 1980-1982 to parents whowere in exactly the 25th national income percentile over the years 1996-2000.2 If a highly income mobile CZ also has high rates of out-migration, and natives who stay do so because theyreceived unusually good income realizations in their home, then the CZ will continue to exhibit high levels of IIMeven after the sample restriction. The related-but-distinct thought experiment I consider is what would happen toIIM in the U.S. if those that would move from their home CZ are somehow restricted from doing so.3 Measured by taking the sample of income-earning college graduates from the 1980-1982 birth cohorts in 2011-2012 and computing the percentage of individuals born in a state who are observed living elsewhere.1Garrett AnstreicherNSF GRFP Application – Research ProposalFigure 1: IIM (Percentile) and College Table 1: OLS Estimates for VariousGraduate Outflow (%) in U.S. States Covariates of State-Level IIMVARIABLES IIMCollege grad outflow 0.0899**(0.037)Share single mothers -0.728***(0.240)Student-teacher ratio -0.336(0.193)Constant 66.81***(13.19)Observations 49R-Squared 0.697Table Notes: Standard errors in parentheses. ***p<0.01, ** p<0.05, * p<0.1. Non-displayed controlsinclude share black, Theil segregation index, collegegraduation rate, labor force participation rate, highschool graduation rate, violent crime rate, and Ginicoefficients.Figure Notes: IIM estimates for top map fromhttp://www.equality-of-opportunity.org/data/. Datafor bottom map from 2011 and 2012 AmericanCommunity Survey (Ruggles et al., 2017).The central mechanism I aim to capture resembles an intranational brain drain: parentsfrom areas with cheap human capital and poor labor market conditions will face incentives toheavily invest in their children, who in turn will leave their home CZ. The process of leaving willallow the child to select their most compatible labor market, greatly increasing their chances ofclaiming a higher wage and bolstering the measured IIM of their place of origin.Broader Impact and Conclusion: In addition to motivating the high IIM of remote areas, Iintend to evaluate the efficacy of various educational policies. An example is New York’sExcelsior Scholarship, which remits tuition under the stipulation that recipients stay in the statefor some time following graduation. Such a policy may work well in increasing the supply ofcollege graduates in states where opportunities are abundant such as New York, but it may not benearly as effective in rural areas with more condensed wage distributions. Expanding this modelto consider general equilibrium effects could also allow me to address myriad issues. How willgeographic wage distributions in the U.S. evolve over time in response to self-selected migrationflows? Will the brain drain I capture lead to further economic deterioration in the rural U.S., orwill its declining living costs induce more highly skilled individuals to return? These areimportant questions that my model may be extended to answer.ReferencesChetty, R.; Hendren, N.; Kline, P. and Saez, E. “Where is the Land of Opportunity? The Geography ofIntergenerational Mobility in the United States.” The Quarterly Journal of Economics, 129(4): 1553-1623, 2014.Becker, G. and Tomes, N. “An Equilibrium Theory of the Distribution of Income and Intergenerational Mobility.”Journal of Political Economy 87(6): 1153-1189, 1979.Ruggles, S.; Flood, S.; Goeken, R.; Grover, J.; Meyer, E.; Pacas, J. and Sobek, M. IPUMS USA: Version 8.0American Community Survey. Minneapolis, MN: IPUMS, 2018. https://doi.org/10.18128/D010.V8.0.2"
70.0,"Measuring Rayleigh Wave Phase Velocity in the Antarctic Upper Mantle from AmbientSeismic NoiseBackground and MotivationTwo compelling questions make the Antarctic region worth studying: 1) Why is there asignificant age difference between West and East Antarctica? And 2) How exactly will globalsea-levels rise in the future? Divided by the Transantarctic Mountains, West Antarctica issignificantly younger than the East Antarctica craton (Hansen et al., 2014). Resolving the agedifference between Antarctica’s two halves will help us understand the tectonic history andevolution of the Antarctic region. On the other hand, sea-level rise has serious implications oninfrastructure displacement as we lose land surface area. Simulations of global sea-level risehave high uncertainty but could benefit from incorporating bedrock uplift, mantle viscosity, andgeothermal heat flux (Gomez et al., 2015). Approaching both questions requires betterconstraints on mantle properties underneath Antarctica.Investigating Antarctica poses unique challenges for manytraditional measurement techniques. About 98% of Antarctica’s landsurface area is underneath a thick ice-sheet (Fretwell et al., 2013), anduncertainty in mantle viscosity convolutes anticipating changes in theEarth’s surface in response to ice-sheet melting and growth. These twofactors make measuring geothermal heat flux impractical. Furthermore,traditional seismic tomography methods rely on earthquakes forseismic signals which are scarce in and around the Antarctic region.An emerging approach in seismic tomography is to use ambientseismic noise – signals primarily generated from interactions between Earthquakes 1966-2017Figure 1: Topography map ofocean water waves and solid Earth – in addition to earthquake data tothe Antarctic plate. Colorscharacterize mantle properties. This method has been published by represent bedrock elevation inBensen et al. (2007) and has been widely adopted by seismologists meters and pink dots areearthquakes that occurred inwith over a thousand citations. One study shows incorporating ambient1966-2017. Tectonic platenoise can increase phase-velocity map resolution in the Indian Ocean boundaries are plotted asby 20% relative to maps generated by relying solely on earthquake white lines.data (Ma & Dalton, 2016). While there exists many studies employingambient noise, the number of similar studies on the Antarctic region are relatively scarce.Proposed MethodologyI will obtain long-period vertical component data from the Incorporated ResearchInstitutes of Seismology (IRIS) for all active, unrestricted stations south of -55 degrees latitudewhich encompasses all of Antarctica. The data will be processed in 4 h stackable (i.e. able to becombined through addition via linearity) windows. To ensure a high signal-to-noise ratio in ourphase arrival time measurements, I will discard 4 h windows that are not entirely full.Correlation measurements between all station pairs will be done in the frequency domain bycomputing the cross-spectrum ρ (ω) as done in Ekström (2014):ijkKevin Trinh NSF Research Statement𝑢 (𝜔) 𝑢∗ (𝜔)𝜌 (𝜔) = 𝑖𝑘 𝑗𝑘𝑖𝑗𝑘√𝑢 𝑖𝑘(𝜔) 𝑢 𝑖∗ 𝑘(𝜔)√𝑢 𝑗𝑘(𝜔) 𝑢 𝑗∗ 𝑘(𝜔)The letter u represents a 4 h seismogram passed through a fast Fourier transform. ω is frequency,i and j are station indices, and k is a 4 h window index. The asterisk * denotes a complexconjugate. Performing an inverse fast Fourier transform on the cross-spectrum yields a cross-correlation in the time domain.Measurements of phase arrival times can be made from cross-correlations. These arrivaltimes will be used to perform inversion and thus yield phase-velocities for many small, discreteregions in Antarctica. Earthquake data can be incorporated with ambient noise data to accountfor regions with low data count (i.e. not many paths traversing the discrete region). Additionally,the smoothing of our inversion will account for regions with little data and can be adjusted toyield the best results. I will conduct numerous tests to identify optimal smoothing parameters andrelative weighing between ambient noise and earthquake-based data. These steps result in a 2Dphase-velocity map and can be repeated to map varying depths of the Antarctic upper mantle.Anticipated ResultsThe speed at which wave phases propagate through solid Earth is related to materialproperties such as temperature, composition, and partial melt. I expect to see West and EastAntarctica to be dominated by slow- and fast-velocity anomalies, respectively, which shouldagree with past studies using p-waves to image the Antarctic mantle. Improved resolution intomographic maps of the Antarctic upper mantle may help me observe undiscovered geologicalfeatures such as cratons and oddly pronounced and heterogenous velocity anomalies.Proposed Timeline:Year 1: Download and process seismogram data from IRIS from 1900 to 2017.Year 2: Generate 2D seismic tomography maps.Year 3: Identify optimal smoothing parameters and relative weights. Repeat mappingprocess for varying depths of the Antarctic upper mantle.ReferencesBensen, G. D., Ritzwoller, M. H., Barmin, M. P., Levshin, A. L., Lin, F., Moschetti, M. P., et al. (2007). Processing seismicambient noise data to obtain reliable broad-band surface wave dispersion measurements. Geophysical JournalInternational, 169(3), 1239–1260. https://doi.org/10.1111/j.1365-246X.2007.03374.xEkström, G. (2014). Love and Rayleigh phase-velocity maps, 5–40 s, of the western and central USA from USArray data. Earthand Planetary Science Letters, 402, 42–49. https://doi.org/10.1016/j.epsl.2013.11.022Fretwell, P., Pritchard, H. D., Vaughan, D. G., Bamber, J. L., Barrand, N. E., Bell, R., et al. (2013). Bedmap2: improved ice bed,surface and thickness datasets for Antarctica. The Cryosphere, 7(1), 375–393. https://doi.org/10.5194/tc-7-375-2013Gomez, N., Pollard, D., & Holland, D. (2015). Sea-level feedback lowers projections of future Antarctic Ice-Sheet mass loss.Nature Communications, 6, 8798. https://doi.org/10.1038/ncomms9798Hansen, S. E., Graw, J. H., Kenyon, L. M., Nyblade, A. A., Wiens, D. A., Aster, R. C., et al. (2014). Imaging the Antarcticmantle using adaptively parameterized P-wave tomography: Evidence for heterogeneous structure beneath WestAntarctica. Earth and Planetary Science Letters, 408, 66–78. https://doi.org/10.1016/j.epsl.2014.09.043Ma, Z., & Dalton, C. A. (2016). Evolution of the lithosphere in the Indian Ocean from combined earthquake and ambient noisetomography. Journal of Geophysical Research: Solid Earth, 122(1), 354–371. https://doi.org/10.1002/2016JB013516"
71.0,"rates of gun-related homicides and emergency department visits are notoriously higher in the U.S.than in other developed countries. Gun violence disproportionately affects low-income andminority residents; homicide is the first (second) leading cause of death for young black (Hispanic)men [1]. Early death and incarceration contribute to staggering numbers of “missing” black men(83 black men per 100 black women), creating other social problems in minority communities [2].Therefore, decreasing gun crime would reduce inequality by improving outcomes fordisadvantaged minority groups. In addition, reducing gun violence would benefit taxpayers bydecreasing Medicaid and Medicare spending on gun-injury related health care.1 Therefore,governments have several reasons to examine potential policy solutions to reduce gun crime.Background: Revitalizing urban areas with green spaces could be part of a policy solution toreduce gun violence, although the effects of vegetation on crime are theoretically and empiricallyambiguous. On the one hand, trees and shrubbery could provide hiding places for criminals andinhibit neighborhood surveillance by obstructing views of the streets. On the other hand, greenerspaces could deter crime (1) by providing community gathering spaces, thereby placing more eyeson the ground and (2) by dampening criminals’ sense of aggression through the physiologicallycalming effects of nature [3]. Understanding how green spaces affect criminal activity hasimportant consequences for safety in urban neighborhoods.Intellectual Merit: A naïve comparison of gun crime between more and less green areaspotentially includes selection bias, since more affluent, less crime-ridden areas also tend to begreener. Even a comparison across time for areas that become green can produce biased estimatesif a confounding factor like gentrification of a neighborhood simultaneously increases green spacesand decreases crime. To circumvent such endogeneity, my research would exploit vacant lotrenovation programs as close-to exogenous variation in green spaces, thereby identifying thecausal impact of greening spaces on gun crime rates: a policy-relevant parameter.By combining this policy-induced variation with objective, detailed data on gunshots andgeographic imagery and data across several American cities, my proposed research seeks touncover how greening urban spaces affects gun violence: one important category of crime.Studying this effect is challenging with typical, reported crime data since renovating lots mightaffect reporting rates (if more people are present to report crime, for instance) and actual amountsof criminal activity. My study will address this issue by using new data from ShotSpotter gunshotsensors (described below). Furthermore, existing studies on the effects of greening vacant lots oncrime rely on relatively sparse crime data in a single locality [4] [5]. In contrast, I would utilizedata on a long time horizon, with more frequent observations,2 across several counties andmunicipalities.3 Therefore, I could contribute longitudinal and more precise, generalizableestimates of the effect of greening spaces on gun crime to the existing literature.Data: ShotSpotter is a technology that captures incidents of gunfire using audio sensors, providingcomprehensive data on these events including precise geographic coordinates and timestamps. Akey advantage of these data is that they are not subject to reporting bias or underreporting, therebyproviding a more objective measure of gun crime: my outcome measure [6]. In order to measure1 Medicaid and Medicare picked up nearly half of the costs of caring for Chicago survivors of gunshot woundsamong costs between 2009 and mid-2016 analyzed by the Chicago Tribune.2 According to Carr & Doleac, 911 reports of shootings (reports of assault with a dangerous weapon) capture just12% (2-7%) of all gunfire incidents recorded by ShotSpotter.3 ShotSpotter has been employed in at least 90 U.S. counties or cities since 2000, of which I currently have access toover 1,500 locality-months’ worth of data representing 27 unique localities.the treatment (greening of vacant lots), I will utilize cities’ databases on vacant lot renovationswhere readily available.4 To allow for analysis in cities where such databases are not available, Iwill use machine-learning algorithms on high-resolution digital aerial imagery and LIght Detectionand Ranging (LIDAR) data to classify areas as green spaces, trees, or other objects of urban spaces,akin to methods described by Zhou & Troy [7]. While I have not worked with these kind ofgeographic data before, my background in machine learning coupled with support from GISexperts at my university’s library would allow me complete this step of my project. I will combinethese data in ArcGIS to create a lot-time level panel dataset.5Methods: The first part of the empirical analysis involves an event study, difference-in-differences(DD) approach before and after greening of vacant lots occur. Control observations will be vacantlots that were not renovated. The primary outcome measure will be the number of ShotSpotter-detected gunfire incidents within a specified radius from each lot. I will check the results usingdifferent radii from lots both (1) as a robustness check and (2) to determine how close to a greenlot one should live to experience its effects: a question that remains underexplored in the currentliterature. Regression analyses will include year and month fixed effects (FE) and lot-, Censusblock-, or city-level FE. In specifications with city FE, I will control for Census block-levelcovariates available from the American Community Survey (ACS) including median income,home ownership rates, racial demographics, and other variables that could be associated with crimelevels. I will cluster standard errors at the lot level: the level of treatment. Further analyses wouldadjust for spatial correlation. In addition to basic DD estimates, I will also employ methods ofsynthetic control to establish comparison groups that better demonstrate common pre-trends.A potential concern with this quasi-experimental approach is that greening spaces mightjust push the same amount of crime to other areas of a city not-yet renovated. However, thisphenomenon would attenuate my estimates of the treatment effect since crime would increase incontrol areas relative to treatment areas.6 To further investigate the prevalence of this phenomenon,I could look at citywide impacts before and after periods of lot restoration.Assuming encouraging findings, I would also complete a cost-benefit analysis. I wouldgather information on the average cost to turn a vacant lot green and maintain it, and associatedadministrative costs. On the benefits side, I will use estimates of the social costs of gun crime(criminal justice claims, loss of life, and other costs [8]) to estimate costs avoided because ofdecreased gun violence. More broadly, I will translate my findings into units like dollars and livessaved that are salient and easily interpretable for policy-makers. I will also submit my findings topeer-reviewed publications and present at crime, urban policy, and economics conferences.This research has important implications for determining the potential of urban renewalpolicies, such as vacant lot restoration programs, to reduce crime. If vacant lot restoration programsare effective at curbing gun violence, minority populations would disproportionately benefit.References: [1] CDC data. [2] Wolfers, J., et al. (2015, April 20). The New York Times. [3] Kuo, F. E., & Sullivan,W. C. (2001). Environment and Behavior, 33(3). [4] Branas, C. C., Cheney, R. A., MacDonald, J. M., Tam, V. W.,Jackson, T. D., & Ten Have, T. R. (2011). American Journal of Epidemiology, 174(11). [5] Garvin, E. C., et al.(2013). Injury Prevention, 19(3). [6] Carr, J., & Doleac, J. L. (2016). Brookings Research Paper. [7] Zhou, W., &Troy, A. (2008). International Journal of Remote Sensing, 29(11). [8] Gani, F., Sakran, J. V., & Canner, J. K.(2017). Health Affairs, 36(10).4 For example, Branas et al use such a database to study Philadelphia, PA.5 A lot-day dataset is possible for cities in which the exact renovations dates are known. Otherwise, I will create adataset using whatever courser time level lot greening could be detected using aerial imagery and LIDAR data.6 To determine the effects of spillover effects more exactly, future randomized trials could consider varying theproportion of lots renovated within a city among a sample of several cities."
72.0,"Bio-production of synthetic rubber using engineered Escherichia coliIntroduction: In a society with both a growing dependence on energy and a depleting reservoirof fossil fuels, it has become increasingly important to design chemical syntheses that aresustainable, renewable and cost-effective. One synthesis of particular concern is that of isoprene,a precursor to synthetic rubber, since the current production relies on finite petrochemicalsources.1 Recent advances in synthetic biology and metabolic engineering have made it possibleto biosynthesize isoprene using glucose extracted from plant biomass, a renewable feedstock.Despite these advances, previously studied synthesis pathways report low product yield due topoor catalytic activity, making them economically unfeasible for large scale production.2,3In order to overcome this roadblock, I propose to use a keto acid-mediated pathway tobiosynthesize isoprene. Keto acids can be used as a selection in directed evolution (DE), a vitaltool in the enhancement of enzyme activity.4 In order to employ DE however, enzyme specificitymust be high enough for the desired conversion. Computational techniques, such as docking andfunnel metadynamics, can be utilized to elucidate key amino acids involved in binding andcatalytic active sites. These amino acids can then be mutated to enhance enzyme specificity. Inthis work, I focus on the conversion of citramalate to butanoic acid, a key step in the synthesis ofisoprene, by expressing carnitine-CoA ligase (CaiC) and carnitine-coA transferase (mvaE) in E.coli. I hypothesize that engineering specificity and activity in heterologous CaiC and mvaEenzymes using a combined theory-experiment approach will increase butanoic acid yield,making the biosynthesis of isoprene more feasible for scale-up.Research Aims: The primary objectives of this project are to (1) manipulate specificity and (2)increase activity of CaiC and mvaE to optimize conversion of citramalate to butanoic acid, whichcan then be converted to isoprene through a series of organic reactions (Fig. 1).Figure 1: Workflow to sustainably produce isoprene from plant biomass by designing enzymes, supplementingexperiment with modeling to engineer protein design, and using results from computation to inform experiment.Preliminary results: During the summer of 2018, I obtained preliminary data by working on theupstream synthesis of isoprene using engineered E. coli in Dr. Kechun Zhang’s lab at theUniversity of Minnesota through the NSF-funded Center for Sustainable Polymers. Specifically,I was able to produce citramalate from glucose via citramalate synthase (CimA) using a ketoacid-mediated pathway (Fig. 1). The next phase of the project is to convert citramalate tobutanoic acid, the next intermediate in the biosynthesis of isoprene.Methods: (Aim 1) CaiC and mvaE enzymes are known to perform the desired reduction oncarnitine, a molecule of similar structure and functional group composition to that ofcitramalate.5,6 I will use computational modeling to design more specific enzyme active sites forcitramalate. The crystal structure for mvaE will be obtained from the Protein Data Bank. I willKristen C. Vogt NSF GRFP Graduate Research Planthen build a homology model for CaiC based on the crystal structure of L-carnitine CoA-transferase (CaiB) and generate force fields describing carnitine and citramalate from quantumchemical calculations. I will run molecular docking of CaiC and mvaE enzymes with citramalatefollowed by molecular dynamics (MD) simulations to equilibrate systems. I will then use funnelmetadynamics to calculate protein-substrate binding free energy to provide an estimate of thebinding affinity between both enzymes and substrates.7 Insights gained from these simulationswill inform key amino acid mutations to improve enzyme specificity.(Aim 2) Next, I will use directed evolution to improve enzyme activity using a keto acid-pathway and growth-based selection.4,8 I will use error-prone polymerase chain reactions (PCR)to create mutant libraries of CaiC and mvaE enzymes. PCR products will be cloned into a DNAbackbone, electroporated into E. coli, and plated on Luria broth (LB) plates containing 100g/mL Ampicillin. I will measure the total product formed from enzyme conversion using a 4-aminoantipyrene assay.8 Enzymes with readings 50% greater than parental standards will beselected for rescreening. Once both enzymes are optimized for activity and specificity, theirDNA sequences will be ordered, replicated using PCR, ligated into a DNA backbone, andtransformed into E. coli. I will then run fermentation for 48 hours in a 37 C thermoshakerrunning HPLC and OD every 12 hours to monitor butanoic acid production.9600Resources and support: In order to address these aims, I will work in collaboration with theZhang and Truhlar groups at the University of Minnesota (UMN) to develop the experimentaland theoretical components of isoprene synthesis, respectively. Access to supercomputing timethrough NSF’s XSEDE will allow for the proposed computational simulations.Intellectual Merit: The combined theory-experiment workflow outlined in this proposal is usedto overcome low yield by improving enzyme activity and specificity in heterologous CaiC andmvaE enzymes. This methodology can be applied in any biosynthetic reaction to synthesizenovel non-natural metabolites at higher yields over varying conditions.7 Even if highly accuratefree energies are challenging to obtain from simulation, mechanistic information obtained fromMD can be used to inform the next stage of experiment. Future directions of this project includeconverting butanoic acid to isoprene and investigating gene knockouts to further increase yield.Broader Impacts: The synthesis of isoprene, a commodity used in countless goods, such asadhesives, tires, and shoe soles, is currently unsustainable due to reliance on limited petroleumresources. Biosynthesizing isoprene using fermentation offers a renewable and cost-effectivealternative. Increasing yield will make it feasible to utilize this technology in large scaleproduction to move away from harmful, depleting syntheses and towards a more sustainablefuture. I plan to regularly present results from this work at conferences and make publicationsavailable to the public via open-access publication methods. Lastly, because parts of the aboveproject, such as using recombinant DNA technology and running fermentation, lend themselvesto undergraduate research, I will mentor and engage undergraduates in order to provide themwith access to authentic research experiences early in their careers.1. Singh, R. Org. Process Res. Dev. 2011, 15 (1), 175–179. 2. Zhao, Y.; Yang, J.; Qin, B.; Li, Y.; Sun, Y.; Su, S.;Xian, M. Appl. Microbiol. Biotechnol. 2011, 90 (6), 1915–1922. 3. Yang, J.; Nie, Q.; Liu, H.; Xian, M.; Liu, H.BMC Biotechnol 2016, 16. 4. Atsumi, S.; Liao, J. C. Appl. Environ. Microbiol. 2008, 74 (24), 7802–7808. 5. Eichler,K.; Bourgis, F.; Buchet, A.; Kleber, H. P.; Mandrand-Berthelot, M. A. Mol. Microbiol. 1994, 13 (5), 775–786. 6.Hedl, M.; Sutherlin, A.; Wilding, E. I.; Mazzulla, M.; McDevitt, D.; Lane, P.; Burgner, J. W.; Lehnbeuter, K. R.;Stauffacher, C. V.; Gwynn, M. N.; et al. J. Bacteriol. 2002, 184 (8), 2116–2122. 7. Limongelli, V.; Bonomi, M.;Parrinello, M. PNAS 2013, 110 (16), 6358–6363. 8. Bloom, J. D.; Labthavikul, S. T.; Otey, C. R.; Arnold, F. H.PNAS 2006, 103 (15), 5869–5874. 9. Kuhn, J.; Müller, H.; Salzig, D.; Czermak, P. Electronic Journal ofBiotechnology 2015, 18 (3), 252–255."
73.0,"humanscalestructuresoutofmodularcomponentsinrealworldenvironments.IntroductionAutonomousunderwatervehicles(AUVs)fittedwithspeciallydesignedgripperswillbuild concrete retaining walls and artificial reefs with modular blocks. Blocks will be designed toprovidepassiveerrorcorrectiononbothpickupandplacement,makingthesystemrobusttonoisefromlocalizationandcontrol. Localizationwillbeachievedwithspeciallydesignedinfrastructurepartiallyembeddedintheblocksthemselvestoprovidecertaintynearassemblyareas.Underwater construction is both dangerous and expensive because specially trained humandivers must build and maintain structures by hand. At moderate depths, extreme measures mustbe taken to protect divers including the use of hyperbaric chambers to treat decompression sick-ness. Small mistakes or equipment failures during a dive are disastrous, often resulting in death.Atextremedepthsbuildinginpersoniseffectivelyimpossibleduetothereducedcapacityofpres-surized air tanks at high pressure. The substantial cost and risk of using divers on site leads us tofavor assembly on land and then transportation to the final location which constrains the types ofstructuresthatcanbebuilt.Though autonomous underwater construction technology could open a frontier of possible ap-plications, no autonomous system exists to date. Teleoperation based solutions have received at-tention in the literature including a rubble leveling robot [2] and a back hoe [1]. Teleoperationeliminates the need for accurate localization, but it requires that large amounts of data are passedback to the human operator at a high rate. Communication underwater is often achieved usingeither a physical tether or acoustic networking. Managing long tethers is difficult and acousticcommunicationislowbandwidth,limitingthepossiblerangeofteleoperationbasedsolutions.Research Plan My initial system will be built around a BlueROV2 underwater robot which isavailableintheDartmouthroboticslab. TheBlueROV2isa10kilogram,0.5meterlongrobotwith6degreeoffreedommotion. Itisanattractiverobottobasethesystemonbecauseitincludesawellmaintainedcodebaseandseveralsensorsforlocalizationincludingtwoinertialmeasurementunits,pressuresensors,twocompasses,alowlightHDcamera,andashortbaselineacousticpositioningsystem. IwillutilizevisualmarkerscalledARTagsforvisualpositioninginformation.Research Question 1 How can we best exploit accurate localization information when it is avail-able, and smoothly switch to coarse grained techniques when it is not? Approach My preliminaryexperiments on localization suggest that sensor accuracy varies depending on the robot’s positionand velocity. I will empirically model the noise and accuracy of the sensors under relevant cir-cumstances, then design a sensor fusion algorithm which exploits the most reliable information atevery instant. Research Question 2 Given some desired structure, how can we design a feasibleand robust build plan for the structure? Approach To design a feasible build order for a structure,itwillbenecessarytoconsiderthephysicalconstraintsofboththebuildingblocksandtherobot’smaneuvering capabilities. I will model the build ordering constraints induced by the structuralandmaneuveringconstraintsasaconstrainedoptimizationproblem. Theobjectivefunctionofthisproblemwillencodetheabilityoftherobottoexploitlocalizationinformationthroughoutthebuild.My initial work on build order optimization in 3D printing can be generalized to suit this applica-tion[3]. ResearchQuestion3Howcanweautomaticallyreacttobuilderrorscausedbychangingenvironmental circumstances? Approach To gain insight into the failure modes of the system, IwilltestitrepeatedlyonaselectedfewbuildplansinboththepoolandLakeSunapee,NH.Duringeach test, I will record and categorize the failures that occur. Based on the most common types oferrors, I will develop automatic recovery mechanisms. For example, if a block is not fully seatedonanotherblock,therobotcouldgentlynudgethemintoplace. Iftherobotpassesthroughacloudofsilt,itcouldstopandwaitforthecloudtosettlebeforecontinuingthebuild.ExperimentPlanFirst,Iwillisolateeachsensoranddetermineitsaccuracylimitations. Toestab-lishtheaccuracyoftheIMUandARTagreadings,Iwillutilizeanindustrialroboticarmavailableinthelabtocollectaccuracyandnoisedatabasedonknownprogrammedmotions. Toevaluatethepositional accuracy of the sensor fusion algorithm on land, I will utilize a highly accurate VICONpositioning system as ground truth. With the sensor quality models in hand, I will begin iteratingon the sensor fusion algorithm. Preliminary debugging style experiments can be conducted in a 6foot diameter water tank in the lab. Frequent field tests in the athletic pool and nearby lakes suchas Lake Sunapee will inform further iterations. Simple tests such as repeatedly moving a blockback and forth on a platform in calm clear waters will help isolate the quality of localization data.Build order and error recovery algorithms will be tested by selecting several simple build plansand repeatedly testing them in varying environmental circumstances. The ultimate goal will be tosuccessfullyexecuteabuildplanneartheshoreoftheCarribeanseaduringoneofthelab’syearlytripstotheBellairsResearchInstituteinBarbados.Intellectual Merit Underwater construction will require the co-development of localization in-frastructure and sensor fusion strategies to rapidly instrument a build site. Rapidly deployablelocalization infrastructure will have applications in any autonomous robotic system attempting toachieve manipulation tasks in a remote or harsh environment. Rather than attempting to jump tounaided manipulation in totally unstructured environments, I will be taking the realistic approachof exploring the minimal amount of additional infrastructure required to complete manipulationtasks. This work will also advance the evaluation of the trade offs between computational andphysical complexity in autonomous robotic systems. For example, designing blocks which moresuccessfullycorrectforerrorcouldallowmanipulationbehaviorstobemoresimplewhilerequiringamorecomplexblockgeometry. Developingtechniquestorigorouslyevaluatetradeoffsbetweencomputationalandphysicalcomplexityforcomputational-physicalsystemsisanimportantstepindesigningrobustroboticsystemsforrealworldenvironments.Broader Impacts A system which can robustly place modular components on one another couldenable the mostly autonomous creation of retaining walls or artificial reefs. As ocean levels con-tinuetorise,retainingwallswillbeincreasinglyimportantindevelopedcoastalareas. Bystackingcomponentsofartificialreefs,wecouldenablelargerscalereefrestorationactivities. Asthetech-nology advances, it could enable more subtle applications as well. It could enable us to moreefficientlybuildoffshoreenergyinfrastructureorenableustoscaleunderwateragriculture.This project is a valuable opportunity for young researchers to gain hands on experience withrobotics and software development, preparing them for a productive career in an exploding field.During the conduction of the preliminary study, I worked with two high school students who con-tributed directly to this research. The students gained exposure to robotic software design andmechanical modeling. I will continue using this work as an opportunity to mentor driven youngresearchers.[1]TaketsuguHirabayashietal.“Teleoperationofconstructionmachineswithhapticinformationforunderwaterappli-cations”.In:AutomationinConstruction15.5(2006).21stInternationalSymposiumonAutomationandRoboticsinConstruction.[2]T.S.Kimetal.“Underwaterconstructionrobotforrubblelevelingontheseabedforportconstruction”.In:201414thInternationalConferenceonControl,AutomationandSystems(ICCAS2014).[3]S.LensgrafandR.R.Mettu.“Animprovedtoolpathgenerationalgorithmforfusedfilamentfabrication”.In:2017IEEEInternationalConferenceonRoboticsandAutomation(ICRA)."
74.0,"Keywords: Aging, stress response, RNA biology, DrosophilaBackground: Aging is characterized by the accumulation of cellular damage, the physiologicaldecline of tissue and an increased susceptibility to disease resulting from the failure to maintainhomeostasis in the face of endogenous and environmental stresses [1]. A key mechanismunderlying protein homeostasis (proteostasis) in response to stress is the assembly of RNA stressgranules (SGs). When stress dissipates, SGs disassemble and cells return to homeostasis, thusSGs dynamic behavior offers a potential molecular mechanism that links aging and cellularstress resilience. Interestingly, changes in SG dynamics have been identified in age-dependentneurodegenerative disorders, yet SGs remain unexplored in normal aging.SGs are non-membrane bound organelles that assemble in the cytoplasm of cells whentranslation initiation is inhibited or during stress (e.g. heat shock, osmotic pressure, oxidativestress) [2]. SG formation has been shown to increase fitness during stress [3]. During transientstress, SGs stabilize mRNA and delay the aggregation of proteins linked to neurodegeneration [4-5]. SGs preferentially sequester long, poorly translated RNAs as well as a diverse set of proteinssuch as nuclear pore complexes, RNA binding proteins and others varying by cell type andstressor [6-7]. SG assembly is rapid: a dense core is formed by an established network of protein-protein interactions, nucleated by G3BP1, followed by the assembly of a dynamic shellcomprising RNA and RNA binding proteins that trigger liquid-liquid phase separation. Afterstress subsides, SGs spontaneously disassemble and allow sequestered factors to return to theirfunctions [8]. When SGs fail to disassemble, such as during chronic stress, they disrupt, notmaintain, proteostasis and facilitate protein aggregation [9]. Two previous studies found that SGcomponents aggregate with age, but it is unknown how normal aging alters the nucleation,stability, or disassembly of SGs [10-11].I hypothesize that (1) the dynamics of SGs will be altered throughout aging and (2) thecomposition of SGs will correspondingly be altered by age.Aim 1: Determine the dynamics of SGs during aging in response to stressUsing a Drosophila model where Rasputin (RIN), the homolog of G3BP1 and the onlyprotein required for SG formation, is endogenously tagged with GFP (RIN-GFP), I will visualizeSG formation in the fly brain [12]. Drosophila share over 60 percent of their genome withhumans, providing a translatable and practical model to study SGs throughout aging (lifespanaverages 100 days). To determine if age impacts SG dynamics (e.g. assembly and disassembly)in fly brains, I will dissect adult fly brains and immunostain for GFP. Using a confocalmicroscope, I will quantify the distribution and sizes of RIN-GFP puncta in various regions ofthe fly brain. Drosophila will be dissected at five time points across aging (1, 20, 50, 80, 100days). To capture the altered dynamics of SGs between types of stress, heat shock and oxidativestress through paraquat ingestion will be used to stress flies just before dissection. Controls willinclude age matched Drosophila that will not be subjected to stress. To distinguish between SGassembly and disassembly, flies will be dissected just after stress (assembly) or two hours afterstress (disassembly). Ten to twenty flies will be studied per time point and treatment.Expected outcomes, potential pitfalls and alternatives. Preliminary experiments show thatSG assembly declines with age in flies (data not shown) thus I expect less robust assembly ofnew SGs for older Drosophila. I expect most SGs to disassemble one hour after stress for youngDrosophila. For old Drosophila, I expect less dynamic SGs or slower disassembly. The assemblymechanism of SGs varies slightly by stressor. Oxidative stress has canonically been used to elicitSG formation [2]. The dynamic behavior of heat shock induced SGs could defy expectationsbased on research into SGs generated by oxidative stress. Previous research shows some SGaggregation in aging [10-11]. If SG disassembly does not visibly change with age, though, changesin SG composition could alter SG dynamics in other ways. These experiments will show for thefirst time how aging fly brains respond to stress by assembling SGs. Future research willexamine the relationship between altered SG dynamics and phenotypes of aging (e.g. behavior).Aim 2: Determine the composition of SGs during aging in response to stressSG dynamics are impacted by the composition of SGs. For example, the recruitment ofprotein kinases ULK1 and ULK2 and the ATPase VCP to SGs is required for SG disassembly[13]. Age-related alterations to SG composition are likely responsible for the expected decline indynamic SG formation during aging. SGs will be isolated using immunoprecipitation (IP) ofGFP tagged RIN protein/RNA complexes from the neurons of the same experimental Drosophilaand controls described above. IPs will be optimized using IgG controls to ensure specificity ofRIN-GFP complexes. Mass spectroscopy and RNA-sequencing will be used to identify theproteins and RNAs comprising SGs throughout the various stress and aging conditions. Key SGmarkers will be cross-referenced. The results will be analyzed using statistical models to identifywhich proteins and RNAs are enriched or depleted in specific conditions and age time points.Expected outcomes, potential pitfalls and alternatives. The differential recruitment ofproteins to SGs has physiological impacts on cells during the stress response and can alter SGdynamics. For example, if SGs formed during old age increasingly sequester nuclear porecomplexes, proteostasis is more likely to be disturbed in aging [6]; similar logic applies to otherpathways. RNAs sequestered to SGs mediate protein recruitment but do not impact globaltranslation [8]. By linking the composition of SGs with different assembly and disassemblyphenotypes, the impact of age on the mechanism of SG-supported stress resilience can be morefully understood. To follow up candidates identified by IP/mass-spec, overexpression orknockdown of genes in Drosophila will be used to identify the specific role of proteins in thealteration of SGs dynamics in the stress response throughout aging.Intellectual Merit and Broader Impacts: For the past three years, I worked with Drosophilastudying RNA biology in aging and the cellular stress response. Through my research, Ioptimized IP procedures to isolate SGs and prepare them for analysis by mass spectroscopy andRNA-seq. To analyze this data, I created statistical models using the R programming language.Given SGs role in stress resilience and neurodegeneration, understanding the impact of age onthe dynamics of SGs is important. These findings will help generate new hypotheses about therole of the stress response in aging. Additionally, researchers are pursuing treatments to theneurodegenerative disease amyotrophic lateral sclerosis that eliminate SGs in humans. SGelimination could significantly impair the ability of neurons to survive especially during aging.The proposed experiments will provide key insights into new strategies to maintain cellularhomeostasis in the human body, specifically the brain, and extend the health span as well aslifespan. As part of my research, I am responsible for leading and training a small team ofundergraduates in our study of RNA biology in aging. In addition to effectively communicatingmy research at two international conferences and to my community, I helped review currentresearch on the role of SGs in protein aggregation for an upcoming publication.References cited: [1] Rieraetal et al., 2016. Annu. Rev. Biochem. 85, 35-64. [2] Parker et al., 2009. Mol. Cell 36,932-941. [3] Riback et al., 2017. Cell 168, 1028–40. [4] McGurk et al., 2018. Mol. Cell 71, 703-17. [5] Bley et al.,2015. Nuc. Acids Res. 43, 23. [6] Khong et al., 2017, Mol. Cell 68, 808–20. [7] Markmiller et al., 2018, Cell 172,590–604. [8] Jain et al., 2016. Cell 164, 487–98. [9] Zhang et al., 2019. eLife 8, 39578. [10] Lechler et al., 2017.Cell Reports 18, 454–467. [11] Moujaber et al., 2017. Bioch. Acta 1864, 475–486. [12] Anderson et al., 2018.Human Mol. Genetics 27, 1366-81. [13] Wang et al., 2019. Mol.Cell 74, 742-57."
75.0,"Investigation of the Catalytic Properties of Cerium(IV) Oxide in Metal Oxide LaserIonization-Mass Spectrometry ImagingBackground: Matrix-assisted laser desorption/ionization-mass spectrometry imaging (MALDI-MSI) is an emerging and powerful analytical technique, which allows the spatially resolvedcharacterization of a wide range of analytes within biological specimens.1 Metal oxide laserionization (MOLI) is a recently described variation on MALDI in which a metal oxide, ratherthan an organic acid, is utilized as the matrix.2255m/z Unlike the other metal oxides, Cerium(IV) Oxide(CeO ) demonstrates a unique property of laser2induced fatty acyl catalysis when applied to281 phospholipids and energized by standard lasersm/zfound in MALDI-TOF MS instruments, as seen inFigure 1. This property of laser-induced catalysis byFigure 1. MOLI-MS using CeO 2 on POPC CeO provides a considerable opportunity in various2biological and clinical applications in which fatty acidprofiling may be needed.3,4 Beyond clinical applications, CeO -based materials also have a2variety of applications as a catalytic system in fuel cells, thermochemical water-splitting, organicreactions, and photocatalysis.5 Because of the involvement of CeO in a variety of fields, and the2potential it has to impact future technologies, a further investigation of the biological catalysisproperties of this compound are warranted.Preliminary Results: The Cox group in Colorado has headed theinvestigation for the use of MOLI techniques in the identification ofbacterial species. This group discovered that CeO could be utilized2for identification with improved stability and reproducibilitycompared to other metal oxides.6 Previously, MOLI has not beenused in conjunction with MSI in order to induce fatty acyl catalysisdirectly from tissue for possible bacterial detection. While MSI isfound to be a promising technique, there are only very few researchgroups that currently have the instrumentation available to conductMSI studies. At Harvard Medical School, the Agar group has beenFigure 2. MOLI-MSI of control working in the field of MSI for over a decade. During my time inmouse brain: A) 255.4 m/z &the group this summer, I developed and optimized a technique forB) 281.5 m/zCeO deposition on biological tissue, which is currently being2prepared for submission. This technique describes the deposition of CeO for MOLI-MSI, such2as in Figure 2, as well as possible clinical applications.Although MOLI using CeO has shown considerable promise, the mechanism for which2fatty acyl catalysis occurs when laser energy is applied to CeO is still unknown. Most2commonly, when MOLI-MS is used, analyte ionization of phospholipids typically occurs byprotonation due to interactions with the Lewis acid/base sites on the metal oxide. However, onlyno protonation occurs with CeO -induced catalysis, indicating that the mechanism of cleavage is2unique. It is postulated that much of the catalytic activity of CeO arises from oxygen vacancy2defects in the surface which occur at MALDI-like conditions (high temperature, low pressure).6Proposed research: My preliminary results have developed a novel technique for theapplication of CeO to clinical problems. However, it has left unanswered a critical question2about the mechanism of fatty acyl cleavage that occurs when CeO is used for MOLI-MS/MSI.2Without understanding the mechanism of catalysis, it is difficult to fully interpret the massMadison McMinn | 2018 NSF GRFP Research Statementspectra generated by this method. My future research plans consist of three specific goals,detailed below. By achieving these goals, I plan to elucidate the mechanism of catalysis that isunique to CeO , when it is used with biological samples.2The first goal is to expand beyond phospholipids, and study compounds that also containfatty acid chains, such as diacylglycerols, sphingolipids, triglycerides, acyl-carnitines, acyl-coenzyme A thioesters and other acyl-bound biomolecules. I am interested in determining if thecleavage of fatty acid chains is unique to phospholipids, or if CeO can also induce this property2on other compounds. This will serve to determine the depth and breadth of this application.Also, if certain compounds are unable to undergo catalysis by CeO , structural differences can be2identified and studied. For these experiments, I plan to use commercially available lipidstandards to evaluate catalysis in a simple, and direct way.The second goal is to apply the knowledge gained in the first goal of this proposal tocomplex systems. Since isolated and purified compounds do not exist naturally, it is critical tosee how effective this technique is when applied to a complex biological specimen. For this, Iplan to correlate mass spectra obtained with a typical MALDI matrix and with CeO . I aim to2correlate the known lipid composition with the fatty acid composition, to see if certain speciesare more prone to cleavage when present in a complex sample. This would be relevant in MOLI-MSI with heterogenous tissue samples, where lipid compositions can vary greatly throughout thespecimen.The third goal is to investigate the surface chemistry of CeO using experimental and2computational methods. I plan to perform studies where CeO particles of varying sizes are2probed by electron/neutron diffraction, since X-ray diffraction is not an ideal technique for thismaterial, due to the low scattering power of oxygen. These studies aim to determine if a greaternumber of oxygen defects contributes to improved catalytic cleavage. Diffuse ReflectanceInfrared Fourier Transform Spectroscopy will be performed to study the surface morphology ofCeO before and after it is subjected to MOLI-MS. Once the surface structure of CeO is well2 2understood, computational studies can be performed using density functional theory calculations.Intellectual Merit & Broader Impact: Elucidating the mechanism of CeO -induced fatty acyl2catalysis will allow scientists to use my developed MOLI-MSI technique and the MOLI-MSdatabase for bacterial identification by the Cox group with increased confidence. Also, theinformation obtained from my first research goal has the potential to advance knowledge infields that use fatty-acid containing molecules, such as cosmetics, nutrition, and metabolomics,in addition to biological applications. Furthermore, elucidation of the catalysis mechanism ofvaried metal oxides, and what induces this variance, can contribute fundamental knowledge tothe field of catalysis chemistry, especially metal oxide catalysis. In regard to the broader impactin biological applications, fatty acid profiling of tissue specimens has been an extensive area ofstudy, dating back nearly a century. Most current approaches use many time-consuming stepsand, more concerningly, result in the loss of spatial relationships between these molecules.Preserving these spatial relationships is critical in the analysis of a wide variety of diseases,including many cancers, which demonstrate heterogenous tissue distribution. We have shownthat MOLI-MSI using CeO can provide in situ fatty acyl characterization of biological tissues2while preserving regional distribution. By better understanding the chemistry of CeO induced2fatty acyl catalysis, a more informed interpretation of resulting MS spectra and therefore thetissue composition can be appreciated. This work will lay the groundwork for a potentially newclinical and translational diagnostic approaches.1.Reyzer ML, Caprioli RM. J Proteome Res. 2005, 4(4), 1138-1142. 2.Schwamborn K, Caprioli RM. Nat RevCancer. 2010, 10(9), 639-646. 3.Voorhees KJ, Saichek NR, Jensen KR, Harrington PB, Cox CR. J Anal ApplMadison McMinn | 2018 NSF GRFP Research StatementPyrolysis. 2015, 113, 78-83. 4. Choe SS, Huh JY, Hwang IJ, Kim JI, Kim JB. Front Endocrinol (Lausanne). 2016,7(30). 5.Hodson L, Skeaff CM, Fielding BA. Prog Lipid Res. 2008, 47(5), 348-380."
76.0,"Evidence from the surface and atmosphere of Mars indicates that Mars was once a much wetterworld. Today, we know that nearly all the water has been lost [6]. Understanding the evolutionof water on Mars and its atmosphere is critical to answering questions such as: did life ever existon Mars, or does it now? What can Mars tell us about possible futures for Earth or evolutionarypathwaysofexoplanetatmospheres?IntellectualMeritTo approach these questions, we need new models that capture the complex chemistry and dy-namics governing escape of water. Thermal escape of hydrogen (H) is the main loss process forbothwaterandtheatmosphereasawhole[6]. BecauseHescapesmoreefficientlythanitsheavierisotope, deuterium (D), understanding variations in the D/H ratio is the key to understand-ing loss of Martian water. However, the most recent atmospheric escape studies that included Dchemistry only considered global averages of D/H, and were 18+ years ago [7, 10]. Since then,D/Hmodelingworkhasstalled,andnostudieshaveinvestigatedoutgoingfluxofD[6].Twenty years of data from the Hubble Space Telescope, ground-based telescopes, and Mars or-biters, landers, and rovers have augmented our knowledge of the D/H ratio on Mars. We nowknow it varies with season, altitude, and geographical location ([9], and references therein). Thisvariability is apparent in atmospheric water vapor, which can form clouds and be transported bydust storms. Studies have shown that planetary boundary layer (PBL) water ice clouds can de-crease the total water column by up to 15% on timescales of a few days [4]. Orbiter data [3, 5]shows that dust storms boost water in the mesosphere, which was demonstrated by Chaffin et al.[2]toenhancelossofHwithinweeks. IproposetousethesenewdataasinputsandconstraintsinthefirststudiesinnearlytwodecadesoftheroleofD/HinMartianatmosphericloss.ResearchGoals1. Explainhowseasonal,altitudinal,geographicalD/Hvariationsaffectatmosphericloss.2. Understandtheeffectsofplanetaryboundarylayercloudsonatmosphericloss.3. Quantifythecontributionofduststormstoatmosphericlossenhancement.ModelingMethodology,Preparation,andCurrentResultsDue to computational resource limitations, 1D photochemical models are required to simulate themartian atmosphere on time-scales of 105+ years. Though limited in space, 1D models can pro-vide context for end-member cases of more expensive 3D calculations. During my first year ingraduate school, I expanded a 1D photochemical model built by my advisor, Michael Chaffin,doubling the number of chemical pathways modeled and adding deuterium chemistry. Fol-lowing Chaffin et al. [2], the model solves a photochemical system of coupled partial differentialequations. Modificationstoaddressresearchgoalsrequireonlyminorchangesasfollows.1. New,high-precisiondatatoconstrainD/HinaltitudeandtimeisforthcomingfromtheESATrace Gas Orbiter to [8]; we can model this data as a time- and altitude-depndent function.Spatialvariancecanbeestimatedwiththis1Dmodelbyindividualruns.2. Bothdiurnalandseasonalvariationsinwatervaporabundanceduetocloudsandduststormscan be included with time-dependent calculations of the water vapor profile, which is pre-scribedinthemodel. CloudaltitudescanbeestimatedusingCuriosityrovermoviesandtheMarsRegionalAtmosphericModelingSystem(MRAMS)[1].IrecentlystudiedtheeffectsofDchemistryandchangestothetemperatureprofileandwaterEryn M. Cangi Research Proposal 2018-2019 NSF GRFP applicationvapor mixing ratios on escape by calculating the fractionation factor f, which represents howefficiently D escapes with respect to H. (A fractional value 0.xx means that D escape is 0.xx% asefficient as H escape). Selected results are shown in Figure 1. This is the first effort to modeldifferential escape of H and D in ∼18 years. Our results show that prior calculations greatlyoverestimated the relative escape of D due to systematic inaccuracies in atmospheric temperaturemeasurementsandphotolysisratecoefficientsavailableatthetime. Isharedtheseresultswithcol-leaguesatthe50thmeetingoftheAmericanAstronomicalSocietyDivisionforPlanetarySciences(DPS),andamcurrentlycompletingworkonwritingtheseresultsinalead-authorpublication.Figure 1: Fractionation factor (percent efficiency atwhich D escapes with respect to H) for 6 model runs,comparedtotworeferences. Labelsindicateadjustmentsto three temperature profile control parameters: T ,surfT , and T . “↓T ”, e.g., means the temperature attropo exo exotheexobasewasloweredforthatmodelrun.Adjustmentsof±25%tothemeanprofileweretested.Thelowsurfacetemperaturecaseproducedanunstableatmosphere(pho-tochemicalsystemhadnosolution)andwasdiscarded.BroaderImpactThe public imagination is already captivated by Mars, as a possible habitat for extraterrestriallife and a target for future crewed missions. As mentioned in my personal statement, next year Iwill join CU-STARS, a departmental program that brings extracurricular science lessons toColoradopublicschools. ExcitementaboutMarsfromallages,andthefactthatatmosphericlosscan be explained without complicated jargon, makes my research an excellent topic for reachingout to schools around Colorado. In terms of engaging the wider public, I also plan to make myresearch available to the public by giving talks at University of Colorado’s Fiske Planetarium,a method of outreach where I can draw on my earlier training in theatre. To understand possiblefuturesforMars,weneedtounderstanditshistory;myfirsttalkwilldiscussthehistoryofmartianatmospheric escape and implications for hypothetical future terraforming efforts (and theethics thereof). I believe in making it easy for the public to access and understand the sciencetheir taxes pay for; in addition to presenting at conferences and publishing papers, I maintain apersonalwebsitewithexplanationsofmyresearchforbothlaypeopleandfellowscientists.MarsresearchcontributesnotonlytoMarsscienceandmissions,butalsotoexoplanetaryscience.Compared to the requirements to understand exoplanet atmospheres, Mars is a cheap and readilyavailable laboratory. It is valuable not only for scientific opportunities, but because for decades, ithas captivated disparate groups of people. Now more than ever, humanity needs goals that unifyusandremindusthatweareallinthistogether;myworktounderstandtheevolutionofwaterandtheatmosphereonMarswilladvancethosegoals.References: [1]Campbell,C.,etal.2018,AAS/DPSMeetingAbstracts,300.03. [2]Chaffin,M.,etal.2017,NatureGeoscience,10,174-178. [3]Fedorova,A.,etal.2017,Icarus,300,440-457. [4]Haberle,R.M.,etal.2017,TheAtmosphereandClimateofMars. [5]Heavens,N.,etal.2018,NatureAstronomy,2(2),126-132. [6]Jakosky,B.,etal.2018,Icarus,315. [7]Krasnopolsky,V.2000,Icarus,148. [8]Villanueva,G.L.,etal.2018,AAS/DPSMeetingAbstracts,303.09. [9]Villanueva,G.L.,etal.2015,Science,348(6231),218-221. [10]Yung,Y.,etal.1988,Icarus,76(1)."
77.0,"Background. How does motivation drive learning? Evidence abounds that reward,motivation, and curiosity can all enhance learning and memory1; these findings have far-reachingimplications for education. However, a fundamental problem undermines our ability to apply thisresearch in classrooms: extrinsic reinforcement can actually decrease intrinsic motivation2. Inother words, although rewards like candy, stickers, and money are often used as incentives forstudents, these secondary reinforcers may decrease internal motivation, curiosity, and fulfillment.In the brain, dopamine pathways are strongly implicated in reward and motivation1.Dopaminergic cells in the ventral tegmental area (VTA) project to the hippocampus andsurrounding medial temporal lobes3, influencing memory by enabling the brain to prioritize andremember important information4. Moreover, high-reward contexts increase sustained VTAactivation and memory encoding5. Past studies of intentional encoding strategies have shown theimportance of elaborative and self-referential processing6, but have yet to link these methods todopaminergic modulation. Importantly, it remains an open question whether cognitive strategiescan act upon dopaminergic pathways to enhance memory, either immediately or over time.Cognitive neurostimulation, the volitional modulation of one’s own brain activity throughmental imagery and thoughts, offers a promising method of enhancing motivation. However,past research has found that without guidance, individuals struggle to self-motivate and modulateVTA activity7. Neurofeedback provides individuals with real-time information about their ownbrain activity. Past studies in the Adcock lab have successfully used neurofeedback to trainparticipants to self-activate the VTA; this activation is associated with self-reported motivation7.A day later, trained participants retained the ability to self-activate, even without neurofeedback.Intellectual Merit. Thus far, no study has shown that self-activation of the VTA caninfluence memory encoding or consolidation. This missing link is essential for elucidatingneural mechanisms of motivation and memory, as well as extending cognitive research toeducation. The proposed research will take a novel approach to address this gap in the literatureby embedding neurofeedback training within a memory task. The present study seeks to: (1)Train participants to drive intrinsic motivation and self-activate the VTA, (2) Test whether self-activation enhances memory encoding and consolidation, and (3) Identify effective motivationalstrategies with a data-driven approach. I hypothesize that with neurofeedback, participantswill learn to self-motivate and drive VTA activation, thus enhancing subsequent memory.Methodology. Fifty healthy participants will be recruited to participate in a study at theDuke University Brain Imaging and Analysis Center, which houses a GE Premier 3T MRIscanner. First, participants will complete two trait motivation surveys, the Motivational TraitQuestionnaire8 and the Behavioral Inhibition-Approach System9 (BIS/BAS). In the scanner, Iwill collect fieldmap, T1-weighted structural, and functional scans (TR=1s, voxels=2x2x2 mm3).Participants will complete three kinds of tasks: Activate task. Participants will beinstructed to self-motivate by using personally-relevant thoughts and mental imagery (e.g., onepast participant reported success with visualizing a cheering crowd7). Using PYNEAL software,previously developed in the Adcock lab, I will calculate real-time VTA activation and informparticipants with a dynamic thermometer display. Watch task. Participants will passively viewthe thermometer display, but will be informed that fluctuations are random, not neurofeedback.On these trials, the thermometer display serves as the control task, equating visual input with theActivate task. Encode task. On each trial, participants will try to memorize a series of 7 objectimages (2 sec. each), sampled randomly without replacement from a set of 336 images.In total, participants will complete 8 runs of 6 trials each. A random half of the trials willbegin with the Activate task (20 sec), and the other half will begin with the Watch task (20 sec).NSF-GRFP Graduate Research Plan: Alyssa H. Sinclair 2The Encode task (20 sec) will conclude every trial. Between runs, participants will verballydescribe the motivational strategies employed on preceding trials. After the MRI scan,participants will be randomly assigned to either the Same-Day group (memory test immediatelyafter the scan, n=25) or the Next-Day group (memory test 24-hours later, n=25). In a behavioraltesting room, participants will complete a recognition memory test of the images from theEncode task (336 old images, 168 novel images), and rate confidence on a 5-point Likert scale.Analyses. In summary, I will employ a 2x2 design (Task: Watch, Activate X Time:Same-day, Next-day) to address the following questions: 1. Does VTA self-activation enhancememory? I expect that within-subjects, average memory accuracy (d', signal detection theory)and event-related VTA activation will be greater on Activate than Watch trials. Moreover, trial-wise VTA activation will be parametrically related to memory for the stimuli encoded on a giventrial. Previous work in the lab has detected similar neuromodulatory effects on single trials10.2. Does self-activation of the VTA influence memory encoding and/or consolidation? IfVTA-activation enhances consolidation, then the Next-Day group will exhibit greater disparity inmemory accuracy between Activate and Watch trials (relative to the Same-Day group), becauseconsolidation is time- and sleep-dependent. Within the Next-Day group, I will compare memoryaccuracy for Activate and Watch trials to control for sleep-consolidation effects that areindependent of VTA-effects. An alternative hypothesis is that VTA-activation will improvememory equally in both groups, reflecting selective effects at encoding.3. What cognitive factors drive motivation and enhance memory? Using the traitmotivation survey data, I will test whether individual differences in personality (e.g., higherscores on trait motivation and the BIS/BAS Reward Responsiveness subscale) predict success onthe Activate task and subsequent memory accuracy. Moreover, I will employ a data-drivenapproach to identify the self-reported motivational strategies that most effectively increasedVTA activation (e.g., verbalizations, various types of mental imagery). Importantly, the existingliterature on motivation and reward is constrained by a limited set of strategies that are imposedby experimenters. Informed by my fMRI findings, I will develop future behavioral studies thattest the efficacy of the diverse motivational strategies that participants intuitively employ.Broader Impacts. Motivation is a core component of learning. Critically, low-incomeand disadvantaged students exhibit low intrinsic motivation, which predicts poor academicoutcomes11. In 2016, a staggering 29.7 million American children (41%) lived in low-incomefamilies12. In classrooms, fostering intrinsic motivation can improve learning outcomes andstudent retention9. Extrinsic reinforcers, such as monetary rewards, can have restricted and short-lived effects; in contrast, intrinsic motivation predicts long-term student success10. The proposedresearch seeks to empower individuals to drive intrinsic motivation and self-activatemotivational brain systems, thus engaging the brain to improve learning. With a novelneurofeedback approach, I will identify cognitive strategies that effectively act upon dopaminesystems to enhance memory. In future behavioral studies, I will directly test whether thesestrategies can successfully bolster motivation and memory without neurofeedback. The presentprogram of research seeks to uncover accessible and non-invasive methods of fostering intrinsicmotivation and improving memory, thus broadly benefiting learning and education.References: 1. Wise, R. A. Nat. Rev. Neurosci. 5, (2004). 2. Butler, R. Br. J. Educ. Psychol. 58, (1988).3. McNamara, C. G. & Dupret, D. Trends Neurosci. 40, (2017). 4. Lisman, J. E. & Grace, A. A. Neuron 46, (2005).5. Murty, V. P. & Adcock, R. A. Cereb. Cortex 24, (2014). 6. Kirchhoff, B. A. Neurosci. 15, 166–179 (2009).7. MacInnes, J. J., Dickerson, K. C., Chen, N. & Adcock, R. A. Neuron 89, (2016). 8. Heggestad, E. D. & Kanfer, R.Int. J. Educ. Res. (2000). 9. Carver, C. S. & White, T. L. J. Pers. Soc. Psychol. 67, (1994). 10. Adcock, R. A., et al.Neuron 50, (2006). 11. Schultz, G. F. Urban Rev. 25, (1993). 12. Koball, H. & Jiang, Y. NCCP, (2018)."
78.0,"Nature versus nurture? Maternal responses to infant distress callsIntroduction & Significance: The brain has the extraordinary capability of attributing differentlevels of importance to the different types of input signals it receives. For example, hearing one’sname, even at very low volume, elicits strong neural signals, as the brain has learned therelevance of that particular sound. The process by which this occurs is known as synapticplasticity, which allows the brain to alter its connections based on experiences associated withparticular sensory inputs. Synaptic plasticity is usually experience-dependent, and a class ofmolecules known as neuromodulators have the role of strengthening specific neural circuitsdependent on particular situations. Here I examine the action of one important neuromodulator,oxytocin, which is involved in a multitude of social interactions, including maternal care.Maternal behaviors are observed in all mammalian species, including mice. As infants,mouse pups are especially helpless, relying on their mother (called a ‘dam’) for all of their needs.Pups become scattered from the nest as the dam moves around, and must communicate with thedam that they have become isolated. To do so, they emit isolation ultrasonic vocalizations(USVs), triggering the dam to respond by retrieving the pup and returning it to the nest[1]. Whiledams retrieve pups with high accuracy, virgin female mice that lack prior experience with pupsfail to exhibit this behavior, generally neglecting the calls of a nearby pup[2]. However, afterbeing cohoused with a dam and pups for several days, virgin female mice can learn to retrievepups with comparable accuracy to the dam[2]. A virgin’s acquisition of this pup retrieval behavioris accelerated by administration of the neuromodulator oxytocin [2]. This behavior can beeliminated by inactivation of the left auditory cortex (A1), which contains a significantly higheramount of oxytocin receptors (OXTR) than the right A1[2].My proposed graduate research is concerned with the specific features of isolation USVstimuli that cause A1 to recognize the behavioral relevance of these sounds. Specifically, Ipropose to examine perceptual attributes of the isolation USV encoded by the maternal A1 thatenable the dam to recognize and respond to this sound, as well as the role of oxytocin-dependentplasticity in acquiring USV-induced pup retrieval behavior by inexperienced virgins. Recentwork demonstrated that human A1 distinguishes screams from conversational speech by anacoustic quality known as ‘roughness,’ defined as the rate at which the volume of soundchanges[3]. By detecting roughness, human A1 rapidly engages subcortical structures to assessdanger[3]. I hypothesize that similar acoustic perceptual features allow the maternal A1 todistinguish, and attribute behavioral relevance to, the sound of nearby pup isolation USVs, andthat learning of pup retrieval behavior through experience relies on oxytocin-dependent synapticplasticity that strengthens the A1 response to such perceptual features.Aim 1A: Which acoustic features differentiateisolation USVs from other vocalizations? Tounderstand how the USV response is encoded inthe dam A1, I will chronically implant electrodearrays into adult female mouse A1, and obtainsingle-unit recordings in response to natural, pup-evoked isolation USVs from a speaker. ThisFigure 1: Example single-unit recordings from process (Figure 1) allows clear visualization oftetrode implants in A1. An increase in activity istemporally-precise spike activity in A1 as itobserved when isolation USV stimulus beginsrelates to sensory input from isolation USVs.Additionally, by observing behavior that dams exhibit when hearing the isolation USV stimulus,Katherine Furman – Graduate Research StatementI can visualize how A1 activity correlates to both sensory input and behavioral output (in theform of pup retrieval).Using the modulation power spectrum (MPS), which can visualize sounds two-dimensionally on both spectral and temporal domains[3], I can examine the portions of acousticspace in which these naturally-produced isolation USVs reside. By comparing this to the MPS ofadult USVs (which are behaviorally neutral to the dam) I will isolate which acoustic features areunique to the pup isolation USV and have behavioral relevance to A1.Aim 1B: Are these acoustic features relevant to A1? If pup isolation USVs contain specificacoustic ‘roughness’ features distinguishable by the maternal A1, synthetically manipulatedUSVs which lack these features should result in a weakened response compared to natural pup-evoked isolation USVs. Using MATLAB I will synthesize audio clips which mimic pup USVs,but lack the spectral/temporal features previously identified as unique to isolation USVs. I willthen play these to maternal animals, measuring behavioral and neural responses to calls withsimilar statistics as USVs, but varying in their frequency, temporal modulation (rhythm), androughness.Using single-unit recordings from electrode arrays in A1, I will observe neural activity ofA1 in dams when they are exposed to synthetically manipulated USVs played from an ultrasonicspeaker. If I’ve successfully identified the differentiating feature(s) making isolation USVsunique from other mouse vocalizations, I expect that dam A1 neurons will exhibit a stronger,more temporally-precise response to hearing pup-evoked isolation USVs (as was observed in[2]), than synthetically manipulated USVs. I also expect that pup retrieval behavior will besignificantly diminished, if not eliminated, when exposed to synthetically manipulated USVs.Aim 2: Are these specific elements dependent on oxytocin signaling? To test the hypothesisthat oxytocin promotes maternal pup retrieval by strengthening the A1 response to uniquespectral/temporal features of isolation USVs, I will observe the changes in A1 activity as a resultof changes to endogenous oxytocin systems. Using transgenic Oxy-Cre mice will allow targetedexpression of light-sensitive opsins in oxytocin-releasing neurons, the activity of which can bemanipulated with light of specific frequencies. I will express channelrhodopsin-2, which is ableto activate neurons in response to blue light, in pup-naïve virgin female mice. These mice will beexposed to pup isolation USVs concurrently with optogenetic stimulation of oxytocin-releasingneurons. By continuously pairing isolation USV audio with stimulation of endogenous oxytocinover a number of days, I hypothesize that the A1 activity of the naïve female will change tomimic the strong, temporally-precise response observed in dams.Intellectual Merit & Broader Impacts: With the help of the NSF GRFP, I will be the first toidentify the specific acoustic features, over both spectral and temporal domains, which areunique to pup isolation USVs when compared to other mouse vocalizations. By identifying theA1 single-unit activity displayed in response to isolation USVs, and by identifying the changesin activity when crucial isolation USV features are eradicated from the stimulus, I aim to observethe specific activity patterns recruited by A1 in attributing behavioral relevance to infant-relatedsounds. By observing the changes in neural activity induced in pup-naïve virgins afteroptogenetic stimulation of oxytocin, I will be able to observe the unique form ofneuromodulatory plasticity evoked by oxytocin in A1 which allows experience-dependentlearning of maternal pup retrieval behavior. By examining maternal auditory processing in suchdepth, the field can better understand the interplay between auditory input and oxytocin to yieldbehavioral output.References: 1. Ehret (2005) Infant rodent ultrasounds – a gate to the understanding of sound communication. BehavGenet 35:19. 2. Marlin et al. (2015) Oxytocin enables maternal behaviour by balancing cortical inhibition. NatureKatherine Furman – Graduate Research Statement520:499 3. Arnal et al. (2015) Human screams occupy a privileged niche in the communication soundscape. CurrBiol 25:2051."
79.0,"An investigation of thermal effects on Anax junius nymph growthBackground:Earth’s climate is changing, and scientists are already observing impacts on biota acrossmany taxonomic groups.1 Odonata are known to be useful biological indicators of environmentalchange at a macro-scale, including climate change.2 Odonata are highly temperature-sensitive,with direct effects on their physiology (e.g., developmental rate) and other life-history traits (e.g.,phenology).1 Additionally, distributional and phenological records for Odonata are extensive, sothey are excellent model organisms for studying the impacts of climate change on animaldistributions, life history strategies, and development. Finally, the fossil record and historic datashow that Odonata have survived rapid and dramatic climatic transitions in the past. However,present-day rates of climate change are substantially greater due to anthropogenic causes.3Historically, Odonata have proven to be resilient and adaptable, but their current response isunknown. In sum, Odonata are considered sentinels of climate change, and there is growinginterest in examining changes in their phenology and physiology as the climate warms.Odonata have been closely researched in the field. Studies using Odonates in appliedresearch areas, such as climate change, are beginning to gain attention, but overall, research inthis area is lacking.4 I believe that Odonata would be an exceptional research subject to help usunderstand how freshwater organisms are responding to warming temperatures. Shifts in airtemperatures will influence lentic water temperatures through convection and by changingevaporation rates.5 Odonata are likely to reflect the mismatches between water and airtemperatures due to climate change, demonstrating a potential temporal decoupling betweenaquatic and terrestrial species.5 Understanding this response in Odonates is particularlyimportant, because they play an important role in structuring food webs, especially in fishlessponds which harbor unique biodiversity among macroinvertebrates, and are quite numerousacross the landscape in many glaciated regions.6Research Proposal:I propose an investigation to examine the effects of warming temperatures on larvaldragonflies using the species Anax junius (the common green darner dragonfly, Order: Odonata,Family: Aeshnidae), in laboratory experiments. Although lab and field observations oftemperature effects on larval Odonate development have been done independently, this proposedstudy will allow for a comparison between the lab experiment and data collected in the field inorder to see if the lab findings in the lab hold up in real ecosystems. This proposal aims to 1)determine a range of temperatures that allow for optimal growth conditions for A. junius nymphs,and 2) compare the laboratory results to water temperature and A. junius emergence timingobserved in the field by students and citizen scientists in order to better understand the effectsthat climate change has on aquatic ecosystems.Preliminary Work:For the past three summers, I have worked with my mentor Dr. Emily Schilling atAugsburg University on projects studying dragonflies from the family Aeshnidae. Our studieshave focused on species showing evidence of modified life history strategies as an adaptiveresponse to climate change. Through this research, we have developed relatively simple, costeffective and trustworthy sampling methods for nymphs, exuviae, and adults, that can easily bereplicated by others.Methods and Materials:Aim 1) For this study, I have selected Anax junius, because this species is commonthroughout North America and adults are easily identified in the field (as opposed to otherAeshnids), which need to be observed in hand for species identification. Additionally, A. juniusis known to be migratory, meaning that this species’ distribution covers a large geographic area.For the laboratory component of my study, I will set up fifteen 20-gallon tanks (30”x12”x12”),each containing ten A. junius nymphs. Each tank will have a heater to regulate the temperatureand a HOBO Dissolved Oxygen Data Logger to monitor the dissolved oxygen differencesamongst the temperature treatments. Tanks will be supplied with emergence supports andcovered with mesh to capture emerging adults. There will be five temperature treatments (10°C,15°C, 20°C, 25°C, and 30°C), with three replicates of each. Nymphs will be measured for theirhead-width-to-wing-sheath ratio three times per week in order to monitor growth, the number ofdays it takes each nymph to emerge will also be recorded. All molts, deaths, and emergences willbe documented each measuring period.Aim 2) In order to get students and the general public more involved in STEM, I amgoing to enlist the help of volunteer scientists to broaden the scope of my data set. I will do thisby contacting high schools and universities with NSF grants, and by posting ads on social media.I will also contact the Dragonfly Society of the Americas to enroll citizen scientists. For therecruited volunteers, I will let them choose a pond to sample and send them a temperature loggerthat continuously records water temperature, dip-nets for sampling dragonfly nymphs, and aguide on nymph and adult identification for A. junius. Lastly, I will create a web page wherevolunteers can easily upload their data and observations from their field sites. By using citizenscientists, I will be able to sample a larger geographic area than would be possible on my own,and collect data from multiple regions simultaneously.Broader Impact:By examining the data I receive from citizen scientists around the country, I will be ableto gain insight as to which regions in North America are seeing the most dramatic changes inwater temperature and gain a better understanding of the biological response to thisenvironmental change. It is important to determine regions of concern so conservation planningcan be prioritized in those areas. All humans and a large proportion of earth’s biodiversityrequire fresh water to survive. That is why research focusing on freshwater ecosystems isessential. Since climate change is a global issue, it is important to involve people in climate-related research that can ultimately inform how we protect freshwater ecosystems, arguably ourmost precious ecological resource. By engaging students and citizens in science, by allowingthem to be a part of the data collection process, I hope to get more individuals interested inhelping preserve and conserve the limited resources we have on Earth for generations to come.References:[1] Hassall, C., D. J. Thompson. 2008. The impacts of environmental warming on Odonata: areview. International Journal of Odonatology 11(2): 131-153. [2] Bried, J. T., C. Hassall, J. P.Simaika, J. D. Corser, J. Ware. 2015. Directions in dragonfly applied ecology and conservationscience. Freshwater Science 34(3): 1020-1022. [3] Pritchard, G. & M. Leggott. 1987.Temperature, incubation rates and the origins of dragonflies. Advances in Odonatology 3: 121-126. [4] Bried, J. T., M. J. Samways. 2015. A review of odonatology in freshwater appliedecology and conservation science. Freshwater Science 34(3):1023-1031. [5] Matthews, J. H.2010. Anthropogenic climate change impacts on ponds: a thermal mass perspective. BioRisk5:193-209. [6] Schilling, E.G, C. S. Loftin, A. D. Huryn. 2009. Macroinvertebrates as indicatorsof fish absence in naturally fishless lakes. Freshwater Biology 54(1):181-202."
80.0,"Regardless of signs of recovery from the economic crisis of 2007-2008, economic issuesremain salient, and many Americans continue to experience stress due to their economicsituation. Economic stress consists of both subjective and objective evaluation of one’sfinancial and employment-related stress1. My research will focus on four forms of economicstress: income inadequacy, financial fragility, underemployment, and job insecurity. Financialstress occurs when individuals perceive their personal financial situation to be insufficient toafford their needs and wants (perceived income adequacy) and are unable to cope withunexpected expenses (financial fragility), consequently leading to financial strain. While theunemployment rate in the United States is relatively low (below 5%), many workers are stillexperiencing employment-related stress due to underemployment and job insecurity.Underemployed workers hold jobs that insufficiently use their skills, abilities, education, orqualifications and may also receive less hours and pay than desired. Job insecurity is anindividual’s subjective evaluation of the “perceived threat to the continuity and stability ofemployment as it is currently experienced”3. Employee perceptions of job insecurity have beenempirically shown to increase stress, reduce mental, physical, and work-related wellbeing, andpredict organizational commitment and turnover intention at work2-6. Perceived job insecuritymay induce more stress than actual job loss or unemployment because the anticipation of job lossmay prevent coping strategies to manage the stress. Along with unemployment, job insecurityhas been a popular focus of economic stress research. However, there has been lessconcentration given to occupational health impacts of broader financial issues (e.g. 19).Economic stressors linked to the Great Recession, such as job insecurity, are associatedwith increased somatic symptoms experienced by individuals and a greater likelihood of alcoholabuse as a potential coping mechanism7. Relationships have also been found between perceivedjob insecurity of men and psychotropic drug use8. Additionally, empirical evidence suggests thatillicit drug use may be a coping strategy for recessions and unemployment9. To my knowledge,research has not examined opioid use as a mediator of the relationships between economicstress and occupational health.For decades, the United States has struggled with an opioid epidemic. Opiates are drugsderived from the opium poppy plant that chemically interact with opioid receptors on nerve cellsin the brain and in the nervous system to produce pain relief and pleasurable effects10. Opioidoverdoses are driven by synthetic opioids (i.e. fentanyl), semi-synthetic opioids (i.e. oxycodone),and heroin and have led to four times more of American opioid-related deaths in 2015 comparedto 199911-12. Long-term opioid use frequently starts with the treatment of acute pain13. Prescribedopioid pain relievers are the prescription drugs most often misused, resulting in a 72.2% increasein deaths due to synthetic opioids – not including illegally produced fentanyl – from 2014 to201514. The number of opioid prescriptions written in 2012 would have been enough for everyadult in the United States to have one bottle of opioid painkillers11. This longstanding problemhas led to many societal consequences. The overuse and misuse of opioid substances costs theUnited States 80 billion dollars each year in healthcare, criminal justice, and productivity costs15.While the consequences of opioid use have been well-researched in other social domains16-17, the relationships between opioid use and the workplace have received less attention.Current research shows that opioid use is related to absenteeism and work productivity18. Myresearch will bridge this important gap in the literature on economic stressors, opioid use, andwork-related outcomes.Gwendolyn Watson NSF Research ProposalBroadly speaking, my goal is to establish a stream of research that will answer thefollowing questions: (1) What is the nature of the relationship between multiple economicstressors (job insecurity, fragility, underemployment, and income adequacy) and opioiduse? (2) What are the causal mechanisms linking opioid use and economic stress? (3) Howdoes opioid use relate to occupational health outcomes? Particularly I am interested in theimpact on employee work engagement such as organizational commitment and turnoverintentions.Opioid UseWork engagementEconomic StressorsRetentionOrganizationalJob insecurity CommitmentFinancial fragility Occupational Health Outcomes PerformanceUnderemployment Other outcomesIncome inadequacyThis project will be a short-term investment with long-term goals. My short-term goalsare to focus on the first two questions to establish the relationship between economic stressorsand opioid use, as well as to identify causal factors to help explain the relationship. I will beanalyzing a series of archival datasets and in-process data collection to test these relationships.Currently, my advisor Dr. Bob Sinclair is administering a longitudinal study online throughAmazon’s Mechanical Turk (MTurk) that includes the variables of interest for my research(economic stressors, opioid use, and multiple occupational health outcomes). The first wave ofthis data collection is complete with over 700 participants. I also have access to multipleadditional sources of survey data including other unpublished MTurk data sets, studies of retailemployees, nurses, and larger population surveys. To test these initial relationships, I willconduct Multiple Regression Analyses and Structural Equational Modeling as applicable to eachdataset. Having access to many existing and in-progress datasets will facilitate my short-termproductivity and provide me with experience so that I can collect my own data in the futureMy long-term goals are to continue expanding on economic stress and opioid use to linktheir relationship to organizational and occupational health outcomes. I would like to furtherexplore the workplace implications and potential interventions if opioid use is found to be acoping mechanism for managing both employment-related and financial economic stress. Mycurrent resources for data sets and collection will allow me to pursue my research interests andbecome a scholar in this area of this research.The NSF Graduate Research Fellowship will help me establish a program of work thatwill advance knowledge and have broader social impact. The proposed program of work willadvance knowledge by bridging gaps in the literature concerning the causal links amongeconomic stressors, opioid use, and work-related outcomes. In the long run, I hope to establish aprogram of multidisciplinary collaborative research connecting my work in appliedpsychology with scholarship in economics, public health, and business. Understanding theintricacies of the relationships between the variables of interest will have broader social impactas we will be able to identify potential antecedents of opioid use to create social ororganizational interventions. This knowledge will help organizations better understand how toaddress issues of economic stressors and opioid use and ultimately mitigate negativeoccupational health outcomes.REFERENCES: [1] Voydanoff, P. (1990). Journal of Marriage and the Family.[2] Probst, T. (2004). Sage Publications Inc. [3] Shoss, M. (2017). Journal of Management [4] Cheng, G. H.-L., & Chan, D. K. S.(2008). Applied Psychology: An International Review [5] De Witte, H., Pienaar, J., & De Cuyper, N. (2016). Australian Psychologist [6] Probst, T. M., Stewart, S. M., Gruys, M. L., & Tierney, B. W. (2007).Journal of Occupational and Organizational Psychology [7] Vijayasiri, G., Richman, J. A., & Rospenda, K. M. (2012). Addictive Behaviors [8] Lasalle, M., Chastang, J. F., & Niedhammer, I. (2015). Journal ofPsychiatric Research [9] Nagelhout, G. E., Hummel, K., de Goeij, M., de Vries, H., Kaner, E., & Lemmens, P. (2017). International Journal of Drug Policy [10] American Society of Addiction Medicine. (2016).[11] Heavey, S. C., Chang, Y., Vest, B. M., Collins, R. L., Wieczorek, W., & Homish, G. G. (2018). International Journal Of Drug Policy [12] Sarpatwari, A., Sinha, M., & Kesselhei, A. (2017). Harvard Law andPolicy Review [13] Shah, A., Hayes, C. J., & Martin, B. C. (2017). MMWR: Morbidity & Mortality Weekly Report [14] Rudd, R., Seth, P., David., F., & Scholl, L. (2016). MMWR: Morbidity & Mortality WeeklyReport, 65(50): 1445-1452. [15] Florence, C.S., Zhou, C., Luo, F., & Xu, L. (2016). Medical Care [16] Centers for Disease Control and Prevention (2014). [17] Zibbell, J. E., Asher, A. K., Patel, R. C., Kupronis,B., Iqbal, K., Ward, J. W., & Holtzman, D. (2018). American Journal of Public Health [18] van Hasselt, M., Keyes, V., Bray, J., & Miller, T. (2015) Journal of Workplace Behavioral Health. [19] Leana, C., & J.Meuris. (2015). The Academy of Management."
81.0,"Investigating morphological variation in SiphonophoresBackground & Proposal: Historically, Siphonophores have been mistaken for jellyfish due totheir transparent bodies, long tentacles, and stinging nematocysts [1], [2]. Like many othercnidarians (e.g., corals), Siphonophores are colonial animals and are made up of multiple animalbodies, calledzooids, which arise from the same embryoand function together as one organism.Within the Siphonophore, zooid types are arranged in a specific pattern, which is repeated acrossthe organism and determined at the growth zones of the Siphonophore [4]. Siphonophores have ahigh degree of functional specialization and precise organization within the colony, which setsthem apart from most other animal species [3]. Though Siphonophores are a diverse group, welack an understanding of how the organizational pattern of zooid type differs across species, andto what degree this morphological variation of patterns is conserved. Understanding theconservation of pattern type, will inform us of the functional specialization structures that areindicative of their survival. To answer these questions, I will use geometric morphometricmethods to compare differences among Siphonophore species. This approach builds on recentwork done in the Casey Dunn lab at Yale, draws directly from myexperience in Dr. DeanAdams’s lab,and is motivated by my own interest in complex trait evolution. Last year, theDunn lab published a transcriptome-based Siphonophore phylogeny and used it to reconstruct theevolutionary history of changes in Siphonophore sexual systems, life history traits, habitats, andzooid types. In their comparisons of zooid type, Munro et al. [1] used only the binarycharacterization of presence/absence of each zooid type, making this study void of any zooidorganizational pattern classification. Previous studies have also suggested that organizationalpatterns of zooid type are species-specific [5].These organizational patterns have never beenexamined from a phylogenetic perspective. I am interestedin extending the work done in Dr.Dunn’s lab by quantifying morphological variationof zooid types to determine their evolutionaryhistory and organizational pattern within the Siphonophore colony.Understanding the evolutionof zooid types is key to unraveling the mechanisms behind coloniality and functionalspecialization. Broadly, this study will improve our understanding of complex traits innon-model organisms from which we lack critical information about their basic biology. Theaim of my study is to determine the evolution of organizational patterns and variation ofzooid specialization in Siphonophores by applying novel methods to quantifythree-dimensional data.To quantify the morphologicalvariation of zooid type beyondpresence/absence descriptions, I will usemicro-computerized tomography (CT) scans tocharacterize organizational patterns. This project willanalyze traits of Siphonophores that is currentlynotunderstood within the scientific community.Methods:I will collect at least three specimens foreach of the 33 species analyzed in the phylogenyproduced by Munro et al [1] to quantify zooidmorphology. Specimens will be collected via bluewater SCUBA diving or remotely operated vehiclesfrom the Monterey Bay Aquarium Research Institute.Collected samples will be stored and preserved insolutions of formaldehyde, as standard procedure [6]. In the Dunn lab at Yale, I will stainsamples using osmium tetroxide to enhance thevisualization of body structures and then use themicro-computerized tomography scanner available on site to scan collected specimens. Using CTscans, I will obtain images of x-rays for every species. To quantify zooid structures from thesescans, I will develop a novel landmark scheme appropriate for use in Siphonophores and obtainthese data using the programAvizo™ (Fig. 1). I will then perform multivariate computationalanalysis for these landmarks using thegeomorph package[7] in R [8]. To test for correlationsbetween zooid morphology and phylogenetic history, I will perform a phylogenetic regressionfor Procrustes shape variables, which will identify patterns of zooid shape variation across theSiphonophore phylogeny. I will then determine the rate of evolution for zooid shape andorganizational pattern by performing morphological disparity tests. These results will indicatethe tempo and mode by which these morphological structures have evolved and the degree ofconservation across species.Feasibility: In the Dunn Lab, I will work with experts in evolutionary and Siphonophorebiology. At Yale, having access to the largest Siphonophore collection to date would allow me toassess all preserved specimens to incorporate into my research. My past research experience inpreparing and maintaining museum specimens, as well as operating and analyzing data from aCT scanner will allow me to successfully complete this project. In the Casey Dunn Lab, I willapply methods from my work with Dr. Dean Adams’s lab including geometric morphometrics,biostatistics, and phylogenetics to complete this project.Intellectual Merit: The collections from this study will illuminate our understanding of thediversity across the Siphonophore phylogeny. It will also aid in the development of newtechniques to maintain and preserve non-model specimens for theYale Peabody Museum. I willuse morphological data to reveal how Siphonophore phenotypes have dispersed throughout theirevolutionary history. Applying computational approaches to compare morphology has been anongoing limitation for research in evolutionary biology. This study will further develop theseapproaches by using a novel combination of techniques such as multivariate analyses and CTscanning. Major outcomes of this study will be the identification of species-specificorganizational patterns, as well as a greater understanding of phenotypic plasticity of zooid types.These results will inform biologists on the evolution of coloniality, functional specialization, andthe morphological specificity of zooids. All CT scans, specimens, and codes from these analyseswill be openly accessible to scientists via data sharing platforms.Broader Impacts: Throughout my dissertation, I willparticipate in public outreach and theeducation of young scholars in science by giving a series of presentations about my experienceas a scientist, research methods, and results at theYale Peabody Museum. At Yale, I will continueto participate in the Society for the Advancement of Chicanos/Hispanics and Native Americansin Science and begin working with Pathways to Science programs to help low-income,first-generation, and underrepresented students pursuing science. I plan to engage students inthese programs and the public by using the unique context of museums. I will work jointly withcurators to help build interactive exhibits by providing field video blogs, preserved specimens,and topics that expand upon the direct implications of my work into more generally societallyrelevant fields such as declining biodiversity and global climate change. Importantly, theseproposed exhibits not only give a diverse public face to scientists, but also use the museum tohelp spark scientific curiosity in the public.References:1) Munro et al.Molecular Phylogeneticsand Evolution127(2018):823-833. 2) Cooke et alClinicalToxicology3(1970):589-595. 3) Mackie, G.O.LowerMetazoa(1963)329-337. 4) Goetz FE.Nanomia bijugawholeanimal and growth zones from http://commons.wikimedia.org/wiki (2018). 5) Dunn, C.W., Wagner, G.P.,DevGenes Evol216(2006):743-754 6) Holst et al.Journalof Plankton Research38(2016):1225-1242. 7) Adams et al.Geomorph(2018) R package version 3.0.6 8) R CoreTeamR(2013)."
82.0,"Rationale: Coral reefs provide services totaling $10 trillion despite covering only ~0.3% of theocean floor1. Their evolutionary success relies on the association between coral animals andsymbiotic algae. Corals provide shelter and nutrients for symbionts which in turn supply sugarsand O to their hosts2. Corals host symbionts within the symbiosome, an intracellular space2defined by the coral-derived symbiosome membrane. This membrane is thought to allow coralsto regulate delivery of nutrients to the symbiont but the specific transport mechanisms are mostlyunknown. Alarmingly, human-caused ocean warming, acidification, and eutrophication disruptthis symbiosis leading to the expulsion of symbionts (known as ‘bleaching’), decreased coralfitness, and death2. However, the lack of mechanistic knowledge of healthy symbiosis impairsour ability to understand why bleaching occurs, identify resilient and vulnerable species, anddesign conservation strategies. The mechanisms that deliver nitrogenous molecules (N ) tomsymbionts are particularly important. Healthy corals must provide symbionts with enough N formthe repair of photosystem proteins and other basic functions; however, excess N could result inmsymbiont overgrowth and bleaching2. Thus, corals must possess yet unidentified mechanisms toregulate N delivery to symbionts.mI propose to study the mechanisms controlling N delivery to symbionts and tomcharacterize responses to environmental stress in two coral species with differential susceptibilityto eutrophication. In other animal models, NH moves across membranes via Rhesus channels3(Rh). When paired with an acidification pathway, NH gas combines with H+ to form NH + that3 4is trapped on the other side of a membrane3. Coral Rh is an ideal candidate for transporting NH3across the symbiosome membrane for N delivery for three reasons: (1) an “Rh-like” gene ismupregulated in anemones upon symbiont acquisition4, (2) corals acidify the symbiosome usingV-H+-ATPases, which would favor NH + trapping in the symbiosome5, and (3) NH + is4 4symbionts’ preferred N source6. I hypothesize that (1) corals supply N to symbionts via Rh inm mthe symbiosome membrane, (2) N supply is controlled via transcriptional and translational Rhmregulation and changes in Rh localization, and (3) future ocean conditions can bypass the Rhpathway resulting in bleaching. Preliminary Results: I cloned the first coral Rh from Acroporayongei (ayRh) (MH025799), developed anti-ayRh antibodies, and confirmed ayRh proteinexpression via Western Blots. I found that ayRh is more abundant in the symbiosome membraneduring daytime compared to the night via immunofluorescence microscopy (IFM) (Fig 1).Aim 1: Establish ayRh Transport and Function. In vitro: Iwill express recombinant ayRh in Xenopus oocytes andmeasure its transport kinetics. Oocytes injected with ayRhcRNA or scrambled cRNA (controls) will be incubated withthe radiolabeled NH /NH + analog [14C]methylammonium,3 4and uptake rates will be measured with a gamma counter.Since some vertebrate Rhs can also transport CO 7, I will2determine ayRh CO permeability by measuring CO -induced2 2changes in oocyte pH with the pH-sensitive dye SNARF1. Iwill run statistics in R and Prism™. I predict that ayRh isNH - and not CO -permeable, supporting my hypothesis of3 2Figure 1. Diel Rh localization to the Rh-mediated N m delivery. If ayRh transports both, I willSM (n = 50) (p = 0.0246*). adjust my hypothesis and explore the role of Rh in providingcarbon and N for symbiont photosynthesis and metabolism. In vivo: I will explore themcorrelation between Rh abundance and N transport rate in isolated coral cells hostingmsymbionts5. I will measure Rh abundance by Western Blot and NH + uptake rates from seawater4using spectrophotometry. I predict a direct relationship between Rh abundance and capacity forN transport. All materials are already available in my collaborating and host labs.mAim 2: Characterize Coral Rh Regulation. I will expand on my preliminary results (Fig.1) toidentify mechanisms that regulate Rh abundance in the symbiosome membrane. In addition to A.yongei, I will work with Stylophora pistillata, which is more resilient to N eutrophication8. Thismcomparative approach may unveil species-specific mechanisms that confer resilience in pollutedoceans. Transcriptional and translational Rh regulation will be tested using qPCR and Westernblotting in coral samples taken during day and night timepoints. Rh’s subcellular localization anddynamics will be assessed in unprecedented detail via IFM on a super-resolution confocalmicroscope. Building on preliminary experiments, I will sample every three hours over a two-day period. Furthermore, I will use the highly specific photosynthesis inhibitor DCMU todetermine if the presence of Rh in the symbiosome membrane depends on photosyntheticactivity or simply on the presence of light2. I will automate IFM data collection and quantitativeanalysis with ZENTM software; I will use my coding experience to create custom workflows toachieve high throughput and bias reduction during analysis. I will run statistics in R andPrism™. I predict the Rh pathway is present in both coral species, that Rh trafficking to andaway from the symbiosome membrane depends on photosynthetic activity, and that Rh mRNAand protein abundance will remain relatively constant reflecting basal turnover rates.Aim 3: Establish Rh Responses to Stress. To determine the effects of future ocean conditionson Rh, I will grow A. yongei and S. pistillata in three conditions: (1) control, (2) elevated N (10mμM NH Cl), and (3) elevated N and CO (10 μM NH Cl, 1000 μatm CO ). I will collect4 m 2 4 2samples at 12:00 and 24:00 daily over a 70-day period (10 days of control, 30 days of treatment,and 30 days of recovery in control conditions) and rapidly analyze Rh expression and subcellularlocalization as described above; this method will also allow me to quantify symbiont density toestimate bleaching. Additionally, I will study symbionts’ photobiology using respirometry andPAM fluorometry and genotype symbionts to explore potential effects of symbiont strain. I willrun statistics in R and Prism™. I predict the Rh pathway will be initially downregulated in bothexperimental treatments. I also predict that corals in elevated N and CO conditions willm 2undergo the highest degree of bleaching due to larger loss of host control over symbiont growth;these effects will be more pronounced in eutrophication-sensitive A. yongei. Finally, I predict theRh pathway will gradually return to normal during recovery and reestablishment of symbiosis.Intellectual Merit/Broader impacts: This study will characterize a novel N transportmmechanism in coral symbioses and develop much-needed biomarkers to evaluate species-specificvulnerability to environmental stress and early detection of bleaching. It also has the potential toreshape our understanding of coral symbioses by establishing a novel diel regulatory mechanismthat traffics proteins to and from the symbiosome membrane. I am well qualified to conduct thisresearch based on my experience with IFM, molecular biology, coral biology, and computerscience. In my PhD, I will continue to mentor undergraduates through my tutoring program,many of whom are Latina females, and I will expand my program to low income high schools.Results from my project will be presented to the scientific community through peer-reviewedpapers and conferences, and to the general public in youth activities, lectures, and exhibitsthrough Sally Ride Science and the Birch Aquarium (which hosts 450k visitors annually). Mycareer goal is to be an R1 professor and these activities will shape my future outreach andeducation programs. References: (1) Global Environ Change 2014, 26, 152-158. (2) Microbiol Mol Biol R,2012, 76, 229-261. (3) Transfus Clin Biol 2006, 13, 85-94. (4) G3-Genes Genom Genet 2014, 4, 277-295. (5) PNAS2015, 112, 607-612. (6) Mar Biol 1983, 167, 157-167. (7) Membranes 2017, 7, 61. (8) Mar Biol 2000, 19, 103-113."
83.0,"One of the most important unknowns in high-z extragalactic astronomy is how reionizationoccurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominatesthe intergalactic medium (IGM). HI attenuates radiation from early stellar populations,masking galaxies from detection. Understanding how and when reionization occurs can revealwhether or not these young galaxies provided the necessary ionizing radiation to completelyreionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the highredshift-space this implies, spectroscopic observations are limited as these galaxies are veryfaint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths.My background in NIR spectroscopy and observational astronomy has prepared me toassist in addressing this question. I propose using Lyα and CIII] to investigate theproperties and ionization state of young galaxies using ground- and space-basedtelescopes, the structure and distribution of HI in the IGM and the circumgalac-tic medium (CGM) of certain galaxies, and implications for the evolution of theneutral fraction of the IGM throughout the EoR. The individual points proposed willbe summarized as follows: (i) small scale testing and building of an analysis technique, (ii)distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. Byunderstanding more about the IGM during the reionization era and of the galaxies within it,we can further constrain the properties of current galaxy evolution and reionization models.Small scale testing & building of analysis technique: In the search for galaxies during theEoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric0surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UVbright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring theescape fraction of Lyα many studies have inferred an increasingly neutral fraction of theIGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emissionhundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent methoduses a complementary spectroscopic tracer not attenuated by HI, with the UV metal lineCIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5].0In my current work, I measure CIII] H-band emission of galaxies found via Lyα emissionusing Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) andattenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGMof these galaxies and surrounding IGM. From my previous work experience, I havedeveloped a proficiency in coding which enabled me to gain a close familiarity withthe MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fixbugs often encountered when working with incredibly faint emission lines and less commondithering patterns for standard star observations. I wrote code to optimally extract my 1Dspectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage ofIRAF and Python, I developed code that can track the photometric variability of my datafrom a frame-to-frame basis – important when working with faint emission lines.Distribution of galaxies and evolution of neutral fraction: Using the foundation built from myprevious work, I will build a statistical sample of galaxies during the last half ofthe reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escapefraction as a function of redshift. Using my optimized extraction technique to improveTaylor A. Hutchison 2measurements, I will use this dataset to constrain the offset between these galaxies’ systemicand attenuated redshifts. This work will significantly increase the sample of high-z galaxieswith both Lyα and CIII] measurements. In addition, it will provide a more significantcomparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses.A current complication for this project is the lack of a complete spectroscopic sample ofLAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines orthe [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to0NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes– it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in therange of NIRCam on JWST. As a first approach to resolving this, I will take the currentsample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using myoptimized extraction technique to improve measurements. This has already been attemptedfor some galaxies [4], providing useful lower limits for determining exposure times andpotential telescopes for future observations, including JWST. We are planning proposals forthe first JWST cycles for this work. I will then take advantage of the deep multi-wavelengthimaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I ampart of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with earlyindications that the proposal has so far been successful. Finally, as a scientific collaborator onan ERS JWST proposal, I will prepare for access to that data – understanding what spectraI will be looking for from running binary stellar population models (eg. BPASS), scaled tomatch expected bandpass magnitudes, through the JWST exposure time calculator.With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will beable to further constrain the amount of hard radiation emitted from these galaxies; as shownwith mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5].By tracking its evolution through the last half of the EoR, I can inform current reionizationmodels. Lastly, through gathering my sample I will map out the distribution of thesegalaxies, identifying whether galaxies with large escape fractions are tracing over-denseluminous regions, located within large ionized bubbles [3,8].Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space-based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of thefluxes of the CIII] doublet, when measurable, I can infer an estimate of the electrondensity of the gas in the CGM. This is closely linked to the metallicity of the CGM,which directly affects the velocity offset of Lyα emission. Not only do recent studies indicatethat a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, theyalso suggest (from mid-z analogs) a strong link between the profile of Lyα emission and theproperties of the gas within the CGM [5]. This can be incredibly important as some high-zgalaxies have been found to have more symmetric Lyα profiles, contrary to the archetypalasymmetric shape, thought to be indicative of high star formation and galactic winds [6].Understandingthe rateand distributionof reionization, including thefactors andprocessesresponsible for it, remains one of the most important unknowns in extragalactic astronomy.My work will aim to shed more light on this question, enabling more precise modeling ofthiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies.References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erbet al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004"
84.0,"One of the most important unknowns in high-z extragalactic astronomy is how reionizationoccurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominatesthe intergalactic medium (IGM). HI attenuates radiation from early stellar populations,masking galaxies from detection. Understanding how and when reionization occurs can revealwhether or not these young galaxies provided the necessary ionizing radiation to completelyreionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the highredshift-space this implies, spectroscopic observations are limited as these galaxies are veryfaint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths.My background in NIR spectroscopy and observational astronomy has prepared me toassist in addressing this question. I propose using Lyα and CIII] to investigate theproperties and ionization state of young galaxies using ground- and space-basedtelescopes, the structure and distribution of HI in the IGM and the circumgalac-tic medium (CGM) of certain galaxies, and implications for the evolution of theneutral fraction of the IGM throughout the EoR. The individual points proposed willbe summarized as follows: (i) small scale testing and building of an analysis technique, (ii)distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. Byunderstanding more about the IGM during the reionization era and of the galaxies within it,we can further constrain the properties of current galaxy evolution and reionization models.Small scale testing & building of analysis technique: In the search for galaxies during theEoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric0surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UVbright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring theescape fraction of Lyα many studies have inferred an increasingly neutral fraction of theIGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emissionhundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent methoduses a complementary spectroscopic tracer not attenuated by HI, with the UV metal lineCIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5].0In my current work, I measure CIII] H-band emission of galaxies found via Lyα emissionusing Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) andattenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGMof these galaxies and surrounding IGM. From my previous work experience, I havedeveloped a proficiency in coding which enabled me to gain a close familiarity withthe MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fixbugs often encountered when working with incredibly faint emission lines and less commondithering patterns for standard star observations. I wrote code to optimally extract my 1Dspectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage ofIRAF and Python, I developed code that can track the photometric variability of my datafrom a frame-to-frame basis – important when working with faint emission lines.Distribution of galaxies and evolution of neutral fraction: Using the foundation built from myprevious work, I will build a statistical sample of galaxies during the last half ofthe reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escapefraction as a function of redshift. Using my optimized extraction technique to improveTaylor A. Hutchison 2measurements, I will use this dataset to constrain the offset between these galaxies’ systemicand attenuated redshifts. This work will significantly increase the sample of high-z galaxieswith both Lyα and CIII] measurements. In addition, it will provide a more significantcomparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses.A current complication for this project is the lack of a complete spectroscopic sample ofLAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines orthe [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to0NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes– it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in therange of NIRCam on JWST. As a first approach to resolving this, I will take the currentsample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using myoptimized extraction technique to improve measurements. This has already been attemptedfor some galaxies [4], providing useful lower limits for determining exposure times andpotential telescopes for future observations, including JWST. We are planning proposals forthe first JWST cycles for this work. I will then take advantage of the deep multi-wavelengthimaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I ampart of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with earlyindications that the proposal has so far been successful. Finally, as a scientific collaborator onan ERS JWST proposal, I will prepare for access to that data – understanding what spectraI will be looking for from running binary stellar population models (eg. BPASS), scaled tomatch expected bandpass magnitudes, through the JWST exposure time calculator.With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will beable to further constrain the amount of hard radiation emitted from these galaxies; as shownwith mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5].By tracking its evolution through the last half of the EoR, I can inform current reionizationmodels. Lastly, through gathering my sample I will map out the distribution of thesegalaxies, identifying whether galaxies with large escape fractions are tracing over-denseluminous regions, located within large ionized bubbles [3,8].Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space-based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of thefluxes of the CIII] doublet, when measurable, I can infer an estimate of the electrondensity of the gas in the CGM. This is closely linked to the metallicity of the CGM,which directly affects the velocity offset of Lyα emission. Not only do recent studies indicatethat a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, theyalso suggest (from mid-z analogs) a strong link between the profile of Lyα emission and theproperties of the gas within the CGM [5]. This can be incredibly important as some high-zgalaxies have been found to have more symmetric Lyα profiles, contrary to the archetypalasymmetric shape, thought to be indicative of high star formation and galactic winds [6].Understandingthe rateand distributionof reionization, including thefactors andprocessesresponsible for it, remains one of the most important unknowns in extragalactic astronomy.My work will aim to shed more light on this question, enabling more precise modeling ofthiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies.References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erbet al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004"
85.0,"Introduction: Large carnivores are re-colonizing North America1,2 and parts of Europe3,following decades of systematic eradication4. The expansion of large carnivore populations iscreating novel and complex predator-prey interactions5. One well-known example is trophiccascades and associated declines in herbivore abundance6. Predator-prey interactions are amongthe most fundamental ecological processes and have been the focus of ecology since its origin6.They are integral processes that shape biological communities, affect coupled human-wildlifesystems, and drive conservation and management ecology7. Despite this, our understanding ofthe effect of predators on prey populations, especially in complex food-webs, is in its infancy5,7.Predator-prey theoretical and empirical research is dominated by single predator-singleprey systems like the Isle Royale wolf-moose system8. While useful, it is unclear if thesesimplified models are capable of predicting dynamics where multiple predators are interactingwith multiple prey species6,7. For example, the recent recovery of wolves, expansion of grizzlybear populations, and expanding range of mountain lions across the Western United States areincreasing the number and complexity of interactions between predator and prey species1,2,4.Mounting evidence that the growing number of interactions can cause previously unknownecological effects suggests that there is much left to be understood in multi-predator, multi-preyfood webs6,9,10. For example, these complex dynamics can spur changes in direct ecologicalinteractions, such as prey switching by predators in response to prey abundance9.A key conceptual way in which single predator-prey interactions differ from morerealistic, complex, multiple predator-prey food webs is the inclusion of competition in additionto direct predator-prey dynamics. For example, predator-prey dynamics can lead to indirectecological interactions, such as apparent competition, where one prey species supports predatorpopulations, thereby reducing alternative prey populations10. With various competitiveinteractions within a trophic level occurring, the complexity of competition must also beconsidered11. Ecology has long studied the tension between how the forces of predation andcompetition structure communities and population dynamics11. Unfortunately, the inherentcomplexity of such systems has often rendered purely statistical/empirical approaches limited intheir utility.Compared to laboratory studies and field experiments, mathematical models, such asmultiple predator-prey models (MPPMs) allow ecologists to study these dynamics12. Complexfood webs cannot be easily resolved with statistical/empirical approaches because of the largenumber of parameters to estimate and the scant data to do so with, as well as the challengespresented by some parameters and mechanisms that are impossible to estimate (e.g., carryingcapacity). MPPMs are also very powerful in evaluating the consequences of managementdecisions12. Commonly, natural resource agencies manage populations using independentmanagement strategies for each species; therefore they do not reflect the complexity of predator-prey population dynamics. By failing to incorporate food web interactions into speciesmanagement strategies and ignoring the role of multi-species predation and competition,agencies may be sub-optimally preserving and managing wildlife populations.I hypothesize that MPPMs which consider alternative interactions will explain empiricalsystems better than single-predator, single-prey models (SPPMs). I will address these majorquestions: a) Are MPPMs better at predicting population dynamics in real-world systems thanSPPMs? b) If so, are the main advantages of MPPMs in terms of predictive performance drivenby predation or inclusion of competitive interactions? c) What are the conditions (e.g.,environmental, stochastic) in which predation vs. competition drive food webs? With theseecological questions answered, I will finally address: d) How does management of one speciesaffect populations of other predators and prey within a food web?Research Approach: I will use wildlife agency-collected datasets from the Idaho Department ofFish and Game for predator and prey populations. Then, through funding from my NSF GRFPproposal, I will generalize my results to other high-profile multi-species predator-prey datasetsfrom Banff National Park, Yellowstone National Park, and Serengeti National Park, with thehelp of my Ph.D. supervisors who have connections to these 3 systems. First, I will gatherinformation about predator or prey population dynamics from previous studies to inform thestructure of my models12. Then, I will estimate functional and numerical responses for eachpredator-prey pair from across systems. I can then incorporate each predator and prey speciesinto a set of coupled equations, one for each species in the food web. If there are i predatorspecies and j prey species, the corresponding predator-prey equations can be written as such:𝑑𝑉 𝑑𝑃𝑗 𝑖[𝟏] = 𝑓(𝑉)−∑𝑓(𝑉,𝑃)−∑𝑓(𝑉) [𝟐] = ∑𝑓(𝑉,𝑃)−𝑓(𝑃)−∑𝑓(𝑃)𝑑𝑡 𝑗 𝑗 𝑖 𝑗 𝑑𝑡 𝑗 𝑖 𝑖 𝑖𝑖 𝑗 𝑗 𝑖where [1] describes the population growth rate for prey (1st term), reduced by the effects ofpredation (2nd term) and competitive interactions with other prey j (3rd term) and [2] representsthe population growth rate for predators (1st term), decremented by predator mortality (2nd term)and, when present, competition from other predators (3rd term). For example, V could represent1white-tailed deer, V elk, whereas P could represent wolves, P mountain lions, and so on. The2 1 2shape and dynamics of these functions, f (.), will be determined from field data.Intellectual Merit: I will address questions fundamental to predator-prey theory, and also morebroadly, the ecological theory about the role of competition vs. predation in driving populationdynamics. For example, I will investigate if functional and numerical responses, thought to beintegral to predator-prey theory7,9,13, are sufficient or even necessary to understand predator-preypopulation dynamics. By applying these models to a broad variety of ecosystems, I will identifygeneral properties that drive not only predator-prey systems, but other consumer-resourcerelationships14. Moreover, I will help natural resource agencies avoid mistakes stemming fromun-integrated management, which can be economically and ecologically costly12.Broader Impacts: Through an increased understanding of how management controls predator-prey population dynamics, wildlife agencies will be able to determine how human harveststrategies of one species will affect others in a food web. Additionally, I will work to establish anaccurate public image of large carnivores throughout local communities, and bridge the gapbetween ecologists-wildlife agencies-citizens. I will do so by giving talks at local high schools,writing articles for newspapers and online blogs, and partnering with local radio/TV programs,much of which I have done in the Falkland Islands (see Personal Statement). In sum, I envisionthat my work will develop ecological principles general enough to transcend ecosystems, butalso specific enough to assist management of the natural resources of local communities.Literature Cited: 1) Mech, L. (1995). Cons. Bio. 9(2):270-278. 2) LaRue, M. et al. (2012). J. Wild. Mgmt.76(6):1364-1369. 3) Chapron, G. (2014). Science. 346: 1517-1519. 4) Ripple, W. et al. (2011). Science. 343:151-162. 5) Berger, J. et al. (2001). Science. 291:1036-1039. 6) Shurin, J. et al. (2002). Ecol. Lett. 5(6): 785-791. 7)Abrams, P. & Ginzburg, L. (2000). TREE. 5(278): 535-541. 8) Messier, F. (1994). Ecol. 75(2):478-488. 9)Hebblewhite, M. (2013). Pop. Ecol. 55(4):511-522. 10) Holt, R. (1977). Theor.Pop.Bio. 12(2): 197-229. 11)Chesson & Kuang. (2008). Nature. 456: 235-238. 12) Serrouya, R. et al. (2015). Am. Natl. 185(5): 665-679. 13)Berryman, A. (1992). Ecol. 73(5):1530-1535. 14) Vucetich, J. et al. (2011). J. Anim. Ecol. 80(6):1236-1245."
86.0,"Keywords: coral reef; resilience; bleaching; climate change; ecosystem based managementIntroduction: Coral reefs shelter 25% of all marine species1 and provide food, protectcoastlines, and support economic opportunities for over 1 billion people worldwide.2 However,coral reefs face multiple threats. Chronic stressors, including overfishing, coastal development,pollution, and ocean acidification, slowly degrade coral reefs by undermining vital ecologicalprocesses, such as grazing by reef fish and coral growth.3 Coral reefs are also threatened by acutestressors, such as cyclones and coral bleaching events, that may severely damage or restructurecoral reef ecosystems.2,3 Climate change is predicted to compound these stressors and has alreadyincreased average ocean temperatures by 0.9°C globally.4 This increase has triggered three globalbleaching events to date, and most coral reefs are projected to face annual bleaching by the 2050s.4Adaptive and innovative management approaches are needed to ensure the longevity ofcoral reef communities in an era of global change.5 One potential approach supported by anemerging body of research3, 5-13 is resilience-based management (RBM).7 Under RBM, scientistsand coral reef managers use a variety of biotic and abiotic indicators to assess coral reef resilience,where resilience is defined as the capacity of an ecosystem to resist and recover from stress withoutshifting to a less desirable ecosystem state.6 However, several knowledge gaps related to coral reefresilience hinder application of RBM theory.5-12 Scientists have not reached consensus on whichbiotic and abiotic indicators are the strongest drivers of coral reef resilience.6, 8 Most assessmentsof coral reef resilience are predictive, and few studies exist that test the accuracy of thesepredictions following major stress events.6, 13 Furthermore, these assessments are conducted acrossa range of geographies using varying methodologies, 5-12 which makes it difficult determine ifobserved variability in resilience is due to context-specific factors or broad biogeographic patterns.In addition, assessments of coral reef resilience are still evolving to incorporate emerging scienceon ecosystem thresholds and phase shifts between coral- and algal-dominated states.3, 11, 13Proposed Research: The destructive 2014–2017 global bleaching event (Event), whichimpacted 51% of the world’s coral reefs,14 presents a unique opportunity to examine the impactsof major stress events on coral reef resilience. By evaluating changes in coral reef condition acrossan array of sites before, during, and after the Event, I will (1) assess the relative importance ofselect biotic and abiotic factors in determining coral reef resilience, (2) examine how theimportance of these factors varies spatially among reefs in the central Pacific, and (3) ground-truthexisting methods used to predict coral reef resilience.Experimental Design: An effective analysis of coral reef resistance to, and recovery fromthe Event will require extensive monitoring data that documents biotic and abiotic conditions oncoral reefs before, during, and after the Event. As a graduate researcher at the Scripps Institute ofOceanography (SIO), I will be well positioned to access comprehensive monitoring data from paststudies and collect data through current monitoring programs.To evaluate coral reef condition before the Event, I will analyze benthic and reef fishmonitoring data collected as part of an extensive SIO study that monitored coral reefs across 56islands in the central Pacific from 2002–2009.15 To assess coral reef resilience during and after theEvent, I will analyze photo surveys and fish transect data collected as part of the 100 IslandChallenge (Challenge), which is currently surveying 100 islands across a range of environmentaland anthropogenic gradients in the Pacific and Caribbean.16 Photo surveys collected as part of theChallenge are orthoprojected to generate comprehensive 3D models of the coral reef benthos thatdocument species composition and spatial arrangement to a resolution of 1cm2. The Challengesurveys sites (many of which were affected by the Event)14, 16 at regular intervals to documentecological changes, and has been monitoring numerous islands since 2013.I will select study sites by cross-referencing monitoring locations from the 2002–2009study15 with locations currently being monitored by the Challenge (I anticipate this will revealseveral dozen sites with sufficient monitoring data). I will evaluate the historic exposure of studysites to chronic stressors using the World Resources Institute’s Reefs at Risk spatial dataset ofanthropogenic impacts.9 Similarly, I will evaluate historic exposure to acute stressors before theEvent, and bleaching alert levels during the Event, using spatial data produced by NOAA’s CoralReef Watch. Based on this evaluation, I will classify sites into eight experimental groups (Fig. 1).I will build off existing meta-analyses of RBM studies6, 12 to identifypriority resilience indicators to measure, such as coral cover/diversity,herbivore biomass/diversity, coral recruitment, coral disease, macroalgaecover, bioerosion, substrate availability, and topographic complexity.3, 5-12I will monitor these indicators as a member of the Challenge research teamand extract relevant data from the 2002-2009 SIO monitoring dataset.15Using this data, I will generate site-level averages for each indicator beforethe Event to determine baseline conditions and analyze indicator varianceto assess pre-bleaching trends in ecosystem health. I will compare thesevalues to indicator averages and variance after the Event to identifyindicators with a statistically significant impact on coral reef resistance toand recovery from bleaching. I will also analyze spatial variability in siteresilience to detect regional or context specific patterns (i.e., whichindicators determine resilience and where), a knowledge gap prior studieshave not addressed. Lastly, I will examine areas of overlap and divergencebetween my findings and those of prior resilience assessments. Fig. 1: Sites will beAnticipated Results: I hypothesize that reef resilience will vary classed into 8 groupsbased on past exposure.depending on each site’s history of chronic and acute stress exposure. Inaddition, I anticipate that the accuracy of resilience predictions and the relative importance ofbiotic and abiotic drivers of resilience will exhibit spatial variability at regional and local scales.Intellectual Merit and Broader Impacts: This study will ground-truth RBM theory usingempirical data to analyze patterns of coral reef resilience before, during, and after a majorenvironmental disturbance. As a co-author of an RBM study,9 I recognize the potential of thisresearch to address knowledge gaps, and thereby enable scientists to further refine RBM methodsto reflect observed patterns of coral reef resilience. This continued refinement will be critical6 asreef managers, communities, and governments move to adopt RBM to inform marine protectedarea design, fisheries regulations, and other conservation measures7-10 that would have broadimpacts for millions of people who depend on coral reef ecosystems.2 In addition, the methodsoutlined in this study could be modified to test RBM assumptions for other ecosystems threatenedby climate change.7 As I conduct this research, I will use connections I have cultivated in themarine non-profit community by collaborating with researchers and practitioners, and draw frommy communications experience as an environmental blogger to widely disseminate my findings.References [1] Plaisance et al. 2011. PLoS one, 6(10): e25026. [2] Hoegh-Guldberg et al. 2007. Science,318.5857: 1737-1742. [3] Anthony et al. 2014. Glob Change Biol, 21: 48–61. [4] Van Hooidonk et al. 2016. ScientificReports, 6: 39666. [5] Nyström et al. 2008. Coral Reefs, 27: 795–809. [6] McClanahan et al. 2012. PloS one, 7.8:e42884. [7] Maynard et al. 2015. Biological Conserv, 192: 109-119. [8] Maynard et al. 2010. Coral Reefs, 29.2: 381-391. [9] Harris et al. 2017. Aquatic Conserv, 27.S1: 65-77. [10] Weeks & Jupiter. 2013. Conserv Biol, 27.6: 1234-1244. [11] Mumby et al. 2013. Conserv Letters, 7.3: 176-187. [12] Lam et al. 2017. PloS one, 12.2: e0172064. [13]Jouffray et al. 2014. Philosph Trans B, 370.1659: 20130268. [14] Eakin et al. 2017. Reef Encounter, 32(1): 33-38.[15] Smith et al. 2016. Proc R Soc B, 283(1822): 20151985. [17] 100islandchallenge.org/study-design/"
87.0,"One of the most important unknowns in high-z extragalactic astronomy is how reionizationoccurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominatesthe intergalactic medium (IGM). HI attenuates radiation from early stellar populations,masking galaxies from detection. Understanding how and when reionization occurs can revealwhether or not these young galaxies provided the necessary ionizing radiation to completelyreionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the highredshift-space this implies, spectroscopic observations are limited as these galaxies are veryfaint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths.My background in NIR spectroscopy and observational astronomy has prepared me toassist in addressing this question. I propose using Lyα and CIII] to investigate theproperties and ionization state of young galaxies using ground- and space-basedtelescopes, the structure and distribution of HI in the IGM and the circumgalac-tic medium (CGM) of certain galaxies, and implications for the evolution of theneutral fraction of the IGM throughout the EoR. The individual points proposed willbe summarized as follows: (i) small scale testing and building of an analysis technique, (ii)distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. Byunderstanding more about the IGM during the reionization era and of the galaxies within it,we can further constrain the properties of current galaxy evolution and reionization models.Small scale testing & building of analysis technique: In the search for galaxies during theEoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric0surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UVbright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring theescape fraction of Lyα many studies have inferred an increasingly neutral fraction of theIGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emissionhundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent methoduses a complementary spectroscopic tracer not attenuated by HI, with the UV metal lineCIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5].0In my current work, I measure CIII] H-band emission of galaxies found via Lyα emissionusing Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) andattenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGMof these galaxies and surrounding IGM. From my previous work experience, I havedeveloped a proficiency in coding which enabled me to gain a close familiarity withthe MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fixbugs often encountered when working with incredibly faint emission lines and less commondithering patterns for standard star observations. I wrote code to optimally extract my 1Dspectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage ofIRAF and Python, I developed code that can track the photometric variability of my datafrom a frame-to-frame basis – important when working with faint emission lines.Distribution of galaxies and evolution of neutral fraction: Using the foundation built from myprevious work, I will build a statistical sample of galaxies during the last half ofthe reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escapefraction as a function of redshift. Using my optimized extraction technique to improveTaylor A. Hutchison 2measurements, I will use this dataset to constrain the offset between these galaxies’ systemicand attenuated redshifts. This work will significantly increase the sample of high-z galaxieswith both Lyα and CIII] measurements. In addition, it will provide a more significantcomparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses.A current complication for this project is the lack of a complete spectroscopic sample ofLAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines orthe [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to0NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes– it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in therange of NIRCam on JWST. As a first approach to resolving this, I will take the currentsample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using myoptimized extraction technique to improve measurements. This has already been attemptedfor some galaxies [4], providing useful lower limits for determining exposure times andpotential telescopes for future observations, including JWST. We are planning proposals forthe first JWST cycles for this work. I will then take advantage of the deep multi-wavelengthimaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I ampart of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with earlyindications that the proposal has so far been successful. Finally, as a scientific collaborator onan ERS JWST proposal, I will prepare for access to that data – understanding what spectraI will be looking for from running binary stellar population models (eg. BPASS), scaled tomatch expected bandpass magnitudes, through the JWST exposure time calculator.With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will beable to further constrain the amount of hard radiation emitted from these galaxies; as shownwith mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5].By tracking its evolution through the last half of the EoR, I can inform current reionizationmodels. Lastly, through gathering my sample I will map out the distribution of thesegalaxies, identifying whether galaxies with large escape fractions are tracing over-denseluminous regions, located within large ionized bubbles [3,8].Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space-based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of thefluxes of the CIII] doublet, when measurable, I can infer an estimate of the electrondensity of the gas in the CGM. This is closely linked to the metallicity of the CGM,which directly affects the velocity offset of Lyα emission. Not only do recent studies indicatethat a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, theyalso suggest (from mid-z analogs) a strong link between the profile of Lyα emission and theproperties of the gas within the CGM [5]. This can be incredibly important as some high-zgalaxies have been found to have more symmetric Lyα profiles, contrary to the archetypalasymmetric shape, thought to be indicative of high star formation and galactic winds [6].Understandingthe rateand distributionof reionization, including thefactors andprocessesresponsible for it, remains one of the most important unknowns in extragalactic astronomy.My work will aim to shed more light on this question, enabling more precise modeling ofthiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies.References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erbet al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004"
88.0,"cues on relationship perception and intent biasesMen are more likely to perceive a woman’s friendliness as sexual interest, and thispattern holds up across surveys, actual behaviors, and beyond lab conditions [1-3]. Althoughcommunication about sexual interest have always been complicated, they recently have becomelegal and societal issues. A more complete understanding of how individuals communicate aboutsex is necessary, especially when 23.1% of college women experience sexual assault [4].Intellectual MeritError management theory (EMT) explains this “sexual overperception” effect in men as astrategic bias favoring specific types of judgment errors over other types of errors. Differentialparental investment theory [5] states that male mammals are less physically obligated to invest inoffspring, so they tend to be more willing to engage in sexual activity, whereas females are moreselective about potentially costly sexual activity. For males, the error of “missing” an interestedfemale is costlier than the “false alarm” error of judging an uninterested female as interested,resulting in a pattern of decisions that adaptively reduces costs and increases benefits, even as itfails to minimize errors overall.EMT is, at its core, Signal Detection Theory (SDT) applied to intersexual relationships[6], using differential parental investment to model the costs and benefits of relationshipdecisions. SDT is a way to describe how observers judge the presence or absence of a “signal”when the given stimuli have some level of ambiguity (“signal + noise”) [7]. The division ofEMT from SDT has resulted in an unnecessarily restricted analysis of data that couldpresent a fuller explanation of behavior if analyzed using signal detection models. AlthoughEMT, like SDT, considers different judgments and possible outcomes, it ignores severalextensions and implications which a full SDT analysis can provide. For instance, EMT does notconsider the base rates of signals compared to noise (that is, the frequencies with which signalsand noise occur in the environment), and how that influences judgments. Very common truesignals, with rare non-signals, will encourage signal-present judgments in ambiguous situations(known as a liberal bias). Conversely, a low signal rate and common non-signals will encourageno-signal judgments (known as a conservative bias). Additionally, SDT provides a measure(sensitivity) of how well people distinguish signals from noise.One benefit of this approach is that concepts already developed within SDT cantransfer to EMT contexts. At the theoretical level, SDT specifies situations in which both menand women should have systematically different signal detection strategy profiles. Individualswith faster (v. slower) life history strategies, more unrestricted (v. restricted) sociosexuality, andmore short-term (v. long-term) mating orientation should show more liberal biases (Hypotheses1-3). Similarly, people high in mate value should show a liberal bias because their experience isof a higher signal base rate (H 4), which EMT cannot predict as it does not take signal/noise ratiointo account. It also is possible to manipulate aspects of the social situation, and thus the value ofdecision outcomes, by manipulating the attractiveness of the stimuli used as signals (H 5) and bychanging the signals-to-noise base rates through exposure to different sex ratios of stimuli (H 6).Additionally, methods and analyses from SDT research can be used to more fullyunderstand and analyze existing EMT results. For preliminary results, I analyzed the data fromPerilloux, et al. [8] using SDT. This confirmed that men are more liberally biased in perceivingsexual interest, but also yielded unanticipated insights: Women are more sensitive to thedifference between sexual interest versus non-interest (d’ in Figure 1), and -surprisingly- bothmen and women in this study are conservatively biased in perceptions of sexual interest (c inFigure 1). Differential parental investment theory predicts why men have a lower sensitivity thanwomen, as females may conceal their signals of sexual interest, making it more difficult for mento differentiate signal from noise. This also may explain why men are more liberally biased thanwomen, since they need to compensate for their lower sensitivity to maintain the same level ofoptimality at detecting sexual interest. This difference in sensitivity led to an additional hypoth-esis that men’s sensitivity will increase as the woman’s sexual cues become more overt (H 7).Methodological Approach – My stimuli will include 96video clips showing heterosexual pairs engaging in conver-sations. Each videotaped person will rate their sexual interest intheir conversation partner, then complete questionnaires toevaluate individual differences (described above). Method:Studies will involve participants watching the muted clips andrating each actor regarding their levels of sexual interest in theirconversation partner. Multiple study variations will look atinfluences of the observers’ life history strategy (H 1), socio-sexual orientation (H 2), mating strategy (H 3), and mate value(H 4). Additionally, experimentally manipulated sets of clipswill be shown to evaluate the causal effects of skewed ratios ofattractiveness of each conversant (H 5) in the video clips, priorexposure of participants to skewed sex ratios (H 6), andexposure to overt sexual cueing (H 7). Analysis will use bothEMT and SDT methods, utilizing multilevel probit regressionto determine c and d’ for this repeated measures design. [9]Broader ImpactsUnderrepresented Minorities in STEM: Efforts will be madeto recruit underrepresented and first-generation undergraduatesas research assistants, who will be encouraged to learn about the research process, present resultsat conferences, and participate in authorship of publications. Increasing Scientific Literacy andPublic Engagement with STEM: Research about romantic relationships often gets publicmedia attention, which will be used to broadly communicate the results of this research and bringattention to current directions in psychological science. This research will also be presented atregional and national conferences. Improving Individual Well-Being: This SDT approach willincrease knowledge about the abilities and biases different people have about sexualcommunication, empowering individuals to make informed, healthy decisions about their sexualand relationship behaviors. Identification of individuals and situations where sexual interest andintents are often misinterpreted will aid in locating at-risk populations, improving sexual assaultprevention policies, and inhibiting interference with the right to receive an education free fromdiscrimination through sexual harassment and sexual violence (per Title IX of the EducationAmendments of 1972).References: [1] A. Abbey, J. Pers. Soc. Psychol.42, 830-838 (1982). [2] A. Abbey, Psychol. Women Quat 11, 173-194 (1987). [3] M. G. Haselton, J. Res. Pers. 37, 34-47 (2003). [4] The Association of American Universities,Report on the AAU Campus Climate Survey on Sexual Assault and Sexual Misconduct (Westat, Rockville, MD, ed.2, 2015). [5] R. L. Trivers in Sexual Selection and the Descent of Man, B. Campbell Ed. (Aldine, Chicago, IL.1972), 136–179. [6] D. Nettle in Evolution and the Mechanisms of Decision Making, P. Hammerstein, J. R. StevensEds. (MIT Press, Cambridge, MA, 2012), 69-79. [7] D. M. Green, J. A. Swets, Signal Detection Theory andPsychophysics, (Wiley, New York, NY, 1966. [8] C. Perilloux, J. A. Easton, D. M. Buss, Psychol. Sci. 23, 146-151(2012). [9] L. T. DeCarlo, Psychol. Methods 3, 186-205 (1998)."
89.0,"extreme hydrodynamic and aerodynamic loads on offshore wind turbines (OWTs),specifically wave and wind loads during hurricanes. To this end, I propose to numericallysimulate OWTs subjected to extreme wind and waves using computational fluid dynamics(CFD). This research aims to advance basic understanding of OWT loads by answeringthe questions:1. How do extreme hydrodynamic and aerodynamic loads on OWTs vary for differentsupport structures and hurricane characteristics?2. How do OWTs (especially floating OWTs) respond to hurricane wind and waves?Motivation: To meet the federal goal of 20% electricity from wind energy by 2030,the U.S. wind industry must expand to include offshore wind development. Offshore windoffers several advantages over onshore wind, including the mitigation of aesthetic and landuse issues, as well as the utilization of abundant, high-quality offshore wind resources inproximity to population centers [1]. However, wind farms off the eastern and southern U.S.coast could be destroyed by hurricanes, unless their support structures are designed withsuch extreme loads in mind [2]. These designs require accurate load data, butexperimental data is mostly unavailable due to the lack of OWTs in hurricane-prone areas.So, current OWT simulations find hydrodynamic and aerodynamic loads using simpleempirical models, which are much less accurate than CFD [3]. This inaccuracy iscatastrophic when designing OWTs to withstand hurricanes, so using CFD to betterpredict extreme loads will inform more robust OWT designs.Methods: To generate hydrodynamic and aerodynamic loads typical of hurricanes, Iwill numerically simulate OWTs subjected to extreme, hurricane-like wind and waves.These simulations will be done in the CFD software Converge from Convergent Science.Unlike most CFD software, Converge doesn’t require user-made meshes, which meansresearchers can complete simulations faster. Converge’s adaptive automatic meshing alsoimproves solution accuracy for moving objects like floating OWTs, since the gridresolution adapts where necessary. Converge could also fix stability issues found whenmodeling floating OWTs in other CFD software like OpenFOAM [4].I will first identify hurricane parameters characteristic of the U.S. coast, focusing onareas where OWT development is likely. I will then validate my predicted hydrodynamicand aerodynamic loads against experimental and numerical data: CFD-based numericaldata for non-extreme waves is available from Benitz [4], while proprietary experimentaldata for loads from Hurricane Irene is available from industry collaborators. Aerodynamicloads will be validated against the open literature for onshore wind turbines. Finally, I willcreate databases of hydrodynamic and aerodynamic loads corresponding to varioushurricane parameters for several support structure types.The predicted non-hurricane hydrodynamic loads will be validated for some OWTstructures prior to the beginning of the proposed project: the proposed team (detailedbelow) is currently collaborating on a 1-year project on OWTs in breaking waves, whichincludes validating the predicted hydrodynamic loads on non-floating structures againstnumerical data and experimental data from industry partners.Deliverables: The main deliverables of the project and their estimated times ofcompletion are:1. Identify representative hurricane characteristics for the U.S. coast (2 months),2. Validate aerodynamic loads against experimental data (3 months),3. Validate hydrodynamic loads against experimental and numerical data (5 months),4. Generate hydrodynamic and aerodynamic load databases for various hurricaneparameters for non-floating structures (12 months) and floating structures (14months).Collaborations: This work will involve collaboration between University ofMassachusetts faculty from two departments, as well as collaboration with industrypartners (Convergent Science and others in development). Dr. David Schmidt and Dr.Matt Lackner (Mechanical Engineering) have previously studied hydrodynamic loads onOWTs [3,4], and have access to the computing resources necessary for CFD. Dr. Schmidtalso worked in Converge on other applications, and his long relationship with ConvergentScience enables their collaboration and assistance in introducing hydrodynamics andaerodynamics as new applications. Dr. Sanjay Arwade (Civil Engineering) brings expertisein OWT support structures and hurricanes’ impacts on OWTs.Intellectual merit: The proposed project furthers basic scientific understanding ofOWT loads, introduces better software for OWT modeling, and provides better data forstructural models of OWTs. First, using CFD to study extreme hydrodynamic andaerodynamic loads on OWTs will improve fundamental understanding of how OWTsupport structures behave during hurricanes, which is currently hindered by overly simplemodels and a lack of experimental data. Second, this project will validate a faster, moreaccurate CFD software for OWTs and other ocean engineering applications. Third, thisresearch will provide more accurate loads used in OWT structural analysis, like that doneby civil engineers. These results will be distributed at wind energy and CFD conferences,in journal articles, and in my PhD dissertation.Broader impacts: By providing more accurate hydrodynamic and aerodynamichurricane loads for use in structural OWT models, the proposed project allows for betterdesigns of OWTs that can withstand hurricanes. Hurricane-resistant OWTs lower the risksof offshore wind, encouraging the widespread development of offshore wind energy in theU.S. and other hurricane-prone countries. In this way, the proposed research contributes tothe growth of renewable energy on the national and global scale.References1 Musial, W., and Ram, B. 2010. Large-scale offshore wind power in the United States:Assessment of opportunities and barriers. Technical Report NREL/TP-500-40745, U.S. Dept.of Energy, 1–221.2 Wei, K., Arwade, S.R., Myers, and A.T. 2014. Incremental wind-wave analysis of thestructural capacity of offshore wind turbine support structures under extreme loading.Engineering Structures 79, 58-69.3 Benitz, M.A., Lackner, M.A., and Schmidt, D.P. 2015. Hydrodynamics of offshore structureswith specific focus on wind energy applications. Renewable and Sustainable Energy Reviews44, 692-716.4 Benitz, M.A. 2016. Simulating the hydrodynamics of offshore floating wind turbine platformsin a finite volume framework. PhD thesis, University of Massachusetts - Amherst."
91.0,"In order to understand evolution, we need to understand the complex relationships betweenbiological systems and fitness. This is a difficult task, because there are an enormous number ofpossible genetic states, and the mutations underlying these states interact non-additively toproduce fitness. We can frame this problem by thinking of evolution as a process occurring on ahigh-dimensional map between this space of genetic possibilities and the fitness of eachpossibility, a function often referred to as the “fitness landscape.” As a population adapts to aparticular environment, it moves between neighboring genotypes, constrained by the force ofselection to follow paths of increasing fitness. By understanding the general properties of thefitness landscape, we can answer questions about the functional nature of a biological system - Ifa mutation knocks out this gene, what effect will that have on fitness? - and ask broad theoreticalquestions - Is evolution predictable, or does it depend on chance events?The growing field of experimental evolution provides an avenue for addressing thesequestions by empirically testing important features of the fitness landscapes of microbes. Wenow know that in the budding yeast Saccharomyces cerevisiae, the effect of a beneficialmutation depends on the fitness of the genetic background where it arises [1], but whether asimilar pattern holds for deleterious mutations is an open question. Deleterious mutations may becommon in populations due to environmental changes or population bottlenecks, and theyprovide a novel way to study adaptation and to test the role of contingency in evolution. In myPhD research, I will study the fitness landscape of S. cerevisiae by investigating theinterplay between deleterious mutations and adaptation. I will complete my PhD research inDr. Michael Desai’s lab at Harvard University, where I am uniquely situated to conduct workthat combines genetics, experimental evolution, and deep sequencing.Aim 1. Changes in the fitness effects of loss of function mutations over adaptive trajectoriesGiven that most loss of function (LOF) mutations are deleterious, competing models makedifferent predictions about how their effects should change with increasing population fitness.The Desai lab recently found that in S. cerevisiae, the fitness effect of a beneficial mutation in aparticular genetic background is primarily predicted by the fitness of the background, creating apattern of “diminishing returns” during adaptation [1]. If this “global epistasis” model holds forall mutations, deleterious mutations should also become less deleterious as the populationbecomes more fit. In contrast, Fisher’s geometric model predicts that the fitness effect of somemutations will change from negative to positive at different levels of adaptation [2]. I will usetransposon mutagenesis and sequencing fitness assays (Tn-seq) to measure the fitness effects of alarge set of loss of function mutations in populations with different initial fitness backgrounds.Hypothesis: I predict that, in accordance with the global epistasis model [1], LOF mutationswill be less deleterious in populations with higher fitness. Methods: First, I will evolve 24 S.cerevisiae populations in standard liquid media for 1000 generations (100days), following a similar protocol to [3]. I will freeze samples every 250generations to create a “frozen fossil record” of each population as itadapts and gains fitness. At each of the five timepoints in this record, Iwill unfreeze my populations and use Tn-seq to systematically probe thefitness effects of a large number of LOF mutations. As shown in thefigure at right, Tn-seq consists of two steps. In A, I transform a genedisruption library into the population, causing a diverse set of singleinsertion mutations. In B, I track the frequency of each mutation over 30generations using deep sequencing of a barcode region in the insertion[4]. Using this method, I can determine the fitness effect of every mutation in parallel byanalyzing the change in its frequency [3,4]. I will create my DNA-barcoded transposon (Tn)gene disruption library using genomic DNA from S. cerevisiae [3], and by sequencing thislibrary, I will associate a unique barcode with each gene disruption. These associations willallow me to connect my data to specific genes, yielding additional biologically relevantinformation about how S. cerevisiae adapts to laboratory conditions.Aim 2. Adaptation after disruption of the genetic systemEvolution often involves transient environmental changes that alter selection pressure orpopulation size, both of which can lead to the fixation of mutations that are not beneficial in theorganism’s primary environment. Do these events affect long-term outcomes of evolution? Thedynamics of adaptation after a population has been “bumped” off of its adaptive trajectory arenot well understood, but they have the potential to distinguish between models of adaptation. Forfitness landscape models in which mutations interact only additively, any deleterious mutationsimply slows adaptation. However, in “rugged” fitness landscape models where mutationsinteract non-additively, it is possible that deleterious steps can lead to exploration of a previouslyinaccessible part of genotype space, potentially allowing a population to ultimately reach higherfitness. I will capitalize on the Tn-seq method to distinguish between these models by evolving“disrupted” populations alongside “undisrupted” populations and comparing their fitnesstrajectories. Hypothesis: I hypothesize that deleterious mutations will be more likely toimprove evolutionary outcomes in poorly adapted populations, as predicted by [5].Therefore, I predict that disruption due to Tn insertions will lead to higher final populationfitness relative to the undisrupted populations only when the original disruption occurs atearly time points from the frozen fossil record. An alternate prediction is that disruption willslow adaptation in all cases, which would support additive landscape models. Methods: I willpropagate “Tn-disrupted” populations from Aim 1 for 500 generations. I will measure meanpopulation fitness every 100 generations in these populations and at the correspondingtimepoints in the “undisrupted” populations using standard fluorescence-based competitions [1].Intellectual MeritMy project aims to connect ideas about the dynamics of adaptation on fitness landscapes to afunctional understanding of how a model organism changes as it adapts. Using massivelyparallel, sequencing-based fitness assays, this project will provide unprecedented resolution ofthe functional changes a population experiences during adaptation, and through evolution of Tn-disrupted clones, this study will test basic questions about the fitness landscape of evolving S.cerevisiae.Broader ImpactsWe now know that large asexual populations, in the form of pathogens or cancer cells, areinvolved in over a quarter of deaths worldwide [4]. While my research is centered on basicscience questions, these basic principles of asexual adaptation are an important part of buildingmodels of how these diseases progress. I will publish my work in peer-reviewed journals aimedat a scientific audience, but I will also use the power of animations and interactivity to make myevolution research come alive on my web site, where it can be shared with the general public.As detailed in my personal statement, I will also use science communication and video toempower young people to pursue STEM careers by showing them the human side of research.[1] Kryazhimskiy S et al. 2014. Science 344: 1519-1522. [2] Fisher RA. 1930. Clarendon Press, Oxford, U.K.[3] Van Opijnen T et al. 2009. Nature Methods 6: 767-772. [4] Levy SF et al. 2015. Nature 519:181–186.[5] Nahum JR, et al. 2015. Proc Natl Acad Sci USA 112:7530–7535."
92.0,"Unlocking success: Neurobiological correlates of grit in adolescents.Intellectual Merit: During adolescence, the brain undergoes extensive structural andfunctional development. Specifically, adolescence is characterized by differentialdevelopment of reward circuitry and cognitive control systems such that cognitive controlregions are relatively underdeveloped compared to reward processing regions.1 Althoughadolescents are able to reason about risky decision making, they are also vulnerable to socialinfluences. In emotionally salient conditions (e.g., the presence of peers), the maturity ofadolescent reward circuitry compared to the less mature prefrontal control system appears toexacerbate risk taking that results in negative outcomes (negative risk taking).2 However,adolescent differential brain development and vulnerability to social influences may also leadto greater recruitment of cognitive control processes used to engage in risk taking that resultsin positive outcomes (positive risk taking), like “grit”. Grit is defined as the determinedpursuit of a superordinate goal in the face of failure.3 Higher levels of grit are associated,over and above IQ, with objectively measured successes (educational attainment, GPA)4 andgreater well-being.5 Neurobiological investigations of behavior can corroborate and challengeour assumptions regarding the neural mechanisms underlying motivational, cognitive, andaffective components of risk taking. Despite the large body of research investigating negativerisk taking, there is a gap in knowledge regarding the neural mechanisms of positive risktaking and whether these mechanisms differ from negative risk taking.Novelty: The brain-based mechanisms of positive risk taking remain unknown, and the onlyempirical investigations of grit are through self-report. This study will address gaps in ourunderstanding of the association between negative and positive risk taking in adolescence,provide the first ecologically valid experimental manipulation of grit, and will determine howgrit behavior relates to external measures of success (e.g., GPA).Experimental Design: Participants will consist of 60 adolescents (14-18 yrs).6 Participantswill undergo an MRI desensitization procedure in the Galván Lab mock scanner beforecompleting a novel computer task in an fMRI scanner.The fMRI task, the “Grit Task”, is a money-earning paradigm I created that builds onextensive delay of gratification and delay discounting literature. There are two types of trials,each worth a fixed amount, lower-value trials (LVTs) and higher-value trials (HVTs).Participants must choose to perform either LVTs or HVTs before beginning (path selection).Participants who select the HVT path will be considered “delayers” who have higher grit thanthose who select the LVT path. If LVTs are selected, money earned will be paid at the end ofthe session up to $10 max. If HVTs are selected, money earned will be paid in 1 week at amin. of $20, max. $30.7 In addition to the delay in payment, the HVT path will requirecompletion of a mental rotation task (MR task; participants must mentally rotate two 3-Dfigures and determine whether they are identical) between each money-earning trial.Requiring completion of the MR task will improve ecological validity compared to delay ofgratification measures that traditionally do not require completion of an effortful task toachieve higher-value rewards. For example, college success requires continued goal-orientedpursuit, not simply an initial decision to delay the receipt of reward for a greater reward.Prior to path selection, all participants will practice the MR task. Participants will betold they must successfully complete (unlimited attempts) the MR task before each money-earning trial if the HVT path is selected. Traditionally, MR tasks are used as a measure ofspatial processing, however here the MR task will facilitate manipulation of “failure”, anessential element of grit. Participants will be told, regardless of performance, that they havefailed at some MR task attempts (randomized). This will require that participants sustain theirchoice of the HVT path and continue to attempt the MR task to receive the higher reward.Between each money-earning trial delayers will decide whether they want to continue withthe HVT path or switch to the LVT path (reward decisions). Path selection and subsequentGraduate Research Planreward decisions are proxies for grit. Those choosing to continue on the HVT path andperform the MR task after they have failed will be considered more “gritty” delayers.Delayers who subsequently switch to the LVT path, and participants who select the LVT pathat the outset, will remain on the LVT path and will be capped at the LVT max award.Restricting low-to-high switching and setting min/max awards for each path will minimizestrategizing. On the LVT path participants will view the MR stimuli before money-earningtrials but will not be required to complete the MR task. MR tasks have been successfully usedin adolescent fMRI studies and adapted to eliminate gender differences.Validated survey measures will assess (1) supportiveness of adolescents’ home andpeer environments,8 (2) grit and impulsivity,9 (3) academic achievement, optimism, IQ, self-esteem, performance anxiety, and well-being.10 The Stoplight Task (ST), a computerizedfMRI task in which participants drive a virtual car, will be administered to determine whethergritty individuals are prone to more negative risk taking. In the ST, participants decidewhether to brake as the car approaches a yellow light at an intersection. Not braking results ina higher crash risk but also a potentially higher monetary reward for finishing quicker.Anticipated Findings: On the Grit Task, more gritty individuals will exhibit greater: (1)perseverance on the Grit Task, (2) activation in mesolimbic reward circuitry (ventralstriatum) at delayed reward presentation, and (3) activation in regulatory control regions(dorsolateral and ventromedial prefrontal cortices; dlPFC, vmPFC) during reward decisions,compared to less gritty individuals. Ventral striatum activation on the ST and Grit Task areexpected to be highly correlated. Gritty individuals are expected to exhibit more PFCactivation during both tasks resulting in more gritty behavior and less risky behavior(measured by ST yellow light decisions).Feasibility: I will work with Dr. Adriana Galván, a developmental neuroscientist withexpertise in adolescent brain development and my advisor, to implement this program ofresearch. Dr. Galván has a database of over 400 ethnically diverse adolescents from which torecruit participants, and her affiliation with the UCLA Center for Cognitive Neurosciencegives me access to state-of-the-art neuroimaging facilities. Scanning fees will be paid by Dr.Galvan’s unrestricted funds.Broader Impacts: Identifying the neural correlates of grit will advance our understanding ofpositive risk taking and inform efforts to improve positive goal-oriented pursuits (e.g.,academic achievement) for adolescents. For disadvantaged adolescents who lack externalencouragement to engage in positive risk taking, this research is critical. As part of UCLAPsychology in Action (PIA), I will share with educators and policy makers atinterdisciplinary symposia how positive risk taking is beneficial for adolescents. I will alsoengage with lay audiences about the implications of my research through PIA’s social mediaplatforms and through community outreach at area schools. I will use my findings toencourage educators and community organizations to provide positive outlets for adolescents.I will advance scientific knowledge by presenting my work in published manuscripts and atconferences, and through transdisciplinary collaboration investigating positive risk takingwith the UC Consortium on the Developmental Science of Adolescence. I will directlyprovide opportunities for adolescents to engage in positive risk taking by conductingleadership workshops at area high schools and will expose underrepresented groups tocareers in STEM fields by actively recruiting women and minority research assistants.References: 1Casey, B.J., Getz, S., & Galván, A. (2008). Dev Rev, 28, 62-77. 2Crone, E.A., & Dahl, R.E.(2012). Nat Rev Neurosci, 13, 636-650. 3Duckworth, A., & Gross, J.J. (2014). Curr Dir Psychol Sci, 23(5), 319-325. 4Duckworth, A.L., Peterson, C., ... (2007). J Pers Soc Psychol, 92, 1087-1101. 5Steger, M.F., Kashdan,T.B., ... (2008). J Res Pers, 42, 22-42. 6Sample size calculated using fmripower.org. 7Amounts based onintertemporal choice heuristic calculation; Ericson, K.M.M, White, J.M., ... (2015). Psychol Sci, 26(6), 826-833.8e.g., NRI-RQV, NRI-SPV. 9 e.g., Grit Scale, DOSPERT, BIS/BAS. 10e.g., LOT-R , WASI-II, Rosenberg Self-Esteem Scale, LSAS-SR, SWLS."
94.0,"Recollection and neurophysiological correlates of fictional memoriesKeywords: autobiographical memory, fiction, episodic memory, cortical potentialThis project aims to understand the differences between experienced and fictionalmemories, from brain processes to behavioral effects. Episodic memories are characterized by asense of re-living and visual imagery, and form the basis for developing an autobiographical self.Rubin et al. assessed the qualities of autobiographical memories by measuring variablesincluding degree of reliving, visual and auditory imagery, emotions, setting, and belief1. Recentinvestigations have begun to probe the shared processes of remembering (“I went to the sciencemuseum 2 years ago”) versus imagining (“I imagine myself graduating from college in thefuture”). Absent from the literature is thorough behavioral data on fictional memories: thememory of an imagined experience without an explicit reference to self, derived from fiction (“Ican visualize Atticus Finch standing in a Southern courtroom”). Fictional memories are encodedand retrieved with subjective characteristics similar to veridical memories, and can be source ofintegrated knowledge about the world2; as such, they occupy an interesting and largelyunexplored niche in memory research.Conway et al. used electroencephalography (EEG) to record the dynamic process ofretrieving true memories from the past3. Using the excellent temporal resolution of EEG, heestablished a distinct neural signature for what retrieving and maintaining autobiographicalmemory broadly looks like in the brain. First, there is activation in the prefrontal cortex,followed by additional temporo-occipital activation once the memory is formed. In a differenttask, subjects constructed future, imagined scenarios that were plausible and involved the self.Conway found that the real and imagined conditions relied heavily on the same brain regions.However, one difference that left prefrontal activation was highest during active maintenance ofplausible imagined memories, presumably because this construction task is more effortful.Secondly, he found that temporal and occipital lobe activation is greater in the recall of realautobiographical memories, suggesting that imagined memories elicit stored sensory data, theydo so less than real autobiographical memories.In order to gain a theoretical and practical understanding of fictional memories, bothbehavioral and neurophysiological data are important; my proposed study will investigate theseperspectives. I expect that many fictional memories can be vividly re-lived, they may not beassociated with a particular time or place. I also expect fictional memories to evidence thedynamic localization of autobiographical memory. And if memories that are explicitlyunderstood to be not real are incorporated into autobiographical memory, then they can influenceidentity and behavior. Developing a clearer picture of the neurophysiology of fiction andmemory could illuminate how fiction-reading contributes to cognitive and affectivedevelopment, or how fictional sources could be used intentionally by educators. If fictionalmemories do not show activity aligned to real and imagined autobiographical memories, then wemust begin to explain how any episodic-like memory can exist without these networks.I propose a research project to be carried out in two phases. Since there is not existingresearch on how to cue a fictional memory and it is critical to have reliable and controlledprotocols for in the next, EEG-centered phase, I will first establish this protocol, as well as gatherbehavioral data through surveys. This first phase will also allow me to find and address anyunanticipated sources of error in this novel process, and yield data to modify the design of thenext phase. I will limit the study to fictional memories generated through the written word (e.g.novels and short stories). Subjects will be given a cue to recall a scene from a written work offiction that can be strongly recalled; I will seek to gather 30 observations per subject. To gatherBrenda Yang Graduate Research Statement NSF GRFP 2015pilot data, EEG will be used to record cortical scalp potentials (more detailed methods arebelow). I anticipate that these memories—like veridical episodic memories— will differ in manyways within and between participants, including time since the last experience, personal interest,and amount of rehearsal. These qualities will be assessed via a questionnaire modified fromRubin et al.1, which asks participants to rate their experience on a scale of 1 to 7 for questionsthat address recollection (like reliving), component processes (like visual imagery, spatial layoutand emotions), and reported properties (like importance, rehearsal, and age of memory). Thequestions will be delivered after each cue through a provided keyboard.In the second phase, I will use an EEG paradigm to examine the temporal dynamics offictional memory construction. In two conditions, I will record scalp potentials with EEG whilesubjects recall and maintain (1) true memories of the past and (2) plausible imagined scenarios ofthe self; these are the scenarios studied by Conway, and will be used as controls. In the third (3)experimental condition, I will elicit the retrieval and maintenance of fictional memories using theprotocol established in phase 1. I will seek to gather 5 observations for each condition perparticipant to balance the need for statistical rigor with maintaining a reasonable length for thestudy. Each trial will begin with the memory instruction “Real Memory,” “Imagined Memory”or “Fictional Memory” on screen for 3s. A fixation stimulus will be presented for 3s, followed bythe cue for one of the three scenarios. The cue will remain on screen until subjects indicate witha bimanual joystick pull that a memory has been successfully retrieved or generated. Participantswill communicate that they were unable to retrieve a memory via a keyboard instruction, whichwill lead to a new trial. After the memory is retrieved, subjects will fixate on the screen and beinstructed to hold in mind the memory for 7.5s. Then, the participant will then type a briefdescription of their memory using the keyboard provided.In designing the cues and trials, it will be critical to balance cues and trials acrossparticipants. Subjects with high shifts in voltage throughout the trials will not be analyzed.Statistical significance will be assessed using a 3-way ANOVA involving the electrode levelsand the 3 conditions of memory instruction. If fictional memories are experienced as episodicmemories, I would expect to find patterns of cortical potential for fictional memories that aresimilar to that of the imagined future events. That is, the activation of posterior brain regionsshould be reduced compared to remembrances of real events. Of interest is the degree to whichthe prefrontal cortex is activated in the absent of a scenario that does not explicitly involve theself. Behaviorally, I expect to find that re-living of fictional memories to be comparable toveridical ones and primarily visual in nature. Of interest are differences in how the event comesto the subject “a coherent story,” the perspective of the experience, and whether the memorycomes back “in words” for a fictional memory that was, after all, delivered through language.To support this work, I am seeking graduate programs which would allow me to combinebehavioral and brain measures. I have established contact with several institutions where thiswould be possible and where training and facilities for EEG is available. For example, ElizabethMarsh at Duke University studies fiction, false memories, and applications to educationalpractice. Conditional on my acceptance to the program, she has offered guidance forcollaboration between her lab and others in the psychology and cognitive neurosciencedepartments. I am confident that with these supports, my experience designing novelexperiments, and solid conceptual background, I can carry out this research within three years.1Rubin, D.C., Schrauf, R.W., Greenberg, D.L. (2003). Belief and recollection of autobiographical memories.Memory & Cognition, 887-901. 2Marsh, E.J., Meade, M.L., Roediger, H.L. (2009) Learning facts from fiction.Journal of Memory and Language, 519 –536. 3Conway, M. A., Pleydell-Peace, C. W., Whitecross, S. E., & Sharpe,H. (2002). Neurophysiological correlates of memory for experienced and imagined events. Neuropsychologia, 1-8."
95.0,"StressKEYWORDS: personality; cognitive appraisal; emotion; stress reactivity; ecologicalmomentary assessmentINTRODUCTION: Everyone experiences stressful life events, but the magnitude of stressexperienced in response to the same stressor can vary considerably between two people. Forexample, certain people tend to express more heightened negative emotion in response to lifeevents and therefore perceive their lives as more stressful.1 Personality factors such as increasedneuroticism, behavioral inhibition, and negative attributional style are also implicated in thisgreater stress reactivity1,2. I propose that these individual differences in response to stressors arestrongly influenced by personality, and lead to variation in cognitive-emotional appraisal andprocessing of the stressful lifeSTRESSORevent. In turn, this cognitive-emotional appraisal contributes to Cognitive Appraisalvarying physiological reactivity, Personality -Worry/Ruminationas measured by immune, -Neurotic/Anxious -Perceived Control Physiologicalcardiovascular, and endocrine -Behaviorally- -Perceived Severity ReactivityInhibited-Immuneresponse (See Figure 1).-Cardiovascular-NegativeCognitive appraisal involves -EndocrineAttribution Style Emotionalthe perceived severity andReactivityperceived controllability of the-Negative Affectstressor, as well as the magnitudeFigure 1. Proposed model of personality, cognitive appraisal,and frequency of persistent,emotional reactivity, and physiological reactivity in response to stressmaladaptive, negative thoughtsleading up to and following the stressor (worry/rumination). Emotional reactivity involves thepresence of positive or negative affect in response to the stressor (See Figure 1). A betterunderstanding of how subjective cognitive-emotional reactions to stress relate to individualvariation in physiological reactivity is needed to better understand key individual differencesbetween people.It is well established that stress relates to immunological dysregulation (e.g. slowed woundhealing and increased viral susceptibility).3 Yet the interplay between individual differences instress reactivity and immune system reactivity has seldom been investigated, particularly withregard to how such changes play out in real time, real-life human contexts.2 One reason for thislack of understanding is that studies of immune reactivity have largely been limited to short-termlaboratory studies that use blood draws for serum/plasma-based biomarkers. With less invasivesalivary biomarker techniques that are currently in development, more ecologically-validstudies of individual differences in stress reactivity could be conducted.RESEACH AIM 1: To investigate how individual differences in personality relate todifferences in physiological reactivity, specifically markers of immune system function.RESEARCH AIM 2: To examine how cognitive appraisal of a stressor and emotional reactivityto a stressor mediate the relationship between personality and physiological reactivity.RESEARCH AIM 3: To determine how relationships between personality, cognitive appraisal,emotional reactivity, and physiological reactivity play out over time in an ecological context.METHODS: I have already independently conducted an extensive literature review to elucidatethe connection between salivary and blood measures of inflammation and plan to publish areview article on this work. Along with Penn State investigators who are spear-heading furtherinvestigation of salivary immune diagnostics, I hope to incorporate salivary measures of immunereactivity into novel methodological approaches to studying individual variation in the stressresponse. My research program during my graduate training will culminate in myimplementation of three original studies described below.STUDY 1 (S1): I will begin by using an existing data set to examine connections betweenindividual differences and immune responses to stress. My advisor is PI on a longitudinalinvestigation of the degree to which inflammation mediates connections between stress andcognitive aging among diverse adults. I will be able to use those data to explore new dimensionsof how personality (e.g. behavioral inhibition, negative attribution style) is related to cognitive-emotional responses to stress and, consequently, to physiological reactivity over a 4-year period.STUDY 2 (S2): To experimentally model the relationship between personality and immunefunction, I will expose participants to an acute social lab stressor in order to measure individualdifferences in stress reactivity through endocrine, immune, and cardiovascular measures andthrough self-reported assessments of cognitive-emotional state. Specifically, immune functionwill be measured through circulating inflammatory markers obtained via saliva and blood.STUDY 3 (S3): The data from S1 and S2 will be used to inform a larger naturalistic study whichwill utilize ecological momentary assessment (EMA), a method whereby participants report whatthey are feeling and/or how they are behaving in real-time in natural settings. In collaborationwith Penn State’s Dynamic Real-Time Ecological Ambulatory Methodologies (DREAM)initiative – a unique program designed to popularize and educate researchers on EMA methods –participants will be given smart phones which will prompt them to fill out assessments ofcognitive-emotional states and to give saliva samples at specific time points. State-of-the-arthierarchical linear modeling and structural equation modeling will be utilized for mediationanalyses in S1 and S2, as well as to examine between-subject and within-subject variation,including how multiple daily assessments change over time in S3.INTELLECTUAL MERIT and BROADER IMPACTS: With the levels of stress that manyin our society are facing, it is imperative to better understand the mechanisms by which somepeople become more susceptible to the physiological consequences of stress. My program ofresearch has potential implications for improving quality of life, coping strategies,interpersonal relationships, and productivity in the workforce, as well as fostering self-actualization through stress reduction. These three studies will advance the field ofpsychoneuroimmunology by examining the underutilized combination of less-invasivesalivary inflammatory biomarkers with respect to individual differences. The additionalassessment of this concept through EMA will allow for greater external validity of results andhelp popularize more ecologically-valid studies. Pre-existing partnerships with a number facultywho already investigate individual differences in the stress response makes me well-poised toimplement the tri-part research initiative I am proposing. In conjunction with the DREAMinitiative and Penn State’s Centers for Healthy Aging and “De-Stress Zone” (a biofeedbackfacility), I also plan to design and hold workshops for education on ecological measurementof stress and stress self-management to implement with diverse populations. With all of theopportunities for research and outreach available at Penn State, my current position places me inan ideal situation to begin explaining the individual variation that is often overlooked inphysiological psychology studies.REFERENCES: [1] Suls, J., Green, P., & Hillis, S. (1998). Emotional reactivity to everyday problems, affectiveinertia, and neuroticism. Pers & Soc Psych Bltn. 24: 127-136. [2] Segerstrom, S.C. (2000). Personality and theimmune system: Models, methods, and mechanisms. Annals of Behav Med. 3:180-190. [3] Contrada, R., & Baum,A. (Eds.). (2010). The handbook of stress science: Bio., psych., & health. New York, NY: Springer Publishing Co."
