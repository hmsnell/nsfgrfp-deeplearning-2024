text	Success
permission. Analysis of Bacterial, archaeal, and viral dispersal between distantly separated acid mine drainage systems through metagenomic analysis Keywords: Acid mine drainage, metagenomics, biogeography Background Acid mine drainage (AMD) is a serious mining-related environmental problem that causes acidification and metal contamination of waters and rivers. The exposure of sulfide minerals such as pyrite to oxygen and water produces sulfuric acid and releases heavy metals into the draining waters (1). It has been shown that microorganisms significantly contribute to this process by catalyzing the limiting step of the involved reaction (1, 2). I propose to study the rates of dispersal of AMD microorganisms, which will provide insight into understanding the processes that initiate and accelerate AMD formation. Microbial communities found in AMD systems are ideal samples to study due to their low diversity and complexity. The most abundant organisms include Leptospirillum group II, Leptospirillum group III, Acidithiobacillus ferrooxidans and Ferroplasma acidamarnus (3). AMD has been widely reported in several countries. The Richmond Mine at Iron Mountain, California, USA is an unusually well studied AMD system (e.g., 2, 3, 4). Metagenomic analysis of microbial biofilms from the Richmond Mine allowed reconstruction of near complete genomes of Leptospirillum group II (4) and Leptospirillum group III (5), among other organisms. On the other side of the equator, Chile is one of the biggest producers of copper in the world, and, due to extensive mining activity, AMD has been found in several abandoned mines along the country. I propose to compare AMD microbial communities at North and South American sites as a function of their geographical separation, using genomics of total community DNA, i.e., metagenomics. It has been reviewed that diversity of microorganisms correlates with distance separation (biogeography) (6). Because AMD organisms are adapted to extreme environments that are separated by long distances, their rates of migration are limited (7). I propose to determine the biogeographic and evolutionary dynamics that can account for strain diversity in the mentioned AMD systems. More specifically, I will determine whether diversification occurred due to contemporary environmental conditions or due to geographical separation, or whether both factors are responsible. This information will provide insights into the rates at which bacteria and archaea responsible for AMD colonize new exposed sites. Prior studies have indicated that similar AMD environments with similar geochemical conditions tend to have similar microbial communities (8, 9). The objective of this proposal, therefore, is to study strain diversification of the most abundant members of these communities. Thus I hypothesize that: 1) Any two strains of a single archaeal species from two different sites will be more evolutionary distant than a pair of bacterial strains from the same two sites. This hypothesis is based on the concept that opportunities for dispersal of acid-adapted microorganisms will be limited (7). Therefore, bacterial and archaeal populations from distantly separated sites should be distinct at the strain level. Archaeal dispersal is more difficult due to the lack of cell wall, leading to more phylogenetic diversity than that of coexisting bacteria, which would disperse more easily. Copyright 2008 Copyright 2008 All rights reserved to original author. Do not copy or use in any way without permission. 2) AMD virus populations will be generally similar at all sites, despite likely rapid evolutionary rates, due to their efficient rates of dispersal. This hypothesis is based on findings that viral populations in other systems are similar at sites separated far apart (10). To test these hypotheses, I propose to sample and analyze microbial communities found in AMD in Chile. Results will be correlated with metagenomic data from the Richmond mine in California available at Dr. Banfield’s lab. Access to sites in Chile will be arranged in collaboration with Drs. David Homes and Raquel Quatrini, from the Millenium Institute in Santiago, and Dr. Cecilia Demergasso from the Northern Catholic University in Antofagasta, Chile. In collaboration with these labs and UC Berkeley, I will apply at the Joint Genome Institute (JGI) for a community-sequencing project. I will collect samples from at least 5 AMD sites in Chile for screening. Initial identification and abundance of organisms will be done using fluorescent in situ hybridization (FISH), a technique I have worked with extensively in the past. I will also do 16S rDNA clone libraries in order to determine initial phylogenetic relationships. I will select two samples that share the majority of species for metagenomic analysis. I will request a total of 70 Mb of metagenomic sequence from JGI. Since complete or near-complete genomes of AMD organisms are available, I will use these genomes to assign DNA fragments to specific organisms. I will assemble genomes using software that our lab has extensive experience with. Then, I will analyze sequence variation within sites and between sites in order to: i) determine whether there is more or less variation within the populations than between them; ii) quantify the extent of diversity within populations of each organism, and iii) for the virus datasets, to determine whether there is evidence for closely related virus populations at the different sites. In summary, I will do a comprehensive genomic and phylogenetic study between microorganisms found in AMD from the US and Chile. My research experience working at Dr. Banfield’s lab, and completing an undergraduate thesis, has well prepared me to do this kind of analysis. Being a coauthor in two publications also demonstrates my ability to work as part of a team. No existing community research project is similar to my proposed work, therefore, this will be a big step in understanding the rates of evolution of microorganisms found in AMD and their relationships within the community. It will serve as a model for studying other microbial communities separated by long distances. My work will be shown at national and international conferences, for both specialized and general audiences. In addition, as a mentor for the SMASH program I will teach my work to minority high school students, hoping to recruit them as future researchers in this field. References (1) Schrenk MO, et al. Science. 1998 Mar 6; 279(5356): 1519-22, (2) Edwards KJ, et al. Chem Geol. 2000 169: 383–397, (3) Bond PL, et al. Appl Environ Microbiol. 2000 Nov; 66: 4962-71, (4) Lo I, Denef VJ, Verberkmoes NC, Shah MB, Goltsman D, et al. Nature. 2007 Mar 29; 446(7135):537-41, (5) Goltsman D, Banfield JF. (In preparation), (6) Hughes Martiny J, et al. Nat Rev Microbiol. 2006 Feb; 4(2): 102-112; (7) Whitaker RJ. Phil Trans R Soc B. 2006 361:1975-1984; (8) Coram N, et al. App Environ Microbiol. 2002 68: 838-845. (9) Dold B, et al. Environ Sci Technol. 2005 39: 2515-2521. (10) Breitbart M, Rohwer F. Trends Microbiol. 2005 Jun;13(6):278-84. Copyright 2008	0
"Motivation: Since the mid-1970s, global natural gas production has steadily risen to its current all-time high, and is projected to continue until at least 2040.1 Although natural gas is a cleaner burning fuel than gasoline, its implementation in the transportation sector has been stymied by its significantly lower volumetric energy density.2 Densification strategies, including liquefaction or high-pressure storage, have inherent safety and cost issues that are widely viewed as prohibitive for passenger vehicles.3 Introduction: Adsorbed natural gas systems that employ porous materials, such as metal-organic frameworks (MOFs) and covalent organic frameworks (COFs), have received considerable attention as potential alternatives to higher pressure and/or cryogenic storage methods.4,5 In these systems, favorable interactions between the gas and the porous solid increase the amount of gas that can be stored at a given temperature and pressure as compared to an empty tank.4 However, the non-molecular nature of these extended structures makes solubility non-existent, reducing compatibility with post-synthetic functionalization and modification that can be leveraged to increase gas adsorption capacity and bulk material properties, such as density and thermal conductivity. Hybrid metal-organic or all-organic molecular analogs of these porous materials, coined porous coordination cages (PCCs) and porous organic cages (POCs), respectively, directly address this issue while retaining the highly sought after permanent porosity and tunability of their expanded, 3-D counterparts.6,7 PCCs often contain open metal sites that provide favorable interactions for increased gas storage as compared to all-organic structures.6 However, these molecules tend to display surface areas that pale in comparison to MOFs and POCs. On the other hand, POCs display surface areas that are on par with many MOFs, but lack the tunable metal cation- gas molecule interactions seen in hybrid metal-organic systems.8 To address these issues, I propose the design, synthesis, and application of novel porous organic cages with integrated metal sites toward the selective adsorption and/or storage of small molecules. Preliminary Results: Although porous organic cages have been heavily investigated over the past decade, the study of the high-pressure storage of gases in these materials is still well in its infancy.7 As a result, relatively little is known about their utility as gas storage materials. Similarly, post- synthetic metalation of these systems to introduce sites with catalytic Figure 1: Known POC activity or selective gas adsorption has been unexplored. The standard targeted for preliminary work metric for porous materials, gravimetric surface area, is a simplistic representation of a material’s ability to store a gas. While the high surface areas that POCs have displayed provide a basis for using such materials as gas storage media, investigations into these materials for the specific task of gas storage is surprisingly limited. Incorporation of metal chelating sites within molecular cages will allow for the precise insertion of a specific metal post-synthetically. Metal cations can play an important role in tuning metal-gas interactions, which is necessary for creating a material for selective gas adsorption or high-pressure storage. I targeted a known POC based on triformylphenol and 1,2- diaminocyclohexane, where a half-salen unit is formed when the cage is constructed (Figure 1).9 Confirmed via SEM-EDX and XPS, initial results show coordination of a Ni2+ center into the cage’s structure, while retaining permanent porosity (Figure 2). Further gas adsorption studies to better understand the selectivity of the metal-integrated POC are currently underway. Aim 1: Create a library of ligands and cage topologies that are conducive to metal integration. Inspiration for this approach can be drawn from recent literature on the reported topologies of cages, where the most straightforward methods involve imine or boronic ester formation to create the covalently linked cage. Although specific angles must be considered within the ligands in order to access these desired topologies, functional groups and sizes of the ligands are typically tunable. Understanding this, cages will be functionalized and built around traditional multi-dentate ligands, such as salen, catechol, and 2,2’-bipyridine to form metal complexes after cage isolation. Aim 2: Isolate porous organic cages and introduce metals to their chelating sites. Typically, solution-based syntheses produce cages. The purity of isolated cages can be confirmed through several techniques due to their molecular nature, most readily being high-resolution mass spectrometry, NMR, and IR spectroscopy. For more complex cages, such as cages containing chiral centers, additional efforts will be put forth to obtain diffraction quality single crystals, utilizing techniques such as vapor diffusion and liquid/liquid diffusion, to further confirm cage Figure 2: Model representing formation. After successful isolation, the porous organic cages will then the integration of Ni2+ be introduced to metal salts to obtain their metalated counterparts via solvothermal methods and solid-state metalations. A plethora of techniques are available at the University of Delaware for the characterization of the metalated POCs, including SEM-EDX, XPS, EPR, and the previously mentioned techniques. Aim 3: Perform gas adsorption studies and identify the roles that both cages and metal-sites play in selective gas uptake and storage. Gas adsorption studies will be performed on both the base cage and the metalated cage to decipher the interplay of porosity and selective gas uptake. Surface area analyses will be performed in the Bloch Lab, using both CO and N as probe molecules, along with systematic high- 2 2 pressure gas storage studies (hydrocarbons, H , etc.) and enthalpy of adsorption calculations. 2 Intellectual Merit: While small molecule storage has been heavily studied in extended frameworks like MOFs and COFs, much less is known for porous molecular materials. These materials retain the sought after permanent porosity of expanded frameworks, as well as impart solubility that can lead to advantageous post-synthetic modification. This project will elucidate how POCs can be utilized as gas storage media and develop the novel field of metalated POCs, including their design, synthesis, and utilization as adsorbed natural gas materials. Broader Impacts: My proposed project has opportunities for collaborations that I will pursue heavily to better understand these systems. Collaboration with computational groups will help predict gas storage capacities and suggest more favorable interactions based on metals and ligands, and work with catalysis- focused groups can utilize my metalated cages as a homogenous setting for catalytic reactions. Just as importantly, I will continue to mentor undergraduate researchers and first year graduate students to teach them essential and advanced laboratory skills. I will share my findings at local, national, and international meetings when they are deemed safe, and until then, I will present my work virtually and continue to publish results. References 1. EIA, ""World Energy Outlook 2019"", 2019, https://www.iea.org/reports/world-energy-outlook- 2019/gas 2. EIA, “How much carbon dioxide is produced when different fuels are burned?”, 2019, https://www.eia.gov/tools/faqs/faq.php?id=73&t=11 3. DOE, “Natural Gas Fuel Safety”, https://afdc.energy.gov/vehicles/natural_gas_safety.html 4. Mason, J. A.; Veenstra, M.; Long, J. R. Chem. Sci. 2014, 5, 32-51. 5. Das, S.; Heasman, P.; Ben, T.; Qiu, S. Chem. Rev. 2017, 117, 1515–1563. 6. Gosselin, A. J.; Rowland, C. A.; Bloch, E. D. Chem. Rev. 2020, 120, 8987–9014. 7. Hasell, T.; Cooper, A. I. Nat. Rev. Mater. 2016, 1, 16053. 8. Zhang, G.; Presly, O.; White, F.; Oppel, I. M.; Mastalerz, M. Angew. Chem. Int. Ed. 2014, 53, 1516- 1520. 9. Petryk, M.; Szymkowiak, J.; Gierczyk, B.; Spólnik, G.; Popenda, Ł. Janiak, A.; Kwit, M. Org. Biomol. Chem. 2016, 14, 7495-7499."	0
Introduction: Applications that demand large cooling capacities, such as high-power electronics, rely on the elevated thermal energy density that is associated with the latent heat of vaporization. Flow boiling of a liquid in a channel is one of the most effective approaches for ensuring high heat transfer efficiencies. For example, single-phase convection can remove heat with a rate up to 20 kW/m²K, whereas flow boiling easily reaches 100 kW/m²K.1 Flow boiling within microchannels increases the heat transfer potential even further, due to the enhanced surface-to-volume ratio. Many electronics and microelectromechanical systems (MEMS) with high power densities thus rely on microchannel flow boiling as their primary cooling strategy. Consequently, an efficient and reliable operation and control of this chaotic multi-phase flow process is hence indispensable. In both macro- and microchannels, the two-phase mixture can be characterized by different flow regimes, which are defined by the relative amounts and configuration of the liquid and vapor phases, and ranges from bubbly flow (few vapor bubbles in the center) to slug flow, annular flow, and eventually mist flow (few liquid droplets in the center). Despite the great promise that two-phase flow boiling poses, it presents many challenges. For example, dry-out, or the critical heat flux (CHF), occurs at the onset of mist flow and leads to a drastic rise in wall superheat that can induce material thermal fatigue and ultimately failure. Unfortunately, its occurrence and location there-of is difficult to predict, leading to the implementation of large safety margins during the initial design of the thermal management system. A different flow instability arises from the rapid expansion of bubbles, which causes pressure oscillations that can ultimately jeopardize the structural integrity of the channel and even initiate flow reversal.2 These instabilities have presented major challenges toward flow boiling applications and serve as a focal point for recent studies. To overcome the existing limitations, I propose to use machine learning (ML) to detect and ultimately control the onset and nature of these flow instabilities. Autonomous detection and self-stabilization of flow boiling instabilities would significantly enhance the reliability of two-phase cooling systems, while reducing costs and greenhouse emissions. Intellectual Merit: The use of ML in thermal management is a novel method for predicting heat transfer properties. A recent publication by Ravichandran et al.3 has shown that neural network models can predict the margin to the boiling crisis in a pool setting. I propose that in flow boiling, the onset detection and solution to mitigating flow instabilities can similarly be accomplished through the implementation of a deep learning algorithm. Aim 1: Collect images and videos of various flow instabilities for different flow conditions. ML requires the collection of large amounts of data to train an algorithm. I will construct an apparatus for recording various instabilities. Korniliou et al.4 describes a method for microchannel fabrication onto a polydimethylsiloxane (PDMS) substrate with an infrared transparent tin indium oxide back wall. I will implement IR imaging to record wall surface temperatures while simultaneously recording high speed optical imagery at identical frame rates. The setup will include a pressure transducer mounted at both ends of the channel to monitor differential oscillations. A micropump will serve to circulate the fluid after it passes through a vacuum degasser. The ratio of heat flux to mass flux has been used to quantify microscale instabilities by defining their oscillation periods.5 I plan to induce various instabilities within the microchannel by adjusting flow rate and heat generation while recording the resultant effects on temperature and pressure. Each instability will be categorized into an appropriate set of resultant conditions. In addition to recording my own footage, I will also use data available in open literature and contact authors of recent publications to share their data. Aim 2: Train a machine learning algorithm to predict each type of instability based on pre- defined conditions. I propose to train an artificial neural network (ANN) model by inputting measurements of pressure oscillations, nucleation site density, surface temperature changes, and bubble movement into each instability category. A feature ranking algorithm will be implemented to define the key measurement parameters. I will incorporate Google’s machine learning library TensorFlow to accomplish these tasks. The data will be split appropriately among sets for training, validating, and testing the model. Model evaluation will be conducted through 10-fold cross validation to ensure proper fitting. I will utilize the model’s mean absolute percentage error as the primary evaluation metric. The results of the model will be used to determine the corresponding flow, thermal, and differential pressure conditions associated with specific instability types. Broader Impacts: This project serves as a vital proof-of-concept for validating the use of machine learning to detect flow instabilities. Time permitting, I plan to program the model into a closed-loop control algorithm. Upon the detection of a specific instability, the algorithm would contain and implement a pre-programmed solution, such as the adjustment of the flow rate or pulsation of ultrasonic waves. This solution will serve as a pioneering step toward the implementation of machine learning in thermal management and demonstrates a potential method for revolutionizing the peak performance and safety of electronics cooling applications. The development of a working control algorithm would allow for system optimization and self-stabilization, which would answer direct needs of the thermal fluids community. During our STEM club sessions for EduMate NYC, as introduced in my personal statement, we received strong positive feedback from the students during the computer science presentation. I therefore intend to use this project to initiate an Introduction to Artificial Intelligence virtual workshop for high school and middle school students in the St. Louis area. The St. Louis metro region is one of the most segregated cities in the United States, and its long history of racial disparity contributes to educational inequity to this day. I will discuss the fundamentals of artificial intelligence, including machine learning, natural language processing, and computer vision in weekly sessions throughout 5 weeks in the summer months. An additional 5 weeks will be spent discussing the applications of artificial intelligence, such as robotics, transportation, and my proposed project. The goal behind these events is to create a foundational understanding of artificial intelligence while inspiring students to pursue these topics in their future studies and careers. Allowing young innovators to develop skills in one of the most important technologies today is valuable for contributing to their future success. I will collaborate with the St. Louis Academy of Science to host these events and provide outreach assistance. References: [1] Bergman, et al. (2011) Fundamentals of Heat and Mass Transfer [2] O’Neill, et al. (2020) International Journal of Heat and Mass Transfer [3] Ravichandran, et al. (2021) Appl. Phys. Lett. [4] Korniliou, et al. (2017) Applied Thermal Engineering [5] Prajapati, et al. (2017) Experimental Thermal and Fluid Science	0
"Radiolytic Production of Gadolinium Nanoparticles for Cancer Therapy Hypothesis: The average particle size of the gadolinium will decrease as total absorbed dose increases within a nuclear reactor. Background and Introduction: In the United States, cancer is the second leading cause of death and is a major public health problem worldwide [1], [2]. There were estimated to be 1,685,210 new cancer cases and 595,690 cancer deaths in the U.S. alone in 2016 [2]. Current treatment for various cancers include chemotherapy, radiation therapy, hormone therapy, and surgery. Although survival rates have improved from these treatments, each one has its drawbacks and limitations. Chemotherapy, for example, distributes the toxic therapeutic agents throughout the entire body, thus, damaging both cancerous and normal cells. This limits the amount of dose to the cancer cells while causing adverse side effects to the patient including weakness, hair-loss, and organ dysfunction [1]. An interest in nanoparticles (NPs) has increased over the last decade for researchers because of their ability to carry both drugs and imaging probes throughout the body [1]. Additionally, they can be uniquely designed to target the molecules of diseased tissues, thus, having the potential to increase the dose to cancerous cells while decreasing the dose to healthy tissues and organs [3]. Gadolinium neutron capture therapy (GdNCT) is another potential method for the treatment of cancer [4]. GdNCT takes advantage of the energy released when stable gadolinium-157 (157Gd) is bombarded by a neutron, producing an excited 158*Gd that decays by gamma emission, conversion electrons, and Auger electrons (Figure 1) [4] [5]. 157Gd is appealing in NCT for its extremely high neutron absorption cross section Figure 1: Gd decay scheme (255000 barn) and short path length in tissue, restricting cell death to the gadolinium-containing regions only [5]. The objective of this research proposal is to fabricate gadolinium nanoparticles for GdNCT applications by irradiation in a nuclear reactor. Average particle sizes of less than 100 nm are desired so that they can pass through the body and localize around tumors. Once the NPs have gathered around the tumor, neutron irradiation can occur using either a nuclear reactor or a neutron accelerator, destroying the tumor with minimal damage to the surrounding healthy tissues and organs. Research Plan: The first step for this research project will be to determine the likely material components. A form of the gadolinium precursor, a reducing agent, a particle capping agent, and a solvent to prevent excess agglomeration will need to be used to create the gadolinium solution [6]. The solution will be distributed into 8 labelled vials, with Sample 1 being used as the un- irradiated reference sample. Samples 2-8 will be irradiated in the High Flux Isotope Reactor at a constant power (200kW) for times ranging from 30 to 1200 seconds (Table 1.) Table 1 – Irradiation Times Sample 2 3 4 5 6 7 8 Irradiation 30 60 180 300 600 900 1200 Time (s) NSF Graduate Research Statement Mikayla Molnar I will complete dose calculations in order to determine the total absorbed dose for each sample irradiated in the reactor. The samples will be subject to both incident neutrons and gammas radiation. Therefore, the dose rate D is given by Equation (1) t 𝐷 = 𝐾 +𝐷 (1) 𝑡 𝑛 𝛾 Where K is the neutron kerma rate and Dγ is the gamma dose rate. These dose rates will be n determined using a Monte Carlo N-Particle (MCNP) code simulation of the reactor. The samples will then be distributed onto a silicon wafer and left to dry. It is important that the radiation levels of the samples are safe and within NRC/reactor limits before removing them from the reactor for analysis. I will then image each sample under a scanning electron microscope (SEM) to determine the particle size distribution. An Energy Dispersive X-ray Spectroscopy (EDS) analysis will also be performed to verify the elemental composition of the particles. Intellectual Merit: My background as a nuclear engineer is essential for the fulfillment of this project. It will combine my understanding of radiochemistry, reactor physics, and electron imaging with an engineering perspective. I completed a similar process for the production of boron nanoparticles in my Reactor Laboratory II class at Missouri University of Science and Technology, so I have first-hand experience in the process that needs to unfold. A much more comprehensive and in-depth analysis will need to be taken, however. I plan to collaborate with members of Oakridge National Laboratory to complete the irradiation procedure and use my knowledge of MCNP and scanning electron microscopy to determine the total absorbed dose for each irradiated sample. Broader Impact: GdNCT is becoming a safe and effective way to destroy cancer cells with minimal damage to surrounding healthy cells. If the results are successful, my research will provide an efficient and valuable mean for creating gadolinium nanoparticles. This will make GdNCT cheaper and more accessible for cancer patients, resulting in the potential savior of countless future lives. [1] K. T. Nguyen, ""Targeted Nanoparticles for cancer therapy: Promises and challenges,"" Journal of Nanomedicine & Nanotechnology, vol. 02, no. 05, 2011. [2] R. L. Siegel, K. D. Miller, and A. Jemal, ""Cancer statistics, 2016,"" CA: A Cancer Journal forClinicians, vol. 66, no. 1, pp. 7–30, Jan. 2016. [3] R. Subbiah, M. Veerapandian, and K. S. Yun, ""Nanoparticles: Functionalization and Multifunctional applications in biomedical sciences,"" Current Medicinal Chemistry, vol. 17, no. 36, pp. 4559–4577, Dec. 2010. [4] G. A. Miller, N. E. Hertel, B. W. Wehring, and J. L. Horton, “Gadolinium Neutron Capture Therapy,” Nuclear Technology, vol. 103, no. 3, pp. 320–331, Sep. 1993. [5] C. Salt, A. J. Lennox, M. Takagaki, J. A. Maguire, and N. S. Hosmane, “Boron and gadolinium neutron capture therapy,” Russian Chemical Bulletin, International Edition, vol. 53, no. 9, pp. 1871–1888, Sep. 2004. [6] A. Abedini et al., “A review on radiation-induced nucleation and growth of colloidal metallic nanoparticles,” Nanoscale Research Letters, vol. 8, 2013."	0
Crossover Regulation During Caenorhabditis elegans Meiosis Keywords: chromosome structure, condensin, crossover interference, meiosis, recombination Meiosis is essential for the generation of genetic diversity. All sexually-reproducing eukaryotes undergo this specialized cell division, consisting of one round of DNA replication followed by two rounds of chromosome segregation. Successful segregation requires crossover recombination, which is initiated by a programmed double strand break (DSB) that causes the reciprocal exchange of genetic information between homologous chromosomes. Crossovers (COs) provide physical links between homologs, but they also facilitate evolution by culling deleterious mutations and creating novel allelic combinations. Due to their importance, COs are subject to strict regulation that guarantees at least one CO per homolog pair and ensures wide spacing of multiple COs. Additionally, COs preferentially occur on genomic intervals called “hotspots.” These flank haplotype blocks, allelic combinations that tend to be inherited together and are evolutionarily more stable.1 Hotspots determine the evolutionary genomic landscape, but efforts to predict their location have only been partially successful.2 CO hotspots are also hotspots for DSBs, though not all DSBs become COs.3 Therefore, CO regulation affects DSB distribution and DSB resolution into COs or noncrossovers (NCOs). 4,5 The nematode C. elegans provides an elegant system to study this control, for it exhibits complete CO interference: each homolog pair only has one CO per meiosis.6 Chromosomes are structured by a number of protein complexes, one being the highly-conserved condensin complex. C. elegans has three condensins involved in dosage compensation, chromosome compaction, and CO control.7 Disruption of the meiotically-active condensin I complex causes chromosomal axis extension, which alters DSB distribution and thus CO distribution.5 Previously, the condensin II complex was thought to act only in mitosis – but work in the Meyer lab has shown that it is also involved in meiosis, downstream of CO regulation.8 Preliminary data from the lab implicates at least one condensin II subunit earlier in meiosis that affects CO distribution in a way that differs from condensin I. Though CO control is widespread, its precise mechanism remains a mystery. I propose to use C. elegans as a model in which to deepen our understanding of CO regulation by examining how CO distribution is affected by both meiotic condensin complexes. Hypothesis: Condensin II regulates crossover frequency at the level of DSB initiation by lengthening chromosome axes, which changes the binding of DNA to each axis. Mutations in condensin I or II will cause a change in CO frequency manifested by altered hotspot distribution. Aim 1. Do changes in chromosome structure affect CO number by altering DSBs? To determine when CO regulation occurs, I will identify the relationship between DSB formation, DSB resolution, and changes in chromosomal structure as revealed by a lengthened axis. For each experiment proposed below, I will test five condensin II subunit mutants, which we have in lab. Previously-characterized condensin I mutants will serve as a positive control and wild type animals as a negative control. I will also generate animals with mutations in both condensin I and II to uncover interactions between the two complexes. a. Measuring CO frequency. I will score six X chromosome SNPs (single nucleotide polymorphisms) in recombinant individuals generated from crosses between two divergent laboratory strains. In males, CO frequency and distribution can be ascertained along single X chromatids. Preliminary data leads me to expect increased CO frequencies in condensin II mutants, implying that condensin II limits CO formation, but a decreased CO frequency would indicate that condensin II acts to trigger CO formation. b. Measuring DSB frequency. To demonstrate that increased CO frequency is due to increased DSB frequency, I will label DSB position throughout meiosis by immunostaining with RAD-51 antibody, which marks recombination intermediates. Correlation of elevated DSB numbers with higher CO frequencies in condensin II mutants would indicate that additional DSBs provide further substrate for COs, while a lower DSB frequency would Copyright 2008 Original Author implicate involvement at the level of DSB resolution. c. Measuring chromosomal axis length. To measure axis length of X chromosomes, I will use fluorescent in situ hybridization to sequences containing the SNPs from Aim 1a. After immunostaining for DSBs and an antibody to the chromosomal axis protein HTP-3, I can trace labeled X chromosome axes through deconvolved 3D image stacks. Computationally straightening these traces with software present in the lab will allow me to measure axis length within microns and analyze DSB foci on individual X chromatids. Unlike previous lower-resolution studies, this will identify whether sub-chromosomal axis expansions correlate with increased DSB frequency in condensin mutants, demonstrating that changes in chromosome structure affect CO number by creating more DSBs. However, any change in DSB frequencies on altered axis intervals would further bolster a relationship between chromosome structure, DSB initiation, and CO resolution. Aim 2. How do condensins I and II exert effects on chromosome structure? If the condensin complexes affect higher-order chromosome structure by modifying axis length (which I will have determined in Aim 1c), they must also change where DNA attaches to the chromosome axis. To examine whether condensin mutants have these structural changes, I will use ChIP-seq (chromatin immunoprecipation sequencing) to detect the binding of REC-8, a meiosis-specific cohesin that marks DNA-axis attachment, and the axis protein HTP-3. ChIP will isolate specific DNA sequences of protein binding to be identified by high-throughput Solexa sequencing. UC Berkeley has two Solexa sequencers readily available to graduate students in my department. I will analyze REC-8 and HTP-3 binding in condensin I mutants, condensin II mutants, and the double mutant, choosing the subunit mutant conditions that show the strongest CO effect from Aim 1a; wild type animals will serve as a control. Antibodies to both proteins suitable to ChIP have been generated in the lab. If condensin mutants exhibit no change in REC-8 or HTP-3 binding, CO regulation may change axis length without affecting DNA-axis attachment. However, differential DNA-axis binding and altered axis lengths in condensin mutants will reveal a direct association between chromosome structure and CO regulation. Aim 3. What are genome-wide trends of CO in C. elegans? To determine the relationship between DSBs and their resolution into COs or NCOs, I will use microarrays to generate recombination maps for wild type and animals mutant for condensin I, II, and both, again choosing mutant conditions with the strongest CO effect. Previous recombination studies have lacked the resolution to detect NCO formation. To address this, I will use hundreds of SNPs on each chromosome that cause differential hybridization between two divergent laboratory strains, choosing markers that are reproducibly observed on high-throughput tiling arrays.9 Several studies in S. cerevisiae have utilized similar technology,10,11 but few other metazoans will prove as tractable to a genome-wide analysis as C. elegans, due to its small genome, numerous SNPs, and clonal individuals. I will define CO hotspots, and therefore haplotype blocks, using the wild type recombination map. I will also uncover, for the first time, whether NCOs have an effect on overall CO regulation in C. elegans. Additionally, if hotspot architecture changes in condensin mutants, I will have identified a chromosome- wide mode of CO regulation consistent with global control of hotspot activity. This project is fundamentally interesting because it will elucidate a conserved and universal phenomenon for generating diversity, but it will also have a significant impact on our understanding of a basic evolutionary mechanism. Condensins have the ability to exert global effects on chromosome architecture – permitting chromosome-wide communication that could explain the appearance and disappearance of CO hotspots within short spans of time. Determining the mechanism responsible for CO regulation and identifying CO hotspots will be crucial to our understanding of genome organization and evolution. All proposed research is original and of my own design. References: (1) Kauppi L et al. 2007. Prot Natl Acad Sci USA. (2) Petes TD. 2001. Nat Rev Genet. (3) Szostak JW et al. 1983. Cell. (4) Bishop DK & Zickler D. 2004. Cell. (5) Mets DM & Meyer BJ. 2008. MS in preparation. (6) Hillers KJ & Villeneuve AM. 2003. Curr Biol. (7) Tsai CJ et al. 2008. Genes Dev. (8) Chan RC et al. 2004. J Cell Bio. (9) Jones MR et al. 2008. BMC Genomics. (10) Chen SY et al. 2008. Dev Cell. (11) Mancera E et al. 2008. Nature.	0
I propose to develop and implement a new approach to quantum-light spectroscopy. I will benchmark the new approach against established classical-light techniques, namely nonlinear coherent spectroscopy, and use classical- and quantum-light spectroscopies to investigate the mechanism of circularly polarized photoluminescence from chiral perovskite thin films. Introduction: Quantum light has been the subject of research in many physics subdisciplines, but until recently has not been considered as a tool for molecular spectroscopy by physical chemists. Much of the current research on spectroscopy using quantum light has focused on using entangled photon pairs (EPPs) for nonlinear spectroscopies involving two-photon absorption1, effectively using quantum light as an analog for classical light in nonlinear applications. I propose to use an established technique in the field of quantum photonics – quantum-state tomography – in a spectroscopic application that treats both the quantum light and the light-matter interaction fully quantum mechanically. The advantage of the proposed quantum-light spectroscopy over known nonlinear ultrafast spectroscopy techniques is the ability to directly investigate the quantum mechanical nature of a material system and its dynamics. By measuring changes in the state of quantum light due to light-matter interactions, we can learn about phenomena such as entanglement of correlated species in matter and spin- orbit coupling. Chiral perovskite thin films are an exciting and important material system to investigate with this spectroscopy. Recent studies have shown that chiral perovskites act as sources for circularly polarized photoluminescence (CPL), a highly sought-after feature applicable to bioresponse imaging, 3D- LED displays, quantum computing, spintronics, and more.2 An investigation of chiral perovskite thin films by Di Nuzzo, et al. has shown that the unknown mechanism of CPL in Ruddlesden-Popper perovskite thin films does not agree with the model of Rashba spin-orbit coupling,3 the primary model used to describe spin-orbit coupling in 2D-semiconductors. I propose to use quantum-light spectroscopy, as well as two- dimensional photoluminescence (2DPL) and photo-induced absorption (2DPIA) spectroscopies, to explore the claim that the CPL of chiral perovskite thin films is due to the charge of photoexcited Wannier excitons and study the chiral symmetry transfer from optically inactive cations to excitons. Background: Quantum-state tomography is a technique used to completely characterize a quantum state by measuring its density matrix. Photonic-state tomography involves the measurement of an ensemble of polarization-entangled photon pairs to determine the full two-photon density matrix4. I propose to use the spectroscopic setup in Figure 1, in which EPPs are generated in a polarization basis via spontaneous parametric down conversion (SPDC) using a pair of BiBO crystals. The two photons, the signal and the idler, are spatially separated along the edges of the SPDC emission cone and sent down distinct optical paths. The idler photon is sent directly to a Figure 1. Proposed quantum-light spectroscopy setup polarimeter composed of a quarter wave plate (QWP), a half wave plate (HWP), and a polarizing beam splitter. The signal photon interacts with a sample before being sent to an identical polarimeter. The polarimeters project both photons onto a polarization basis defined by horizontal (H), vertical (V), diagonal (D), and right (R) polarizations. Finally, the photons are detected by single photon detectors that drive a coincidence counter. By determining the change in the full density matrix of the biphoton state, we can identify the transformation matrix associated with the light-matter interaction in the polarization basis. Previous Work: The experimental set-up in Figure 1 has been realized in the Silva lab and validated by taking tomographic measurements of several prepared biphoton states of the form and HH VV c |HH〉+c |VV〉 with a quarter waveplate (QWP) in the place of a sample. In the defined polarization basis, a QWP performs a unitary transformation defined by a 90° rotation of the Poincaré sphere about an axis dependent on the waveplate angle. Measurements taken with our experimental setup of a QWP at 0°, 22.5°, and 45° indicate such a transformation, corresponding to a rotation without loss of purity in the biphoton state. The proposed experimental set up is also easily combined with other spectroscopic techniques familiar to the Silva lab, such as pump-probe spectroscopy. Preliminary data taken with a similar experimental set up in which the sample is excited by a pump laser has shown that the quantum state of the probe biphoton pair is altered by scattering off of a triplet-triplet intermediate state involved in singlet fission. Through experimental measurements of bis(triisopropylsilylethynyl)tetracene (TIPS-tetracene) samples obtained through collaboration with Prof. John Anthony at the University of Kentucky and theoretical development done in collaboration with Prof. Eric Bittner at the University of Houston, we have studied the nature of the long-lived correlated triplet pair of TIPS-tetracene, which has been assumed to be entangled in the spin basis without experimental evidence.5 The experimental data, along with simulations of the TIPS-tetracene system and the many-body scattering theory of the biexciton probe have all contributed to a manuscript to be submitted to the Journal of Physical Chemistry C. Aim I – Theoretical comparison of quantum-light and 2D spectroscopies: The implementation of quantum-light spectroscopy is nontrivial; working in the photon counting regime and with ensemble measurements requires precise environmental control and careful error analysis. It is necessary to show the proposed quantum-light spectroscopy will yield information that is inaccessible with known spectroscopic techniques and worth the challenge. I intend to do this theoretically using Lindbladian models to predict the data that can (and cannot) be obtained for chiral perovskite systems comparatively with 2D and quantum-light spectroscopies as shown in our paper published on ArXiv6 for TIPS-tetracene. Aim II – Experimental investigation of CPL of chiral perovskite thin films: In concert with the proposed theoretical development, I intend to interrogate the mechanism by which chiral Ruddlesden- Popper perovskite thin films impart circular polarization on unpolarized incident light using 2DPL, 2DPIA, as well as the proposed quantum-light spectroscopy. I plan to synthesize the thin films with the help of Esteban Rojas-Gatjens, a groupmate co-advised by Prof. Seth Marder. To my knowledge, these thin films have not been characterized with any 2D-spectroscopy which could probe their exciton dynamics. Intellectual Merit: I am ideally positioned to do this research. As part of the Silva group, I have access to a full toolbox of 2D- and ultrafast spectroscopic techniques which are regularly used by our group to investigate perovskites. Moreover, the project will benefit from continued collaboration with Prof. Eric Bittner and future collaboration with Andrei Piryatinski at the Center for Nonlinear Studies at Los Alamos National Laboratory. The proposed research has potential to establish the experimental and theoretical basis for a new form of quantum-light spectroscopy while also learning about the fundamental mechanism of CPL from chiral perovskites, which could improve understanding of spin-orbit coupling in chiral materials. Broader Impacts: Understanding the chiroptical properties of chiral perovskite thin films would contribute to many possible applications, as cited above. However, chiral perovskite thin films are just one of many interesting material systems that the proposed quantum-light spectroscopy can be used to study. Development of a new quantum-light spectroscopy would expand the capacity of spectroscopists to study fundamental quantum mechanical phenomena. It could be used to characterize gates used in quantum computation, singlet fission materials, spin-based memory devices, and so on. [1] S. Mukamel, et al. J. Phys. B., 53, 072002 (2020). [2] G. Long, et al. Nat. Rev. Mat., 5, 423-439 (2020). [3] D. Di Nuzzo, et al. ACS Nano, 14, 7610-7616, (2020). [4] J. Altepeter, et al. Adv. Atom. Mol. Opt. Phys, 52, 105-159 (2005). [5] C. Yong, et al. Nat. Comm., 8, 15953 (2017). [6] arXiv:1909.12869	0
framework for a planet’s formation, evolution into its present state, and past and present geophysical properties such as magnetic fields and atmospheric conditions. In this era of prolific exoplanet discovery, the quest to investigate planetary interiors and surface conditions is more pressing than ever. With the growing number of exoplanets ranging in size from super-Earths to sub-Neptunes, and the omnipresent goal of “finding a new Earth”, it is becoming evident we need to concentrate our studies on such planets. Currently, radial velocity and transit methods used to detect exoplanets give mass and radius data for exoplanets but offer no compositional information. Comparing the mass and radii of exoplanets with mass- radius relationships of pure materials such as iron, silicates, and ice, and stoichiometric mixture thereof, offer a glimpse into their plausible bulk compositions [2-4]. However, attempts to infer bulk composition have resulted in degeneracy with many interior composition combinations fitting mass and radius values for a particular exoplanet. Many models assume planets are fully differentiated, yet previous works using density functional molecular dynamics (DFT-MD) simulations at high pressures and temperatures have predicted deviations from this model. For example, the miscibility of H O with H and He [5] and miscibility 2 of Fe and MgO [6]. Contrary to traditional models, we infer “fuzzy layering” if material is miscible at boundary conditions, which would result in the gradual mixing of heavier elements into the upper, less dense layers. DFT-MD simulations are a powerful tool in predicting the equation of state (EOS) of a wide variety of planetary materials and mixtures at conditions that are difficult to achieve empirically. Hypothesis. Some super-Earth and sub-Neptune exoplanets, termed waterworlds, contain a significant amount of water (H O) ice overlaying a magnesium silicate interior [7,8]. The stability of such a stratified 2 internal structure depends on whether these simplified two-layer models reflect realistic water world structures. Instead of relying on static two-layered models, I am motivated to explore the dynamics of material mixing at the magnesium silicate-ice boundary layers under P-T conditions relevant to the interior conditions of waterworlds. Understanding whether these two materials are miscible will help us better resolve the internal composition and stratification of water worlds. I propose to study the miscibility of a common high-pressure planetary magnesium silicate, enstatite (MgSiO ) [9], and high-pressure 3 water (H O) ice using DFT-MD. MgSiO will be referred to as rock and ice is assumed to be water ice for 2 3 the remainder of this proposal. Research Plan. (Research Goal 1: Building Rock-Ice Systems and running DFT Simulations) I will build the rock-ice systems and equilibrate each to a respective pressure in gigapascals (GPa) (Table 1). Table 1. Systems with MgSiO and H O crystal phases, space groups, and initial system pressure 3 2 System MgSiO phase Space group (MgSiO ) H O phase Space group (H O) P[GPa] x 3 3 2 2 1 ppv Cmcm [63] ice X Pn-3m [224] 120 0.29 2 pv Pnma [62] ice X Pn-3m [224] 60 0.29 3 pv Pnma [62] ice VIII I41/amd [141] 30 0.26 The crystal phases chosen for each material present at 0K were the most stable structures at each respective pressure (Table 1.). I will run simulations on each system using a canonical ensemble (constant number of particles N, constant volume V, and constant temperature T) increasing the temperature of each system from 500 K to 8000 K with the Nosé-Hoover thermostat [10]. I will perform simulations in 500 K increments in a “heat- until-mixes” approach, similar to the “heat-until-melts” method [11]. Although this approach is prone to overestimating melting temperatures, my goal is to calculate the upper bound P-T conditions for rock-ice mixing which will be accomplished with my MD. I describe each system by its ice to rock mass ratio [𝑚 /(𝑚 +𝑚 )], for example, System 1 has an ice to 𝐻2𝑜 𝐻2𝑜 𝑀𝑔𝑆𝑖𝑂3 rock mass ratio of 0.29. Then to investigate in which proportions rock and ice will mix I will simulate additional systems with ice mass ratios of 0.29 and 0.20. I will accomplish this by increasing the number of rock molecules while keeping the number of ice molecules constant. If the two materials spontaneously mix during the simulations, I will know rock and ice are miscible at this temperature. Preliminary results from my MD simulations of System 1, run at 8000 K (Fig 1.) show exciting, novel results of miscibility. (Research Goal 2: Determining Miscibility) An efficient way to detect mixing is to analyze how far atomic species move from their original positions by calculating their mean squared displacement (MSD). When the diffusion coefficient for all species is above zero, I will consider the system fluid. However, I will not know based on MSD alone whether the atoms crossed the rock-ice interface. The system could contain molten rock and water which remain immiscible instead of forming a homogeneous mixture. Therefore, I will also visualize each trajectory and verify that diffusion occurs across the boundary. My final method for confirming miscibility is to calculate the radial distribution functions (RDF) of magnesium (Mg) and silicon (Si) versus the oxygens (O) in MgSiO , termed rock oxygens, and oxygens associated 3 with ice, termed ice oxygens. My goal is to show that Mg and Si lose coordination with the rock oxygens and interact with the ice oxygens. For example, when I plot Mg−O and Mg−O , at the same temperature, rock ice if rock and ice are miscible, their radial distribution curves should overlap. Intellectual Merit. In addition to working with my Ph.D. advisor, Dr. Burkhard Militzer at U.C. Berkeley, I will collaborate with Dr. Sarah Stewart, a professor in the Earth and Planetary Science Department at U.C. Davis, to perform dynamic smoothed particle hydrodynamic (SPH) simulations of colliding planetary bodies. This will give insight into whether these large impact events produce the conditions necessary for material mixing. It is important to determine post impact conditions because giant impacts govern an important stage of planet formation, mold their interiors, and drive geophysical properties [12]. Multicomponent EOSs of material mixing will shift the planetary science community’s focus from static planetary models, where fully differentiated layers are modeled, to dynamic ones which include chemistry deep within the planet. If we neglect the presence of “fuzzy layers” within planets, we may miss key planetary properties such as its thermal evolution and magnetic field generation, which influence other properties such as tectonics, outgassing, dynamics, and volcanism. I plan to continue investigating rock-ice miscibility by considering other rocky material, such as Mg SiO or MgO with H O, and exploring lower 2 4 2 pressure regimes [8]. Moreover, provided that the necessary conditions are reached, I will further my research to elucidate whether a homogeneously mixed rock-ice layer could persist over long periods of time and even become stably stratified within water worlds. This will affect overall heat flow throughout the planet which will help us better understand the evolution of water worlds. Broader Impacts. My proposal has applications in a diverse range of disciplines such as condensed matter physics, high energy density physics, geochemistry, and geophysics. I will publish my work in journals (e.g. PNAS), present at conferences (e.g. AGU and APS), and most importantly continue outreach by presenting my research at local, public seminars (e.g. BASIS, SLAM, and Compass Lectures). I spent my first summer as a graduate student volunteering at two workshops recruiting students to pursue graduate school in planetary science. I also began tutoring environmental sciences at San Quentin Prison which helps keep me informed on challenges facing our most at risk communities and how to aid in their success as aspiring geoscientists. After only one year in graduate school, I have already helped form the first Unlearning Racism in the Geosciences (URGE) pod at Berkeley. We will present our pod’s work of integrating URGE deliverables into the Berkeley EPS department-level strategic plan for enhancing diversity at AGU 2021. The NSF fellowship will allow me to produce and share my findings with my peers as well as a general audience, increase equity in my field, and recruit the next generation of geoscientists. References. [1] B. J. Fulton et al. The Astronomical Journal 154, 109 (2017). [2] A. Vazan et al. arXiv preprint arXiv:2011.00602 (2020). [3] O. Shah et al. Astronomy & Astrophysics 646, A162 (2021). [4] S. Seager et al. The Astrophysical Journal 669, 1279 (2007). [5] F. Soubiran et al. The Astrophysical Journal 806, 228 (2015). [6] S.M. Wahl et al. Earth and Planetary Science Letters 410, 25 (2015). [7] M.S Marley et al. Journal of Geophysical Research 100,348 (1995). [8] T. Kim et al. Nature Astronomy, 5, 815-821 (2021). [9] T Duffy et al. Front. Earth Science 7, 23 (2019). [10] N. Shuichi Progress of Theoretical Physics Supplement 103, 1 (1991). [11] G. Robert et al. Physical Review B, 82, 104118 (2010). [12] P.J. Carter et al. Journal of Geophysical Research: Planets 125, 1 (2020).	0
Assessing Heterogeneity in Organic Municipal Solid Waste Across City-Scales for Optimized Urban Biogas Production Keywords: biogas, sustainable development, urbanization, waste management, OFMSW Hypothesis: Biogas projects have found success in supplying renewable energy for niche markets with homogenous waste streams, but are limited by heterogeneous waste streams at the urban scale. Disaggregating waste streams to homogenize anaerobic digestion feedstocks will aid stability of biogas production at the urban scale. Introduction: Bangkok currently produces one of the highest municipal solid waste generation rates of megacities within the developing world, at over 11,000 tons per day1. The majority of Bangkok’s organic fraction of municipal solid waste (OFMSW) is landfilled, with adverse impacts on both public health and the environment through degradation of water resources and large releases of the potent greenhouse gas methane. One commonly accepted method for management of the OFMSW is anaerobic digestion (AD). Anaerobic digestion of OFMSW has numerous benefits: voluminous production of biogas (a biogenic gas that may be combusted for electricity and heat production), reduction of landfilled waste volume, reduction of methane emissions, and production of a high-quality organic fertilizer by- product2. However, variations in biochemical composition of organic waste streams largely dictate the stability of biogas production, as heterogeneities in feedstocks can cause inhibition of the microbiological processes that produce biogas3,4. For example, significant differences in moisture content between two areas may necessitate the implementation of different AD technologies, such as wet, dry, or a wet-dry combination of AD. Additionally, OFMSW collection infrastructures can be complicated and expensive due to waste originating from numerous sources over large spatial areas. Understanding how generation of OFMSW varies over urban to exurban spatial scales will better inform strategic homogenization of AD feedstock waste streams, more effective collection infrastructure, and appropriate siting of future biogas production plants for the sustainable management of OFMSW. Research Plan: My intended research will fundamentally address the following questions: 1. Do urban “pockets” exist in which municipal solid waste is predominantly composed of organic, digestible waste? 2. Can waste collection infrastructure and waste facility siting be restructured to better reflect spatial variations in OFMSW generation? Methods: I will combine spatial mapping, waste-transport charts, and waste sampling to assess how generation of organic waste is distributed over Bangkok’s urban environment. 1. Delineate urban, suburban, and rural-urban fringe (RUF) zones. I will use Geographic Information Systems (GIS) to demarcate urban, suburban, and RUF zones of Bangkok based on census data, land use maps, and aerial photography5. The demarcation of the three urban zones will inform my in-field waste sampling during the summer of 2015. 2. Identify waste management plants/landfills that collect waste within each of the above-listed zones, and sample waste from May – August, 2015. Municipal solid waste in Bangkok is not source separated, thus I will conduct a waste composition study under ISO 14001 standard6. Depending on the waste center, I will either collect samples from waste screens and grinding operations, or will hand sort the waste to collect samples of the organic fraction. I will then assess the waste sample for moisture content and biochemical composition through local university facilities7. I Graduate Research Proposal NSF GRFP will collect waste samples two times per day from ten replicates within each urban zone to best account for expected high variability in waste structure. 3. Identify appropriate sites for siting of future AD facilities. By comparing samples of organic waste with existing waste collection routes, inferences can be made as to whether or not organization of waste management facilities are appropriate for the waste composition originating from Bangkok. I expect that city areas with high densities of malls (often with large food courts) and food markets will have disproportionately high food waste suitable for AD. 4. Model the potential biogas production based on computer simulation of biogas production, and scale the waste streams for their representative urban areas to the city scale. I will input the elemental compositions of organic wastes into the Anaerobic Digestion Model 1 (ADM1) computer simulation to gain rough estimates of biogas productions8. I will then scale the production rates from each urban area type to its full city-wide extent to estimate Bangkok’s biogas production potential. 5. Model future urban development and its implications for organic waste based on the Bangkok Development Plan released in 2013. Similar to step 4, I will use projections of future shifts in urban environment (e.g., from suburban to urban) to model future growth in urban waste/biogas. Research collaborations: Through Yale University’s Urban Resources Initiative (URI), I will conduct a pilot project to assess neighborhood-wide waste streams during the spring semester of 2015. Working with URI will give me valuable experience and insight into potential pitfalls that may arise during my summer data collection, and will allow me to extend my research to New Haven’s local community. Additionally, my previous research collaborations through the Joint Graduate School of Energy and Environment in Bangkok will afford me access to university facilities for biochemical analyses and support from Thai professors currently involved with waste-to-energy projects. Intellectual merit: My proposed research aims to bridge the gap between the use of AD technology in the developed and the developing world. Understanding how waste-streams respond to urban growth will allow city planners to best implement future waste management plans for developing urban environments in both the developing and developed world – such as the U.S. My intended research will contribute to the broader knowledge on waste management through the goal of a peer-reviewed publication by the end of my second year, as well as presentations of findings at university-based and international conferences. Broader impacts: Capitalizing on OFMSW for production of biogas is a comprehensive sustainable development strategy that tackles the increasing challenges of managing waste, providing stable electricity, and mitigating greenhouse gas emissions. The findings of the research will be particularly valuable in developing urban environments, for example those in India or sub-Saharan Africa, in which putrefying organic waste directly contributes to public health concerns and ecological damage. Furthermore, generation of electricity from biogas may become significant for assisting intermittent renewable energies, such as wind and solar photovoltaics, in future provisioning of base load electricity supply. This work will directly aid local professors, students and urban planners, as few comprehensive waste-structure studies of Bangkok currently exist in the literature. References: 1 Udomsri et al. 2011. Energy for Sustainable Development 15: 355-364. 2 Wellinger, A. et al. 2013. The Biogas Handbook. 3 Curry and Pillay. 2012. Renewable Energy. 41: 200-209. 4 Browne, J.D. et al. 2013. Applied Energy 128: 307-314. 5 Pryor, R.J. 1969. Geografiska Annaler. Series B, Human Geography. 51:33-38. 6 ISO 14001. 2004. Environmental Management. 7 Zhang, R. et al. 2007. Bioresource Technology 98: 929-935. 8 Batstone, D.J. et al. 2002. Water Science Technology 45: 65-73.	0
I propose to study planetary system structures around low-mass stars by conducting radial velocity (RV) observations of known single-planet transiting systems observed by Kepler/K2 and the Transiting Exoplanet Survey Satellite (TESS). RVs and spectroscopy will be completed with MAROON-X1, an instrument with which I will have guaranteed clear-sky observing time as a student at the University of Chicago. I will address if there is a separate population of single planet low-mass stellar systems or if they have an inclined inner-most planet. This study will result in the publication of compelling individual systems as they are identified and a full statistical analysis once the full sample has been obtained. Background & Research Proposal: Planet formation models predict planetary systems form in the same orbital plane. However, 𝚹=15° there are several systems2 that suggest the inner-most planet may be inclined by a significant degree. Heavily inclined planets Figure 1: The inner-most planet would go undetected during exoplanet transit surveys (observing (black) lies 15° off the plane of the stellar flux over time), Figure 1. The analysis of Kepler/K2 system and does not transit, while an outer planet (purple) does. transiting exoplanet system yield an overabundance of single transiting planets, Figure 2.3 It is possible the single transits detected are the inner-most inclined planet and the remaining planets have a different orientation. The overabundance of single transiting planets is most notable around low-mass M-dwarf stars (70% of the Milky Way population). M-dwarfs are smaller and cooler than the Sun (< 0.7 M , T < 4000K). With nearly 300 current planet detections from NASA’s Kepler/K2 missions, Sun eff it is estimated each M-dwarf hosts at least 2 small planets (< 4 R ); TESS is predicted to find 500 additional planets Earth orbiting M-dwarfs.4 By studying planets around M dwarfs, I will be studying the dominant environment of planet formation in our galaxy. Follow-up RV measurements of planetary systems will allow us to identify planets that do not transit. I will be searching for non-transiting companions to known Figure 2: Comparison of the Kepler multi-planet yield (blue) to the best-fit transiting planets to understand the population of planetary models (red). Models under-predict systems with mutual inclinations. By conducting a large RV the number of singly transiting survey of transiting planets with the MAROON-X systems and over-predict the number of multiple transiting systems. spectrograph, under construction at the University of Chicago and to be commissioned on the Gemini North observatory in early 2019, I will answer the following questions: Do low-mass stars have 1 Seifahrt, A., Bean, J. L., Stürmer, J., et al. 2016 “Development and construction of MAROON-X,” in [Ground- based and Airborne Instrumentation for Astronomy VI], Proc. SPIE 9908, 990845 2 Montet, B. T., Morton, T. D., Foreman-Mackey, D., et al. 2015, ApJ, 809, 25; Rodriguez, J. E., Becker, J. C., Eastman, J. D., et al. 2018, ArXiv e-prints, arXiv:1806.08368 3 Ballard S. and Johnson J. A. 2016 ApJ 816 66 4 Barclay, T., Pepper, J., & Quintana, E. V. 2018, ArXiv e-prints,arXiv:1804.05050 !1 significantly inclined inner-most planets? If so, what physical mechanisms are responsible for causing the inclination shift? If not, why do these systems only host one planet? I will compile an initial catalog of M-dwarf systems from the Exoplanet Archive and the TESS Input Catalog prior to data release. Once data are available, I will run the TESS data through my developed software, ELLIE, to search for planet candidates. I will vet candidates, prioritizing stars in my catalog, and determine RV follow-up feasibility based on predicted planet radius and levels of stellar activity identified in the light curve. This process will be repeated until July, 2020, when the entire sky observable by MAROON-X has been observed by TESS. I am choosing MAROON-X to conduct this study because it is optimized for the red optical (peak emission of low-mass stars) and obtains spectra of the stars. By obtaining spectra of stars, I will be able to calculate accurate stellar parameters, following methods developed during my internship at NASA Goddard Space Flight Center. Additionally, MAROON-X has a predicted precision of <1 m/s, allowing for potential detections down to 1 M . By pursuing my PhD at Earth the University of Chicago with the MAROON-X instrument science team, I am guaranteed to obtain data, regardless of bad scheduled nights. I propose to conduct 5 full nights of observing per semester for 3 years, for a total of 240 observing hours. I will obtain mass and density measurements of transit detected planets in addition to identifying new planets. Measuring both parameters will prove essential when choosing planets for atmospheric characterization with the James Webb Space Telescope, currently set to launch in 2021. Obtaining both a radius (via the transit) and mass (via RV) measurements will help us better understand the compositions of relatively nearby planets, improving our understanding of planet formation around the most common stars in the Milky Way. Intellectual Merit & Broader Impacts: My years of previous research experience, including publishing papers and giving talks, have prepared me well for the challenges which lie ahead. I have the necessary knowledge of Python and other tools necessary to complete the project proposed here. I have experience efficiently and accurately analyzing large data sets5 and observations6, and vetting planet candidates.7 I am ready to undertake the project presented here. My current outreach experiences have prepared me for future plans to speak about my research at local K-12 schools. I plan on working with local teachers to improve STEM education and educator development at the elementary/middle school level by creating educational in-class astronomy activities as well as create a website that follows my work that teachers will be able to use in the classroom. Interactive tools will include how to create light curves and how the presence of different objects will change a light curve. I will attend the National Science Teachers Association national conference to give a demonstration on how to use my website. I also plan on taking advantage of events hosted in Chicago. Such events include Astronomy on Tap: Chicago, which brings the universe into a local Chicago pub, and Soapbox Science, which promotes women in STEM by placing them on a box in highly populated areas in downtown Chicago to talk about their work. 5 Feinstein, A. D., Montet, B. T., et al (in prep) 6 Feinstein, A. D., Schlieder, J. E., Livingston, J. H., et al, 2018, AJ (submitted) 7 Liang, Y., Crossfield, I. J. M., Schlieder, J. E., et al, 2018, ApJ 156 22; Crossfield, I. J. M., Guerrero, N., David, T., et al, 2018, ArXiv e-prints, arXiv:1806.03127 !2	0
Question Answering Alvin Wan Keywords: visual question answering, attention, computer vision I. Introduction II. Proposition Visual Question Answering (VQA) is a task To broaden the scope of information that fuses both computer vision and natural incorporated, I propose running object language processing, where a computer detection on the original image to extract all must answer questions about a provided identifiable entities; this can be achieved image. State-of-the-art approaches leverage with a pretrained model, such as YOLO9000. attention networks in deep learning, which The set of attributes obtained from help to focus the neural network on portions captioning the image as well as describing of the image. In particular, these approaches these objects could then be fed into external see improved performance when applying information queries. attention to both the question and the image Provided that attention has improved 1. Further improvements are supported by performance when applied to questions and use of external knowledge bases. images alike, coupled with the fact that leveraging external information likewise Related Work improves performance, I propose applying attention to external information as well. Attention networks can currently leverage Once performance has been verified, I then external information by captioning the treat previous background in the sequence of image, extracting a set of attributes, and questions, as external information. This then supplanting the final answer-generator with allows the VQA bot to utilize previous summaries of facts related to those context. attributes. However, what if the provided In preparation for real-time inference and caption already ignores important context in memory constraints, we will additionally the image? A caption may ignore a beach consider size and space-efficient methods for ball in the background, but a pointed accomplishing these improvements, so that question may not. What’s more--what if the bot is able to build conversation. additional context in the conversation is needed? The user may ask for “the number Evaluation of people wearing jeans” and “of those ​ people, the number of people wearing red A number of public benchmarks provide ​ shirts”. I propose broadening the scope of adequate testbeds for both more higher-level information incorporated, when querying for processing and more rote, simple questions. external information, and applying attention The COCO-QA and DAQUAR benchmarks to all the data retrieved, to build more would serve this purpose, additionally stateful, conversational VQA bots. providing comparison with existing state-of-the-art methods. Other benchmarks that address subsets of the VQA task, such as image captioning, exist for intermediate evaluation. 1 Lu et. al. “Hierarchical Question-Image Co-Attention for Visual Question Answering“ (2016) III. Research Plan architecture and information retrieval both will be optimized by minimizing Year I: Question Attention, Broadened fully-connected layers, using convolutional Information Retrieval layer reductions, and rigorous memory profiling. Attention networks should be Objective: Test a number of varying trained and validated as part of an online implementations for raw information-based learning algorithm. attention. Methodology: Employ object detection and IV. Conclusion image tagging to increase the quantity of information queries. We can use both Intellectual Merit - Should this work ​ shallow and deep featurizations to produce a succeed in proving an effective method of rich set of data descriptors. Then, apply information distillation for incorporation, attention to external sources to determine visual question answering as a field could both quality and relevance of external take several steps forward in studying not information--we can begin by modeling just computer vision for images, or natural relevance as a binary classification problem, language processing for questions, but then by regressing to continuous-valued distributed systems for large-scale relevance. information retrieval and processing. Year II: Stateful Visual Question Broader Impacts - One application of an ​ Answering improved bot, with enhanced visual question answering capabilities, is personal assistants Objective: Utilize previous conversation for the visually-impaired. Considering context to aid in question answering. computer-aided descriptions are substitutes Methodology: Use attention networks to for the user’s vision, the quality of visual determine what types of external information question answering is highly-valued. are needed, whether the information However, the challenge is more complex pertains to objects, an intangible quality, or than simply answering a number of isolated previous conversation context. Attention questions; the bot must be able to leverage networks should successfully discern the conversation context itself i.e., previous relevance of samples collected prior to questions, the user’s behavioral tendencies, learning. This work may extend to year and finally, external information that may three, as various modalities of external make answers more accurate--for example, information may require significantly more if a price tag is shown but the store-wide data processing and filtering. discount is announced online. Thus, with more refined methods of incorporating Year III: Towards more Conversational outside data, the bot could see major Bots improvements in its value for the visually-impaired. Objective: Run such evaluation and answering in real-time. Methodology: This interest directly stems from my current work in reduced model sizes and real-time inference for self-driving cars. In a similar manner, we cannot always rely on network connections for high-level, fast-paced conversation. Neural network	0
gene expression. Transcription factor (TF) activity itself is difficult to measure experimentally in high-throughput; however, many insights can be gained from applying statistical methods to infer activity from the expression of TF target genes. This indirect quantification of TF activity has been made possible by gene expression microarrays, which simultaneously profile thousands of genes. There has been extensive research on how to test for differential expression of a priori defined gene sets such as TF target genes.1 One method recently developed in the Kleinstein lab, Quantitative Set Analysis of Gene Expression (QuSAGE), is unique in that it produces a probability density function for each set by convolution of the expression profiles of individual genes.2 Still, the efficacy of all these methods relies on the chosen TF target gene set. The adjacent figure shows how the NFkB activity inferred after stimulation with TNF (an inducer of NFkB) is highly dependent on the choice of gene set. Thus, there is a need to improve methods for generating and refining such sets. There are numerous ways to generate putative TF target gene sets both computationally and experimentally. Inferred NFkB Activity Computational predictions provide candidate targets by scanning for a specific binding motif in promoter sequences genome-wide. However, this method is known to generate many false-positives. Protein-DNA binding experiments (e.g. ChIP-Seq, ChIP-ChIP) provide experimental evidence for TF-DNA interactions. However, there are a large number of binding interactions observed, many not in the promoter of known genes, and these interactions may be specific to the cell line used. In addition, the accuracy of both of these prediction methods suffers because the occurrence of a TF binding site or the actual binding of a TF to a gene promoter does not necessarily imply transcriptional regulation. Networks from pathway databases (e.g. KEGG) provide some additional information about which genes interact at a transcriptional level. However, the number of interactions in pathways is limited. Currently each TF is associated with a single target gene set. This is problematic because, in reality, TF target genes depend on the cellular and environmental context of the cell. To infer TF activity more accurately, candidate target gene sets from many sources can be refined to include only the genes under a TF's control in the specific context being studied. I will develop a method for generating context-specific transcription factor target gene sets (Aim 1), and apply these gene sets to infer transcription factor activity during infection and vaccination responses (Aim 2). Aim 1: Develop a method for generating context-specific TF target gene sets. I will begin with a large set of proposed candidate TF target genes and then to utilize co- expression patterns from gene expression data to select candidate genes having a similar gene ytivitcA FT 1.5 x 1.0 x 0.5 x 0.0 x x Set 1 Set 2 Set 3 Set 5 Set 4 (149 genes) (206 genes) (301 genes) (2757 genes) (96 genes) 100.0− 500.0− 10.0− 50.0− 1.0− 1 1.0 50.0 10.0 500.0 100.0 TF Target Gene Sets expression pattern. One limitation of current approaches is that each gene is either a candidate of a TF, or it is not. I plan to integrate multiple information sources including computationally predicted binding sites from motif scanning algorithms, protein-DNA binding data, and pathway information to compute a prior probability for each gene being a candidate target of each TF. This prior probability represents the strength of the evidence for a certain gene being a target of a certain TF. Because the relative importance of each of the data sources is not known, I will estimate them as parameters in a Bayesian network. I hypothesize that this extra information will improve correct identification of TF targets. The method will allow for overlap of genes between target sets but I will explore whether this is necessary, since dependence between target sets is often problematic for quantification of TF activity. The proposed method will build on the framework proposed by Fertig et al.3 Some TFs are transcriptionally regulated themselves, allowing for estimation of activity directly from gene expression measurements. I will evaluate my method by comparing the activities inferred from the proposed and published methods for these transcriptionally regulated TFs. Aim 2: Apply these gene sets to infer transcription factor activity during infection and vaccination responses. One natural application of these TF target gene sets is to infer TF activity. Thus, I plan to apply the developed method to specific time-series gene expression data sets of influenza infection and vaccination responses. I have access to these data through the NIAID funded Program for Research on Immune Modeling and Experimentation (PRiME) and the Human Immunology Project Consortium (HIPC). In the case of the influenza infection data, the different contexts correspond to four different strains of in vitro influenza infection; while in the case of vaccination response data, the contexts correspond to vaccine responders or non-responders. Generating context-specific gene sets will allow us to answer two fundamental questions: Are there changes in which genes are regulated by certain TFs across contexts? And how do the activities of TFs differ between contexts? I will answer the first question by applying differential network analysis, a method to identify how the regulatory network is rewired in different contexts. To answer the second question, I will infer activity of each gene set using QuSAGE to find quantitative differences in TF activity between contexts. Significance and Broader Impacts TFs are key regulators in development and disease. The ability to better characterize TF activity thus has implications for understanding disease states and the mechanisms underlying development. The proposed integrative approach to generate context-specific TF target gene sets will improve understanding of transcriptional regulation and allow for a more accurate inference of TF activity. 1. Hung, J.-H., Yang, T.-H., Hu, Z., Weng, Z. & DeLisi, C. Gene set enrichment analysis: performance evaluation and usage guidelines. Brief. Bioinform. 13, 281–91 (2012). 2. Yaari, G., Bolen, C. R., Thakar, J. & Kleinstein, S. H. Quantitative set analysis for gene expression: a method to quantify gene set differential expression including gene-gene correlations. Nucleic Acids Res. 41, e170 (2013). 3. Fertig, E. J., Favorov, A. V & Ochs, M. F. Identifying context-specific transcription factor targets from prior knowledge and gene expression data. IEEE Trans. Nanobioscience 12, 142–9 (2013).	0
billion breeding birds11. Among the species that showed some of the steepest declines were migratory shorebirds, one of which is the Lesser Yellowlegs (LEYE, Tringa flavipes). LEYE, a once common bird that breeds in the boreal forest, has declined by 80% range-wide since 19661,3 and is estimated to lose an additional 50% of its global population within 11 years9. As a result, LEYE has been designated as federally threatened in Canada and a species of high conservation concern in the U.S.9 This species likely encounters multiple threats during its 8000-mile migration journey3, but agricultural practices in one of their most critical stopover regions, the Prairie Pothole Region (PPR) have the potential to impact much of the breeding population3. Reductions in survival due to exposure to agricultural insecticides in the PPR is one novel hypothesis that has been proposed to explain why many shorebirds in North America have declined, including LEYE. However, this hypothesis has not been thoroughly explored. Investigating population-level threats to rapidly declining species like LEYE is a critical conservation priority. The migratory period may be the most critical to annual survival due to the high energetic demands that if not fulfilled can lead to reduced survival and reproductive success10. However, to ensure a timely arrival at breeding and wintering sites, migratory birds must balance their time spent refueling at stopover sites with their migration speed10. The optimal bird migration theory predicts that migrants constrained by time should adjust their stopover duration to their refueling rate, and thus minimize time spent on migration to maximize their fitness2. Research has shown that individuals with low refueling rates depart later from their stopover sites relative to individuals with higher refueling rates, indicating that birds wait until they reach a threshold of fuel stores before departing10. This suggests that the quality of stopover habitat affects the decision of when to leave a stopover site, which is of critical importance for migration success. During migration, shorebirds are exposed to neonicotinoids6, the most widely used class of insecticides in the world, which pose significant risks to birds and other wildlife2,4. Neonicotinoids cause impaired immune function, rapid reduction in food consumption, and lower reproductive success, which can result in greater energetic demand, reduced fat stores, delayed migration and low survival1,4. Because migration delays can carry over to affect survival and reproduction1, neonicotinoids have the potential to impose population-level impacts. Although their adverse impacts have been established in songbirds1,4, we have little information regarding their effect on shorebirds, highlighting a critical information gap. In a recent study, GPS transmitters were deployed on over 100 LEYE in Alaska and Canada3. Of the birds that bred west of James Bay, Ontario, 90% stopped in the PPR to refuel during their migration to South America, with stopover duration times varying from a few days to over a month, indicating the importance of this region during migration. High presence of neonicotinoids has been reported in these prairie wetlands and agricultural fields5, which are important foraging habitats for migrating shorebirds. Proposed Research: Using the optimal bird migration theory as a framework, I will investigate the threat of neonicotinoids on the fitness and migration of fourteen shorebird species of high conservation concern9 that heavily rely on the PPR. This study will investigate a potential contributor to the observed population declines of shorebirds and will help guide on-the-ground management decisions for agricultural solutions. Hypothesis: Migrating shorebirds with high plasma concentrations of neonicotinoids will be physiologically and behaviorally impaired relative to birds with low concentrations. Similar to what has been observed in studies of captive birds1, I predict that wild shorebirds with high neonicotinoid concentrations will exhibit: A) lower plasma triglyceride and higher uric acid levels, indicating lower fueling rates and fat deposition, B) poorer body condition (measured by body mass and fat scores), C) reduced foraging behavior, D) prolonged migration stopovers, and E) later migration departure dates. Research Plan: To establish an environmental gradient in pesticide contamination, I will pre-screen wetlands by measuring neonicotinoid concentrations in water samples. At sites with low and high concentrations of this pesticide, I will capture LEYE and thirteen other shorebird species, collect blood samples, and measure body mass and fat over two fall and two spring migration seasons. I will measure the concentrations of neonicotinoids in bird plasma as well as key metabolites in blood using cutting- edge LC-MS/MS techniques8. Prediction A: I will measure plasma concentrations of triglycerides and uric acid and correlate them to plasma neonicotinoid concentrations7, thereby testing for a link between pesticides and fuel deposition rates. Prediction B: I will compare body mass and fat scores of birds with high, moderate, and low neonicotinoid concentrations to better understand how neonicotinoids affect body condition. Prediction C: I will conduct behavioral surveys on shorebirds at high and low contamination wetlands to determine if there is a relationship between neonicotinoid exposure and foraging behavior. After randomly selecting an individual, I will record the length of time spent in different behavior categories (foraging, resting, etc.) for a duration of 5 minutes. This will be repeated for 10 individuals per wetland. To account for time and weather, I will conduct surveys in the morning and will record temperature, wind, and cloud cover. Predictions D & E: To understand if neonicotinoids are impairing migratory ability and causing migration delays, I will deploy Lotek PinPoint GPS transmitters that will allow me to track the migration, departure dates, and stopover durations of birds with varying levels of neonicotinoid exposure. The results of this study will provide critical information on how environmental contaminants interfere with optimal migration. To minimize confounding factors, I will only capture adults, and will stratify results by sex, species, and migration season. Facilities & Mentorship: I have two mentors: Dr. Christy Morrissey at the University of Saskatchewan and Dr. Courtney Conway at the University of Idaho (UI) where I will matriculate. Dr. Morrissey is a global leader in avian ecotoxicology and has developed novel and extremely sensitive methods for neonicotinoid analysis8. Dr. Conway is a renowned expert in ecology and migration of birds. Intellectual Merit: Regional efforts to study neonicotinoids in songbirds1 and wetlands6 in the PPR are ongoing and our project expands on this by investigating the effects of neonicotinoids on shorebird health, a novel yet timely research topic. This project would build upon an existing and growing partnership among 8 state agencies, federal agencies, South American agencies, and universities in both the U.S. and Canada, as well as farmers and landowners in both countries. My findings will advance the fields of migration ecology and ecotoxicology and will be highly applicable to developing conservation strategies for shorebirds in the PPR because it will improve our understanding of the effects of agricultural insecticides. This project aligns with the 3-Billion Birds Campaign11 to reverse population declines and is part of an international effort to understand threats impacting LEYE throughout their annual cycle. This study fills a critical information gap by investigating a major threat during migration that may have carry over effects to survival and reproduction and will inform managers and farming communities about the effects of agricultural insecticides on birds. Broader Impacts: To increase participation of underrepresented minorities in STEM, I will develop an internship opportunity through the Doris Duke Conservation Scholars Program at UI that will engage students from diverse backgrounds to participate in my research and develop their own independent projects. To improve STEM education and outreach, I will create a program called Backyard Bird Banding for underrepresented students from rural schools and tribal communities to watch how we capture and band shorebirds and participate when deemed appropriate. I will enhance the experience with engaging kid-friendly games and shorebird ID cards for teachers and students to use while out in the field. This event will be recorded and made publicly available world-wide on social media. I plan to develop this program with the following rural ND schools: Glenburn, Kenmare, and Turtle Mountain Community High School. In addition to hands-on field activities, I will use real-time shorebird migration data to link schools through social media platforms in ND and Alaska, and through the Outreach International Environmental program in South America. Students will be able to track the migrations of birds tagged in or passing through their neighborhoods via Movebank, an animal tracking database. Our outreach goal is to engage with at least 200 students in our programs. To increase public engagement, I will develop high-impact outreach and educational materials about shorebird friendly agricultural practices and alternative biological pesticides in the PPR. I will work closely with the Lesser Yellowlegs working group, the Coalition for Conservation & Environmental Education, farmers, and landowners in the PPR to find practical, long-term solutions that will benefit bird populations and farming communities. 1Eng et al. 2019. Sci. 365:1177; 2Alerstam et al. 1990. Bird Mig. 331-351 ; 3McDuffie et al. 2021. Pro. 1- 134; 4Gibbons et al. 2015. Env. Sci. Poll. Res. 22:103; 5Main et al. 2014. PLOS 9:1; 6Malaj et al. 2020. Sci. Tot. Env. 1-10. 7Li et al. 2020. Nat. Sus. 8Bianchini et al. 2018. Env. Sci. & Tech. 52:13562; 9U.S. Shore. Cons. Plan. 2016. 10Zhao et al. 2017. Move. Eco. 5-23; 11Rosenberg et al. 2019. Sci. 120-124.	0
vectors such as mosquitoes and ticks. Discovered in a screen of over 7,000 molecules, DEET was developed for the U.S. Army for application on human skin in 1946. Although DEET is the world’s most widely used insect repellent, the neurobiological mechanisms of how DEET mediates avoidance remain controversial. Better understanding of the processes underlying DEET’s effectiveness would lead to the development of safer and more effective insect repellents. Background & preliminary data: Previous research shows that DEET’s mechanism of action is multimodal1,2,3. It acts through mosquitoes’ sense of smell, evidenced by animals lacking orco2, a necessary subunit of insect olfactory receptors, and through bitter-sensitive receptors in the labellum3 (Fig. A). Through behavioral assays and video analysis, we have shown that mosquitoes sense and are repelled by DEET on contact through their legs (tarsi)4 (Fig. A) Although the olfactory mechanisms of DEET are better understood, little is known about how DEET repels on contact. Our results demonstrate that when mosquito tarsi come into contact with DEET, the mosquitoes will not blood feed from that surface (Fig. C). Previous work suggests that DEET avoidance acts through bitter receptors in the labellum3 (Fig. A). We found that high concentrations of bitters (such as quinine and lobeline), which are sufficient to prevent feeding when contacted by the labellum3, are ineffective at mediating avoidance Figure | A. Female Aedes aegypti mosquito through tarsal contact when applied to human skin or feeding on a human arm. B. DEET-treated arm an artificial surface (Fig. B & C). My preliminary work with 25mm circle of accessible skin. C. Blood- suggests that DEET repels mosquitoes on contact feeding with indicated compounds applied to through an avoidance pathway more strongly than or human arm. independent of bitter taste sensation, and that this deters them from blood-feeding. But nothing is known about the cellular or molecular mechanisms by which the tarsi detect DEET. For my PhD thesis work, I propose to decipher this contact-mediated pathway to elucidate the biological mechanism of action for DEET in mosquito tarsi. Using genetic tools recently developed in the mosquito, I will locate DEET-sensitive cells in the tarsi and identify receptor(s) that mediate DEET avoidance. Aim 1: Which neurons in Aedes aegypti tarsi respond to DEET? To investigate DEET contact repellency further, I will use in vivo calcium imaging to compare neural activity responses to different chemical compounds in sensory cells within the mosquito tarsi. I will use an Ae. aegypti pan-neuronal promoter to express the genetically-encoded calcium sensor, GCaMP6s, in all tarsi sensory neurons. Using a recent protocol from my lab5, I will present chemical solutions over intact tarsi while recording neural activity with a two-photon microscope and analyze the location and response dynamics of activated sensory cells. To determine if neurons in the tarsi are specialized to encode different chemical cues, I will compare responses within these tarsi sensory cells to DEET, sweet compounds, and bitter compounds. In behavioral assays, Aedes aegypti respond very differently to bitter, sweet, and DEET cues, therefore I hypothesize that chemosensory cells in the tarsi have different population response patterns to these different chemical cues. This may be in the form of different cells reacting to different compounds, or discrimination of chemical cues encoded through differences in population-level activity. This work will advance our understanding of the peripheral sensory response that chemical cues elicit in Ae. aegypti tarsi. Aim 2: Is DEET-sensitivity conferred by different RNA expression patterns? In C. elegans, a subset of neurons express a G-protein coupled receptor, str-217, that is necessary for DEET behavioral response6. To determine if sensory cells Ae. aegypti tarsi express a similar specialized receptor, I will perform RNA expression analysis on separate populations of cells responsive to DEET, bitters, and sugars. Depending on the location and pattern of cells identified in Aim 1, I will develop methods for dissociating sensory cells from mosquito tarsi using either laser-capture microdissection (LCM) or fluorescent activated cell sorting (FACS) on photoactivatable GFP to precisely isolate and harvest these separate chemosensitive cell populations. Collaborating with the Rockefeller University genomics core, I will use single-cell RNA sequencing (sc-RNAseq) to compare RNA expression patterns of DEET-responsive cell populations to tarsal cells responsive to bitters and sweet compounds. I expect cell populations that respond to different tastants to differentially express a number of proteins and receptors. Through this Aim, I will develop a method for in-depth investigation of mosquito tarsal RNA expression while creating a list of candidate molecules that may be sufficient or necessary for DEET sensitivity. Aim 3: Can candidate genes for DEET-sensitivity be validated through genetic knock-out? To identify the functional relevance of genes identified in Aim 2, I will use CRISPR-Cas9-based gene editing2 to create knock-out animals for candidate genes that may confer DEET- responsiveness. I will then use the behavioral screen in Figure B to determine if the mutant animal has become DEET-insensitive, or partially insensitive. Ultimately, this work would result in the first identification of a receptor in mosquito tarsi that mediates avoidance behavior upon contact. Intellectual merit: This work has the potential to advance the field of chemosensation and solve a long-standing mystery in neurobiology. Although DEET is highly effective in repelling a wide range of evolutionarily divergent invertebrates, the mechanism of DEET avoidance is still controversial 70 years after its discovery. Uncovering the mechanism of DEET avoidance promises to elucidate new principles underlying how chemosensation is encoded and subsequently translated into behavior. Broader impacts: Mosquitoes and ticks that blood-feed on human hosts can transmit pathogens that cause a number of devastating diseases, threatening hundreds of millions of lives yearly. Identifying biological processes that mediate avoidance of blood-feeding, such as those underlying the mechanism of DEET, may lead to the development of more effective insect repellents that could last longer at lower doses and reduce the exposure of human populations to dangerous vector-borne diseases. In addition, because the general public has experience with insect repellents, DEET presents itself as a very tractable example for public and youth engagement with the sciences. I am excited to continue my efforts in science communication to inspire young potential scientists with such an accessible topic. I will participate again this year in Rockefeller University’s Science Saturday, an annual science festival for over 1000 children in grades K–8 and their families, where I will host a demonstration around DEET to illustrate basic principles of chemosensation. I will also teach a class on chemosensation at the Rockefeller Summer Neuroscience Program, a graduate student-led course for disadvantaged high school students from New York City public schools. I am eager to share my passion for sensory perception and chemosensation, through a topic that children will already be familiar with. Using my research project on DEET, I hope to help students realize that their personal observations can be of scientific value, potentially inspiring them to pursue their own interests in science. References: 1. M. Ditzen, et al., Science 319, 1838-1842 (2008). 2. M. DeGennaro, et al., Nature 498, 487-491 (2013). 3. Y. Lee, et al., Neuron 67, 555-561 (2010). ). 4. E.J. Dennis, O.V. Goldman, L.B. Vosshall, in revision at Current Biology. 5. B.J. Matthews, M.A. Younger, et al., bioRxiv (2018). 6. E.J. Dennis, et al., Nature 562, 119–123 (2018).	0
impending possibility of “The Big One”—a M8 or greater earthquake along the San Andreas Fault System (SAFS) in California. While strike-slip systems exist across the world, the SAFS is unique in that it terminates at the Mendocino Triple Junction (MTJ), which is the only known example of a modern FFT system—a tectonic triple point bounded by two transform (F) boundaries and a subducting trench (T)[1]. The MTJ formed at approximately 30 Ma when the Pacific-Farallon ridge system was subducted beneath the North American Pacific Trench, creating the northward migrating triple point, and changing the tectonic regime of the plate boundary from a convergent style to a current strike slip style at the latitude of the SAFS.[1]. This change has had drastic effects on the Pacific Coast ever since, resulting in high levels of deformation and seismic activity. The project proposed herein seeks to better understand this geologically complex and socially meaningful system, by aiming to identify and characterize the first recognized ancient equivalent of the MTJ at the southwestern extent of the Paleozoic-era Norumbega Fault System (NFS) of Maine. My PhD work will investigate a ridge-subduction model for the Norumbega Triple Junction (NTJ) put forth by Kuiper 2016[2] by 1) utilizing field and microstructural analysis to evaluate whether dextral faults that curve in a direction opposite to Riedel faults in the NFS are equivalent to stepover faults in the SAFS and 2) testing whether these faults are progressively younger to the southwest, indicating a southwestward migration of the NTJ, analogous to the modern northward migration of the MTJ. If the southern terminus of the NFS is an appropriate type locality for the FFT style triple junction, then the region may provide insight into the evolution and behavior of strike-slip tectonics in the SAFS and around the world. The identification of such an analogue would be particularly significant given the strong outcrop exposure of structural features long the Maine coast in contrast to the more veiled features of the modern Mendocino Triple Junction. Regardless of model success, this work will provide a furthered paleogeographic model for the evolution of New England—a complex and meaningful region in its own right. Context Along its ~300 km extent from SW New Brunswick to S Maine, the mid-Paleozoic NFS is a NE-trending right-lateral transpressive system that parallels the Appalachians[3]. The NFS terminates to the SW, along high grade metamorphic rocks and migmatites associated with the Nashoba terrane of Eastern Massachusetts[4]. The youngest age of partial melting in the Nashoba (~360 to 380 Ma) is coincident with ages of dextral shear in southern parts of the NFS obtained by Ar/Ar age-dating[5,3]. These coeval ages suggest that while dextral movement occurred along the NFS in Maine, convergent style tectonics were still taking place in the nearby Nashoba terrane, indicating a triple point existed between the two. Furthermore, work done by Gentry et al., 2016 illustrates a lack of dextral NE-trending subvertical lineaments, shear zones, and faults in Massachusetts. These observations suggest an abrupt southern termination of the NFS, not unlike the modern MTJ placement against the northern Cascade arc volcanoes in the SAFS[6,7]. Hypotheses Splay-shaped faults in the NFS and SAFS play a key role in testing the Kuiper 2016 model. These structures look like expected Riedel-shaped faults, but curve in a direction opposite to the expected Riedel orientation [2]. In the SAFS, these features are known to be associated with dextral activity, and are possibly either slip-transfer faults from the SAFS to the Mendocino Triple Junction or linkage faults between various strands of the SAFS[8]. If the subducted ridge model is correct, the Norumbega Triple Junction would have moved SW, mirroring the current northward migration of the MTJ. If this is the case, the splay-shaped dextral faults in the NFS should be younger to the south, paralleling the age pattern inferred from the slip-transfer and linkage faults in the northern SAFS[8]. Alternatively, such features in the NFS could be original sinistral (Riedel- expected-orientation) structures which were subsequently reactivated, resulting in dextral overprinting. This alternative will be investigated alongside the work proposed herein. Methods and Work Plan Beginning in 2018, I will add to my background of coursework in high temperature geochemistry, structure, and geodynamics through graduate-level work in microstructure, kinematics, and geochemistry. During this time, I will identify optimal sites for mapping based on high quality LIDAR imagery in collaboration with previous workers and the Maine Geological Survey. Over the next two summers, I will mentor an undergraduate field assistant and conduct detailed structural mapping at sites across southern Maine, with the goal of identifying pre-NFS convergent folds displaying overprinted dextral shear indicators such as S-C fabrics, shear bands, and stretching lineations, all of which indicate the formation of the Norumbega Triple Junction.[9] Following sample acquisition, Backscattered Electron Imaging (BSE) analysis will be utilized to reveal spatial relationships between mineral assemblages and deformation fabrics sampled along the NFS (extending from the north towards hypothesized younger southwestern extents). This will allow for the selection of mineral candidates for U-Th- Pb age-dating. Analyses will be conducted using either an electron microprobe (EMP), or a laser ablation system coupled to a high-resolution, single collector inductively coupled plasma mass spectrometer (LA-ICPMS). Given that the EMP provides only an elemental age and has a smaller spot size than ICPMS, the exact methodology will depend on the size of the domains chosen for analysis. Both methods have been shown to display strong enough precision to resolve geological events in the study region (1-3% for the EMP and <3% for LA-ICPMS).[10] Results from each of the two field seasons will be synthesized and used to inform a workplan for future field and analytical work. Final results will take the form of my PhD dissertation and associated peer reviewed publications. Results will also drive the outreach campaign outlined in my Personal Statement. While this project is ambitious, my prior educational, professional, and research experiences pair with the resources available at Colorado School of Mines and elsewhere to ensure that I have the skillset, perspective, and support necessary to succeed. Broad Impact The affirmation of a ridge-subduction model for the NFS not only has the potential to establish a type locality for the enigmatic FFT style triple junction, but moreover would inform scientists seeking to decipher the kinematics of the currently active MTJ by allowing for interpretations from well exposed outcrops along the NFS to be applied to the SAFS—extending our base of knowledge about an active and dangerous natural system. Even if the model cannot be confirmed, the completion of an in depth structural history of the region will enable a stronger understanding of the mid-crustal dynamics associated with the evolution of the New England Appalachians. Finally, the completion and communication of this work will not only be scientifically relevant, but will enable me to build upon the goals outlined in my personal statement by establishing a career that will impact public discourse and geoscience education. References[1] Furlong, K.P., and Schwartz, S.Y., 2004, Annual Review of Earth and Planetary Sciences. v. 32. [2] Kuiper, Y.D., 2016. Geology, v. 44. [3] West, D.P., 1999, Geological Society of America Special Paper, v. 331. [4] Goldsmith, R., 1991, United States Geological Survey, Professional Paper 1366 E–J. [5] Buchanan et al., 2014, Geological Society of America, Abstracts with Programs, Vol. 46, No. 2. [6] Gentry et al., 2016, Geological Society of America, Abstracts with Programs, Vol. 48, No. 7.[7] Nicholson et al., 1994, Geology, v. 22. [8] Wakabayashi et al. 2007, Tectonophysics, v. 392. 9] Swanson, M.T.,1999, Geological Society of America Special Paper, v. 331. [10] Neymark et al., 2016, Economic Geology, v. 111.	0
Background and Motivation: Observation of the 21cm hydrogen emission line has the potential to provide tremendous insights into the evolution of the universe, and is one of the most exciting frontiers in cosmology. Roughly 400,000 years after the Big Bang, the universe began cooling enough for neutral atoms to form in a period known as recombination. After recombination came a period known as the ‘dark ages’, during which the universe consisted mainly of neutral hydrogen. It gets that name because, although the universe was transparent, no stars had formed yet, so the only radiation was photons from the CMB and 21cm emission coming from the hyperfine spin-flip transition of neutral hydrogen. Thus far, researchers have been unable to directly observe this period of the universe. Eventually, gravitational collapse allowed the first stars and galaxies to form. Radiation from these galaxies then began ionizing the neutral hydrogen in a time period known as the Epoch of Reionization (EoR). Roughly one billion years after the Big Bang, reionization was complete, and the universe became observable again. My research aims at detecting the cosmological 21cm emission line, which will allow us to study the mechanisms driving the evolution of the early universe. Improving our understanding of this period of the universe is crucial to the field of cosmology. In the most recent decadal survey by the National Academy of Sciences, experiments aimed at detecting the cosmological 21cm signal were listed as the highest priority in radio astronomy [1]. There are many barriers to observing the 21cm emission line, including the weakness of the cosmological signal and instrumentation challenges. Most experiments searching for this signal use interferometers, rather than traditional dish telescopes, due to their higher resolution, which is particularly important for long wavelength radio waves. The signal we are searching for is 4-5 orders of magnitude weaker than the foreground sources, which means that our instruments must achieve an extremely high level of precision. With interferometry, calibration of the array can be extremely challenging, but without exceptionally precise calibration, systematic errors dominate the experiment and will prevent any attempt to recover the 21cm signal. There are two primary methods of calibrating an interferometer: sky-based calibration and redundant calibration. Sky-based calibration, performed here using Fast Holographic Deconvolution (FHD), relies on an a priori sky model to solve for the antenna gains. Incompleteness in current sky models has been shown to produce sufficient contamination of the power spectrum to obscure the desired signal [2]. Redundant calibration makes use of the redundancies in the arrangement of array antennas in solving the calibration equations. This method can largely be performed without the use of a sky model, and in fact an estimated sky model is actually produced during calibration without using prior knowledge [3]. My research will improve the redundant calibration pipeline and use the estimated sky model to inform the model used for sky-based calibration, thus increasing the precision of current calibration techniques. Research Project: For my research, I will work primarily with data from the Murchison Widefield Array (MWA), which is a low-frequency radio interferometer containing 256 tiles, each of which is composed of 16 dipole antennas. Specifically, my work will be based on phase II of the MWA, which has been running since 2016, and whose data is yet to be fully analyzed. The MWA provides unique opportunities for studying interferometric calibration because it is composed of two highly redundant hexagonal arrays and a pseudo-random extended array, which makes it workable for both sky-based and redundant calibration. I began working with this data set during my research as an undergraduate, where I created the first images of the sky model NSF Graduate Research Plan Statement Dara Storer produced through redundant calibration and uncovered sources of error that were propagating into the model. My undergraduate work was aimed at finding relative agreement between the estimated sky model produced through redundant calibration and the observed data. If achieved, this estimated sky model could be used to inform the sky model used for sky-based calibration, thus lowering the necessary precision of the a priori sky model. Through this research, I found evidence that positional errors in the antennas were propagating into the estimated sky model and contaminating the calibration solutions. As a first year graduate student at the University of Washington, I have already joined the radio cosmology group, led by Miguel Morales. This group is one of the leaders of the MWA and the Hydrogen Epoch of Reionization Array (HERA) collaborations, and has developed some of the most important data analysis pipelines for precision calibration. By joining this group, I have gained access to better resources and more robust software, which positions me well to study the calibration systematics I uncovered as an undergraduate. My research will proceed as follows: Phase I: I will run simulated data through the same pipeline I used on the real data in my undergraduate research, which will allow me to more precisely examine the systematics contributing to positional error propagation. I will subtract the model produced through redundant calibration of the simulated data from the a priori sky model given by FHD. This result will provide insight into significant sources of flux that may be missing from our sky model. Phase II: I will work with collaborators at the University of Melbourne, led by Professor Rachel Webster, to supplement any missing sources in the catalog currently being used for sky-based calibration. During my semester abroad in Melbourne I worked on a piece of code, PUMA, that is used to produce and combine source catalogs, so I am well prepared to study these catalogs further. Then, I will run the MWA Phase II data back through this adjusted pipeline, and reexamine the propagation of positional uncertainties. Phase III: I will work with the HERA collaboration as they begin collecting data from the telescope, which is expected to happen in Spring, 2019. I will compare the results from HERA with those from the MWA, which will allow me to better determine which systematics are specific to the instrumentation of the MWA, and which are due to the calibration pipeline. This research plan will allow me to systematically track and eliminate the propagation of position errors into the sky model and calibration solutions, which will greatly increase the precision of interferometric calibration, and bring us closer to a true detection of the 21cm hydrogen emission line. Broader Impacts: Observing the EoR will provide insight into what the early universe looked like and the processes that led to the formation of the first stars and galaxies. Understanding the early universe is fundamental to understanding the modern one, and measurement of the 21cm signal will have tremendous influence in almost every area of astrophysics. In addition to my research, I will work with Professor Morales’ group to continue their long history of supporting community college transfer and other historically underrepresented students through the CHAMP program, as detailed in my personal statement. [1] National Academy of Sciences, New Worlds, New Horizons in Astronomy and Astrophysics [2] Barry, N., Hazelton, B., Sullivan, I., et al. 2016, MNRAS, 461, 3135B [3] Li, W., Pober, J., Hazelton, B., et al. 2018, ApJ, 863, 170	0
cues on relationship perception and intent biases Men are more likely to perceive a woman’s friendliness as sexual interest, and this pattern holds up across surveys, actual behaviors, and beyond lab conditions [1-3]. Although communication about sexual interest have always been complicated, they recently have become legal and societal issues. A more complete understanding of how individuals communicate about sex is necessary, especially when 23.1% of college women experience sexual assault [4]. Intellectual Merit Error management theory (EMT) explains this “sexual overperception” effect in men as a strategic bias favoring specific types of judgment errors over other types of errors. Differential parental investment theory [5] states that male mammals are less physically obligated to invest in offspring, so they tend to be more willing to engage in sexual activity, whereas females are more selective about potentially costly sexual activity. For males, the error of “missing” an interested female is costlier than the “false alarm” error of judging an uninterested female as interested, resulting in a pattern of decisions that adaptively reduces costs and increases benefits, even as it fails to minimize errors overall. EMT is, at its core, Signal Detection Theory (SDT) applied to intersexual relationships [6], using differential parental investment to model the costs and benefits of relationship decisions. SDT is a way to describe how observers judge the presence or absence of a “signal” when the given stimuli have some level of ambiguity (“signal + noise”) [7]. The division of EMT from SDT has resulted in an unnecessarily restricted analysis of data that could present a fuller explanation of behavior if analyzed using signal detection models. Although EMT, like SDT, considers different judgments and possible outcomes, it ignores several extensions and implications which a full SDT analysis can provide. For instance, EMT does not consider the base rates of signals compared to noise (that is, the frequencies with which signals and noise occur in the environment), and how that influences judgments. Very common true signals, with rare non-signals, will encourage signal-present judgments in ambiguous situations (known as a liberal bias). Conversely, a low signal rate and common non-signals will encourage no-signal judgments (known as a conservative bias). Additionally, SDT provides a measure (sensitivity) of how well people distinguish signals from noise. One benefit of this approach is that concepts already developed within SDT can transfer to EMT contexts. At the theoretical level, SDT specifies situations in which both men and women should have systematically different signal detection strategy profiles. Individuals with faster (v. slower) life history strategies, more unrestricted (v. restricted) sociosexuality, and more short-term (v. long-term) mating orientation should show more liberal biases (Hypotheses 1-3). Similarly, people high in mate value should show a liberal bias because their experience is of a higher signal base rate (H 4), which EMT cannot predict as it does not take signal/noise ratio into account. It also is possible to manipulate aspects of the social situation, and thus the value of decision outcomes, by manipulating the attractiveness of the stimuli used as signals (H 5) and by changing the signals-to-noise base rates through exposure to different sex ratios of stimuli (H 6). Additionally, methods and analyses from SDT research can be used to more fully understand and analyze existing EMT results. For preliminary results, I analyzed the data from Perilloux, et al. [8] using SDT. This confirmed that men are more liberally biased in perceiving sexual interest, but also yielded unanticipated insights: Women are more sensitive to the difference between sexual interest versus non-interest (d’ in Figure 1), and -surprisingly- both men and women in this study are conservatively biased in perceptions of sexual interest (c in Figure 1). Differential parental investment theory predicts why men have a lower sensitivity than women, as females may conceal their signals of sexual interest, making it more difficult for men to differentiate signal from noise. This also may explain why men are more liberally biased than women, since they need to compensate for their lower sensitivity to maintain the same level of optimality at detecting sexual interest. This difference in sensitivity led to an additional hypoth- esis that men’s sensitivity will increase as the woman’s sexual cues become more overt (H 7). Methodological Approach – My stimuli will include 96 video clips showing heterosexual pairs engaging in conver- sations. Each videotaped person will rate their sexual interest in their conversation partner, then complete questionnaires to evaluate individual differences (described above). Method: Studies will involve participants watching the muted clips and rating each actor regarding their levels of sexual interest in their conversation partner. Multiple study variations will look at influences of the observers’ life history strategy (H 1), socio- sexual orientation (H 2), mating strategy (H 3), and mate value (H 4). Additionally, experimentally manipulated sets of clips will be shown to evaluate the causal effects of skewed ratios of attractiveness of each conversant (H 5) in the video clips, prior exposure of participants to skewed sex ratios (H 6), and exposure to overt sexual cueing (H 7). Analysis will use both EMT and SDT methods, utilizing multilevel probit regression to determine c and d’ for this repeated measures design. [9] Broader Impacts Underrepresented Minorities in STEM: Efforts will be made to recruit underrepresented and first-generation undergraduates as research assistants, who will be encouraged to learn about the research process, present results at conferences, and participate in authorship of publications. Increasing Scientific Literacy and Public Engagement with STEM: Research about romantic relationships often gets public media attention, which will be used to broadly communicate the results of this research and bring attention to current directions in psychological science. This research will also be presented at regional and national conferences. Improving Individual Well-Being: This SDT approach will increase knowledge about the abilities and biases different people have about sexual communication, empowering individuals to make informed, healthy decisions about their sexual and relationship behaviors. Identification of individuals and situations where sexual interest and intents are often misinterpreted will aid in locating at-risk populations, improving sexual assault prevention policies, and inhibiting interference with the right to receive an education free from discrimination through sexual harassment and sexual violence (per Title IX of the Education Amendments of 1972). References: [1] A. Abbey, J. Pers. Soc. Psychol.42, 830-838 (1982). [2] A. Abbey, Psychol. Women Quat 11, 173- 194 (1987). [3] M. G. Haselton, J. Res. Pers. 37, 34-47 (2003). [4] The Association of American Universities, Report on the AAU Campus Climate Survey on Sexual Assault and Sexual Misconduct (Westat, Rockville, MD, ed. 2, 2015). [5] R. L. Trivers in Sexual Selection and the Descent of Man, B. Campbell Ed. (Aldine, Chicago, IL. 1972), 136–179. [6] D. Nettle in Evolution and the Mechanisms of Decision Making, P. Hammerstein, J. R. Stevens Eds. (MIT Press, Cambridge, MA, 2012), 69-79. [7] D. M. Green, J. A. Swets, Signal Detection Theory and Psychophysics, (Wiley, New York, NY, 1966. [8] C. Perilloux, J. A. Easton, D. M. Buss, Psychol. Sci. 23, 146-151 (2012). [9] L. T. DeCarlo, Psychol. Methods 3, 186-205 (1998).	0
Evidence from the surface and atmosphere of Mars indicates that Mars was once a much wetter world. Today, we know that nearly all the water has been lost [6]. Understanding the evolution of water on Mars and its atmosphere is critical to answering questions such as: did life ever exist on Mars, or does it now? What can Mars tell us about possible futures for Earth or evolutionary pathwaysofexoplanetatmospheres? IntellectualMerit To approach these questions, we need new models that capture the complex chemistry and dy- namics governing escape of water. Thermal escape of hydrogen (H) is the main loss process for bothwaterandtheatmosphereasawhole[6]. BecauseHescapesmoreefficientlythanitsheavier isotope, deuterium (D), understanding variations in the D/H ratio is the key to understand- ing loss of Martian water. However, the most recent atmospheric escape studies that included D chemistry only considered global averages of D/H, and were 18+ years ago [7, 10]. Since then, D/Hmodelingworkhasstalled,andnostudieshaveinvestigatedoutgoingfluxofD[6]. Twenty years of data from the Hubble Space Telescope, ground-based telescopes, and Mars or- biters, landers, and rovers have augmented our knowledge of the D/H ratio on Mars. We now know it varies with season, altitude, and geographical location ([9], and references therein). This variability is apparent in atmospheric water vapor, which can form clouds and be transported by dust storms. Studies have shown that planetary boundary layer (PBL) water ice clouds can de- crease the total water column by up to 15% on timescales of a few days [4]. Orbiter data [3, 5] shows that dust storms boost water in the mesosphere, which was demonstrated by Chaffin et al. [2]toenhancelossofHwithinweeks. Iproposetousethesenewdataasinputsandconstraints inthefirststudiesinnearlytwodecadesoftheroleofD/HinMartianatmosphericloss. ResearchGoals 1. Explainhowseasonal,altitudinal,geographicalD/Hvariationsaffectatmosphericloss. 2. Understandtheeffectsofplanetaryboundarylayercloudsonatmosphericloss. 3. Quantifythecontributionofduststormstoatmosphericlossenhancement. ModelingMethodology,Preparation,andCurrentResults Due to computational resource limitations, 1D photochemical models are required to simulate the martian atmosphere on time-scales of 105+ years. Though limited in space, 1D models can pro- vide context for end-member cases of more expensive 3D calculations. During my first year in graduate school, I expanded a 1D photochemical model built by my advisor, Michael Chaffin, doubling the number of chemical pathways modeled and adding deuterium chemistry. Fol- lowing Chaffin et al. [2], the model solves a photochemical system of coupled partial differential equations. Modificationstoaddressresearchgoalsrequireonlyminorchangesasfollows. 1. New,high-precisiondatatoconstrainD/HinaltitudeandtimeisforthcomingfromtheESA Trace Gas Orbiter to [8]; we can model this data as a time- and altitude-depndent function. Spatialvariancecanbeestimatedwiththis1Dmodelbyindividualruns. 2. Bothdiurnalandseasonalvariationsinwatervaporabundanceduetocloudsandduststorms can be included with time-dependent calculations of the water vapor profile, which is pre- scribedinthemodel. CloudaltitudescanbeestimatedusingCuriosityrovermoviesandthe MarsRegionalAtmosphericModelingSystem(MRAMS)[1]. IrecentlystudiedtheeffectsofDchemistryandchangestothetemperatureprofileandwater Eryn M. Cangi Research Proposal 2018-2019 NSF GRFP application vapor mixing ratios on escape by calculating the fractionation factor f, which represents how efficiently D escapes with respect to H. (A fractional value 0.xx means that D escape is 0.xx% as efficient as H escape). Selected results are shown in Figure 1. This is the first effort to model differential escape of H and D in ∼18 years. Our results show that prior calculations greatly overestimated the relative escape of D due to systematic inaccuracies in atmospheric temperature measurementsandphotolysisratecoefficientsavailableatthetime. Isharedtheseresultswithcol- leaguesatthe50thmeetingoftheAmericanAstronomicalSocietyDivisionforPlanetarySciences (DPS),andamcurrentlycompletingworkonwritingtheseresultsinalead-authorpublication. Figure 1: Fractionation factor (percent efficiency at which D escapes with respect to H) for 6 model runs, comparedtotworeferences. Labelsindicateadjustments to three temperature profile control parameters: T , surf T , and T . “↓T ”, e.g., means the temperature at tropo exo exo theexobasewasloweredforthatmodelrun.Adjustments of±25%tothemeanprofileweretested.Thelowsurface temperaturecaseproducedanunstableatmosphere(pho- tochemicalsystemhadnosolution)andwasdiscarded. BroaderImpact The public imagination is already captivated by Mars, as a possible habitat for extraterrestrial life and a target for future crewed missions. As mentioned in my personal statement, next year I will join CU-STARS, a departmental program that brings extracurricular science lessons to Coloradopublicschools. ExcitementaboutMarsfromallages,andthefactthatatmosphericloss can be explained without complicated jargon, makes my research an excellent topic for reaching out to schools around Colorado. In terms of engaging the wider public, I also plan to make my research available to the public by giving talks at University of Colorado’s Fiske Planetarium, a method of outreach where I can draw on my earlier training in theatre. To understand possible futuresforMars,weneedtounderstanditshistory;myfirsttalkwilldiscussthehistoryofmartian atmospheric escape and implications for hypothetical future terraforming efforts (and the ethics thereof). I believe in making it easy for the public to access and understand the science their taxes pay for; in addition to presenting at conferences and publishing papers, I maintain a personalwebsitewithexplanationsofmyresearchforbothlaypeopleandfellowscientists. MarsresearchcontributesnotonlytoMarsscienceandmissions,butalsotoexoplanetaryscience. Compared to the requirements to understand exoplanet atmospheres, Mars is a cheap and readily available laboratory. It is valuable not only for scientific opportunities, but because for decades, it has captivated disparate groups of people. Now more than ever, humanity needs goals that unify usandremindusthatweareallinthistogether;myworktounderstandtheevolutionofwaterand theatmosphereonMarswilladvancethosegoals. References: [1]Campbell,C.,etal.2018,AAS/DPSMeetingAbstracts,300.03. [2]Chaffin,M.,etal.2017,Nature Geoscience,10,174-178. [3]Fedorova,A.,etal.2017,Icarus,300,440-457. [4]Haberle,R.M.,etal.2017,The AtmosphereandClimateofMars. [5]Heavens,N.,etal.2018,NatureAstronomy,2(2),126-132. [6]Jakosky,B.,et al.2018,Icarus,315. [7]Krasnopolsky,V.2000,Icarus,148. [8]Villanueva,G.L.,etal.2018,AAS/DPSMeeting Abstracts,303.09. [9]Villanueva,G.L.,etal.2015,Science,348(6231),218-221. [10]Yung,Y.,etal.1988,Icarus, 76(1).	0
Key Words: Peptide Release, Ribosomal Stalling, Gene Regulation, Student Mentoring Abstract: The aim of the proposed research is to gain a mechanistic understanding of peptide release and nascent peptide mediated ribosome stalling by employing both a synthetic and structural approach. This project will broaden our understanding of protein synthesis and gene regulation by the ribosome and promote teaching and learning in all educational levels through mentoring and collaboration. This research will be completed with the guidance of Dr. Scott Strobel at Yale University, with all the requested resources and collaborations available to successfully accomplish the following aims. Background and Significance: Protein synthesis by the ribosome is a fundamental process found in all life. A set of highly conserved nucleotides located in the active site of the large subunit of the ribosome are responsible for two biologically important reactions: peptide bond formation and release. Termination of protein synthesis occurs when one of three stop codons are recognized in the small ribosomal subunit and decoded by release factor proteins (RFs) 1. Upon recognition, a highly ordered water molecule nucleophilically attacks the aminoacyl ester linkage of peptidyl-tRNA hydrolyzing the ester bond which links the nascent polypeptide to the peptidyl-tRNA. As seen in Figure 1a, it is hypothesized that as the ordered water molecule attacks the ester linkage, the carbonyl carbon proceeds through a tetrahedral transition state containing a developing negative charge, an oxyanion. While termination of translation has Figure 1. a) General mechanism of peptide release. b) Generic structure of peptide release been known for transition state analogs. c) Generic structure of peptide formation transition state analogs. some time now2, it is less studied than elongation and the underlining mechanistic processes are only starting to emerge. Thus transition state characterization and structural studies can help define how the ribosome catalyzes this challenging reaction. In addition to catalyzing the formation and release of polypeptides, the ribosome has also been found to have the ability to monitor the structure of the growing polypeptide during elongation, a process which is poorly understood. Accumulating evidence shows that some nascent peptides result in ribosomal stalling due to specific RNA interactions within the exit tunnel of the ribosome. Many of these have been found to play a role in regulating the expression of genes such as erythromycin resistance in bacteria3. Recent cryo-EM reconstructions of the stalled ribosome have suggested that certain interactions within the tunnel are relayed to the peptidyltransferase center (PTC) to arrest translation4. However, how this information is communicated to the PTC is essentially unknown. By further understanding the mechanism of ribosome stalling it may yield insights into the events that regulate gene expression from bacteria to humans, which can lead to the rational design of more efficacious drugs. Specific Aims: Aim 1: To synthesize and measure the binding affinity of a series of transition state analogs. In the Strobel lab, I will create a series of peptide release transition state analogs containing functional groups of varying shapes, charge distributions, and hydrogen-bonding potentials and measure their relative affinity for the ribosome using RNA chemical footprinting (Figure 1b). With this technique, nucleotides in the 23S rRNA will be probed using dimethyl sulfate as a function of inhibitor concentration using established protocols5. All of these inhibitors have the same basic geometry and each is synthesized as a pair of diastereomers that allow both non- bridging oxygens to be characterized independently. Transition state theories predict that enzymes bind the tightest to the transition state of the reactions they catalyze. Therefore, the inhibitors that best complement the electrostatic environment of the active site will bind the tightest, and from changes in the extent of modification of ribosomal residues, the relative affinities will allow us to draw conclusions about the geometry and charge distribution of the active site during release. Aim 2: To gain a structural understanding of peptide release and induced ribosomal stalling. Given the implications of the ribosome in peptide release, its role in translational arrest, and its essential yet understudied role in gene regulation, it will be vital to develop a mechanistic understanding of how the ribosome performs all these actions. Using high-resolution crystal structures I will investigate how important structural features of the ribosome, peptidyl tRNA and release factor proteins position a water molecule for optimal attack of the aminoacyl ester linkage of peptidyl-tRNA. I will also elucidate how specific conformations of the nascent polypeptide chain and subtle conformational changes in the ribosome can feedback inhibit the PTC. I will thus collaborate with the Steitz lab at Yale, which is preeminent in ribosomal X-ray crystallography, to get a crystal structure of release factor 2 bound to the ribosome with the best peptide release transition state analog from Aim 1 (Figure 1b). Using solid phase synthesis, I will also synthesize transition state analogs of peptide bond formation with an attached polypeptide of known stalling ability that can be tethered into the exit tunnel (Figure 1c). By visualizing the peptide-exit tunnel interactions through crystallography together with biochemical and computational data, it is possible to propose a more accurate mechanistic model of nascent polypeptide chain-mediated translational stalling. Aim 3: To promote teaching, mentoring and collaboration in multiple educational levels. In collaboration with Dr. Nicolas Carrasco at Quinnipiac University, I am in the unique position to teach and mentor possible undergraduate students from both Yale and Quinnipiac who wish to participate in this project by helping them experience graduate-level research, and teaching them how to communicate their findings at conferences. These students will work toward the synthesis of various oligonucleotide-peptide conjugates in order to further research stalling peptides and investigate the role of various cofactors in the formation of the stable stalled ribosome complex. I will also use this work as a teaching tool during TA sessions at Yale for science and non-science oriented undergraduate classes by teaching students how to think critically and analytically. Additionally, I will become involved in the New Haven Science Fair Mentor Program (NHSFMP) to help elementary school students and teachers become more excited about science. I will facilitate weekly brainstorming sessions meant to teach students to form a hypothesis, develop an experimental approach, and analyze their results. My long term goal is to help create a better science curriculum to show their students how to become future scientists. References: 1. Weixlbaumer, W. et al. (2008) Science. 322: 953-956. 2. Capecchi, MR. et al. (1967) Proc. Natl. Acad. Sci. 58: 1114-51. 3. Ramu, H. et al. (2011) Mol. Cell. 42: 321-330. 4. Seidelt, B. et al. (2009) Science. 326: 1412-15 5. Parnell, K. et al. (2002) Pro. Natl. Acad. Sci. U. S A. 99: 11658-1166	0
Keywords: Aging, stress response, RNA biology, Drosophila Background: Aging is characterized by the accumulation of cellular damage, the physiological decline of tissue and an increased susceptibility to disease resulting from the failure to maintain homeostasis in the face of endogenous and environmental stresses [1]. A key mechanism underlying protein homeostasis (proteostasis) in response to stress is the assembly of RNA stress granules (SGs). When stress dissipates, SGs disassemble and cells return to homeostasis, thus SGs dynamic behavior offers a potential molecular mechanism that links aging and cellular stress resilience. Interestingly, changes in SG dynamics have been identified in age-dependent neurodegenerative disorders, yet SGs remain unexplored in normal aging. SGs are non-membrane bound organelles that assemble in the cytoplasm of cells when translation initiation is inhibited or during stress (e.g. heat shock, osmotic pressure, oxidative stress) [2]. SG formation has been shown to increase fitness during stress [3]. During transient stress, SGs stabilize mRNA and delay the aggregation of proteins linked to neurodegeneration [4- 5]. SGs preferentially sequester long, poorly translated RNAs as well as a diverse set of proteins such as nuclear pore complexes, RNA binding proteins and others varying by cell type and stressor [6-7]. SG assembly is rapid: a dense core is formed by an established network of protein- protein interactions, nucleated by G3BP1, followed by the assembly of a dynamic shell comprising RNA and RNA binding proteins that trigger liquid-liquid phase separation. After stress subsides, SGs spontaneously disassemble and allow sequestered factors to return to their functions [8]. When SGs fail to disassemble, such as during chronic stress, they disrupt, not maintain, proteostasis and facilitate protein aggregation [9]. Two previous studies found that SG components aggregate with age, but it is unknown how normal aging alters the nucleation, stability, or disassembly of SGs [10-11]. I hypothesize that (1) the dynamics of SGs will be altered throughout aging and (2) the composition of SGs will correspondingly be altered by age. Aim 1: Determine the dynamics of SGs during aging in response to stress Using a Drosophila model where Rasputin (RIN), the homolog of G3BP1 and the only protein required for SG formation, is endogenously tagged with GFP (RIN-GFP), I will visualize SG formation in the fly brain [12]. Drosophila share over 60 percent of their genome with humans, providing a translatable and practical model to study SGs throughout aging (lifespan averages 100 days). To determine if age impacts SG dynamics (e.g. assembly and disassembly) in fly brains, I will dissect adult fly brains and immunostain for GFP. Using a confocal microscope, I will quantify the distribution and sizes of RIN-GFP puncta in various regions of the fly brain. Drosophila will be dissected at five time points across aging (1, 20, 50, 80, 100 days). To capture the altered dynamics of SGs between types of stress, heat shock and oxidative stress through paraquat ingestion will be used to stress flies just before dissection. Controls will include age matched Drosophila that will not be subjected to stress. To distinguish between SG assembly and disassembly, flies will be dissected just after stress (assembly) or two hours after stress (disassembly). Ten to twenty flies will be studied per time point and treatment. Expected outcomes, potential pitfalls and alternatives. Preliminary experiments show that SG assembly declines with age in flies (data not shown) thus I expect less robust assembly of new SGs for older Drosophila. I expect most SGs to disassemble one hour after stress for young Drosophila. For old Drosophila, I expect less dynamic SGs or slower disassembly. The assembly mechanism of SGs varies slightly by stressor. Oxidative stress has canonically been used to elicit SG formation [2]. The dynamic behavior of heat shock induced SGs could defy expectations based on research into SGs generated by oxidative stress. Previous research shows some SG aggregation in aging [10-11]. If SG disassembly does not visibly change with age, though, changes in SG composition could alter SG dynamics in other ways. These experiments will show for the first time how aging fly brains respond to stress by assembling SGs. Future research will examine the relationship between altered SG dynamics and phenotypes of aging (e.g. behavior). Aim 2: Determine the composition of SGs during aging in response to stress SG dynamics are impacted by the composition of SGs. For example, the recruitment of protein kinases ULK1 and ULK2 and the ATPase VCP to SGs is required for SG disassembly [13]. Age-related alterations to SG composition are likely responsible for the expected decline in dynamic SG formation during aging. SGs will be isolated using immunoprecipitation (IP) of GFP tagged RIN protein/RNA complexes from the neurons of the same experimental Drosophila and controls described above. IPs will be optimized using IgG controls to ensure specificity of RIN-GFP complexes. Mass spectroscopy and RNA-sequencing will be used to identify the proteins and RNAs comprising SGs throughout the various stress and aging conditions. Key SG markers will be cross-referenced. The results will be analyzed using statistical models to identify which proteins and RNAs are enriched or depleted in specific conditions and age time points. Expected outcomes, potential pitfalls and alternatives. The differential recruitment of proteins to SGs has physiological impacts on cells during the stress response and can alter SG dynamics. For example, if SGs formed during old age increasingly sequester nuclear pore complexes, proteostasis is more likely to be disturbed in aging [6]; similar logic applies to other pathways. RNAs sequestered to SGs mediate protein recruitment but do not impact global translation [8]. By linking the composition of SGs with different assembly and disassembly phenotypes, the impact of age on the mechanism of SG-supported stress resilience can be more fully understood. To follow up candidates identified by IP/mass-spec, overexpression or knockdown of genes in Drosophila will be used to identify the specific role of proteins in the alteration of SGs dynamics in the stress response throughout aging. Intellectual Merit and Broader Impacts: For the past three years, I worked with Drosophila studying RNA biology in aging and the cellular stress response. Through my research, I optimized IP procedures to isolate SGs and prepare them for analysis by mass spectroscopy and RNA-seq. To analyze this data, I created statistical models using the R programming language. Given SGs role in stress resilience and neurodegeneration, understanding the impact of age on the dynamics of SGs is important. These findings will help generate new hypotheses about the role of the stress response in aging. Additionally, researchers are pursuing treatments to the neurodegenerative disease amyotrophic lateral sclerosis that eliminate SGs in humans. SG elimination could significantly impair the ability of neurons to survive especially during aging. The proposed experiments will provide key insights into new strategies to maintain cellular homeostasis in the human body, specifically the brain, and extend the health span as well as lifespan. As part of my research, I am responsible for leading and training a small team of undergraduates in our study of RNA biology in aging. In addition to effectively communicating my research at two international conferences and to my community, I helped review current research on the role of SGs in protein aggregation for an upcoming publication. References cited: [1] Rieraetal et al., 2016. Annu. Rev. Biochem. 85, 35-64. [2] Parker et al., 2009. Mol. Cell 36, 932-941. [3] Riback et al., 2017. Cell 168, 1028–40. [4] McGurk et al., 2018. Mol. Cell 71, 703-17. [5] Bley et al., 2015. Nuc. Acids Res. 43, 23. [6] Khong et al., 2017, Mol. Cell 68, 808–20. [7] Markmiller et al., 2018, Cell 172, 590–604. [8] Jain et al., 2016. Cell 164, 487–98. [9] Zhang et al., 2019. eLife 8, 39578. [10] Lechler et al., 2017. Cell Reports 18, 454–467. [11] Moujaber et al., 2017. Bioch. Acta 1864, 475–486. [12] Anderson et al., 2018. Human Mol. Genetics 27, 1366-81. [13] Wang et al., 2019. Mol.Cell 74, 742-57.	1
"Impacts of Radioactive Cs on Marine Bacterioplankton: Effects of the Fukushima Disaster on Hawaii’s Kaneohe Bay Bacterial Communities Introduction Marine bacteria are unmatched in their diversity and abundance. They exhibit mutualism with economically significant organisms, synthesize life-saving natural products, and play a vital role in oceanic nutrient cycling. Despite our dependence on marine bacteria, very little research has been conducted on how they respond to large-scale disasters. One such catastrophe, a tsunami off the coast of Japan, occurred on March 11, 2011. The tsunami caused the Fukushima-Daiichi Nuclear Power Plant to emit 10 PBq of radiation2, the largest ever release of anthropogenic radionuclides into the ocean4. The main pollutant, 137Cs, has a half-life of 30 years and will first hit the US territories at the Hawaiian Pacific Islands in early 2014, diluted by only three orders of magnitude2 (figure 1). While 63 marine species have already exceeded the Japanese limit for radioactive Cs (100 Bq/kg), the impacts of radioactive waste on marine microorganisms are largely unknown6. Due to their short reproductive lifecycle and unicellularity, bacteria evolve faster than most eukaryotes when exposed to radiation, so much so that radiation is used in laboratories to induce mutagenesis. This project aims to assess the impacts of radiation on the bacterioplankton community of Kaneohe Bay in Oahu, Hawaii. The bay is in the direct path of Fukushima’s radioactive waste and has a bacterioplankton community that was well-characterized pre-disturbance1, making it the ideal case study for the microscopic impacts of radioactive pollution. I will compare trends after radiation exposure to previously documented annual/seasonal fluctuations. This is possible because Fukushima bacterial populations were catalogued bimonthly over an 18-month period. Hawaii Hawaii Figure 1: Predicted spread of 137 Cs after 2.5 and 5 years2; color scale shows dilution factor Research Questions 1. How has the bacterioplankton species composition in Kaneohe Bay (as determined by 16S small-subunit ribosomal RNA (SSUrRNA) barcodes) changed since the Fukushima leak? 2. Has there been a significant increase in single nucleotide polymorphisms (SNPs) since the radiation event, as compared to mutation rates that would occur due to random chance? Methods I will work within the Rappé laboratory for aquatic microbial ecology at Hawaii Institute of Marine Biology (University of Hawaii at Manoa), which is equipped with all necessary instruments and sampling materials. Rappé is at the forefront of bacterioplankton ecology, and having established the 2006-2007 baseline1, his lab will provide an excellent knowledge base for collecting comparable data. Seawater will be sampled at a depth of 1m at 2 sites (reef flat and lagoon) separated by 600m near Coconut Island in southern Kaneohe Bay. Samples will be taken twice monthly from January 2015 to July 2018 between 07:00 and 08:30h. In situ measurements of temperature, salinity and pH will be taken at 1m depths using a multi-parameter sonde, and radiation levels + will be monitored with a scintillation probe. Dissolved inorganic nutrient concentrations (NH , 4 – 3– NO2 , PO4 , silicate) will be measured using a continuous segmented flow system. Bacteria will be isolated by filtering 1L of water through a 1.6 µm microfiber membrane pre-filter followed by a 0.2 µm polyethersulfone membrane and stored at –80°C in DNA lysis buffer. Genomic DNA will be extracted using the DNeasy Tissue kit1. Bacterioplankton will be characterized by PCR of SSUrRNA and sequenced in a barcoded Illumina HiSeq run. The bacterial primers 27F-B-FAM and 519R will be used1. OCTUPUS and UC-LUST will be used to process raw reads, which will then be clustered into operational taxonomic units using MegaBLAST3. Mutation and species compositional shifts due to random chance will be determined from the 2006-2007 data1 using a Poisson distribution and extrapolated to determine the number of mutations that should occur from 2015 to 2018. The experimental 2015-2018 community structure and SNP prevalence will be compared against these values to identify changes that are due to radiation. Anticipated Results 1. The bacterial community structure will change significantly more than due to random chance. 2. Post-Fukushima species will have significantly more nonsense and missense mutations in non-essential genes and neutral mutations in housekeeping genes than would have accumulated due to random chance. Broader Impacts This research will help characterize the full repercussions of radioactive pollution at its first outset, providing insight that will allow us to prepare for future radiation leaks and the arrival of the contaminants to the California coast6. It will reduce the knowledge gap of what potential harm radioactivity causes marine microbial communities, and give policy makers the information they need to manage affected ecosystems. In light of the recent shift towards increased nuclear power reliance, this research will inform the tradeoffs of pursuing various energy sources in future development, as well as allow policy makers to establish and enforce adequate safety standards for nuclear power plants. In doing so, this research will protect the ecosystem services that marine bacterioplankton provide for humanity, including the nutrient cycling that supports economically important fisheries and large-scale oceanic biodiversity. Resultant policies will protect the biodiversity of marine microbes, which has already proven itself a priceless source of natural products that combat neurological disorders, infections, and cancer5. This study will also characterize the impact of radiation on pathogenic bacteria in coastal communities, which is crucial to fully assessing the impact of radioactive waste on human and environmental health. Literature Cited 1. Apprill, A. and M. S. Rappé (2011). ""Response of the microbial community to coral spawning in lagoon and reef flat environments of Hawaii, USA."" Aquatic Microbial Ecology 62: 251-266. 2. Behrens, E., et al. (2012). ""Model simulations on the long-term dispersal of 137Cs released into the Pacific Ocean off Fukushima."" Environmental Research Letters 7(3): 034004. 3. Bik, H. M., et al. (2012). ""Sequencing our way towards understanding global eukaryotic biodiversity."" Trends in ecology & evolution 27(4): 233-243. 4. Rossi, V., et al. (2013). ""Multi-decadal projections of surface and interior pathways of the Fukushima Cesium-137 radioactive plume."" Deep Sea Research Part I: Oceanographic Research Papers. 5. Villa, F. A. and L. Gerwick (2010). ""Marine natural product drug discovery: Leads for treatment of inflammation, cancer, infections, and neurological disorders."" Immunopharmacology and immunotoxicology 32(2): 228-237. 6. Wada, T., et al. (2013). ""Effects of the nuclear disaster on marine products in Fukushima."" Journal of environmental radioactivity 124: 246-254."	0
Investigation of the Catalytic Properties of Cerium(IV) Oxide in Metal Oxide Laser Ionization-Mass Spectrometry Imaging Background: Matrix-assisted laser desorption/ionization-mass spectrometry imaging (MALDI- MSI) is an emerging and powerful analytical technique, which allows the spatially resolved characterization of a wide range of analytes within biological specimens.1 Metal oxide laser ionization (MOLI) is a recently described variation on MALDI in which a metal oxide, rather than an organic acid, is utilized as the matrix.2 255 m/z Unlike the other metal oxides, Cerium(IV) Oxide (CeO ) demonstrates a unique property of laser 2 induced fatty acyl catalysis when applied to 281 phospholipids and energized by standard lasers m/z found in MALDI-TOF MS instruments, as seen in Figure 1. This property of laser-induced catalysis by Figure 1. MOLI-MS using CeO 2 on POPC CeO provides a considerable opportunity in various 2 biological and clinical applications in which fatty acid profiling may be needed.3,4 Beyond clinical applications, CeO -based materials also have a 2 variety of applications as a catalytic system in fuel cells, thermochemical water-splitting, organic reactions, and photocatalysis.5 Because of the involvement of CeO in a variety of fields, and the 2 potential it has to impact future technologies, a further investigation of the biological catalysis properties of this compound are warranted. Preliminary Results: The Cox group in Colorado has headed the investigation for the use of MOLI techniques in the identification of bacterial species. This group discovered that CeO could be utilized 2 for identification with improved stability and reproducibility compared to other metal oxides.6 Previously, MOLI has not been used in conjunction with MSI in order to induce fatty acyl catalysis directly from tissue for possible bacterial detection. While MSI is found to be a promising technique, there are only very few research groups that currently have the instrumentation available to conduct MSI studies. At Harvard Medical School, the Agar group has been Figure 2. MOLI-MSI of control working in the field of MSI for over a decade. During my time in mouse brain: A) 255.4 m/z & the group this summer, I developed and optimized a technique for B) 281.5 m/z CeO deposition on biological tissue, which is currently being 2 prepared for submission. This technique describes the deposition of CeO for MOLI-MSI, such 2 as in Figure 2, as well as possible clinical applications. Although MOLI using CeO has shown considerable promise, the mechanism for which 2 fatty acyl catalysis occurs when laser energy is applied to CeO is still unknown. Most 2 commonly, when MOLI-MS is used, analyte ionization of phospholipids typically occurs by protonation due to interactions with the Lewis acid/base sites on the metal oxide. However, only no protonation occurs with CeO -induced catalysis, indicating that the mechanism of cleavage is 2 unique. It is postulated that much of the catalytic activity of CeO arises from oxygen vacancy 2 defects in the surface which occur at MALDI-like conditions (high temperature, low pressure).6 Proposed research: My preliminary results have developed a novel technique for the application of CeO to clinical problems. However, it has left unanswered a critical question 2 about the mechanism of fatty acyl cleavage that occurs when CeO is used for MOLI-MS/MSI. 2 Without understanding the mechanism of catalysis, it is difficult to fully interpret the mass Madison McMinn | 2018 NSF GRFP Research Statement spectra generated by this method. My future research plans consist of three specific goals, detailed below. By achieving these goals, I plan to elucidate the mechanism of catalysis that is unique to CeO , when it is used with biological samples. 2 The first goal is to expand beyond phospholipids, and study compounds that also contain fatty acid chains, such as diacylglycerols, sphingolipids, triglycerides, acyl-carnitines, acyl- coenzyme A thioesters and other acyl-bound biomolecules. I am interested in determining if the cleavage of fatty acid chains is unique to phospholipids, or if CeO can also induce this property 2 on other compounds. This will serve to determine the depth and breadth of this application. Also, if certain compounds are unable to undergo catalysis by CeO , structural differences can be 2 identified and studied. For these experiments, I plan to use commercially available lipid standards to evaluate catalysis in a simple, and direct way. The second goal is to apply the knowledge gained in the first goal of this proposal to complex systems. Since isolated and purified compounds do not exist naturally, it is critical to see how effective this technique is when applied to a complex biological specimen. For this, I plan to correlate mass spectra obtained with a typical MALDI matrix and with CeO . I aim to 2 correlate the known lipid composition with the fatty acid composition, to see if certain species are more prone to cleavage when present in a complex sample. This would be relevant in MOLI- MSI with heterogenous tissue samples, where lipid compositions can vary greatly throughout the specimen. The third goal is to investigate the surface chemistry of CeO using experimental and 2 computational methods. I plan to perform studies where CeO particles of varying sizes are 2 probed by electron/neutron diffraction, since X-ray diffraction is not an ideal technique for this material, due to the low scattering power of oxygen. These studies aim to determine if a greater number of oxygen defects contributes to improved catalytic cleavage. Diffuse Reflectance Infrared Fourier Transform Spectroscopy will be performed to study the surface morphology of CeO before and after it is subjected to MOLI-MS. Once the surface structure of CeO is well 2 2 understood, computational studies can be performed using density functional theory calculations. Intellectual Merit & Broader Impact: Elucidating the mechanism of CeO -induced fatty acyl 2 catalysis will allow scientists to use my developed MOLI-MSI technique and the MOLI-MS database for bacterial identification by the Cox group with increased confidence. Also, the information obtained from my first research goal has the potential to advance knowledge in fields that use fatty-acid containing molecules, such as cosmetics, nutrition, and metabolomics, in addition to biological applications. Furthermore, elucidation of the catalysis mechanism of varied metal oxides, and what induces this variance, can contribute fundamental knowledge to the field of catalysis chemistry, especially metal oxide catalysis. In regard to the broader impact in biological applications, fatty acid profiling of tissue specimens has been an extensive area of study, dating back nearly a century. Most current approaches use many time-consuming steps and, more concerningly, result in the loss of spatial relationships between these molecules. Preserving these spatial relationships is critical in the analysis of a wide variety of diseases, including many cancers, which demonstrate heterogenous tissue distribution. We have shown that MOLI-MSI using CeO can provide in situ fatty acyl characterization of biological tissues 2 while preserving regional distribution. By better understanding the chemistry of CeO induced 2 fatty acyl catalysis, a more informed interpretation of resulting MS spectra and therefore the tissue composition can be appreciated. This work will lay the groundwork for a potentially new clinical and translational diagnostic approaches. 1.Reyzer ML, Caprioli RM. J Proteome Res. 2005, 4(4), 1138-1142. 2.Schwamborn K, Caprioli RM. Nat Rev Cancer. 2010, 10(9), 639-646. 3.Voorhees KJ, Saichek NR, Jensen KR, Harrington PB, Cox CR. J Anal Appl Madison McMinn | 2018 NSF GRFP Research Statement Pyrolysis. 2015, 113, 78-83. 4. Choe SS, Huh JY, Hwang IJ, Kim JI, Kim JB. Front Endocrinol (Lausanne). 2016, 7(30). 5.Hodson L, Skeaff CM, Fielding BA. Prog Lipid Res. 2008, 47(5), 348-380.	0
(sRNA-seq) technologies have fueled the discovery of many new classes of biologically relevant non-coding sRNAs. Accumulating evidence suggests that sRNAs are critical contributors to the pathogenesis of various diseases and play an essential role in regulating gene expression levels1. RNA-seq analysis has revealed diverse classes of sRNAs circulating on various lipid and protein carriers, including high-density lipoproteins (HDL) 1. The most well characterized sRNAs are microRNAs (miRNA) and the sRNA-seq analysis tools currently available are designed to focus mostly on miRNA quantification. Due to the limitations of previous analysis tools, our lab developed a novel sRNA analysis pipeline (i.e. TIGER) which profiles many classes of host (e.g. mouse, human) and non-host (e.g. bacteria, archaea, fungal) sRNAs present on lipoproteins2. Using our new TIGER pipeline, we discovered that the overwhelming majority of circulating sRNAs on HDL are classified as host and non-host ribosomal RNA-derived fragments (rDF)2. Motivation: Although the functional role(s) of rDFs are poorly understood, mounting evidence suggests that rDFs are not products of random degradation, but regulated by specific endonucleolytic cleavage processes, similar to that of transfer RNA-derived fragments (tDFs)3. Indeed, various stressors were shown to induce transfer RNA cleavage events, producing stable tDRs3. Angiogenin is an RNase A-family enzyme that is thought to be primarily responsible for stress-induced tRNA fragmentation within mammalian cells4. Angiogenin has also been shown to induce rRNA fragmentation, although to a lesser extent4 Interestingly, the cleavage of tRNAs . and rRNAs are inherently linked to their chemical modifications (i.e. m1A and m5C)5. Both tRNAs and rRNAs represent the most abundant sRNAs, and the two most heavily modified RNAs in the eukaryotic genome. Although the identification of chemical modifications on tDFs has been actively pursued, few studies address the modifications found on rDFs. However, preliminary data I have generated using 2D-thin layer chromatography (2D-TLC) identified abundant base modifications (e.g. m5C, m6A) on sRNAs isolated from human HDL samples. A major limitation when exploring the sRNA world is that many base modifications can disrupt Watson/Crick base pairing and impede first-strand synthesis by reverse transcriptase (RT) 4. These chemical modifications therefore affect the detection and quantification of sRNAs, limiting the power of discovery. Although recent improvements to our TIGER pipeline have greatly enhanced our ability to assess sRNA content on HDL, base modifications would significantly impair efficient detection of these modified sRNAs. To circumvent these issues, a relatively new method was developed called AlkB-facilitated RNA methylation sequencing (ARM-seq) which exploits the RT roadblocks created by chemical base modifications (i.e. m1A, m3C and m1G) in tRNAs6. The E. coli AlkB homologs (ALKBH1 and ALKBH2) act as “eraser” proteins, catalyzing the demethylation of specific chemical base modifications6. This method represents a large step forward in the quantification of tRNAs, however a very limited number of studies have used ARM-seq for rDFs. Similar to tDFs and miRNAs, which were once readily discarded from RNA-seq datasets, rDFs may play important roles in the regulation of gene expression. As such, accurately quantifying rDFs and their modification status on HDL is key to gaining a more complete understanding of the biological functions of the epitranscriptome. Based on our previous studies and preliminary results I hypothesize that: (1) Improved sRNA- seq methods will increase the inclusion and identification of rDRs in HDL-sRNA datasets. (2) Stress factors induce parent rRNA fragmentation leading to an increase in circulating rDRs. I will address these hypotheses through two central aims. Aim 1: Enhance HDL-sRNA identification by characterizing the landscape of chemical base modifications found on sRNAs. To achieve this goal, we must capture and identify all host and non-host rDRs. This will include a.) Expanding bioinformatic analyses for rDRs, b.) Improving the identification of modified rDRs, and c.) Removing modifications on sRNAs for enhanced rDF inclusion in sequencing analyses. To address this aim I will first collect blood from healthy individuals and isolate their sRNAs found circulating on HDL using fast protein liquid chromatography. I will then pretreat HDL-sRNAs with the purified AlkB enzymes prior to cDNA synthesis (RT step) and library preparation. By comparing AlkB-treated and untreated samples, I will reveal the positional modification profile of HDL-sRNAs, including rDFs. The TIGER pipeline will be used to identify the diverse classes of sRNAs on HDL particles. The power of ARM-seq will be maximized by taking advantage of RNA modification databases, such as Modomics and RMBase. I expect ARM-seq to efficiently reveal chemical base modifications in the sRNA samples and increase the repertoire of rDFs. Aim 2: Characterize changes in parent rRNA fragmentation and cellular rDF export to HDL in response to environmental stress. Overwhelming evidence supports the role for specific environmental stressors to induce tRNA cleavage; however, very few studies have looked at rRNA fragmentation during environmental stress7. To determine whether oxidative stress, heat and cold stress, or γ-irradiation promote rRNA cleavage events, and the export of rDFs to HDL, I will treat human hepatic and non-hepatic cell lines with various environmental stressors (hydrogen peroxide, cold or heat shock, or irradiation with UV). Afterwards, the cells will be fractionated into nuclear and cytoplasmic extracts, and HDL will be isolated using a FPLC. To examine stress-induced rRNA fragments within these cellular fractions, I will use improved sRNA-seq approaches and confirm candidate rDFs using northern blot techniques. Moreover, we will quantify the export of hepatic rDFs to HDL in response to stresses to using HDL-sRNA export assays. I fully expect that exposure of specific environmental stressors will induce distinct parent rRNA fragmentation patterns and alter hepatic rDF export to HDL Broader Impact: Circulating sRNAs have been shown to be differentially altered in several diseases and hold great potential for the discovery of novel biomarkers and highly promising therapeutics. Given the value of potential biomarkers, the field of sRNA has led to cutting edge research. However, there are still gaps in our understanding of sRNA diversity on circulating HDL. My proposal helps to address this gap and may lead to the identification of yet unknown RNAs. With novel classes or sRNAs being discovered, and the validation of modified sRNAs, it is paramount that RNA-modification and sRNA databases are updated. I will disseminate my findings to web portals and servers dedicated to compiling databases for RNA modifications. Intellectual Merit: It was not very long ago that many sRNAs were considered “junk” and often removed from RNA-sequencing data analysis. However, we now know that sRNAs can regulate several aspects of gene expression. The novel pipeline generated by our bioinformatics team allows us to discern several classes of small RNAs found in both eukaryotes and prokaryotes. This interdisciplinary proposal applies techniques from bioinformatics, transcriptomics, microbiology, and biochemistry, and represents the first study aimed at identifying modified small RNAs on HDL. Successful completion of this proposal will not only expand the repertoire of sRNAs and rDRs but will also show how rDRs are important biological molecules. References: [1] Vickers et al. 2011. Nature Cell Biology. [2] Allen et al. 2018. Journal of Extracellular Vesicles. [3] Lambert et al. 2018. Non-coding RNA Investigation. [4] Su et. Al. 2019. J Biol Chem. [5] Rashad et al. 2020. Neural Regeneration Research. [6] Cozen et al. 2015. Nature Methods. [7] Thompson et al. 2009. Cell.	1
Progress in neuroscience is limited by the lack of proper tools available to biologists and neuroscientists to study neural circuits with high spatial resolution and cell type specificity. One area of neuroscience that is particularly affected by this absence is the study of somatosensory and motor control systems. Currently available tools used to study these systems and mimic their functions consist of electrode arrays, such as the polymer cuff electrode, attached to the peripheral nervous system1 or the Utah electrode array implanted directly into the motor cortex2. These electrode arrays, however, lack the ability to induce or record neural activation with cell specificity. Herein, I propose the development of a novel class of miniaturized, battery-free, wireless, soft, implantable neural machine interfaces (NMIs) utilizing optogenetics to study the somatosensory system in non-human primate (NHP) models via the peripheral nervous system (PNS). This proposal considers the recent advancements within the fields of optogenetics and photometry, advanced micro- and nano-fabrication methods, and the necessary collaborations to bring this project to fruition within a three-year period. Background Optogenetics is a growing neuroscience tool which utilizes viral injections to genetically modify neuron populations to express light-sensitive ion channels. The targeted neurons can be selectively stimulated among other tissues by selecting viral vectors and opsins with preferential tropisms. In NHPs, initial research in optogenetic stimulation of the peripheral nervous system shows channelrhodopsin-2 (ChR2) and Chronos delivered via adeno-associated virus and stimulating muscle injection to be successful3. Once the opsins are virally delivered, these neuron populations can be excited or silenced by targeting them with varying wavelengths and stimulation frequencies from light-emitting diodes. Recent papers have shown the success of optogenetics in stimulating the central nervous system via the brain and the spinal cord4,5. Similarly, genetically-encoded calcium indicators (GECI’s) and photometry can be used to visualize neural activation of defined cellular populations in-vivo6. These tools have significant advantages over electrical probes which lack the stimulation and recording specificity required for high resolution research into light touch information propagation through the low-threshold mechanoreceptor afferent neurons in the dorsal root ganglia (DRG). Neuron populations of particular interest for this study are the low-threshold mechanoreceptor afferent neurons within the DRG located in cord segments C6, C7, and C8, which are responsible for light touch information propagation from the lower forelimb and hand7. Aim 1: Optical Recording and Stimulation of Low-Threshold Mechanoreceptor Afferent Neurons The primary functions of the proposed device are to optically record the neural activity of low-threshold mechanoreceptor afferent neurons in a healthy NHP’s DRG, and to stimulate those neurons to replicate light touch information being transmitted through the neural circuit up A- and A- fibers. The cells will be targeted following the methods outlined by Williams et al. using ChR2 and Chronos, and with GECI’s. To enable both recording and stimulation, the device will employ a colloc𝛽𝛽ated micr𝛿𝛿oscale inorganic light- emitting diode (μ-LED) and photodetector (μ-IPD), both interfaced with a microcontroller for stimulation control and data processing respectively. Additional functional requirements to ensure device reliability are highly deformable mechanics and a usable lifetime of 10 years or longer. To ensure the device function for applications lateral the spinal column serpentine geometries, polymer substrate and encapsulations, and thin annealed metal traces will be employed to keep local strains under fatigue limits even under high bending and linear loads. To extend the usable lifetime of these devices, dielectric interlayers of thermally grown Silicon Dioxide and Hafnium Oxide will be used. These interlayers, employed in a total thickness up to 100µm, work to extend usable lifetime by retarding the ingress of ions and water vapor and disrupting pin-hole defects while remaining translucent8. Device encapsulation and fatigue mechanics will be tested using accelerated life testing (ALT) in an 87℃ phosphate buffered saline bath with complex mechanical loading conditions for 4 months, simulating an implanted lifetime of 10 years and 8 months. While ALT is being performed, the long-term reliability of the stimulating and recording capabilities will be assessed by measuring the irradiance of the μ-LED and the recorded signal of an external light source via the μ-IPD over time. Device electronics and wireless power harvesting will be assessed via continuous data logging. Aim 2: In-Vivo Testing in Non-Human Primates The final aim of the proposed research is to implant the proposed device into NHPs to conduct research on light touch propagation via low-threshold mechanoreceptor afferent neurons in the DRG. The anatomical similarities of mechanoreception between NHPs and humans allows for the study of light touch perception in the forelimbs that could not be studied in small animal models. This work will be done with collaborators who conduct NHP behavioral studies, external to the Yoon Lab at the University of Michigan where I propose to do my PhD. One study of interest includes training the NHPs to perform two-alternative forced choice tasks involving the differentiation of textures on their fingertips or palms and studying the neural activity for each of the presented textures. After the behavior is learned with accuracy of 95% or greater and the neural activity has been recorded and decoded, the NHP will perform the task again. However, this time the NHP will receive light touch information from the device via optogenetic stimulation of the mechanoreceptor afferent neurons in the C6 C7 and C8 DRG without any textures presented. Future Directions Upon completion of this work, the goal is to transition from the study of the peripheral nervous system’s role in light touch perception to implantation within a NHP amputee. Once implanted, this device will interface with an upper-limb prosthetic via near field communication and used to replace the lost light touch perception abilities of the amputated limb. If shown to be successful, the next step is to use this device in conjunction with functional magnetic resonance imaging to study the long-range neural circuits of the somatosensory system, a study never before possible. Intellectual Merit The development of this device will utilize recent advances in materials science, fabrication, and optogenetics to advance neuroscience tools. The design process and in-vivo testing will also require collaboration with the departments of Biology and Neuroscience as well as external collaborators to inform the selection of virus, opsin, μ-LED, and μ-IPD. Once developed, these devices would directly promote an advancement in the understanding of the peripheral nervous system’s role in somatosensory processing and propagation and introduce a platform of devices for the targeted study of the somatosensory system and other short- and long- range neural circuits in-vivo. Broader Impacts These devices would be applicable not only in the study of light touch information but any neuron type and thus have broad impacts in neuroscience, neurotherapies, and limb rehabilitation or replacement for paralyzed or amputated individuals. Outside of the medical field, the ability to transmit somatosensory from an external input to the peripheral nervous system could also be used to advance entertainment systems and virtual reality to include touch perception. References 1. Elyahoodayan, S., et al.. Acute in vivo testing of a polymer cuff electrode with integrated microfluidic channels for stimulation, recording, and drug delivery on rat sciatic nerve. J. Neurosci. Methods 336, 108634 (2020). 2. Maynard, E., et al.. The Utah Intracortical Electrode Array: A recording structure for potential brain-computer interfaces. Electroencephalogr. Clin. Neurophysiol. 102, 228–239 (1997). 3. Williams, J., et al.Viral-Mediated Optogenetic Stimulation of Peripheral Motor Nerves in Non- human Primates . Frontiers in Neuroscience vol. 13 759 (2019). 4. Ausra, J. et al. Wireless, battery-free, subdermally implantable platforms for transcranial and long- range optogenetics in freely moving animals. Proc. Natl. Acad. Sci. 118, e2025775118 (2021). 5. Kathe, C. et al. Wireless closed-loop optogenetics across the entire dorsoventral spinal cord in mice. Nat. Biotechnol. (2021) 6. Burton, A. et al. Wireless, battery-free subdermally implantable photometry systems for chronic recording of neural dynamics. Proc. Natl. Acad. Sci. 117, 2835 LP – 2845 (2020). 7. Vanderah, T. W. & Gould, D. J. Nolte’s the Human Brain: An Introduction to its Functional Anatomy. (Elsevier, 2021). 8. Jeong, J. et al. Conformal Hermetic Sealing of Wireless Microelectronic Implantable Chiplets by Multilayered Atomic Layer Deposition (ALD). Adv. Funct. Mater. 29, 1806440 (2019).	0
"Introduction: Glass-Ceramics (GC’s) are critically relevant materials for industry and scientific research, primarily due to their outstanding mechanical, thermal, and optical properties. High-grade GC’s such as lithium- aluminosilicate (LAS) are commonly used as insulation materials for high-performance aircrafts and missiles, optical bodies of precision optics, and biomaterials for medical equipment[1]. Although the material properties of GC’s are very attractive in many engineering fields, the cost of manufacturing complex geometries can be prohibitive, primarily due to the high cost of tooling and limited machining capabilities of present manufacturing methods. Therefore, it is the goal of the proposed work to implement a novel method of manufacturing GC’s with predictable, tailorable, and optimized material properties. GC’s are classified as two-phase materials; one being the glass matrix, the other being small volume fractions of nanocrystal inclusions. Typically, GC’s are manufactured through casting or forming methods based on glass-making techniques. In these methods, the glass matrix is heated to high temperatures using a two-step process. In the first step, known as nucleation, the GC is heated just past its devitrification temperature to create a high density of nuclei throughout the interior of the glass. The second step involves re-heating the GC to a highly controllable temperature which directly impacts the growth rate, crystal size, and region of crystallization[2]. Vat Photopolymerization (VP) is an Additive Manufacturing (AM) process which utilizes UV light to selectively cure a polymer-based resin in a layer-by-layer fashion. VP can offer unparalleled resolution, complex internal and external features, and high-solid loadings of GC’s to further enhance their applications. Digital Light Processing (DLP) is a sub-category of VP which uses a UV projector instead of a laser to expose cross-sections of the design geometry onto a resin vat. Therefore, the curing characteristics of the polymer resin are controlled by the light intensity and the effective pixel size from the projector. Additionally, by carefully tailoring the monomer, oligomer, and photo-initiator concentrations in the resin, high solid-loading of GC’s within the resin could be achieved[3]. The polymer matrix is finally burnt off through a debinding step before the sintering process, resulting in a highly pure and fully dense GC part. To further improve the mechanical, thermal, and optical properties of the GC’s, an Ion-Exchange (IX) process will be implemented after the sintering step. During this process, cations of small atomic size from within the glass matrix surface are replaced by larger cations from the molten salt bath through a diffusion mechanism driven by temperature difference, resulting in a permanent compressive force in the surface of the part, which suppresses the growth of surface flaws and reduces crack propagation within the glass[4]. Proposed Research Activities: Objective 1: Understanding primary and secondary parameter influence on material properties of Glass-Ceramics. From my previous and ongoing research, it has been noted that the concentrations of monomer, cross-linker, and photo-initiator in respect to the solid-loading of the matrix material greatly impact the printability and final properties of the GC. Therefore, it would be highly beneficial to understand the primary and secondary parameters during the printing, debinding, and sintering processes and their impact on the final material properties. Primary parameters from the printing step could include UV exposure times, light intensity, and layer thickness; secondary printing parameters could include layer waiting time, lifting speed, and vat temperature. Degree of Conversion (DoC) is a common measurement tool that utilizes Fourier-Transform Infrared (FTIR) characterization data to rapidly quantify the progress of the photopolymerization reaction. Primary and secondary printing parameters will therefore be directly quantified and compared through DoC by use of FTIR. For debinding, primary parameters could include ramping rates and holding temperatures; secondary parameters could include crucible materials and furnace atmosphere. Initially, Thermogravimetric analysis (TGA) will be performed on printed samples to determine ideal holding The University of Texas at El Paso 1 Sebastian Vargas NSF GRFP Research Statement temperatures in efforts to ensure complete removal of organic compounds. Additionally, Energy Dispersive Spectroscopy (EDS) will be used as an elemental analysis tool to compare anticipated composition of the GC’s to actual results. In terms of sintering, primary parameters could include final sintering temperatures and furnace atmosphere; secondary parameters could include ramping rates and cooling rates. X-Ray Diffraction (XRD) analysis will be performed on sintered samples to broadly determine the degree of crystallization by comparing results to literature and standards. Finally, by identifying the relationships (linear or non-linear) between parameters and material properties, a novel, reliable, and well-understood manufacturing method for GC’s based on the DLP process could be achieved. Objective 2: Inclusion of alkali modifiers for chemical strengthening through Ion Exchange (IX). Based on the recent work of Gy, R.[4] and Macrelli, et al.[5], lithium, potassium, and sodium ion modifiers will be added to the GC resin formulation in preparation for chemical strengthening through the IX process, which will be carried out directly after Objective 1. The depth of penetration of the cationic exchange layer, also known as case depth, is a direct metric of the effectiveness of the IX process and will be evaluated at various depths using a refractometer. As mentioned before, the strengthening process intrinsically develops a residual compressive stress at the surface of the GC. Therefore, the effect of cation modifier concentration on the final mechanical properties will be assessed by compressive, flexural, and hardness testing based on ASTM standards and will be performed with equipment available at the Keck Center and SMP lab. Extended Objective: Supporting the development of a machine-learning-based model to predict material properties of glasses from compositional data. Based on the recent work by Ward. et al[6], a framework capable of extracting predictive models from existing materials data is being developed by the Kansas City National Security Campus (KCNSC). My research will serve to provide the model with characterization and testing data obtained from Objectives 1 and 2 in order to effectively populate the model. More detailed information may not be suitable for public release at this time. Intellectual Merit: The proposed work represents a novel method for manufacturing two of the most relevant materials to society: ceramics and glasses. My research would directly advance the limited understanding of the intricate relationships between input parameters and material properties at different steps of the DLP manufacturing process. This critical understanding could potentially overcome a common barrier towards further development and implementation of DLP-based AM as a prevalent manufacturing method. Broader Impact: The development of DLP-based AM could enable previously unachievable part geometry of GC’s and therefore, become a highly tailored process to impact many industries including aerospace, defense, and medical. Additionally, this work could drive further research in STEM, including fields such as Additive Manufacturing, Machine Learning, and materials science. Finally, the proposed work would directly support ongoing research efforts at the KCSNC, and thereby, the National Nuclear Security Administration. References: [1] Elan Industries. https://www.elantechnology.com/glass/glass-materials/las-glass-ceramics/ [2] Rawlings, R. D., J. P. Wu, and A. R. Boccaccini. ""Glass-ceramics: their production from wastes—a review."" Journal of Materials Science 41.3 (2006): 733-761. [3] Kotz, F, et al. ""Three-dimensional printing of transparent fused silica glass."" Nature 544.7650 (2017): 337-339. [4] Gy, René. ""Ion exchange for glass strengthening."" Materials Science and Engineering: B 149.2 (2008): 159-165. [5] Macrelli, Guglielmo, Arun K. Varshneya, and John C. Mauro. ""Ion Exchange in Silicate Glasses: Physics of Ion Concentration, Residual Stress, and Refractive Index Profiles."" arXiv preprint arXiv:2002.08016 (2020). [6] Ward, Logan, et al. ""A general-purpose machine learning framework for predicting properties of inorganic materials."" Nature: npj Computational Materials 2.1 (2016): 1-7. The University of Texas at El Paso 2"	0
Introduction: Organic (polymeric) semiconductors (OSCs) are readily processible, leading to numerous advantages over traditional inorganic semiconductors.[1] These advantages include the ability to be fine-tuned for properties such as solubility, thermal processing, and optoelectronic properties, which allows OSCs to be leveraged for a variety of applications.[2] Through this molecular design, the charge carrier mobility of OSCs has been improved over the past three decades by seven orders of magnitude.[2] OSCs have been targeted for use in many future technologies, such as organic thermoelectric devices and organic solar cells, which can be used as alternative, green energy sources. One challenge in realizing this potential is the inherent structural and electronic disorder in polymers used as OSCs. In analogue to inorganic semiconductors, OSCs can be doped to increase electron or hole conductivity. These dopants take the form of small acceptor/donor molecules, which can be introduced into the polymer matrix a variety of ways. These methods include mixing the polymer and dopant in solution (solution doping), immersing or casting the dopant solution on top of the previously-cast polymer film (sequential doping), and subliming the dopant into the previously-cast polymer film (vapor doping). Although solution doping can be utilized for scale- up procedures such as roll-to-roll processing, solution doping leads to polymer aggregation and eventual precipitation, which leads to poor quality films. This causes a lower conductivity than as sequential and vapor doping, which have better local and long range ordering.[3] Despite the progress in doping methods, the dynamics for molecular doping into polymer films are not well known. This is due to the complexity of the charge transfer mechanism of molecular dopants in OSCs. In consequence, I propose to develop an easy-to-use platform to study the dynamics of OSC doping in situ, which will reveal both fundamental doping mechanisms as well as efficient and effective doping methods. The Institute for Molecular Engineering (IME) at The University of Chicago is a vibrant place for collaboration, and in conjunction with the Rowan group (synthetic) developing novel polymers for OSCs and the de Pablo group (computational) modelling advanced doping mechanisms, this platform would provide a method for probing the dynamics and optimization of a polymer/dopant pairing. Further understanding on the dynamics of OSC doping will inform further fine-tuning of OSCs and their development for use in novel, alternative-energy sources. Preliminary Results: In order to develop a platform for studying the dynamics of doping, I have used a model polymer-dopant pairing of Poly(3-hexylthiophene) (P3HT) and 2,3,5,6- Tetrafluoro-7,7,8,8-tetracyanoquinodimethane (F4TCNQ). P3HT has been widely studied as a conductive polymer due to its processability and ability to form an ordered semi-crystalline morphology. The conductivity of doped P3HT has been shown to increase six orders of magnitude from pristine to highly doped states.[5] Our custom-built chamber allows for measurement of in situ conductivity response to doping (Fig. 1), which is crucial for this platform. I have performed ) m c /S ( y t iv it 112 ... 050 )m c/S( ytivitcudnoC 111111 000000 ------ 1654321 00 R R Ru u un n n 1 2 3 Dopi1 n0 g1 time (s) 102 R RR u uu n nn 2 31 F c o P s l oao n 3i gmg n H -u d p lT our lee g-tc h F s t i r .4o pe T 2 n le oIC: n td N sa e oQtI s ta fn e s pt h oaa o nrks a swi et etnu e s t c regime, showing u A B d n 0.5 Onset Linear Saturation growth from 2E-6 to 2 Figure 1: (A) Chamber holds electrode array with o C regime regime regime S/cm over nine 0.0 minutes of vapor tungsten contacts. (B) Chamber sits above 0 200 400 600 doping. Data collected subliming dopant, with contacts for in situ conductivity measurements. Doping time (s) 14 Sept., 16:00 CST. 1 Mark DiTusa NSF GRFP Research Statement initial conductivity measurements on P3HT/F4TCNQ polymer-dopant pairing spun coat onto an interdigitated electrode array (IDA) we designed in the Pritzker Nanofabrication Facility at UChicago. These samples were vapor doped in our chamber for over nine minutes while in situ conductivity measurements were made. Dopant mass, surface area, and doping temperature were held constant between measurements. These measurements showed a profile that increased six decades in close agreement with multiple literature values.[1,4,5] These measurements were shown to be consistent over multiple runs (Fig. 2). My measurements also showed three doping regimes: an onset regime, a linear regime, and a saturation regime. These three regimes, which has not been previously reported in literature, will be individually probed for evidence on the dynamics of molecular dopants during the doping process. I have also conducted Grazing-Incidence Wide- Angle X-ray Scattering (GIWAXS) on pristine P3HT and P3HT doped with F4TCNQ at the 8-ID- E beamline in the Advanced Photon Source (APS) at Argonne. This technique showed us detailed information about the crystallinity of the film and how the dopant disrupts this structure. These measurements indicate that, for P3HT, the polymer backbone spacing is reduced and sidechain spacing increased by the presence of molecular dopant. Research Plan: With the platform for measuring conductivity in situ complete, I plan to use this platform in conjunction with complementary characterization techniques to study novel polymer-dopant systems. These polymer-dopant systems will be provided by our collaboration with the Rowan groups within the Institute for Molecular Engineering at UChicago. This project also allows me to be trained further at the Pritzker Nanofabrication Facility to produce the interdigitated electrode arrays necessary to measure polymer-dopant properties in situ. I will employ a complementary group of characterization techniques to probe these systems. UV-Vis-NIR spectroscopy in conjunction with FTIR spectroscopy will be used to identify charged species in films at all three doping regimes, allowing us to measure the concentration of charge carriers in the polymer films. Raman spectroscopy and microscopy will be used to measure and map the distribution of charge carriers in our systems in different doping regimes. Conductive Atomic Force Microscopy will allow us to map conductivity on the surface of our systems. These techniques will occur at the Materials Research Center at UChicago, an NSF MRSEC facility. Our relationship and proximity with Argonne Nation Laboratory allows us to utilize advanced techniques to characterize the polymer-dopant system. Synchrotron (high energy) X- rays are required for this project to probe the small length scales of our polymer films. As previously detailed, GIWAXS obtains measurements of the local crystallinity of the polymer film. Beamline 8-ID-E at the APS also has expertise in X-ray Photoelectron Correlation Spectroscopy, which allows for in situ measurement of the slow dynamics of the doping system. Resonant Soft X-ray Scattering at beamline 29-ID-D obtains morphology over large length scales (10-1000 nm). Conclusions and Broader Impacts: The data I will obtain from in situ measurements and characterizations will give us a more complete picture of how molecular dopants infiltrate and modulate conductive polymers throughout the entire doping process. Through our collaborations with the Rowan and de Pablo groups in the Institute for Molecular Engineering at UChicago, we will be able to use our platform with novel polymer-dopant systems, aiding in the discovery of systems with high conductivities. This, in turn, will allow for the improvement of organic semiconductor devices, furthering the development of OSCs for use in critical (opto)electronic applications such as thermoelectrics and organic photovoltaics. Receiving the NSF GRFP will allow me to realize these goals and to present at public outreach events about the need for alternative energy sources and how my research brings these technologies closer to realization. 1. Adv. Mater. 2017, 1703063 2. MRS Commun. 2015, 5 (3), 383-395 3. J. Mater. Chem. C. 2016, 4, 3454-3466 4. Macromolecules. 2017, 50 (20), 8140-8148 5. Phys. Rev. B. 2015, 91, 085205 2	0
which humans can run experiments and the enormous size of the search space.1 Recently, the materials- design loop has been accelerated through several different mechanisms, including robotic high-throughput experimentation (HTE), atomistic simulations, and machine learning (ML).1 While each of these techniques has independently shown promise for accelerating discovery, an approach that applies all three harmoniously would revolutionize chemical research with wide-ranging implications, enabling faster and cheaper discovery of new pharmaceuticals, catalysts, and photovoltaic devices. I aim to advance this effort in my own research by coordinating the interplay between these three methods for the discovery and design of new dye molecules. Dyes are a suitable class of molecules for testing an autonomous, integrated design platform because they have several readily measurable properties that must be optimized simultaneously for use cases ranging from solar cells to medical imaging. Specifically, I am focusing on the following objectives: (1) developing ML models to predict UV-Vis absorption and emission spectra accurately given a dye molecule and solvent pair, (2) creating a generalizable, automated active machine learning framework to improve the prediction models, and (3) utilizing this framework to design a novel near-infrared (NIR) dye for biomedical sensing and diagnostics. Objective 1 - Model Development for UV-Vis Spectra Predictions: Accurate prediction of UV-Vis optical properties is essential to dye design for any application. Previous work toward predicting UV-Vis spectra with ML has mostly consisted of the simpler task of predicting two scalars, the wavelengths of the peaks of maximum absorption and emission (𝜆 and 𝜆 )2, and has been limited by data sparsity. Since abs em starting my work with Prof. Rafael Gómez-Bombarelli in January, I have addressed this limitation by collecting all openly accessible UV-Vis data from seven online repositories (29,811 measurements in total) and standardizing it into a consistent format. I then used a combination of a directed message-passing neural network (DMPNN)3 and a feed-forward neural network to predict a value for 𝜆 given an input molecule- abs solvent pair and an analog of 𝜆 computed with time-dependent density functional theory (TD-DFT). abs Using this method, my model has achieved a test-set mean absolute error (MAE) of 8.68 nm (a 17% reduction in error over the previous best model) on the largest dataset for which ML predictions have been published.2 My first step toward extending this method to predict full spectra will be to train my model to predict the peak widths and intensities for each 𝜆 using the data I assembled. I foresee the limited abs quantity of available data presenting a challenge for predicting full spectra since the majority of openly accessible data contains only 𝜆 values for each molecule-solvent pair. I will address this issue with a abs pretraining strategy in which I train my model with lower-fidelity data and use the resulting neural network weights as the initial weights when training my final model (as opposed to a random initialization). I will then estimate the epistemic and aleatoric uncertainty in my model’s predictions using a deep ensembling approach.4 Finally, I will replicate the previous steps for predicting emission spectra. My accurate models for predicting absorption and emission spectra will aid experimentalists in choosing which molecules to test, even before I further automate this process in the following objective. Objective 2 - Active Learning: My models’ abilities to make predictions with corresponding uncertainties will fulfill an important prerequisite for implementing active learning (AL), which improves models by focusing the sampling of new data on molecules with high uncertainties in their predictions. The additional components needed for active learning are (1) a set of new molecules from which to sample, and (2) a method of measurement for each sample. My experimental collaborators in Prof. Klavs Jensen’s group have created (1) by extracting a list of 7 million purchasable compounds from chemical vendor websites. Further, they have created a method for (2) by building an HTE apparatus for measuring 96 UV-Vis spectra simultaneously. Since TD-DFT calculations are faster and cheaper than experiments, I propose using these to augment the strategy for (2) by reducing the number of necessary experiments. I will design a computational framework to automatically deploy calculations for molecules with a high epistemic uncertainty and retrain my models using this new data. From molecules that still have high uncertainty after the calculations, my system will use the uncertainty values along with molecular similarity to choose 96 molecules to recommend for measurement in the HTE apparatus. My models will automatically receive data from the experiments and repeat the previous steps iteratively until their predictive performances reach asymptotes of aleatoric uncertainty. The proposed AL framework integrates TD-DFT and HTE in an automatic fashion, which will enable significant time and cost savings and will be readily generalizable to many areas of chemistry and materials science research. Objective 3 - Design of a Novel Dye for Biomedical Imaging: Once I am able to demonstrate that my ML models are sufficiently accurate over a large region of chemical space, I will adapt my AL framework to design novel molecules with optimized properties for biomedical imaging. Specifically, it is favorable for dyes to have absorption and emission peaks in the NIR-II range (1000-1700 nm) because this range has deeper tissue penetration compared to visible or shorter-wavelength NIR-I light.5 High Stokes shift (𝜆 - em 𝜆 ) and high quantum yield are also desirable.5 Additionally, I will leverage the ongoing work of my abs collaborators in Prof. Bill Green’s group who are predicting solubility, toxicity, and photodegradation, as these are also important properties for this application.5 I will employ the generative models of Jin et al.6 to create new molecules out of substructures that are likely responsible for desired properties of interest in known molecules. Next, I will make predictions on these new molecules with my ML models. I will then modify my AL framework to explore the new chemistries proposed by the generative models; it will deploy TD-DFT calculations as necessary for molecules with uncertain predictions and ultimately recommend novel molecules with predicted properties in the target ranges to my experimental collaborators in the Jensen group. They will use automatic retrosynthesis methods7 to synthesize the novel compounds and will then use their HTE apparatus to test which proposed molecules indeed have the desired properties. Finally, I will propose the best-performing molecules to Prof. Angela Belcher’s group for further study and in vivo testing. If successful, this strategy could serve as a blueprint for combining experiments, theory, and ML for multi-objective molecular design across the field of chemistry. Intellectual Merit: Design problems in chemistry and materials science often suffer from a combinatorial explosion of configurations to explore, which makes solution of these problems intractable by brute force, or with HTE, physics-based calculations, or ML alone. By using all three methods simultaneously and automating the interactions between them, my work will be an advancement toward a “closed-loop” system that can explore massive chemical spaces with minimal need for human intervention beyond the specification of design objectives. Conducting my work at MIT gives me the opportunity to collaborate with experts who have proven records of integrating chemistry and computer science methods, and it gives me access to computing resources to run atomistic calculations and train ML models. An NSF fellowship would supplement my current computing resources with access to XSEDE and would ensure the necessary funding for my completion of this project. Broader Impacts: I will design a novel dye that could be applied to guide surgery or to detect cancer at earlier stages. My work’s flexible multi-objective optimization will also be able to design new dyes for additional applications such as dye-sensitized solar cells. Furthermore, the AL framework I develop could be widely adopted to design other types of molecules and materials, such as those in batteries and catalysts. I plan to make all code and datasets I develop openly available online with detailed documentation, which will enable other researchers to replicate and build upon my work more easily. Additionally, I will host a workshop to demonstrate my framework, with the goal that even experimentalists with little computational experience would learn to utilize the AL component of my framework to accelerate their progress in molecular or materials design. My work ultimately aims to encourage greater collaboration between experimental, theoretical, and computational researchers by automating the connections between their work in pursuit of design challenges that would otherwise be intractable. References: [1] Angew. Chemie Int. Ed., 2019, doi:10.1002/anie.201909987. [2] ChemRxiv, 2020, doi:10.26434/chemrxiv.12111060.v1. [3] J. Chem. Inf. Model., 2019, 59 (8), 3370–3388. [4] J. Chem. Inf. Model., 2020, 60 (6), 2697–2717. [5] J. Mater. Sci., 2020, 55 (23), 9918–9947. [6] ICLR, 2020, arXiv: 2002.03244. [7] Science, 2019, 365 (6453), eaax1566.	0
Dynamics of Alliance Formation in Pueblo Societies Keywords: Climate Change, Agent-based modeling, Alliance, Puebloans, Cooperation, Conflict Introduction: Previous work within the Village Ecodynamics Project (VEP) has successfully established a detailed, semi-realistic, household-level model for Puebloan ecodynamics1. I propose to extend this household model to create agent-based models for conflict and cooperation in the context of the 700-year archaeological record of the central Mesa Verde region. Here, the formation of larger groups is linked in a complicated way with conflict, but it is also probable that mutualistic activities not motivated by between-group conflict contributed to these larger group sizes. This model will help us understand the years of peace within the Mesa Verde region, and the circumstances under which Puebloan people resorted to violence, as these cycles have been locally described by Cole2. The models I create will be applicable to other small-scale societies—and elsewhere, with appropriate caution. Background: As resources dwindle, climate change is reshaping the earth, leaving us faced with problems with potentially dire consequences3. Repeated calls have recently been made to apply agent-based modeling to contemporary affairs4, both to understand crises as they unfold, and to anticipate them. In these efforts, archaeology assists by providing a long-term view of the relationship between demography, distribution of human group sizes, environmental factors and violent conflict. My place at Washington State University in the context of the VEP will allow me to address these questions with support from archaeologists, geologists, geographers, computer scientists and economists engaged in VEP empirical and modeling efforts. Hypotheses: My research will investigate how human cooperation affects the demographic success and spread of human groups. Specifically, my research will examine the following hypotheses: 1) both kinship- and non-kinship-based coalitions formed in response to environmental pressures, such as dwindling per-capita resources due to climate change or population growth; 2) coalitions do not form only as a response to external conflict; rather, they are leveraged by humanity’s evolved sociality5 and can serve to provide positive returns to increasing group size; 3) coalitions may fracture when within-group competitive pressures become too great, or when between-group competitive pressures relax. Research Plan: Working with Dr. Timothy Kohler and the VEP, I will participate in ongoing field research in the Mesa Verde region. As an NSF Graduate Research Fellow, I will generate spatial goodness-of-fit measures between VEP simulations and the archaeological record to assess the general fidelity of the simulation to archaeological data from Mesa Verde, as well as to analyze and interpret the residuals. I aim to understand how accurately the existing agent- based models (ABM) predict the spatial distribution of households, subsistence and technology, and to evaluate the extent to which the simulations generate the archaeological record. Moreover, I will explore various methods of assessing spatial goodness-of-fit using over 4,000 archaeological sites in the VEP study area from AD 600 to 1280. Next, I will create a model describing the emergence of alliances based on kinship and economic ties. Currently, the Village simulations do not allow for cooperation beyond that provided by exchange, or conflict beyond that generated through household-level competition for resources. Building upon Dr. Sergey Gavrilets’ (University of Tennessee Knoxville, Biology) framework for alliance formation6, I propose to create a stochastic model describing the emergence of cooperation resulting from between-group competition for key resources. I will gradually add levels of complexity to the unidimensional model as described by Gavrilets, which accounts for alliance formation only through competition for rank or mates. I will introduce a means of incorporating scalar stress in order to generate nested groups, in contrast to the Stefani Crabtree Proposed Research Essay exponential growth of alliances in Gavrilets’ model. An additional shortcoming of the previous model is that it only accounts for alliance-formation as a response to conflict, which ignores altruism and mutually beneficial relationships in coalition formation. Using the experimental test bed provided by the ABM, I will see whether approaches to generating cooperative networks modeled on the sodalities seen in Southwestern societies provide a better fit to the known facts of the archaeological record than do alliances generated out of between-group conflict. In modern Hopi societies, for example, sodalities form around a specific clan, “which own[s] the ceremonies, kivas, and ritual items used by each sodality. However, while sodalities are managed by specific clans, sodality members can come from any clan”7. My benchmark for comparison will be the well-known and precisely-dated archaeological record of the central Mesa Verde region, which provides a dataset that is unparalleled in the Neolithic world. Broader Impacts: This novel approach will provide critical information on the nature of human alliance formation. As my research will analyze how issues such as control of resources influence the formation of alliances, I will be able to determine how these alliances break down when resources become scarce. My results may have widespread applicability as the human population continues to grow worldwide, stretching the resources of our fragile planet. Understanding what lead to the dissolution of civilizations in the Neolithic world may help policymakers anticipate future challenges. My research will inform efforts to understand sociopolitical impacts of climate change. Through agent-based models of the archaeological data, I will analyze how people reacted to fluctuating temperature, reduction of key resources such as woody fuels and water, crop failure, and inter and extra-tribal hostilities, which may have been induced from the changing environment. Additionally, this research will examine the extent to which alliances form out of conflict, or as a means of providing positive per capita return in procurement of resources, and help us to understand not only the years of peace dominating the Mesa Verde region, but also the wave of violence that swept the area in its final years2. Future researchers will be able to build on these models to understand the complex dynamics of human relations in other societies. We are poised at a cross-roads as a civilization, plagued by many of the same issues that our ancestors faced. An understanding of our past will help us make informed decisions about our future. 1 Kohler, T., et. al. 2007. Settlement Ecodynamics in the Prehispanic Central Mesa Verde Region. In The Model-Based Archaeology of Socionatural Systems, edited by T. A. Kohler and S. van der Leeuw, pp. 61-104. SAR Press, Santa Fe. 2 Cole, S. 2007. Population Dynamics and Sociopolitical Instability in the Central Mesa Verde Region, A.D. 600-1280. Unpublished Master’s Thesis, Department of Anthropology, Washington State University, Pullman. 3 Cabrera, D. et al. 2008. What is the crisis? Defining and prioritizing the world’s most pressing problems. Frontiers in Ecology and the Environment 6(9):469–475. 4 for example, see: Buchanan, Mark. 2009. Meltdown modeling: Could agent-based computer models prevent another financial crisis? (News Feature) Nature 460(6):680-682. 5 Henrich, J., et.al. (2005) ‘Economic Man’ in Cross-Cultural Perspective: Ethnography and Experiments from 15 small-scale societies. Behavioral and Brain Sciences, 28, 795-855.. 6 Gavrilets S., et. al. 2008. Dynamics of Alliance Formation and the Egalitarian Revolution. PLoS ONE 3(10): e3293. doi:10.1371/journal.pone.0003293 7 Kantner, J., 2004. Ancient Puebloan Southwest. Cambridge University Press. Cambridge, UK. p. 262	0
NSF GRFP Application – Research Proposal Research Question and Intellectual Merit: How does internal migration influence the geographic diversity of intergenerational income mobility (IIM1) in the U.S.? The IIM literature has seen a surge in activity, in part thanks to Chetty et al. (2014) (henceforth CHKS), who link several years of IRS tax data to investigate IIM in the U.S. on an unprecedented scale. Among their key findings is that the expected economic outcomes of a child vary drastically based on their commuting zone (CZ) of origin. Both CHKS and much of the literature that has followed it have focused on the importance of the characteristics of where an individual is from in influencing their expected income mobility as opposed to where (or whether) they go. This may be in part because CHKS themselves appear to put the issue to rest: they find that their IIM estimates do not change meaningfully after limiting their sample to individuals who stay in their original commuting zone, nor do they appear to be strongly correlated with CZ-level net migration rates. However, limiting the sample to stayers is insufficient to fully investigate the role of self- selected migration in forming the landscape of IIM in the U.S. if this sample is endogenously determined2, and focusing on more narrow migration patterns than net rates uncovers a more suggestive relationship. Figure 1 juxtaposes state-level IIM estimates with the college graduate outflow rate3 in each state. With few exceptions, the most income-mobile states in the country (namely, those in the rural Midwest and the Mountain States) also exhibit some of the highest rates of out-migration. Table 1 reveals this visual association to be statistically robust on a basic level after controlling for the most important correlates of IIM that CHKS identify. This project will more meticulously consider the importance of internal migration in generating spatial variation in IIM through the development and estimation of a structural model. In doing this, I will provide new insight on an oddity that has not been thoroughly probed hitherto: the fact that children from underprivileged backgrounds seem to fare the best when coming from some of the most remote and forgotten-about places in the country. Creating a formal model will also allow me to add to the relatively much smaller recent literature that carefully evaluates policy counterfactuals regarding IIM. Methodology: I intend to construct and solve a lifecycle model that follows the migration and child-rearing decisions of agents from high school graduation into early adulthood. The model will expand the classic Becker and Tomes (1979) framework to incorporate local labor market conditions and moving opportunities. Agents are born in a home CZ to parents of a certain income level, who also endow them with a set of inherited attributes and human capital investments. The children then choose whether to stay or move to a new location, after which they select how many children to have of their own and how much to invest in them. Investments in children are differentially costly across locations to reflect heterogeneity in public school quality. In this framework, local labor market quality will induce dual effects on IIM: stronger labor markets will improve the outcomes of stayers but will also depress incentives for agents to leave and find a better match. This may provide motivation for recent empirical findings that conventional measures of local labor market quality have little predictive power for IIM. 1 Measured as the expected national income percentile in 2011-2012 of a child born in 1980-1982 to parents who were in exactly the 25th national income percentile over the years 1996-2000. 2 If a highly income mobile CZ also has high rates of out-migration, and natives who stay do so because they received unusually good income realizations in their home, then the CZ will continue to exhibit high levels of IIM even after the sample restriction. The related-but-distinct thought experiment I consider is what would happen to IIM in the U.S. if those that would move from their home CZ are somehow restricted from doing so. 3 Measured by taking the sample of income-earning college graduates from the 1980-1982 birth cohorts in 2011- 2012 and computing the percentage of individuals born in a state who are observed living elsewhere. 1 Garrett Anstreicher NSF GRFP Application – Research Proposal Figure 1: IIM (Percentile) and College Table 1: OLS Estimates for Various Graduate Outflow (%) in U.S. States Covariates of State-Level IIM VARIABLES IIM College grad outflow 0.0899** (0.037) Share single mothers -0.728*** (0.240) Student-teacher ratio -0.336 (0.193) Constant 66.81*** (13.19) Observations 49 R-Squared 0.697 Table Notes: Standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.1. Non-displayed controls include share black, Theil segregation index, college graduation rate, labor force participation rate, high school graduation rate, violent crime rate, and Gini coefficients. Figure Notes: IIM estimates for top map from http://www.equality-of-opportunity.org/data/. Data for bottom map from 2011 and 2012 American Community Survey (Ruggles et al., 2017). The central mechanism I aim to capture resembles an intranational brain drain: parents from areas with cheap human capital and poor labor market conditions will face incentives to heavily invest in their children, who in turn will leave their home CZ. The process of leaving will allow the child to select their most compatible labor market, greatly increasing their chances of claiming a higher wage and bolstering the measured IIM of their place of origin. Broader Impact and Conclusion: In addition to motivating the high IIM of remote areas, I intend to evaluate the efficacy of various educational policies. An example is New York’s Excelsior Scholarship, which remits tuition under the stipulation that recipients stay in the state for some time following graduation. Such a policy may work well in increasing the supply of college graduates in states where opportunities are abundant such as New York, but it may not be nearly as effective in rural areas with more condensed wage distributions. Expanding this model to consider general equilibrium effects could also allow me to address myriad issues. How will geographic wage distributions in the U.S. evolve over time in response to self-selected migration flows? Will the brain drain I capture lead to further economic deterioration in the rural U.S., or will its declining living costs induce more highly skilled individuals to return? These are important questions that my model may be extended to answer. References Chetty, R.; Hendren, N.; Kline, P. and Saez, E. “Where is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States.” The Quarterly Journal of Economics, 129(4): 1553-1623, 2014. Becker, G. and Tomes, N. “An Equilibrium Theory of the Distribution of Income and Intergenerational Mobility.” Journal of Political Economy 87(6): 1153-1189, 1979. Ruggles, S.; Flood, S.; Goeken, R.; Grover, J.; Meyer, E.; Pacas, J. and Sobek, M. IPUMS USA: Version 8.0 American Community Survey. Minneapolis, MN: IPUMS, 2018. https://doi.org/10.18128/D010.V8.0. 2	0
Introduction: The Channeled Scablands is a striking landscape that captures a remarkable moment in Earth’s history when enormous quantities of glacial meltwater poured across the region. These glacial floods carved deep canyons, referred to as coulees, into basalt bedrock within the otherwise subdued topography of the Columbia Plateau. The most impressive of these, Grand Coulee, is the largest flood-carved canyon on Earth at 200 m deep and nearly 100 km long. Once thought to be glacially-carved, the recognition that Grand Coulee formed due to the upstream erosion of what must have been one of the largest known waterfalls on Earth [1] brought about a revolution in geological thinking by proposing that catastrophic events—rather than slow, uniformitarian processes—can dominate the evolution of Earth’s surface. Though a catastrophic flood origin of the Grand Coulee is now accepted, many questions still remain regarding the size and number of floods that carved it [1]. I propose to use cosmogenic nuclide exposure dating to measure the retreat rate of the Grand Coulee waterfall, and to combine field evidence of sediment transport with numerical flood models to constrain the discharge of the outburst floods that carved Grand Coulee. This approach will address longstanding questions concerning the role of catastrophic events in shaping Earth’s surface, make inferences about the hydrology of early Mars (which contains canyons similar in form to Grand Coulee). Additionally, my project will highlight the process of scientific discovery to the public via Grand Coulee’s status as a National Natural Landmark and a key feature on the Ice Age Floods National Geologic Trail. Questions: The project will address two fundamental questions regarding the role of catastrophic floods in eroding Grand Coulee. Question 1: Did Grand Coulee form geologically instantaneously during a single flood, or by flooding throughout the last ice age or even earlier glaciations? Question 2: What was the magnitude of the flood(s) that carved Grand Coulee and how did flood volume change as the landscape evolved via canyon incision? Research Plan: I propose to integrate field, geochronological, and numerical modeling methods to unravel the geomorphic history of the Upper Grand Coulee, under the advisement of Dr. Isaac Larsen at the University of Massachusetts. Question 1 will be addressed using primarily geochronological methods. Determining exposure ages along the length of the canyon rim of Upper Grand Coulee will constrain the location of the waterfall as it retreated upstream. I have collected samples of fluvially-transported granite boulders and flood-carved basalt surfaces from the study area for exposure dating [2]. Granites will be processed for 10Be dating in the UMass Cosmogenic Nuclide Laboratory, and basalts will be dated using 3He in labs of collaborators. If the waterfall experienced gradual retreat in response to multiple floods, we expect to find a decrease in exposure ages of flooded basalt surfaces with distance upstream. Alternatively, similar ages of flooded basalt along the rim of Upper Grand Coulee would support a more rapid landscape response driven by a single flood or several floods occurring in rapid succession. Question 2 will be addressed by field and numerical methods. I will use a 2D, depth-averaged shallow water hydraulic script to numerically simulate floods of varying magnitudes in Grand Coulee, with discharges ranging from the minimum required to barely inundate the canyon floor, to that which fills the canyon to the brim. This script will be run on the Massachusetts Green High Performance Computing Center, a supercomputer accessible from UMass. Field measurements will be used to determine which of these modeled discharges is most consistent with the geochronological evidence. I measured boulder dimensions on depositional bars in Grand Coulee, and will use these to constrain the threshold bed stresses and flood discharges required for their transport. Similarly, field measurements of basalt columns and physics-based estimates of bed stresses required to erode the bedrock channel floor will be used to constrain the canyon-forming discharge [3]. These discharge constraints will allow me to assess whether Grand Coulee was filled to the brim by floods, as is often assumed in flood reconstructions, or whether smaller, but still exceptional, floods carved the canyon. Moreover, the geochronology will allow me to independently assess the predictions of my modeling, as the dating will indicate whether the appropriate paradigm of incision requires huge, brim-full floods or smaller floods. Intellectual Merit: The evidence to support extensive flooding in the Channeled Scablands is overwhelming, but it remains a challenge to quantitatively constrain the pace and timing of the evolution of the bedrock landscape and the coevolution of flooding and canyon incision [3]. Our understanding and interpretation of the roles that floods of varying magnitudes may have played in generating the topography of the Channeled Scablands therefore remains far from complete. By addressing the magnitude of the floods that carved Grand Coulee, my work will address long- standing questions regarding the balance between catastrophic and gradual processes in shaping topography. There is great value in constraining the discharge of megafloods: large freshwater releases can alter ocean circulation and trigger abrupt climate change [4], so understanding the magnitude of paleo-floods is key to understanding Earth’s past climate and sensitivity for future climate change, given current ice melting in polar regions. Additionally, understanding the processes and formation rates of Grand Coulee can yield insight into the evolution of the much larger Martian Outflow Channels and contribute to a clearer picture of the volume of water that flowed on the surface of early Mars, where direct dating is not yet possible. Broader Impacts: The story of the megafloods is exciting, coherent, and illustrative of the nature of scientific research, and outreach on this topic can motivate the next generation of scientists. In fact, watching a documentary on the Channeled Scablands played a major role in my own decision to pursue graduate research. Outreach will therefore be a significant broader impact of this work. I participate in UMass’ Graduate Women In STEM’s Science Café, and already have multiple Channeled Scablands presentations scheduled at a local middle school, through which I hope to ignite interest in science and make research relatable. I will also work with Eureka!, a branch of Girls, Inc., which runs summer programs at UMass for pre-college girls from an underserved community. I will play a leading role in developing a series of local discovery-based field trips to introduce girls to earth science, which is largely absent in the standardized state curriculum. Floods from ice and landslide dam failures are hazards worldwide, and the quantitative methods for estimating flood discharge developed in my work can be directly transferred to smaller floods, and thereby used to assess geohazards using paleoflood evidence and to predict risks from future flooding scenarios. As the usefulness of scientific research is limited until it is communicated to policymakers and members of the public, I will post project updates to my field blog to convey the research process, discuss my findings, and provide a personal perspective of geoscience. I also intend to collaborate with other researchers to develop a field trip to the Scablands at a national conference to showcase current research on the megafloods, and will work with state parks in the Grand Coulee area to develop interpretive displays to communicate the story of the Scablands’ dynamic past to the public. References [1] Bretz, J. 1932. American Geog. Soc. 15. [2] Lal, D. 1991. Earth and Plan. Sci. Letters. 104, 424-439. [3] Larsen, I. and Lamb, M. 2016. Nature. 538, 229-232. [4] Barber, D.C., et al. 1999. Nature. 400, 344-348.	0
fluorescence from satellites Across the globe, the terrestrial biosphere is responding to growing climate extremes, including more frequent heatwaves, droughts, and high-impact weather events1. For North American forests, this means increasing physiological stress on one of the continent’s most important carbon sinks, along with the vast quantity of biodiversity that these ecosystems support. A fundamental way to understand and potentially mitigate the ecological impacts of extreme heat and drought events is by tracking carbon uptake through photosynthesis (i.e., gross primary productivity, GPP), across seasons at the forest-scale2. However, large- scale monitoring of GPP is challenging when considering the highly dynamic and remote nature of mountain biomes, which make up a substantial portion of North American biomass3. Montane landscapes exhibit vast gaps in spatial coverage of surface-level GPP measurements, along with complex topography that makes land surface models and atmospheric tracer approaches prone to significant uncertainty4. Thus, remotely-sensed data from space present a promising tool to fill in these spatial gaps to better understand forests’ response to stress and the implications for the terrestrial carbon cycle. Traditionally, forest-level GPP has been derived from satellites using reflectance-based indices that quantify the “greenness” of a land surface. However, the temperate and boreal forests that comprise many North American mountain biomes consist mainly of evergreen conifer trees which retain their needles, and therefore, greenness, even in photosynthetically dormant seasons (e.g., drought or winter). Thus, studies that use reflectance-based indices as metrics of conifer GPP face significant challenges in capturing seasonal to decadal changes of photosynthetic activity5,6. In contrast, solar-induced chlorophyll fluorescence (SIF), which is emitted by chlorophyll pigments as a byproduct of the photosynthetic process and can be measured via satellite instruments, has been shown to closely follow the seasonal cycle of photosynthetic production in evergreen forests7. The combination of newly available remotely-sensed high- resolution SIF data, in conjunction with measures of complex terrain characteristics (e.g., slope angle, aspect and elevation), represents a unique opportunity for understanding GPP over mountain biomes. In my proposed work, I will analyze GPP derived over the Sierra Nevada mountain range in California using ground- based flux tower data, biogeochemical models, and remotely-sensed SIF and reflectance-based data in order to test the hypotheses described below and in Figure 1. Hypothesis 1 (H1) - SIF is an improved way to measure GPP over montane conifer ecosystems compared to traditional reflectance-based remote sensing indices. High-resolution SIF from satellites provides extensive spatial data coverage, but satellite- Fig. 1: Satellite-based SIF will be analyzed against based SIF has yet to be analyzed for fine-scale reflectance-based indices and modeled GPP in mountains spatial and elevation gradients in complex to assess drought-induced stress. terrain. To test H1, I will analyze SIF data from the TROPOMI and OCO-2/3 satellite instruments over the Sierra Nevada range, comparing to GPP measured at eddy-covariance flux towers from the NSF-funded National Ecological Observatory Network (NEON) and Southern Sierra Critical Zone Observatory sites in the Sierra Nevada range. I will then compare SIF and traditional reflectance-based indices (NDVI, EVI, CCI) to quantify differences in their ability to predict seasonal and interannual GPP as a function of elevation and terrain characteristics. Hypothesis 2 (H2) - Characterization of mountain conifer forests response to drought can be improved with the aid of remotely-sensed SIF. The Sierra Nevada range has consistently experienced extreme drought conditions over the past decade; these forests’ productivity in response to drought can be estimated using biogeochemical models, but does the spatial resolution of a model limit its ability to resolve such dynamic processes over complex terrain? Based on insights from H1 and using SIF as a means of constraining modeled GPP, I will test H2 by comparing remotely-sensed SIF to GPP modeled using the Community Land Model version 5 (CLM5) over the Sierra Nevada region for timeframes with available high-resolution SIF data, detecting mismatch between satellite- and model-derived GPP over seasonal and interannual cycles during observed drought periods. Hypothesis 3 (H3) - SIF can provide information towards early warning capabilities for forest health in response to drought conditions. Assuming a direct linkage between forest productivity and physiological stress, remotely-sensed measures of GPP could act as highly localized indicators of forest health in drought- stricken regions. Using information gleaned from H1 and H2, I will analyze high-resolution SIF data with recent records of forest drought disturbances to quantify trends in SIF as they are correlated with drought events. Through this analysis I will uncover statistical relationships between SIF and drought stress in the context of variables such as elevation, aspect, and snow cover to determine the extent to which SIF can act as an early warning system for forest health over land-use management scales. Collaborations and Computing Resources: To complete this work, I will build on an existing NSF- funded collaboration between the University of Utah and researchers at California Institute of Technology (led by Prof. Christian Frankenberg) to access high-resolution, pre-processed topography-corrected SIF data products over the Sierra Nevada range. I will also work with the University of California–Irvine Innovation Center for Advancing Ecosystem Climate Solutions to engage regional Sierra Nevada stakeholders in scientific discussion. To accommodate the computational needs associated with my work, I will utilize dedicated group-access nodes (purchased by advisor Prof. John Lin) on two supercomputers maintained by the University of Utah’s Center for High-Performance Computing. Intellectual Merit – Global Carbon Budget: Remotely-sensed SIF has the potential to track forest productivity over global scales and is a promising tool for rectifying uncertainties in the carbon budget of mountains. My work will be among the first to examine high resolution SIF over complex terrain in order to address uncertainties in forest drought response. As heat and drought stress grow increasingly prevalent due to climate change, understanding the highly dynamic response of montane carbon stocks will be critical for improving terrestrial biosphere models that inform global climate policy. Forest Management: High resolution SIF can help diagnose features of forest wellbeing and tree mortality at scales meaningful to land use management. Through assessment of H3, I will examine how SIF can be used to provide early warning capabilities for near-term forest disturbances and allow land managers to adapt to rapid changes in forest health from critical stress events. These capabilities ultimately aid in emergency preparedness capabilities to mitigate damage from wildfires and bark beetle die-off. Broader Impacts – Stakeholder Engagement: The U.S.D.A. Forest Service Region 5 and Sierra Nevada Conservancy have a vested interest in central California’s forest health and wildfire management, motivated by public protection, forest resource care, and conservation advocacy. In addition, the California Air Resources Board is seeking accurate methods to estimate forest carbon stocks for implementation of carbon accounting policies. Through collaboration with UC–Irvine (see above), I will engage stakeholder representatives from these institutions through ongoing virtual correspondence and regularly held stakeholder meetings to disseminate my findings on forest drought response and early warning capabilities. Code Sharing: As I have already done in my previous publications, I plan to provide open access to R and python scripts to the entire research community produced through my personal Github webpage (github.com/lkunik). References: [1] IPCC 6th Assessment Report, (2021) [2] E. Tomppo, et al. (2021) Remote Sens. 13, 597. [3] D. Schimel, et al. (2002) Eos Trans. AGU. 83(40), 445–449. [4] M. Rotach, et al. (2014) Bull. Amer. Meteor. 95(7), 1021-1028. [5] K. Springer, et al. (2017) Remote Sens. 9, 1–18. [6] D. Sims, et al. (2006) J. Geophys. Res. 111, G04015. [7] T. Magney, et al. (2019) Proc. Natl. Acad. Sci. 116(24), 11640-11645.	0
Introduction: Large carnivores are re-colonizing North America1,2 and parts of Europe3, following decades of systematic eradication4. The expansion of large carnivore populations is creating novel and complex predator-prey interactions5. One well-known example is trophic cascades and associated declines in herbivore abundance6. Predator-prey interactions are among the most fundamental ecological processes and have been the focus of ecology since its origin6. They are integral processes that shape biological communities, affect coupled human-wildlife systems, and drive conservation and management ecology7. Despite this, our understanding of the effect of predators on prey populations, especially in complex food-webs, is in its infancy5,7. Predator-prey theoretical and empirical research is dominated by single predator-single prey systems like the Isle Royale wolf-moose system8. While useful, it is unclear if these simplified models are capable of predicting dynamics where multiple predators are interacting with multiple prey species6,7. For example, the recent recovery of wolves, expansion of grizzly bear populations, and expanding range of mountain lions across the Western United States are increasing the number and complexity of interactions between predator and prey species1,2,4. Mounting evidence that the growing number of interactions can cause previously unknown ecological effects suggests that there is much left to be understood in multi-predator, multi-prey food webs6,9,10. For example, these complex dynamics can spur changes in direct ecological interactions, such as prey switching by predators in response to prey abundance9. A key conceptual way in which single predator-prey interactions differ from more realistic, complex, multiple predator-prey food webs is the inclusion of competition in addition to direct predator-prey dynamics. For example, predator-prey dynamics can lead to indirect ecological interactions, such as apparent competition, where one prey species supports predator populations, thereby reducing alternative prey populations10. With various competitive interactions within a trophic level occurring, the complexity of competition must also be considered11. Ecology has long studied the tension between how the forces of predation and competition structure communities and population dynamics11. Unfortunately, the inherent complexity of such systems has often rendered purely statistical/empirical approaches limited in their utility. Compared to laboratory studies and field experiments, mathematical models, such as multiple predator-prey models (MPPMs) allow ecologists to study these dynamics12. Complex food webs cannot be easily resolved with statistical/empirical approaches because of the large number of parameters to estimate and the scant data to do so with, as well as the challenges presented by some parameters and mechanisms that are impossible to estimate (e.g., carrying capacity). MPPMs are also very powerful in evaluating the consequences of management decisions12. Commonly, natural resource agencies manage populations using independent management strategies for each species; therefore they do not reflect the complexity of predator- prey population dynamics. By failing to incorporate food web interactions into species management strategies and ignoring the role of multi-species predation and competition, agencies may be sub-optimally preserving and managing wildlife populations. I hypothesize that MPPMs which consider alternative interactions will explain empirical systems better than single-predator, single-prey models (SPPMs). I will address these major questions: a) Are MPPMs better at predicting population dynamics in real-world systems than SPPMs? b) If so, are the main advantages of MPPMs in terms of predictive performance driven by predation or inclusion of competitive interactions? c) What are the conditions (e.g., environmental, stochastic) in which predation vs. competition drive food webs? With these ecological questions answered, I will finally address: d) How does management of one species affect populations of other predators and prey within a food web? Research Approach: I will use wildlife agency-collected datasets from the Idaho Department of Fish and Game for predator and prey populations. Then, through funding from my NSF GRFP proposal, I will generalize my results to other high-profile multi-species predator-prey datasets from Banff National Park, Yellowstone National Park, and Serengeti National Park, with the help of my Ph.D. supervisors who have connections to these 3 systems. First, I will gather information about predator or prey population dynamics from previous studies to inform the structure of my models12. Then, I will estimate functional and numerical responses for each predator-prey pair from across systems. I can then incorporate each predator and prey species into a set of coupled equations, one for each species in the food web. If there are i predator species and j prey species, the corresponding predator-prey equations can be written as such: 𝑑𝑉 𝑑𝑃 𝑗 𝑖 [𝟏] = 𝑓(𝑉)−∑𝑓(𝑉,𝑃)−∑𝑓(𝑉) [𝟐] = ∑𝑓(𝑉,𝑃)−𝑓(𝑃)−∑𝑓(𝑃) 𝑑𝑡 𝑗 𝑗 𝑖 𝑗 𝑑𝑡 𝑗 𝑖 𝑖 𝑖 𝑖 𝑗 𝑗 𝑖 where [1] describes the population growth rate for prey (1st term), reduced by the effects of predation (2nd term) and competitive interactions with other prey j (3rd term) and [2] represents the population growth rate for predators (1st term), decremented by predator mortality (2nd term) and, when present, competition from other predators (3rd term). For example, V could represent 1 white-tailed deer, V elk, whereas P could represent wolves, P mountain lions, and so on. The 2 1 2 shape and dynamics of these functions, f (.), will be determined from field data. Intellectual Merit: I will address questions fundamental to predator-prey theory, and also more broadly, the ecological theory about the role of competition vs. predation in driving population dynamics. For example, I will investigate if functional and numerical responses, thought to be integral to predator-prey theory7,9,13, are sufficient or even necessary to understand predator-prey population dynamics. By applying these models to a broad variety of ecosystems, I will identify general properties that drive not only predator-prey systems, but other consumer-resource relationships14. Moreover, I will help natural resource agencies avoid mistakes stemming from un-integrated management, which can be economically and ecologically costly12. Broader Impacts: Through an increased understanding of how management controls predator- prey population dynamics, wildlife agencies will be able to determine how human harvest strategies of one species will affect others in a food web. Additionally, I will work to establish an accurate public image of large carnivores throughout local communities, and bridge the gap between ecologists-wildlife agencies-citizens. I will do so by giving talks at local high schools, writing articles for newspapers and online blogs, and partnering with local radio/TV programs, much of which I have done in the Falkland Islands (see Personal Statement). In sum, I envision that my work will develop ecological principles general enough to transcend ecosystems, but also specific enough to assist management of the natural resources of local communities. Literature Cited: 1) Mech, L. (1995). Cons. Bio. 9(2):270-278. 2) LaRue, M. et al. (2012). J. Wild. Mgmt. 76(6):1364-1369. 3) Chapron, G. (2014). Science. 346: 1517-1519. 4) Ripple, W. et al. (2011). Science. 343:151- 162. 5) Berger, J. et al. (2001). Science. 291:1036-1039. 6) Shurin, J. et al. (2002). Ecol. Lett. 5(6): 785-791. 7) Abrams, P. & Ginzburg, L. (2000). TREE. 5(278): 535-541. 8) Messier, F. (1994). Ecol. 75(2):478-488. 9) Hebblewhite, M. (2013). Pop. Ecol. 55(4):511-522. 10) Holt, R. (1977). Theor.Pop.Bio. 12(2): 197-229. 11) Chesson & Kuang. (2008). Nature. 456: 235-238. 12) Serrouya, R. et al. (2015). Am. Natl. 185(5): 665-679. 13) Berryman, A. (1992). Ecol. 73(5):1530-1535. 14) Vucetich, J. et al. (2011). J. Anim. Ecol. 80(6):1236-1245.	0
control gene expression levels. In eukaryotes, a key contributor to transcriptional regulation is the chromatin architecture; that is, the arrangement of nucleosomes, transcription factors (TFs), and other proteins that bind along the genome at any point in time. One way to predict binding sites is by scanning the sequence for motifs commonly associated with TF binding; this method is fairly sensitive but also prone to a large number of false positives. Thus, to ascertain the in vivo chromatin architecture, scientists have developed experimental methods, such as ChIP-seq and DNase-seq, to pinpoint the genomic locations of protein-DNA interactions. ChIP-seq is considered the “gold standard” for locating transcription factor binding sites (TFBSs). This assay identifies the locations at which a particular TF binds to DNA in vivo. However, ChIP-seq requires a separate experiment and antibody for every TF, making the procedure time-consuming and costly. An alternative method, DNase-seq, gauges the accessibility of DNA at specific regions in the genome. In this protocol, the nuclease DNase I makes cuts throughout the genome, and the locations of the cuts are then mapped to a reference genome for analysis. Since regions bound by proteins are less accessible to the DNase enzyme than unbound regions, composite plots of DNase digestion patterns at TFBSs reveal reduced levels of digestion at many TFBSs, creating “footprints” in the data (1). Thus, DNase-seq allows detection of binding sites for multiple TFs using just one experiment. Recent studies showed that unique and dramatic fluctuations exist within the DNase footprints of specific TFs, often at the resolution of a single base pair, distinguishing any one TF’s footprints from those of other TFs (1). Some recent studies have suggested that these patterns mirror the small-scale biochemical interactions between the bound protein and the DNA molecule (1). If true, this might allow identification of a specific TF’s binding sites based on the DNase digestion pattern alone. However, follow-up studies challenged this claim, demonstrating that most high-resolution fluctuations in DNase digestion are an artifact of DNase’s inherent sequence binding preferences (2). Most TFs only bind certain DNA sequence motifs, and some specific positions within these binding site motifs are cut more often than others due to DNase’s own sequence preferences. In fact, some have suggested that most TF footprints are not even footprints at all, but rather false positives where DNase cut counts are depleted due to an inherently DNase-resistant sequence context (2). I have analyzed DNase’s sequence biases using experimental data from DNase digestion of naked (protein-free) DNA. I determined the relative “cuttability” of every possible 6-mer nucleotide sequence and used this information to produce corrected TF footprint plots in the budding yeast S. cerevisiae. My results confirmed that some TFs had almost no visible footprint after bias correction, but almost half of the 102 TFs I tested still showed visible footprints. For those TFs that still had footprints, correcting the sequence bias produced smoother footprints with a lower variance in cuts at individual positions, but for some TFs, such as MCM1, a clear factor-specific footprint shape was still present even after bias correction. Furthermore, different transcription factors have different footprint widths. Thus, although the factor-specific footprint patterns are not as striking as was originally suggested, it is still quite likely that a statistical model could incorporate this information to infer which specific TF binds at a given location. I plan to investigate how the chromatin landscape influences transcriptional regulation in eukaryotes, using yeast as a model organism in order to understand basic regulatory principles that might later be extended to human data. My project will explore how differences in this landscape across time and across cell types are correlated with differences in gene regulation. There are two stages to the project. I will first improve upon existing methods for predicting the locations of transcription factor binding sites and identifying the specific transcription factors that bind at each one. I will then apply my model to existing datasets to investigate to what extent changes in the chromatin landscape correlate with changes in gene expression levels. I have spent over 2 years analyzing data from DNase-seq experiments, with the broad goal of determining transcription factor binding sites. To date, no model exists that explicitly models sequence bias in DNase experiments to fully harness the high resolution of the data. My goal is to develop a novel statistical method that uses DNase data to compute the likelihood that each position is bound by a particular protein. Specifically, I will use a hidden Markov model (HMM) in which the DNase cut counts at each position in the genome are the observed data, and each of the model’s “hidden” states represents a specific bound protein, which may be a TF, a nucleosome, or no protein at all. The HMM will include separate parameters for the individual positions within each TF, in order to take full advantage of the high resolution of DNase data. To avoid confounding the “actual” footprints with the sequence biases of DNase, I will include the sequence at each position as an additional predictor of DNase cut counts, using expectation maximization (EM) to simultaneously infer the parameters that govern the bias and the parameters of the underlying footprint patterns. To validate my results, I will compare my predicted binding patterns with published results of ChIP-seq experiments and nucleosome positioning experiments previously performed on the yeast genome. Once I have developed a reasonable model for inferring the chromatin landscape, I will extend my results to directly investigate how this configuration of proteins affects gene expression. Specifically, I will predict gene transcription rates using features derived from the adjacent protein landscape. Previous attempts to solve this problem have achieved moderate success, but only within a certain subset of test cases (3). Most of these models have used sequence data such as kmer counts as the sole predictors of gene expression (3). One downside of this approach is that these features lack clear biological significance: it is still difficult to understand the indirect process by which a given kmer’s appearance in the promoter influences the level of gene expression. In my proposed machine learning model, the features, or predictive variables, will be easily interpretable features that represent attributes of the gene’s surrounding protein landscape. These features will include the number and location of TFs and nucleosomes in a gene’s promoter region, and I will obtain them using the model that I developed in the first phase of my project. Thus, the two phases form a pipeline: first I will infer the chromatin architecture, and then I will use the model output as predictors for modeling transcription rates. Developing tools to elucidate chromatin architecture is crucial for understanding gene expression. Differential gene expression drives many critical processes within an organism, allowing cells to adapt to changes in the environment and even become specialized through the process of cell differentiation. Despite our awareness of gene expression’s importance to an organism, the processes regulating gene expression are far from fully understood. My proposed research project will bring us closer to understanding these fundamental cellular dynamics. The DNase-seq data for this project were generated by the Crawford Lab at Duke and the Stam Lab at the University of Washington, and are freely available online. 1) Hesselberth, J. R., et al. (2009). Global mapping of protein-DNA interactions in vivo by digital genomic footprinting. Nature Methods, 6(4), 283-289. 2) Sung, M. H., et al. (2014). DNase Footprint Signatures Are Dictated by Factor Dynamics and DNA Sequence. Molecular Cell. 3) Meyer, P., et al. (2013). Inferring gene expression from ribosomal promoter sequences, a crowdsourcing approach. Genome Research, 23(11), 1928-1937.	1
Introduction: Traditional stair construction, in which stair flights are rigidly connected to the structure at both ends (“fixed-fixed” connections), has been shown to cause damage to stairs and surrounding structural members during earthquakes due to stairs being stretched and compressed due to the relative displacement between a building’s floors1. (See Figure 1.) In response to this, alternative designs (“fixed-free” connections) have been developed2 that permit stairs to accommodate the relative deformation between stories by detaching the stair at one of the floors. Recent tests2 have confirmed that fixed-free connections have the potential to eliminate damage due to seismic forces being distributed to stairs, but further testing is needed to understand these novel designs. Scissor stairs, a common configuration in which the stair turns back on itself at a mid-story landing, have not yet been tested in conjunction with fixed-free connections. Stairs have been shown to affect a structure’s seismic response3, so it is essential to investigate scissor stairs with fixed-free connections not only in isolation but also interacting dynamically with a building. One concern is that releasing degrees of freedom for fixed-free connections may cause a whiplash effect due to the stair’s mass being less constrained, damaging nearby building components. Additionally, this whiplash effect may cause undesirable torsion in the building. Therefore, a key challenge in designing fixed-free stairs is to remove enough restraints to permit some movement while preventing completely free oscillation. I propose to observe and quantify the dynamic characteristics fixed-free scissor stairs and their effect on the lateral response of buildings. Three variations of fixed-free connections will be considered: 1) removing all connections from the lower end of the stairs so that it may slide freely on its landing, 2) use slotted connections to allow some movement while restraining most movement, and 3) using a sliding hanger connection to allow translation in all three dimensions without leaving the stair completely unattached. Hypothesis: Fixed-free stairs will prevent damage that traditionally constructed stairs would otherwise suffer by allowing relative movement between stairs and floors and dissipating energy through friction. Research Goal 1: Develop Models for Stairs with Fixed and Free Connections I will model four scissor stair configurations: three with fixed-free connections and a control case with traditional, rigid connections. Because building prototypes and performing dynamic testing is typically cost-prohibitive, I will develop finite element models of the stairs and their connections using the finite element program LS-DYNA. An essential feature of LS-DYNA is its ability to model friction and the interaction between components that come into contact with one another4. The models will be subjected to cyclic and dynamic loading protocols to identify the force-deformation behavior of the stair system under earthquake loading. Research Goal 2: Model Scissor Stairs in Structures Using the methodology described by Wang et al. (2015)5, I will represent fixed-free stairs in structures by developing a system of nonlinear springs using the force-deformation relationships found in Goal 1, which will be then integrated into full-building structural models to observe the effect of fixed-free stairs on the seismic response of buildings. For this task, I will use the structural finite element framework OpenSeesPy, which is more appropriate for modelling an entire building. In order to evaluate the effects of fixed-free stair connections, I will subject the models to dynamic loading using a variety of ground motions, then compare member forces and nodal displacements between models without stairs, with traditional stair connections, and with fixed-free stair connections. Research Goal 3: Improve Full-Building Models by Comparing to Shake Table Test Working under Professor Keri Ryan at the University of Nevada, Reno, I will participate in the shake table testing of a full-size, 10-story timber building at the NSF’s National Hazards Engineering Research Infrastructure (NHERI) facility at UC San Diego in 20216. Because this structure will include scissor stairs with various types of fixed-free connections, this will be an unprecedented opportunity to gather physical data showing the effects of stair-structure interaction. Using this data, I will develop a model of the building including the stairs using OpenSeesPy, which I will validate and calibrate with data from the shake table test. Discoveries from this test will allow me to improve the stair-structure models developed for Goal 2. Intellectual Merit: Although prior research efforts have investigated the performance of fixed-free connections2 and the interaction between scissor stairs and structural systems5, no study has yet addressed coupling the two. Previous tests have demonstrated the potential of fixed-free connections to mitigate damage in stairs2 sufficiently to warrant further investigation. Accurately characterizing the nonlinear force-deformation relationship of scissor stairs with fixed-free connections will facilitate future research into the seismic response of buildings. Furthermore, developing a nonlinear spring model in Goal 2 will be an important step in helping practicing engineers integrate the results of this research into design practice. This research will also advance the quality of computational analysis in structural engineering. To my knowledge, I will be the first student at the UNR to extensively use OpenSeesPy, an adaptation of the finite element framework OpenSees for Python. Python has many data science libraries that will improve analysis of data from shake table tests and computational models. Linking Python and OpenSees will improve the quality of structural research by facilitating pre- and post-processing of data from tests and simulations. Broader Impacts: Failure to account for differential movement between floors has led to stairs collapsing in the recent Wenchuan and Christchurch earthquakes2. Fixed-free connections can prevent similar collapses in the future, but first their effects on building response need to be considered before this life-saving technology can be fully implemented. Since stairs are the primary means of egress from a building during a catastrophic event, protecting stairs from collapse during seismic events is an essential task for preventing loss of life. Furthermore, better modelling of stair-structure interaction will ensure safer design and reduce damage, which will in turn facilitate rapid recovery after disasters by reducing building downtime and preventing economic losses. In order to promote the use of safer stair systems, I will disseminate my findings through publications in structural engineering journals, the NHERI TallWood outreach webpage, and through seminars hosted by UNR’s Earthquake Engineering Research Institute chapter. The NHERI TallWood project will give me opportunities to work closely with industry collaborators to further develop and promote fixed-free connections. I will also present a simplified version of my research to K-12 classrooms to foster youth interest in earthquake research. References: [1] D. Bull, (2011). Canterbury Earthquakes Royal Commission. [2] C. Black, et al., (2020). 17th World Conference on Earthquake Engineering. [3] J. Zhu, et al., (2011). Applied Mechanics and Materials. [4] https://www.lstc.com/products/ls-dyna [5] X. Wang, et al., (2015). Earthquake Engineering & Structural Dynamics. [6] K. Ryan, et al., (2020). Colorado School of Mines.	0
coral and is a crucial reef-building coral. Due to mass mortality from white band disease (WBD) in the Caribbean, however, A. cervicornis is currently designated as critically endangered by the IUCN Red List of Threatened Species. While the etiological agent of this disease is unknown, my lab recently discovered that a bacterium associated with the disease is strongly stimulated by nutrient pollution. In early 2017, I assembled the genome of this obligate intracellular parasite of A. cervicornis and, based on its phylogenetic position and genome content, I hypothesize that it is responsible for WBD and the destruction of Caribbean Acropora. Yet we do not yet know its mechanisms of transmission and disease development (pathogenesis) and further experiments are needed to confirm its role as the agent of disease. Using a combination of comparative genomics and field experiments, I aim to evaluate the gene expression, biogeography, and evolution of this bacterium to help prevent and manage this disease on coral reefs in the future. Background: WBD has been observed in the Caribbean, Red Sea, and the Pacific, but its mechanisms of pathogenicity are uncharacterized. Transmission experiments suggest that WBD is caused by bacteria,1 with species of Rickettsiales implicated as possible etiological agents,2,3 yet these taxa are present in both healthy and diseased coral microbiomes. Recent studies4,5 led by my advisor at Oregon State, Dr. Rebecca Vega Thurber, indicated that exposing corals to nitrogen stimulates the growth of an intracellular bacterium (order Rickettsiales). A strong negative correlation was found between the abundance of this taxon and coral growth (r2=0.9987, p<0.001) in the presence of nutrients.5 In A. cervicornis, this taxon increased from <11% of the microbial community to ~88% after 8 weeks of enrichment. In an unpublished study, tissue homogenates were generated from diseased and healthy A. cervicornis. The diseased homogenate, which caused a sixfold increase in mortality of exposed corals, was found to have a relative abundance of >50% Rickettsiales, while these species comprised only 0.1% of the healthy homogenate. While known pathogens in the genus Rickettsia clustered together in a 16S rRNA phylogenetic tree, I discovered that the intracellular parasite of A. cervicornis clustered most closely with uncultured symbionts of marine invertebrates, primarily corals and sponges.6 Based on strong statistical support for the distinction of these intracellular symbionts of marine invertebrates from other Rickettsiales, I proposed a new genus, Marinoinvertebrata gen. nov., in a publication in preparation for the ISME Journal.6 I named the newly-discovered parasite M. rohwerii sp. nov. after the lab that first observed it.3 Given their abundance in corals exhibiting reduced growth and signs of WBD, I hypothesize that: (1) Marinoinvertebrata spp. are members of the healthy coral microbiome but have evolved mechanisms of pathogenesis (encoded by virulence genes) that are modulated by environmental factors. (2) The increased expression of these genes due to nutrient exposure, in tandem with weakened host immune function caused by pollution, leads to coral disease. I will address these hypotheses through two central questions: Q1: How do mechanisms of pathogenesis and environmental response differ in species of Marinoinvertebrata from distinct regions? (Aim 1) I will compare the genome of M. rohwerii to genomes of disease-causing Rickettsia to identify homologs to virulence genes. I will also compare the pathogenicity of this organism with that of related species from different regions. Q2: How does concurrent exposure to nutrients and infection by Marinoinvertebrata spp. alter host physiology and induce disease? (Aim 2) I will conduct nutrient enrichment experiments on six genotypes of A. cervicornis to induce growth of Marinoinvertebrata spp. I will track parasite abundance using quantitative PCR as well as changes in microbiome composition, host/pathogen gene expression, growth rate, and disease progression. Lastly, I will use nanoscale secondary ion mass spectrometry (NanoSIMS) to trace parasite nutrient assimilation. Research Plan: Aim 1: Via the KAAS server,7 I will use reference genomes of pathogenic Rickettsia to guide my search for homologous virulence genes in the assembled genome of M. rohwerii. Using this method, I previously uncovered a complete Type IV secretion system in this genome, which is involved with host infection and genetic exchange in related bacteria8, as well as the NtrY-NtrX two-component system involved in sensing extracellular nitrate levels. However, additional virulence and environmental response genes present in Rickettsia may have more distant homologs in Marinoinvertebrata spp. As part of the Global Coral Microbiome Project and Tara Pacific, I have access to microbial community data from hundreds of Caribbean and Indo-Pacific coral samples and have identified species of Marinoinvertebrata in samples from Australia, Saudi Arabia, and Colombia. I will assemble the genomes of these species and determine whether they also possess virulence genes, and whether these are modulated by environmental-sensing genes. Aim 2: I will expose six different genotypes of A. cervicornis (raised in nurseries at Mote Marine Lab (MML)) to various levels of inorganic nitrogen to stimulate Rickettsiales proliferation as shown previously.4,5 Three of these genotypes are sensitive to WBD and three are resistant, as determined by Dr. Erinn Muller, who will be my host at MML. From the genome of M. rohwerii, I will generate quantitative PCR markers to track its absolute abundance throughout the enrichment experiments. Coral fragments will be sampled weekly with the help of students from MML’s after- school program, and high-throughput RNA sequencing will be used to assess Rickettsiales and host gene expression. I will track coral health through growth, photosynthesis, and respiration rates and changes to the host microbiome using 16S rRNA amplicon analysis. Finally, I will spend the summer of 2019 working with Dr. Xavier Mayali of Lawrence Livermore National Laboratory, using NanoSIMS to isotopically trace whether M. rohwerii scavenges nutrients from the host, from symbiotic algae, or from the environment. Intellectual Merit: Our ability to directly alter host-microbe interactions using nutrient enrichment provides a reliable model to ascertain which genes play a role in disease initiation and host response. Beyond its implications for coral disease, the opportunity to reconstruct the genome of a novel pathogen is rare and may uncover new mechanisms of transmission, especially when this pathogen is traced throughout different regions and coral hosts. As coral reef fish are crucial to the economy of many tropical regions, it is critical to combat the rapid destruction of their habitat by disease. The PCR primers I develop for this experiment can be used to quantify disease progression and track the spread of M. rohwerii, and our understanding of the host and pathogen transcriptomes will contribute to further studies on antibiotic treatment of WBD. Lastly, as MML’s coral nurseries were damaged by Hurricane Irma, my research will inform recovery efforts as genotypes most resistant to Marinoinvertebrata infection will be used in new nurseries. Broader impacts: Given my extensive advising and leadership experience, the robust educational programs already established by MML will provide an excellent framework to involve the local community in my research. Through MML’s Research-Based After-School Program for Students, I will work directly with high school students passionate about ocean conservation to introduce them to lab- and field-based research by helping them develop short-term experiments on how nutrient dose effects parasite growth. Dr. Muller and I will lead volunteer teams to propagate new nurseries and learn about the effects of coral disease. These teams will “adopt” their own corals to observe over time, increasing their personal investment in the reef. I will also work with the Oregon Coast Aquarium as a Scientific Interpreter to lead demonstrations showing how nutrient pollution negatively effects the health of all marine organisms, not just coral systems. Citations: [1] Gignoux-Wolfsohn et al. (2012) Sci. Rep. [2] Miller et al. (2014) PeerJ. [3] Casas et al. (2004) Environ. Microbiol. [4] Zaneveld et al. (2016) Nat. Commun. [5] Shaver et al. (2017) Ecology [6] Klinges et al. (in prep.) ISME J. [7] Moriya et al. (2007) Nucleic Acids Res. [8] Cascales et al. (2003) Nat. Rev. Micro.	0
Motivation: Humans are inherently good at cooperation in teams, and our ability to work together enables us to overcome challenges that a single individual would otherwise be unable to complete. The same can be said for autonomous robotic systems: for many applications a team of robots working together can complete a task more efficiently than an individual robot working alone. Another advantage of multi-robot systems is that the agents can be mechanically simpler than a single, general purpose agent required to complete the same set of tasks. Introducing heterogeneous robot types into the team allows specialized agents to focus on the tasks they are good at, and in many cases, increases the efficiency of the team. Specialization improves teams, but introduces a new challenge: if a team is confronted with a task that no members of the team have the expertise to complete, the team will fail. The current solution to this challenge is to create large teams with a high diversity of agents, but this solution is inefficient and leaves highly specialized agents in the team under-utilized. Humans solve this problem intuitively: if the team does not have a capable member then one is recruited from an outside source, and when that task is complete and that person's skill set is no longer required, they are released from the team. Consider a situation where a child is lost in a theme park and security for the park sends out a quadrotor-based search team to look for the child. The quadrotors may be able to fly over the park’s walkways and above open areas searching for the lost child, but might determine that the child may have entered an area that they are unable to investigate. In this case the search team should be able to recruit the help of other agents in the area, like concession service robots on the ground or park employees, to search the areas inaccessible to the quadrotors. Then when the search is over, the recruited agents can return to their previous assignments. Background: This question of dynamic team building is largely unexplored in robotics, but represents a necessary functionality as the diversity of robot systems increases. Previous research in this problem domain has focused on defining this challenge and presenting assessment metrics [1]. However, formal methods for coordination of these types of teams are limited. In 2017, I worked with researchers at Oregon State University on the design and testing of a novel distributed coordination and task planning algorithm for heterogeneous robot teams called Distributed Monte Carlo tree search (Dist-MCTS). By generalizing the tasks, reward functions, and agent abilities, Dist-MCTS remains agnostic to the type of agents in the team, enabling coordination of teams composed of agents with different abilities. Simulated trials showed that Dist-MCTS teams earned 47% more cumulative team reward than teams coordinated using a distributed auction-based approach. While it is effective at organizing complex teams, this algorithm does not allow for dynamic team formation. Research Proposal: I propose to explore the challenge of creating dynamically formed teams by modifying the Dist-MCTS high-level planner and implementing it in a larger framework for the dynamic formation of heterogeneous multi-robot teams. Two essential extensions must be made to the Dist-MCTS algorithm before it can be used for dynamic formation of teams. These focus on scalability and p olicy estimation. A primary restriction of the current Dist-MCTS algorithm is scalability. If too many agents are added to the team (20+) or the number of tasks increases beyond a certain threshold (100+), the planning space becomes too high-dimensional and the solution quality declines. To address the challenge of scalability, I will incorporate autonomous sub-teaming and task space segmentation into the existing algorithm. This adaptation reduces the complexity of the planning operations for all agents across the macro-team. Extensibility is a key requirement for applications to dynamically formed teams because the planning space has the potential to expand rapidly during complex missions. The second I will make to Dist-MCTS is the incorporation of an adaptive task selection model for inclusion of independent agents like humans or unknown robots in the team. Previous work [2] has demonstrated preliminary results for a method to estimate the policy of an independent agent like a human and use this to inform the actions of multi-agent teams. This is necessary because it cannot be guaranteed that the recruitable agents in the operating environment of a dynamically formed team will be able to communicate with the team. Policy estimation for these independent agents will allow the existing team to coordinate itself around agents without requiring direct communication and enable teams to be formed of robots that are not built by the same research group or manufacturer. Methods: I plan to develop a novel algorithm for coordination of dynamically formed teams in three phases: algorithm design, simulation testing and refinement, and h ardware validation. I. The first phase will focus on integrating the modified Dist-MCTS in a larger software framework and developing the components necessary to facilitate dynamic team formation. This will include creating a method for assessing newly discovered tasks and determining a functional set of protocols for recruiting new agents to the teams. II. With the new planner in development stages, I will test and refine the system with a modified version of the simulator used to assess the Dist-MCTS algorithm. A key point in this phase will be investigating how teams should recruit and release members as tasks are discovered and completed. III. With a refined result, I will apply this high-level planner to multi-robot teams in a real-world hardware trials in unknown, dynamic environments to assess its validity in application. For a complete test of this algorithm, these hardware trials will be focused on not only validating the team’s basic functionality, but in testing its applicability to complex multi-robot task domains like collective construction. Broader Impact: This proposal addresses a key challenge in the design and operation of collaborative multi-robot systems, and will provide a platform for a variety of other potential research topics including rapid deployment of heterogeneous teams, and optimization of agent structures in such teams. Dynamic formation of multi-robot teams will revolutionize robotics by increasing the versatility of the multi-agent teams and the complexity of possible missions. As advances in robotics continue and more robots become integrated in everyday life, these types of teams will become increasingly useful in a wide variety of application domains. One application for these types of teams could be in STEM education, where students learn about how robots interact with both the real world and each other through demonstrations involving these teams, allowing them to draw parallels between human teamwork and robot teamwork. [1] Jones, E. G., Browning, B., Dias, M. B., Argall, B., Veloso, M., & Stentz, A. (2006). Dynamically formed heterogeneous robot teams performing tightly-coordinated tasks. In Proceedings 2006 IEEE International Conference on Robotics and Automation, ICRA 2006 (Vol. 2006, pp. 570-575). [1641771] [2] L. Milliken and G. A. Hollinger, “Modeling user expertise for choosing levels of shared autonomy,” in Proc. Planning for Human-Robot Interaction Shared Autonomy and Collaborative Robotics Workshop, Robotics: Science and Systems Conference, 2016.	0
Stress in the city: investigating the effect of urbanization on coyote oxidative stress and diet Background: As the world urbanizes, animals are adapting to novel environmental disturbances in human- dominated landscapes. The type and magnitude of these stressors vary considerably with human activity, culture, and socioeconomic status (SES), and each can influence the amount of biodiversity within a city1,2. Wealthier neighborhoods (higher SES areas) generally exhibit higher biodiversity and greater food availability, collectively known as the luxury effect1,2. Paradoxically, some low SES areas exhibit high biodiversity by providing greater refugia (e.g., abandoned buildings) for prey2. Overall, the luxury effect has repeatedly been shown to affect ecological dynamics at the community level, principally shaping species assemblages and interactions, which ultimately affect population and organismal ecology1,2. Few studies, however, have investigated how the luxury effect and urban stressors (e.g., light pollution) interact to affect the ecology and physiology of urban wildlife. Wildlife in urban and low SES environments face distinct environmental pressures relative to their conspecifics in rural and high SES areas1. However, how within-city differences in disturbance (e.g., noise pollution) affect stress levels and fitness outcomes is largely unexplored. Given that stressors in low SES neighborhoods are magnified1, organisms in these areas may experience greater oxidative stress, an imbalance between free radicals and antioxidants in the body4,5. Biological responses to oxidative stress can vary with environment, genotype, and activity levels5. Further, antioxidants gained through food can provide a defense against oxidative stress6. Anthropogenic food, however, is often protein-poor and low in antioxidants5. Hence, wildlife that exploits such resources are likely to develop health risks7 (e.g., hyperglycemia, which is correlated with low immunity8) and are at higher risk of the deleterious effects of free radicals4, including apoptosis5. Because these effects can reduce fitness4, it is imperative to uncover how human-driven impacts on habitat and diet shape an individual’s ability to cope metabolically with anthropogenic stressors (Fig. 1). My research will investigate the effects of within-city and among- city variation on the physiology of coyotes (Canis latrans). Specifically, I will test how oxidative stress and diet vary along an urban-socioeconomic gradient. Urban coyotes are well-suited model organisms to address the phenotypic consequences of variation in human disturbances within urban systems. Coyotes are ubiquitous across North America and have assumed the apex predator role in urban areas following the local extirpation of tertiary carnivores (e.g., wolves, Canis lupus)3. Moreover, coyotes often consume anthropogenic food9, but exhibit variation in diet9,10. As apex predators, stressors that affect coyote behavior or physiology will have top-down effects in urban ecosystems3. I predict low SES areas will represent poor habitat and diet quality via a reduction in prey diversity1. Therefore, coyotes will supplement their diets with a greater proportion of anthropogenic food subsidies relative to conspecifics in rural and high SES areas. I will trap coyotes (n=60) at 15 predetermined locations along an urban-socioeconomic gradient based on land cover, household density, and median household income11 across the Seattle-Tacoma, WA metropolitan region over three years during winter and summer. Coyotes show more restricted home ranges in urbanized areas and are unlikely to forage in non-adjacent territories12. I will collect blood and hair samples from captured animals and scat samples from within and around trapping sites. I will deploy GPS collars and use spatial data to determine the mean habitat type and SES area used by each individual, and relate these habitat measures to physiological data. Aim 1: Quantify oxidative stress variation in coyotes along an urban-socioeconomic gradient. H1: Coyotes in low SES areas are exposed to more stressors, leading to greater oxidative damage and hyperglycemia relative to rural and high SES coyotes. Alternatively, coyotes may cope with urban stress via access to higher prey diversity in low SES areas, where refugia for prey is more common. To quantify oxidative damage, I will analyze lipid erythrocyte proteins from blood samples for peroxidation4. Additionally, I will test for hyperglycemia by examining glycated serum protein levels8. 1 Cesar O. Estien Research Statement Aim 2: Determine the effect of urbanization on coyote diet composition and how diet influences their ability to mitigate oxidative stress. H2.1: Urban coyotes consume less natural food and more anthropogenic food than rural coyotes. Urban coyote fecal and hair samples will have lower nitrogen (δ15N) signatures, demonstrating a protein-poor diet, and higher carbon (δ13C) signatures, reflecting anthropogenic food consumption13. H2.2: Urban coyotes increase their antioxidant capacity by up- regulating antioxidant enzymes to cope with urban stress. To evaluate how coyotes mitigate stress, I will evaluate their (a) total antioxidant capacity, (b) activity of antioxidant enzymes4, and (c) diet composition. To evaluate (a), I will use the ferric reducing ability of plasma assay to describe the global antioxidant balance4; (b), I will measure glutathione peroxidase and superoxide dismutase activity4; (c), I will perform stable isotope analysis (using δ13C and δ15N)11 on hair and scat samples to determine diet composition (i.e., anthropogenic vs. non-anthropogenic food sources). Using a linear mixed-effects model framework with AIC model selection, I will analyze the effect of site characteristics (rural/low SES/high SES), sex, and reproductive status on oxidative stress and the effects of urbanization on diet and oxidative stress. Feasibility: My research project will bring a new avenue of research to an established system, the Grit City Carnivore Project (Dr. Christopher Schell, University of Washington). I will collaborate with the Urban Wildlife Information Network to effectively identify coyotes near trapping locations through existing camera trap data. Field sites and permits have already been approved. My field experience (trapping/sampling) and computational skills (database management/R), along with access to cutting-edge equipment and collaborators, will lead to the success of this project. Intellectual Merits: Understanding the consequences of urban systems on wildlife physiology and stress is essential to developing effective wildlife conservation plans. My proposed research will fill knowledge gaps by exploring the luxury effect in coyotes and identifying links between urbanization, SES, and oxidative stress. These results will bring a novel perspective to an emerging field investigating the influence of urbanization on the life-history of urban wildlife. This study will form a foundation for future studies on fitness outcomes and adaptation to oxidative damage and could establish coyotes as bioindicators that reflect the health of urban environments (e.g., high oxidative stress may reflect exposure to pollutants14). Further, this project will advance our knowledge of the biological processes within cities. Broader Impacts: (1) Community Engagement and Education: In addition to disseminating my results throughout the scientific community, I will also present these findings locally (e.g., Tacoma News Tribune, high schools). I will also engage directly with residents and students near urban trapping sites to observe coyotes closely and showcase the vibrancy of the urban biome. I will partner with Environmentalists of Color and The Nature Conservancy to create community engagement opportunities and accessible education materials in urban ecology, focusing on Black and Brown communities in the Seattle- Tacoma metropolitan area. I will work with Treehouse, an organization aiming to close the education gap between underrepresented foster youth and their peers, to develop engaging interdisciplinary assignments with real data that links math, science, and urban history. (2) Management Implications: I will leverage existing connections to work with Point Defiance Zoo and Aquarium and Woodland Park Zoo to develop workshops about urban wildlife natural history and conservation. My research will reveal how wildlife are modifying their behavior in urban areas and how wealth disparities in humans influence wildlife stress, which will help managers develop natural areas for urban wildlife. Through my collaborations, I will be able to interact directly with Seattle and Tacoma city officials to help develop environmental justice and urban conservation policies. References: [1] Schell et al. (2020) Science. [2] Kuras et al. (2020) Landscape Urban Plan. [3] Prugh et al. (2009) BioSci. [4] Herrera-Dueñas et al. (2017) Front. Ecol. & Evol. [5] Isaksson (2015) Funct. Ecol. [6] Arnold et al. (2010) Biol. J. Linn. Soc. [7] Strandin et al. (2018) Phil. Trans. R. Soc. B Biol. Sci. [8] Schulte et al. (2018) Cons. Physio. [9] Morey et al. (2007) Am. Midl. Nat. [10] Newsome et al. (2015) Oecologica. [11] Magle et al. (2015) Anim. Conserv. [12] Gehrt (2007) Proc. 12th Wildl. Damage Mgmt. Conf. [13] Windberg et al. (1991) J. Wildl. Dis. [15] Pérez-Coyotl et al. (2019) Env. Poll. 2	0
"OPTIMIZING THE SYNTHESIS OF METAL ORGANIC FRAMEWORKS USING SEGMENTED FLOW TUBULAR REACTOR TECHNOLOGY Keywords: metal organic framework, scale up, continuous flow processes I. BACKGROUND The segmented flow tubular reactor (SFTR) is an exciting breakthrough in the realm of nano-scale technology. I first encountered this reactor during my research at Sandia National Labs (see personal statement), where it showed much promise in scaling up the synthesis of titania nanowires. Due to the limitations of reactor size, high chemical costs, and uncontrollable side effects of chemical impurities, mass production of nanoparticles remains an ever increasing challenge of engineers today. The expansion of a typical batch process will often lead to low quality synthesis due to the inability to control particle size and morphology under heterogeneous conditions. This same challenge is also paralleled within the metal organic framework (MOF) industry. How can MOFS, having synthesis routes and material properties that depend on nucleation at a reaction surface, be produced at a large scale where quality is often sacrificed for quantity? Fortunately, the SFTR is able to address this issue in much the same way as it did for nanomaterials. The key principle behind the design is the segmentation of the reactants into micro-reactors in a continuous tubular process. Each microvolume is separated by an immiscible fluid or gas as shown in Figure 1. Within this arrangement, borrowing from the concept of ‘plugs’ in plug flow reactors, reagents are perfectly mixed in the radial direction but not in the axial direction, thus removing the possibility of axial Fig. 1. Schematic Representation of SFTR [1]. back-mixing, and ensuring that all reactants are endure a similar history (residence time and heat exchange). This process allows the synthesis of homogeneous products with narrow particle size distributions, enhanced control of particle morphology, polymorph selectivity and better stoichiometry control. II. MOTIVATION Current literature demonstrates that continuous MOF synthesis is possible, even at the scale of several kilograms per day [2]. The largest MOF manufacturing company, BASF, whose pilot plant is located in Germany, can produce MOFs on order of several kilograms per batch using the solvothermal method. What will be done as the demand for these hydrogen-capturing materials increases, especially as the United States is becoming an increasingly hydrogen-based economy [3]? NSF GRFP Research Proposal III. RESEARCH PLAN a. Year I –Determine Optimal Reacting Conditions for Common MOF Compounds It is desirable to know exactly what operating conditions the SFTR is expected to perform under before a prototype reactor can be prepared, and thus its effectivity in optimization tested. The respective MOF compounds under analysis include, Mn3[(Mn4Cl)3(BTT)8]2, Zn4O(BDC)3, Cu3(BTC)2(H2O)3. These should all be explored due to their ability to their differing levels of hydrogen storage capacity and differing size and geometry. The optimal reacting conditions will also be depend on what pore size is desired for the respective MOFs and what substrate is being used in synthesis. b. Year II- Determine Optimal Reactor Conditions for the MOF Synthesis Whereas year I focused exclusively on the chemistry of the MOFs to be prepared and demonstrating that specific reaction conditions generate material with the desired properties, year II will be one of matching those conditions to that of a SFTR. Here, I will be sizing an SFTR that meets all of the requirements given the reaction conditions. c. Year III- Design a Bench Scale Model and Test Performance At this point in research, it is expected that the prototype reactor can be developed as as bench scale model. Necessary parameters such as tube size, material construction, volume, pumping efficiency and the need for a cooling or heating bath have been determined based on analysis in years I and II. The performance will have to become compared against traditional MOF synthesis routes such as lab scale layer-by-layer deposition, and microwave synthesis. IV. ANTICIPATED RESULTS Just as the segmented flow tubular reactor has shown much promise in optimization of calcium carbonate nanomaterial production [1], it is expected that, it will also be successful regarding synthesis of the three chosen MOF compounds. The many similarities between MOF synthesis and nanoparticle synthesis is what will be exploited in this study, to hopefully achieve similar results. Solvothermal synthesis is useful for growing crystals suitable to structure determination, because crystals grow over the course of hours to days. It is expected that by optimizing the SFTRs models to each respective MOF mechanism, a continuous production of uniform and low–defect material can be achieved over this same length of time or even a shorter duration. V. INTELLECTUAL MERIT & BROADER IMPACTS The implications of this research project are far reaching, even beyond its potential to mass produce MOFs and thus meet the demand of our growing hydrogen economy. It has the potential to introduce an energy efficient way of producing compounds designed for energy efficient applications to begin with. Double threat! This projects represents the cross between the chemistry and chemical engineering discipline to address the issues in sustainability that plague our nation. The scale-up of MOFs is not an area that has not been widely studied, and thus this research study represents a much needed contribution to the world of sustainable chemistry. NSF GRFP Research Proposal References: [1] “Precipitation of nanosized and nanostructured powders: process intensification using SFTR”, applied to BaTiO3, CaCO3 and ZnO - Chem. Eng. & Techn., 34(3) 344-352 (2011). [2] “BASF Develops Method for Industrial-Scale MOF Synthesis; Trials Underway in Natural Gas Vehicle Tanks.” Green Car Congress. 5 October 2010. [3] ""Global Hydrogen Fuel Cell Electric Vehicle Market Buoyed as OEMs Will Launch 17 Vehicle Models by 2027, IHS Says"". IHS Inc. 4 May 2016. Retrieved 13 May 2016."	0
Nature versus nurture? Maternal responses to infant distress calls Introduction & Significance: The brain has the extraordinary capability of attributing different levels of importance to the different types of input signals it receives. For example, hearing one’s name, even at very low volume, elicits strong neural signals, as the brain has learned the relevance of that particular sound. The process by which this occurs is known as synaptic plasticity, which allows the brain to alter its connections based on experiences associated with particular sensory inputs. Synaptic plasticity is usually experience-dependent, and a class of molecules known as neuromodulators have the role of strengthening specific neural circuits dependent on particular situations. Here I examine the action of one important neuromodulator, oxytocin, which is involved in a multitude of social interactions, including maternal care. Maternal behaviors are observed in all mammalian species, including mice. As infants, mouse pups are especially helpless, relying on their mother (called a ‘dam’) for all of their needs. Pups become scattered from the nest as the dam moves around, and must communicate with the dam that they have become isolated. To do so, they emit isolation ultrasonic vocalizations (USVs), triggering the dam to respond by retrieving the pup and returning it to the nest[1]. While dams retrieve pups with high accuracy, virgin female mice that lack prior experience with pups fail to exhibit this behavior, generally neglecting the calls of a nearby pup[2]. However, after being cohoused with a dam and pups for several days, virgin female mice can learn to retrieve pups with comparable accuracy to the dam[2]. A virgin’s acquisition of this pup retrieval behavior is accelerated by administration of the neuromodulator oxytocin [2]. This behavior can be eliminated by inactivation of the left auditory cortex (A1), which contains a significantly higher amount of oxytocin receptors (OXTR) than the right A1[2]. My proposed graduate research is concerned with the specific features of isolation USV stimuli that cause A1 to recognize the behavioral relevance of these sounds. Specifically, I propose to examine perceptual attributes of the isolation USV encoded by the maternal A1 that enable the dam to recognize and respond to this sound, as well as the role of oxytocin-dependent plasticity in acquiring USV-induced pup retrieval behavior by inexperienced virgins. Recent work demonstrated that human A1 distinguishes screams from conversational speech by an acoustic quality known as ‘roughness,’ defined as the rate at which the volume of sound changes[3]. By detecting roughness, human A1 rapidly engages subcortical structures to assess danger[3]. I hypothesize that similar acoustic perceptual features allow the maternal A1 to distinguish, and attribute behavioral relevance to, the sound of nearby pup isolation USVs, and that learning of pup retrieval behavior through experience relies on oxytocin-dependent synaptic plasticity that strengthens the A1 response to such perceptual features. Aim 1A: Which acoustic features differentiate isolation USVs from other vocalizations? To understand how the USV response is encoded in the dam A1, I will chronically implant electrode arrays into adult female mouse A1, and obtain single-unit recordings in response to natural, pup- evoked isolation USVs from a speaker. This Figure 1: Example single-unit recordings from process (Figure 1) allows clear visualization of tetrode implants in A1. An increase in activity is temporally-precise spike activity in A1 as it observed when isolation USV stimulus begins relates to sensory input from isolation USVs. Additionally, by observing behavior that dams exhibit when hearing the isolation USV stimulus, Katherine Furman – Graduate Research Statement I can visualize how A1 activity correlates to both sensory input and behavioral output (in the form of pup retrieval). Using the modulation power spectrum (MPS), which can visualize sounds two- dimensionally on both spectral and temporal domains[3], I can examine the portions of acoustic space in which these naturally-produced isolation USVs reside. By comparing this to the MPS of adult USVs (which are behaviorally neutral to the dam) I will isolate which acoustic features are unique to the pup isolation USV and have behavioral relevance to A1. Aim 1B: Are these acoustic features relevant to A1? If pup isolation USVs contain specific acoustic ‘roughness’ features distinguishable by the maternal A1, synthetically manipulated USVs which lack these features should result in a weakened response compared to natural pup- evoked isolation USVs. Using MATLAB I will synthesize audio clips which mimic pup USVs, but lack the spectral/temporal features previously identified as unique to isolation USVs. I will then play these to maternal animals, measuring behavioral and neural responses to calls with similar statistics as USVs, but varying in their frequency, temporal modulation (rhythm), and roughness. Using single-unit recordings from electrode arrays in A1, I will observe neural activity of A1 in dams when they are exposed to synthetically manipulated USVs played from an ultrasonic speaker. If I’ve successfully identified the differentiating feature(s) making isolation USVs unique from other mouse vocalizations, I expect that dam A1 neurons will exhibit a stronger, more temporally-precise response to hearing pup-evoked isolation USVs (as was observed in [2]), than synthetically manipulated USVs. I also expect that pup retrieval behavior will be significantly diminished, if not eliminated, when exposed to synthetically manipulated USVs. Aim 2: Are these specific elements dependent on oxytocin signaling? To test the hypothesis that oxytocin promotes maternal pup retrieval by strengthening the A1 response to unique spectral/temporal features of isolation USVs, I will observe the changes in A1 activity as a result of changes to endogenous oxytocin systems. Using transgenic Oxy-Cre mice will allow targeted expression of light-sensitive opsins in oxytocin-releasing neurons, the activity of which can be manipulated with light of specific frequencies. I will express channelrhodopsin-2, which is able to activate neurons in response to blue light, in pup-naïve virgin female mice. These mice will be exposed to pup isolation USVs concurrently with optogenetic stimulation of oxytocin-releasing neurons. By continuously pairing isolation USV audio with stimulation of endogenous oxytocin over a number of days, I hypothesize that the A1 activity of the naïve female will change to mimic the strong, temporally-precise response observed in dams. Intellectual Merit & Broader Impacts: With the help of the NSF GRFP, I will be the first to identify the specific acoustic features, over both spectral and temporal domains, which are unique to pup isolation USVs when compared to other mouse vocalizations. By identifying the A1 single-unit activity displayed in response to isolation USVs, and by identifying the changes in activity when crucial isolation USV features are eradicated from the stimulus, I aim to observe the specific activity patterns recruited by A1 in attributing behavioral relevance to infant-related sounds. By observing the changes in neural activity induced in pup-naïve virgins after optogenetic stimulation of oxytocin, I will be able to observe the unique form of neuromodulatory plasticity evoked by oxytocin in A1 which allows experience-dependent learning of maternal pup retrieval behavior. By examining maternal auditory processing in such depth, the field can better understand the interplay between auditory input and oxytocin to yield behavioral output. References: 1. Ehret (2005) Infant rodent ultrasounds – a gate to the understanding of sound communication. Behav Genet 35:19. 2. Marlin et al. (2015) Oxytocin enables maternal behaviour by balancing cortical inhibition. Nature Katherine Furman – Graduate Research Statement 520:499 3. Arnal et al. (2015) Human screams occupy a privileged niche in the communication soundscape. Curr Biol 25:2051.	1
I propose to develop new theoretical and computational methods for investigating activated chemical processes. Specifically, I will develop a method for calculating derivatives of rate constants, transport properties, and other dynamical timescales with respect to temperature (T), pressure (p), and chemical potential (µ). I will use these to calculate activation energies (E ) and activation volumes (∆V‡) of transport properties in CO -expanded liquids (CXLs). a 2 Motivation: There is a growing interest in green alternative solvents for use in catalytic reactions.1 Onesuchalternativemedia, CXLs, areofparticularinterestduetotheirincreased safety, cost-effectiveness, and transport properties when compared with traditional organic solvents.2 Using a CXL can give up to a five-fold reduction in the amount of solvent needed for a reaction compared to the neat solvent, while significantly increasing mass transport important for catalysis where reactions are often diffusion-limited. Diffusion in CXLs has generally been seen to be monotonic with changes in p, T; however a separate, computation- ally expensive, vapor-liquid coexistence simulation is currently required at each phase point (p,T) before transport calculations can be run with molecular dynamics (MD). Introduction: Instead, I propose a direct method by which the entire T-, p-, and µ- dependence of transport properties in CXLs may be evaluated from simulations at a single phase point. Traditionally, the T-dependence and activation energies of transport properties are calculated from a series of simulations at different temperatures and evaluating their Arrhenius behavior. While this is generally satisfactory, there are systems in which calcu- lations over large temperature ranges are difficult or inconvenient, as in the case of CXLs where small changes in T and p change the composition of the liquid phase. Preliminary Work: A general method has been devel- opedinpreviousworkbywhichtheT-dependenceoftrans- port properties and their activation energies can be ex- tracted from simulations at a single temperature. This is achieved by launching non-equilibrium MD trajecto- ries from different points along a single NVT trajectory. This was originally applied to the reactive flux correla- tion functions and was later generalized by our group to work for any rate constant, transport property, or dynam- ical timescale calculated from a time-correlation function Figure 1: The ratio of the energy- (TCF).3−4 This also allows for decomposition of E into weighted mean-squared displacement a (MSD (t)) to the mean-squared dis- kinetic and potential energy contributions, providing oth- H placement (MSD(t)) is presented in erwise unobtainable mechanistic insight into the E . a red and is equal to E at the long a,D With this method, the first derivative of transport time limit, presented in blue. properties with respect to state variables (e.g., T, p) can be calculated. This has been successfully applied previously to the diffusion coefficient (D) to calculate the E of diffusion of bulk water, as pictured in Figure 1. A typical Arrhenius a calculation finds E should be 3.5 kcal/mol while this direct method found a value of 3.48 a kcal/mol. The second derivative is also similarly calculable with respect to these variables. For a system with a monotonic dependence on T, and p, as in the case of CXLs, these derivatives and a single value of the transport property are all that is needed to determine its value at other state points. This greatly reduces the number of simulations necessary to evaluate the T-dependence and drastically cuts down the computational expense while also gaining additional insight into the decomposition of the E . a 1 Graduate Research Plan Ezekiel A. Piskulich Aim-1 Application to the NpT Ensemble: With the NpT ensemble, fluctuations occur in both energy and volume that allow for the calculation of both E and ∆V‡. Using a a similar derivation to our previously published works, we have been able to show that ∆V‡ = k T∂ln(D) = lim (cid:104)δV(0)[r(t)−r(0)]2(cid:105) . Experiments and simulations have previously D B ∂p t→long (cid:104)[r(t)−r(0)]2(cid:105) used an ”Arrhenius-like” plot of ln(D) vs p to calculate ∆V‡; however, these calculations require that measurements be taken across an enormous range of pressures (1- 10,000 bar) to distinguish a measurable change in D with respect to p. Yet in some cases ln(D) is not linear in p. This should allow us to probe this non-linearity which is generally inaccessible using traditional methods. Additionally, cross-derivatives, such as ∂Ea, can be calculated, ∂p allowing us to extract the p-dependence of E from simulations at a single p. a Aim-2 Application to the µVT ensemble: In the µVT ensemble, the first derivative of a transport coefficient or other dynamical timescale with respect to the chemical potential can be written as ∂CAB(t) = β(cid:104)δN(0)A(0)B(t)(cid:105) . In practice, this derivative can be calculated ∂µ µVT byrunninganNVT simulationinwhichasub-volumehasbeendefined, allowingthenumber of molecules in the sub-volume to fluctuate. This provides a means by which changes in rate constants, diffusion coefficients, or other dynamical timescales with respect to the chemical potential can be calculated without separate simulations at different compositions. Aim-3 Application to CXLs: I propose to apply this method to a variety of CXLs of industrial importance, including CO -expanded ethylene oxide (EO) and methanol (MeOH) 2 which are involved in a non-phosgene route for the industrial commodity chemical, dimethyl carbonate.6 The methods described in the previous aims will be used to probe the T-, p-, and µ-dependence of their transport properties and other relevant dynamical timescales from simulations at a single point. Ideally, this will allow for the calculation of the entire surface of diffusion coefficients, reorientation times, and other dynamic timescales for these CXLS without requiring more than a single phase coexistence simulation. Intellectual Merit: The methods proposed within this work are simple to apply, signif- icantly decrease computational costs, and provide deeper insight into the mechanisms of transport properties. Additionally, they provide a convenient means by which first (and higher-order) derivatives with respect to T, p, and µ may be calculated. This will allow for entire dependence of these properties on these macroscopic variables to be calculated without requiring expensive phase coexistence calculations at each phase point. Broader Impacts: The EO–CO and MeOH–CO system is a green alternative solvent 2 2 consideredfortheproductionofdimethylcarbonate, animportantprecursorinpolyurethane production and will be investigated in collaboration with chemical engineers at the Center for Environmentally Catalysis.6 I will also be co-organizing the 2019 Liquid Gordon Research Seminar for graduate and postdoctoral students. I will continue to mentor undergraduate students throughout the course of my studies. Furthermore, I will pursue the outreach program proposed in my personal statement. The methods proposed in this work can be extended to solve a wide variety of chemical problems. [1]P.AnastasandN.Eghbali. Chem. Soc. Rev.,39,pp. 301-312(2010). [2]P.Jessop,andB.Subramaniam Chem. Rev.,107,pp. 2666-2694(2007). [3]O.O.MeseleandW.H.Thompson. J.Chem. Phys.,145,134107 (2016). [4] Z.A. Piskulich, O.O. Mesele and W.H. Thompson. J. Chem. Phys., 147, 134103 (2017). [5] K. Krinicki, C. Green, and D. Sawyer. Faraday Discuss. Chem. Soc., 66, pp. 199-208 (1978). [6] Z.A. Piskulich, B.B. Laird and W.H. Thompson. Fluid Phase Equilib., Submitted, (2017). 2	0
The problem: While most plants rely on soil nitrogen (N), plants capable of symbiotic N fixation (SNF) can acquire N directly from the atmospheric N . Because N is inexhaustible, 2 2 SNF is convenient. However, SNF has a high cost2 due to the need to break the triple bond of N . 2 Following previous literature1,2, I define the C cost of SNF as the respiration (CO flux) needed 2 to drive SNF divided by SNF itself (N flux). Biochemical calculations estimate the C cost of 2 SNF to be slightly higher than using nitrate and much higher than using ammonium1. Measurements of these costs in nodules (the root structures that house symbiotic bacteria) have been close to the biochemical predictions2. However, these measurements have been carried out at constant temperatures. As explained below, the cost might vary widely across temperature. The cost of SNF helps determine its effectiveness, both within a plant (using SNF vs. soil N) and across species (competition between N-fixing and non-fixing plants). A lower cost makes SNF viable even when soil N is abundant, whereas a higher cost makes SNF untenable even when soil N is scarce. Therefore, variation in the cost of SNF across temperature would have far- reaching implications. For example, it could help explain why N-fixing trees are successful in warm areas3, and could also affect how SNF will change with climate. Despite its importance to fundamental biology, research on temperature responses of SNF has long been beset by technological constraints. Using a novel method that overcomes these constraints, I will ask one main question: What is the temperature response of the C costs of SNF? I will address this question using the tree Robinia pseudoacacia, which lives across a wide climatic range, accounts for 64% of tree-based SNF in the contiguous USA5, and is common across Eurasia3. Hypotheses: My hypotheses are based on previous measurements of the components of the cost: respiration and SNF itself. Previous work6 has observed that SNF plummets at low (near 0°C) and high (near 50°C) temperatures. There are few data on nodule respiration at different temperatures, but leaf respiration continues across 0-50°C7, suggesting nodule respiration might too. Respiration rates well above 0 divided by SNF rates near 0 mean that (H1) the C cost of SNF will be well above the biochemical predictions at low and high temperatures. My hypotheses about temperature optima stem from measurements from my lab, which recently developed a system for non-destructive, extremely sensitive, and continuous measurements of nitrogenase activity6. Preliminary research using this system has shown that the optimal temperature for SNF is much higher (29-36°C) than previously assumed (25°C)4, as shown in Fig. 1a. I do not know how nodule respiration will change with temperature, so I have competing hypotheses. (H2a) If respiration peaks near the same temperature as SNF (green curve, Fig. 1b), then the C cost will have a similar temperature optimum as SNF (green curve, Fig. 1c). (H2b) Alternatively, if respiration rises continually (blue curve, Fig. 1b), as leaf respiration does6, then the temperature optimum of the C cost will be lower than the optimum of SNF (blue curve, Fig. 1c). Hypotheses H1, H2a, and H2b are represented in Fig. 1c, which also shows the equation for the C cost of SNF that is used in many models8(black curve). This equation assumes that the change in C cost of SNF with temperature is inversely proportional to the SNF rate and is scaled to the biochemical C cost of SNF (7.5-12.5 g C g N−1). As explained above, I believe this model is flawed as it does not account for how nodule respiration changes with temperature. Methods: Using growth chambers at Columbia University, I will grow 30 Robinia pseudoacacia seedlings (from seed) under a temperature regime of 26°C during day and 20°C during night using a 14-hour light and 10-hour dark photoperiod with relative humidity and CO₂ concentrations of 70% and 400 ppm to emulate controlled climate conditions6. The seedlings will be inoculated with slurries of crushed nodules as well as bacteria cultured from these nodules to ensure the plants can establish symbiotic partnerships, and will be fertilized with limited levels of N (1.5 g N m−2 yr−1) but ample amounts of all other nutrients to promote SNF. The nodules will be measured for SNF and respiration continuously across 1-50°C over the course of 3 hours. The excised nodules will be placed in a sealed chamber with 2% acetylene (the concentration at which the system measures nitrogenase activity most precisely and accurately6). After accounting for leakage and other factors6, the rate at which acetylene is reduced to ethylene (measured with a Picarro G2106 laser) gives a measurement of nitrogenase activity9. Preliminary work in our lab has shown that Robinia nodules have stable nitrogenase activity at least 6 hours after excision, and that the ratio of 15N to acetylene reduction is stable 2 across the temperature range of our study (TA Bytnerowicz, pers. comm.). CO₂ flux in the chamber will be synchronously measured by a Licor LI-62626 to determine the temperature response of nodule respiration. I will process and analyze the data by modifying R scripts previously developed in the Menge lab (ref. 4 for processing, TA Bytnerowicz, pers. comm. for temperature responses of SNF). The analysis will yield temperature response curves for SNF, respiration, and the ratio of the two (the C cost of SNF). Intellectual Merit: This research will answer questions fundamental to the biology of the symbiotic relationship between legumes and N-fixing bacteria. At the level of plant ecophysiology, at what temperatures is it energetically favorable for Robinia pseudoacacia to fix N? At the level of community ecology, how does Robinia pseudoacacia compete against non- fixing plants if it relies on SNF? Broader Impacts: The paucity of knowledge on how SNF and its C cost respond to temperature has been a major constraint on global biogeochemistry and climate modeling. As described above, temperature response functions for SNF and for the C cost of SNF are already in use in terrestrial biosphere models, despite few data for the temperature response of SNF itself and zero data for the temperature response of the C cost of SNF. My work will lead to direct improvements in the representation of SNF in these models, and thus will directly influence our ability to predict global biogeochemistry and climate change. In addition to publishing in academic journals, I will present my work at academic conferences, such as SACNAS’ National Diversity in STEM Conference, and outreach programs, such as the Ecological Society of America’s SEEDS program and Women In Science at Columbia (WISC). As a former McNair Scholar, I am well aware of the disparity of resources within underrepresented populations. Because of this, I willalso contribute my mentorship to the Environmental Justice and Urban Ecology Summer Research Program, a funded program for high school students at the Washington Heights Expeditionary Learning School. 1Gutschick 1981, The American Naturalist. 2Tjepkema & Winship 1980, Science. 3Steidinger et al. 2019, Nature. 4Houlton et al. 2008, Nature. 5Staccone et al. 2020, Global Biogeochemical Cycles. 6Bytnerowicz et al. 2019, Methods in Ecology & Evolution. 7Heskel et al. 2016, PNAS. 8Fisher et al. 2010, Global Biogeochemical Cycles. 9Hardy et al. 1968, Plant Physiology.	0
Introduction: Conventionalseismicmomentresistingframes(SMRFs)aredesignedtoresist anddissipateseismicenergybytransferringtheloadsanddistributingpermanentyieldingtothe primary members, such as beams and columns. While this design philosophy performs well in providing strength and collapse resistance to a structure, damage to major structural members presents a major drawback in economical repairs. Therefore, most recent alternative designs not onlyseektodissipateseismicenergyandavoiddamageintheprimarymembers,butalsotomitigate damageintheconnectionsthemselves. Inessence,thedevelopmentofsuchaconnectionincreases seismicresiliencyinstructures,avoidingexpensiveanddisruptivereplacementofentiremembers whilealsolimitingexcessivedeformationsattheconnections. Onesuchmethodistheslidinghinge joint(SHJ):1 Figure1showsthe proposedSHJconnectionwithmodifications. Theflangesofthe beamareconnectedtoslidingplateshingedtothecolumn. Thus,asthecolumnandbeamrotate duringseismic events, theconnection’srotation causesthe platestoslip, andenergyis dissipatedin theformoffriction. Because the moment resistance is dependent on the friction provided betweenthe platesand flanges, challenges intheir design includeensuringthattheconnectionsstillprovideadequatestrength and safety under service. Additionally, the benefit in mitigating theneedtoreplaceconnectionsafterdamagecanonlyberealized by ensuring that the joint return to its original position. Since it is desired that the connection also be economical and simple to build, the proposed research will be the first to examine the performance of posttensioned (PT) strands as the self-centering mechanismcoupledtoaslidinghingejoint(SHJ-PT).Theproposed study will be focused around the achievement of the following primaryobjectives: 1)developmodelsfortheSHJconnectionand Figure1: SHJconnectionwith PTstrands,and2)performsimulationandanalysesonthemodels PTstrands1 and,resourcespermitting,experimentaltestingofthesubassembly. Hypothesis: Posttensionedstrands providean economical, easily-constructable, and geometri- callyflexiblemechanismofrecenteringfortheslidinghingejointconnections. Objective 1: Development of Models for SHJ and PT Strands The proposed connection consists of a beam and column connected with shear plates. The SHJ as described by Clifton (2005)2 willbeemployedasthemethodofenergydissipation. AsshowninFigure1,steelstrands paralleltothebeamareanchoredtothecolumnsinordertostressthebeam. Whiletheexperimental testing of full-scale structures under seismic loads is one method to examine response behavior, suchtestsarecostlytoperformandgenerallystillrequiresupplementalanalysestoassesslocalized materialresponse. Expandingonexistingmethods,3 ananalyticalprocedurewillbedevelopedto establishtherelationshipbetweentheconnectionrotationandthemechanicalresponsedevelopment in the PT steel strand. For a range of rotation induced by seismic loads, the response of the PT strandwillbedeveloped. SimilartoworkbyKhoo(2012),4 theSHJcanbemodeledusingasystem of rotational springs, thus providing the SHJ’s response to seismic load. When coupled with the previousmethod, thetotalstructuralresponseandbehavioroftheSHJ-PTcanbestudied; output 1GCCliftonetal.“Slidinghingejointsandsubassembliesforsteelmomentframes”. In: 2007. 2GCClifton. “Semi-rigidjointsformoment-resistingsteelframedseismic-resistingsystems”. PhDthesis. 2005. 3ConstantinChristopoulosetal.“PosttensionedenergydissipatingconnectionsforsteelMRFs”. In: (2002). 4Hsen-HanKhooetal.“Developmentoftheself-centeringSHJwithfrictionringsprings”. In: (2012). 1 strainsanddisplacementsfromthePTstrandsmodelarepassedtotheSHJspring-model’srotations andanalyzed (andvice versa)in orderto fullyunderstandthe interactionbetween thetwo systems. Additionally,finiteelementmethodanalysiswillbeperformedtocheckthatthestressesontheplate donotexceedthefailurelimitsoftheplates2. Objective 2: Perform Seismic Simulations and Analyses on Models Working with Dr. Pa- tricia Clayton (UT), I will numerically investigate the performance of the SHJ coupled with PT strands under earthquake simulations. Using the component models developed under Objective 1, building models will be constructed. These building models will be subjected to time-history analyses simulating a series of ground motions representing the maximum considered event and servicelevel earthquake. In thisobjective, thegoalsare toarrive atadequate responseresultsfrom whichaproceduretodeterminedesignparameterscouldbeestablished. Asametricofachievement, the results will be subjected to validation using existing test data from previous studies5,6. Upon the model structure’s achievement of data validation and providing sufficient response to bench- markloads,thecomputationalstudycanbeexpandedtoobservetheperformanceofthestructures utilizingvariationsofthedesignedconnection,suchasusageataspecifiedspacingsandbaysor with varying strengths of the sliding hinge plates. Resources permitting, testing of the proposed SHJ-PTconnectionsubassemblycanbeperformedusingthefacilitiesattheFergusonStructural EngineeringLaboratoryattheUniversityofTexas. IntellectualMerit: Thoughthereisexistingresearchonthedevelopmentoftheslidinghinge joint6,1,2,4 andtheanalysisandtestingofreplaceableconnectionsutilizingPTstrands7,8,5,efforts to couple these concepts in a design have yet to be extensively studied. Results9,4 from previous simulationsand physical testinghavedemonstrated theSHJ’sability to mitigate damage aswell as itspotentialtobecoupledwithself-centeringmechanismsandwarrantsfurtherresearchintonovel designs. Asitstands,theSJH-PT’sadvantagesare: 1)mitigationofdamagetoprimarymembers andlimitingdamageontheconnection;2)constructionusingconventionalmaterialandskills;3) abilitytoself-centerafterevents. Thesebenefitswarrantfurtherstudyintotheconnections’strength andserviceabilitypotentials. ThestudyalsoaimstoaddressconcernsbasedonpracticalityofPT strands in connections based on the effects of gap-opening and the compression that the strands induceinthebeamflanges1. BroaderImpacts: DrawbacksincurrentpracticeinSMRFsthatemployweldedconnections include seismic damage to beams and columns that require disruptive and costly replacement of thecomponent. ThesuccessfuldevelopmentofSMRFsusingenergy-dissipationmethodsmeans removingtheneedtoreplaceconnectionsafterseismicevents, leadingtoamoreflexiblerecovery processforstructuresafternaturaldisasters. Thisplaysahugeroleinpresentingretrofittingoptions toboostresilience forstructuresinareaswhere seismicrisksarerisingdue toenvironmental and industrial impacts. I will also work to increase the acceptance of the SHJ-PT connection in the structuralengineeringcommunitythrough1)publicationsinnotablejournals,suchasJournalof StructuralEngineeringandJournalofConstructionalSteelResearch,2)attendingconferencesinthe discipline,and3)workingcloselywithindustryentities,especiallythosealreadywithproprietary work in semi-rigid moment connections such as Simpson Strong-Tie and Skidmore Owings & Merrill,inordertofurtherdevelopandpopularizeenergydissipationbasedSMRFconnections. 5JamesMRiclesetal.“Posttensionedseismic-resistantconnectionsforsteelframes”. In: (2001). 6HHKhooetal.“Experimentalstudiesoftheself-centeringSlidingHingeJoint”. In: (2012). 7Ying-ChengLinetal.“Seismicperformanceofalarge-scalesteelself-centeringMRF”.in: (2012). 8PatriciaMClaytonetal.“Seismicdesignandperformanceofself-centeringsteelplateshearwalls”. In: (2011). 9GregoryAMacRaeetal.“Theslidinghingejointmomentconnection”. In: (2010). 2	0
Introduction: Recent advancements in technology have enabled new ways of constructing metamaterials that possess desirable mechanical properties. Materials that have high elastic stiffness and low density are considered some of the strongest, stiffest, lightest materials available today [1]. By controlling their microstructures, we can tune mechanical properties of metamaterials that endure extreme conditions. A trait of a metamaterial microstructure that is much ignored to date is randomness (aperiodicity), owing to the limitation of current design approaches based on unit cells. Aperiodic structures are likely to result from natural self-assembly and self-organization processes and may be more robust against uncertainty. Examples of robust microstructures can be seen in natural formations such as wood and nacre, or in parts of the human body such as bone [2]. Materials that are robust against uncertainty perform well under various forces and stresses they may encounter. Currently, state-of-the-art approaches for designing metamaterial microstructures for desirable properties can be categorized either as parameterized design or topology optimization methods. Both approaches build their foundation on the assumption that material microstructure consists of periodically repeated motifs (or unit cells). Parametrized design is a simpler design method to design structures like lattices; it allows for a few design parameters to map directly to specific properties, but it needs an ad-hoc design to start with. Since the design space is quite narrow, there is a narrow range of achievable material property ranges. Meanwhile, topology optimization is a design method that allows for freeform design of a structure with almost any geometry; it is mathematically well defined, but computationally expensive, leading to unpredictable geometries that are hard to manufacture [3]. Both approaches are difficult to use as tools to efficiently and effectively explore the vast design space of material microstructures. By combining optimal features of each method, this research aims to expand the microstructure design space to maintain local parametric behavior while enabling global freeform design. Through numerical approaches, one can program aperiodic material microstructures towards desirable properties using a “growth”-like process that is encoded by “DNA”-like pairwise combination rules. With this “growth” process, a method for physical self-assembly is desired as it allows for rapid production of programmable metamaterials. The mechanical self-assembly process has been explored in several applications across different scales [4, 5], but none have been able to achieve mechanical self-assembly of an aperiodic microstructure. This design approach allows us to efficiently generate new random yet ordered microstructures. It enables effective exploration of the material design space and pushes the boundaries of applying new stronger materials to applications such as shock absorption and acoustics. Research Plan: I aim to develop self-assembly methods to investigate how the shape and design of cellular automata base cells affect mechanical properties of programmable metamaterials. I will then develop specific tunable metamaterials for applications that desire the particular properties. Numerical, experimental (Fig. 1), and application phases can be achieved with FEA software, 3D printing, and mechanical testing equipment commonly used in any mechanical engineering lab. My previous research experience doing this with square-shaped tiles demonstrates my technical capability. Specific Aim I. Develop Baseline Samples: I will first develop a wave function collapse algorithm [6] for various polygons, such as pentagons and hexagons. The versatile algorithms will be developed in Python for both 2D and 3D self-assembly with flexibility for varying polygons. To do this, I will use a so-called wave function collapse algorithm, which performs a “growth” process similar to cellular automata. We define fundamental building blocks and connectivity rules over a cellular space. Once the first cell is set, connectivity rules are enforced to determine surrounding cell states; this process then repeats in propagating cycles. This will generate 2D and 3D samples for multiple shapes to be transformed into 3D objects in Rhino with Grasshopper C# that will be tested using FEA software. This forms a baseline mapping of the programmable microstructure design space as a success assessment. Specific Aim II. Perform Physical Experiments: From my numerical analysis of these structures, I will 3D print specific samples generated by the algorithms and physically perform the same mechanical tests as in the numerical phase using mechanical testing equipment, such as an Instron machine. The results of these tests can be compared to the numerical results to evaluate their degree of error. Physical 2D and 3D self-assembly methods for multiple shapes will be developed to gain further insight into this self-assembly method. Inspired by the mechanics of DNA self-assembly processes [7], I will design tiles for each shape with their respective channel types dictated by the polygon’s interior angle. This tile design and experimental setup must ensure some randomness and adhere to the algorithm- determined connectivity rules. The self-assembled 3D printed tiles will be used as a carrier casting mold in which to pour a plastic material to generate the metamaterials to be tested. Using the same mechanical testing methods as the previous samples, I will then compile and analyze data for information about the design space and microstructure properties with a focus on how base cell shape and corresponding interior angles affect mechanical properties of the metamaterial microstructures. At this point, a new physical method for developing aperiodic programmable metamaterials will have been created. The design space can be studied by constructing a material database. If the physical self-assembly method is not experimentally reliable, the initial algorithm-based numerical study and mechanical tests still provide a wealth of data to be used in the applications phase. Specific Aim III. Develop Metamaterials for Applications: Based on my findings from the studies performed in the previous phases, I will then study special properties of certain metamaterials that this self-assembly method yielded, such as shock absorbency and acoustic capacity. My experimental results will yield a desired mechanical property by tuning the specific base cell shape and quantity of channel type. With these settings, the physical self-assembly method can be used to create an array of samples used for additional testing for specific properties, such as impact testing in the shock absorbency case. These studies will demonstrate how the metamaterials yielded from the developed self-assembly method can be applied as lighter, stronger, and more flexible alternatives to materials currently used in aerospace and medicine. Intellectual Merit: This project will develop a mechanical self-assembly method of aperiodic programmable metamaterials, contributing new 2D and 3D self-assembly methods to metamaterials and mechanics research; this will improve the fundamental understanding of material microstructures and their properties. This project will also enhance understanding of the mechanics of self-assembly such as attractive forces and interlocking as seen in DNA self-assembly [7]. Working on this project at a research institution with a strong mechanical engineering program will provide the proper resources to research and publicize my findings related to metamaterials, bio-inspired processes, and their applications to medical and aerospace fields. Broader Impacts: This project will contribute new 2D and 3D self-assembly methods to metamaterials and mechanics research while developing new aperiodic programmable metamaterials with large degrees of tunability. Scale-independent metamaterials will be created that can be applied to aerospace materials, soft materials, and medical devices for its capabilities of enhanced shock absorbency, acoustic properties, elasticity, and strength. Additionally, through its biologically linked process of self-assembly, the microstructures developed have the potential to be applied to sustainable, environmentally friendly structures and devices. This project also has a parallel educational impact to introduce high school and undergraduate students to numerical and experimental methods in mechanical engineering and STEM. I plan to mentor students in researching metamaterials. [1] J. B. Berger et al., “Mechanical metamaterials,” pp. 533–537. [2] H. Wagner et al., “Bone microstructure,” pp. 1311–1320. [3] O. Sigmund et. al, “Topology optimization,” pp. 1031–1055. [4] E. Klavins, “Programmable Self-Assembly,” pp. 43–56. [5] G. M. Whitesides, “Self-Assembly,” pp. 2418– 2421. [6] Heaton, R. (2018). Wavefunction Collapse Algorithm. [7] S.-S. Jester et al., “DNA nanostructures,” pp. 1700–1709.	0
An Adaptive Chemistry Reduction Method for Detailed Modeling of Advanced Combustion Systems Key words: combustion, mechanism reduction, stiffness removal, CFD Combustion of hydrocarbon fuels provides 85% of energy in the modern United States [1]; the current energy crisis is in reality a fuel crisis. While renewable forms of energy are being pursued to supplement combustion-based sources, hydrocarbon fuels will remain the major component for the next few decades. Currently, there is high demand to improve the efficiency of combustion technology to decrease the amount of fuel consumed and to reduce the emissions in an effort to lessen the environmental impacts; in addition, fuel-flexible designs that can run on both conventional and alternative fuels are desired. Computational modeling drives the design of new combustors and engines for aerospace, transportation, and energy applications, but accurate prediction of fuel consumption and pollutant emissions requires detailed chemical reaction mechanisms. Detailed mechanisms for liquid hydrocarbons of interest contain large numbers of species and reactions; for example, the reaction mechanisms for n-heptane (C H ) and iso-octane (C H ), important molecules for 7 16 8 18 gasoline modeling, contain almost 1000 species and 8000 reactions [1]. Despite rapid advancements in computing power, it is generally formidable to integrate such detailed reaction mechanisms into large-scale computational simulations, in terms of CPU time and memory requirements. In addition, the wide range of time scales (from nanosecond to second) and the nonlinear coupling between species and reactions induces stiffness when governing equations are solved. Due to these computational demands, practical simulations using detailed chemistry are impossible with modern computational tools. Mechanism reduction schemes are used to allow quantitative modeling while keeping realistic chemistry effects. Non-adaptive reduction methods perform reduction based on a predicted range of conditions typically by removing unimportant species and reactions and identifying the species with fast time scales for further reduction, providing a single global mechanism. Most adaptive reduction methods, on the other hand, operate by storing chemical kinetics information and retrieving necessary data during the simulation to avoid direct integration of the differential equations; newer techniques use multiple mechanisms reduced prior to the simulation at various points in the flow. I propose the development of a novel adaptive and computationally friendly reduction method that will remove unimportant species and reactions and eliminate stiffness on the fly. I aim to explore and develop new algorithms while using existing reduction methods as a basis. Non-adaptive reduction methods attempt to provide a valid reduced mechanism by predicting the range of conditions (pressure, temperature, mixture composition) of interest in a simulation. However, the size of the reduced mechanism is limited by the locations in the computational domain that require more detailed chemistry due to high reaction activity. Many methods have been developed to reduce mechanisms in this manner, but the application of directed relation graph (DRG) theory [2] to describe reacting systems is particularly useful. In this method, nodes of the DRG represent species and directed edges represent dependences between species defined by normalized contributions to production rates. Important target species are defined (e.g. fuel, oxidizer, pollutants) and a graph-searching algorithm finds the dependent set of species needed to accurately predict the production rate of targets. Species with contributions below a certain error threshold are removed from the dependent set, and the final reduced mechanism contains the union of all dependent sets. The algorithm then eliminates reactions containing the unimportant species. For further elimination of stiffness in reaction 1 NSF GRFP 2008 – Proposed Plan of Research – Kyle Niemeyer systems, the quasi steady state (QSS) and partial equilibrium (PE) assumptions are applied [3]. QSS species and PE reactions have very short time scales, causing stiffness, and the approximations seek to replace differential equations with algebraic relations to solve for species’ concentrations. Computational singular perturbation (CSP) and intrinsic low dimensional manifold (ILDM) [3] are traditional methods for finding QSS species and PE reactions by separating fast and slow processes. Adaptive reduction methods rely on different approaches to increase computational efficiency during simulations. Approaches such as in situ adaptive tabulation (ISAT) and artificial neural networks (ANN) [4] perform storage and retrieval of chemical kinetics information to save processing time. Newer adaptive methods such as genetic algorithms (GA) [5] and optimization-based approaches [6] use various techniques to provide multiple reduced mechanisms for use during the simulation at different points in the flow. Highly detailed chemistry needs to be considered at locations where reactions are actively occurring, while regions with little reactive activity can use extremely reduced mechanisms. However, all of the methods currently rely on predictive reduction, which will not provide the highest level of accuracy or reduction. I propose the investigation of a new adaptive reduction methodology that will perform on the fly removal of species and reactions and elimination of stiffness. Identification and removal of unimportant species and reactions based on local conditions allows for the highest level of reduction and therefore the least computational demand, while keeping high accuracy. I will explore a novel algorithm for species and reaction removal using the DRG concept as a starting point; previous studies [2] based on DRG have shown it to be fast and reliable, suitable characteristics for on the fly application. Traditional methods for identifying QSS species and PE reactions such as CSP and ILDM are time-intensive [3] and therefore not well suited for on the fly stiffness removal; as such, I will also investigate efficient methods for identifying these fast processes. The adaptive reduction method I have proposed can be directly applied to the simulation of combustion processes for aeropropulsion, transportation, and energy applications. The incorporation of detailed chemistry while providing speedy simulation will allow accurate modeling of fuel consumption and emissions and help drive the design of next-generation engines and combustors. A method based on graph theory could be also be applied to the modeling of other complex systems; broader applications consist of food web/ecosystem modeling, disease spreading modeling, climate modeling, and biological systems modeling. Also, a new methodology developed to perform mechanism reduction could also be used to mine important information about complex systems. For example, a CSP-based method was used to gather information about explosive processes in a simulation of a hydrogen/air turbulent lifted jet flame [7]- the new method I propose could be used similarly for data mining. References [1] “Basic Research Needs for Clean and Efficient Combustion of 21st Century Fuels,” DOE/BES Workshop Report (2006). [2] T.F. Lu, C.K. Law, Combust. Flame 144 (2006) 24-36. [3] T.F. Lu, C.K. Law, J.H. Chen, AIAA 2008-1010 (2008). [4] J.Y. Chen, J.A. Blasco, N. Fueyo, C. Dopazo, Proc. Combust. Inst. 28 (2000) 115-21. [5] I. Banerjee, M.G. Ierapetritou, Combust. Flame 144 (2006) 619-33. [6] O.O. Oluwole, P.I. Barton, W.H. Green, Combust. Theory Model. 11 (2007) 127-46. [7] T.F. Lu, C.S. Yoo, J.H. Chen, C.K. Law, AIAA 2008-1013 (2008). 2	1
An Empirical Analysis of the Role of Ecological Filters in Grassland Restoration Keywords: restoration, ecological filters, community assembly, invasive plant management Introduction: (cid:39)(cid:72)(cid:74)(cid:85)(cid:68)(cid:71)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:75)(cid:68)(cid:86)(cid:3)(cid:85)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:71)(cid:3)(cid:72)(cid:70)(cid:82)(cid:86)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:86)(cid:72)(cid:85)(cid:89)(cid:76)(cid:70)(cid:72)(cid:86)(cid:3)(cid:82)(cid:89)(cid:72)(cid:85)(cid:3)(cid:81)(cid:72)(cid:68)(cid:85)(cid:79)(cid:92)(cid:3)(cid:75)(cid:68)(cid:79)(cid:73)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:40)(cid:68)(cid:85)(cid:87)(cid:75)(cid:182)(cid:86)(cid:3)(cid:79)(cid:68)(cid:81)(cid:71)(cid:3) area, making an understanding of ecological restoration a critical issue for ecology [1]. However, restoration has historically been practiced on an ad hoc basis, without adequate planning or proper application of the scientific method. As a result, most restoration projects fail to achieve lasting change and seldom provide insights that may be broadly applicable and advance restoration theory. Experts in the field of restoration ecology are now calling for studies that apply ecological principles to empirically test basic ecological theories that are pertinent to restoration, such as community assembly and succession and, specifically, the role of trophic interactions [2]. My research attempts to respond to this call by answering questions regarding the interactions of herbivory, seed-predation and biotic soil disturbance in the restoration of a grassland ecosystem in California. Through my efforts, I hope to advance the science and practice of large-scale grassland restoration. Background: The ultimate goal of restoration ecologists is to manipulate assembly and succession in ways that produce the most desirable stable state, either by speeding up natural processes or by overcoming thresholds that might be insurmountable without human intervention. Ecological filters are biotic or abiotic variables that favor the assembly of certain species over others. If well-understood, these filters can be used to favor desirable species while inhibiting the establishment of undesirable ones, thereby directing community assembly towards the most desired state. In grasslands that have been invaded by exotic annual grasses, managers could use ecological filters to promote the re-assembly of native bunchgrasses to improve habitat quality for native plants and animals and improve forage quality for livestock [3]. The role of plant-herbivore interactions such as herbivory, seed-predation, and biotic soil disturbance as potential ecological filters is poorly understood. Evidence suggests that physical soil disturbance caused by burrowing rodents, such as the endangered Giant Kangaroo Rat (Dipodomys ingens) in my study system, promotes the invasion of exotic annual grasses [4]. However, recent work has shown that kangaroo rats also preferentially eat the large seeds of exotics and thus the net effect of their presence on plant recruitment is currently unknown [5]. Another study found that the exotics responded to defoliation with more vigorous regrowth than natives did, and the authors therefore concluded that grazing by large herbivores promotes dominance by exotics [6]. However, the effects of grazing are not limited to defoliation; animals also exhibit preferential selection and alter soil characteristics through compaction and nutrient addition [7]. Thus, the net or synergistic effects of these interactions on native plant restoration remain unclear. Hypotheses: (1) Soil disturbance by kangaroo rats will favor the assembly of exotic grasses while compaction caused by cattle will favor native bunchgrasses. (2) Nutrient addition by both animal species will favor the assembly of exotics and have a greater impact on the re-assembly process than physical soil disturbance. (3) Cattle with help export excess nutrients by selectively grazing nutrient rich vegetation and kangaroo rats will control the abundance of exotics by preferentially consuming their seeds. (4) A combination of cattle and kangaroo rats will be most successful in directing re-assembly towards a state dominated by native bunchgrasses. Research Plan: To test the effects of herbivory, seed-predation, and soil disturbance on re- seeding efforts I will establish 1-m2 restoration plots within an existing framework of nested cattle and kangaroo rat exclosures(cid:178)allowing for the quantification of both the individual and Page 1 of 2 Proposed Plan of Research Christopher M. Gurney combined effects of cattle and rodents. Two plots will be established in each of the three test areas, one on rodent disturbed soil and one on undisturbed soil (n = 10 exclosures). Additionally, soil samples will be taken on and off disturbed soil in each test area and will be analyzed for bulk density and chemical composition. These data will allow for the artificial decoupling of physical soil disturbance from nutrient addition. Additional plots will be established in the kangaroo rat exclosures(cid:178)one to simulate physical soil disturbance, one to simulate nutrient addition, and one to simulate both types of disturbance (for comparison with genuinely disturbed plots) for both animal species. Differences in bulk density will be simulated using a soil corer (to reduce density) or a rammer (to increase density). To simulate nutrient addition, fertilizer will be added to plots in an amount necessary to achieve the observed soil chemical composition where kangaroo rats or cattle have been active. All plots will first be surveyed in the spring using a pinframe method, then sprayed with herbicide and sown with four rows of seeds in the following winter. Each row will be randomly assigned to one of four native species of varying forage quality(cid:178)two were preferred and two were avoided in kangaroo rat feeding trials [5]. Plots will be monitored weekly through the growing season. Soil disturbance, seed germination, and herbivory on seedlings will be recorded. Plant cover will be monitored annually each spring. Data will be analyzed using mixed-model ANOVAs. Logistics and Support: The nested cattle and kangaroo rat exclosures were constructed two years ago as part of a concurrent project. The effectiveness of this experimental framework has already been demonstrated [5], and the kangaroo rat exclosures are checked on a regular basis for evidence of rodent activity. Our partners at the Bureau of Land Management (BLM) are in full support of this project and have generously agreed to provide the required seed and equipment. As a graduate student at UC Berkeley, I will also have access to the resources provided by the Graduate Group in Range Management(cid:178)including the support of expert faculty who specialize in grassland ecology and restoration. Broader Impacts: The results of my research will advance ecological theory by helping to elucidate the roles of herbivory, seed predation, and biotic soil disturbance on plant community assembly. Since many of the issues addressed in my research are ubiquitous throughout grassland ecosystems, my findings could be broadly applied in restoration all over the world. These results will also be useful to land managers and ranchers who hope to reduce the damage caused by invasive plants(cid:178)currently estimated at $2 billion annually in US grasslands [8]. Besides preparing the results for peer-reviewed publication, I will also collaborate with various stakeholders to determine how my findings can best be applied to large-scale management and restoration. At a local level, the joint managing partners of Carrizo Plains National Monument (the BLM and the Nature Conservancy) have already demonstrated a keen interest in applying the results of my research in future restoration projects at the Carrizo Plains(cid:178)(cid:38)(cid:68)(cid:79)(cid:76)(cid:73)(cid:82)(cid:85)(cid:81)(cid:76)(cid:68)(cid:182)(cid:86)(cid:3)(cid:79)(cid:68)(cid:85)(cid:74)(cid:72)(cid:86)(cid:87)(cid:3) remnant grassland and home to 13 endangered species. References: [1] G.C. Daily, Science, 269, 350 (1995). [2] V.K. Temperton et al. Assembly Rules and Restoration Ecology (Island Press, 2004). [3] L.F. Salo, Journal of Arid Environments, 57, 291 (2004). [4] Schiffman, P.M. Biodiversity and Conservation 3, 524 (1994). [5] L.R. Prugh, Carrizo Exclosure Experiment Report (Prepared for The Nature Conservancy, 2008). [6] S. Kimball, and P.M. Schiffman. Conservation Biology 17, 1681 (2003). [7] M.R. Stromberg, J.D. (cid:38)(cid:82)(cid:85)(cid:69)(cid:76)(cid:81)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:38)(cid:17)(cid:48)(cid:17)(cid:3)(cid:39)(cid:182)(cid:36)(cid:81)(cid:87)(cid:82)(cid:81)(cid:76)(cid:82)(cid:17)(cid:3)(cid:3)California Grasslands Ecology and Management (University of California Press, 2007). [8] J.M. DiTomaso, Weed Science, 48, 255 (2000). Page 2 of 2	0
Applications to Airborne Wind Energy Systems Introduction: While wind energy capacity has tripled in the past decade[1], the installation of towered wind energy systems in remote and deep-water offshore locations, as well as the ability to harness wind resources above 100m, is severely limited by tower and foundation constraints.[2] Airborne wind energy (AWE) systems solve this problem by replacing the conventional tower with tethers and a lifting body (usually a kite or wing). Two approaches to AWE systems have been adopted: (i) ground-based generators, where the lifting body is cyclically spooled in and out and power is generated on the ground[3], and (ii) airborne generators, in which the generator is a turbine attached to the lifting body.[2][4] Both technologies possess the potential for vastly increased energy generation through the execution of cyclic crosswind flight, which results in apparent wind speeds that can far exceed the true wind speed on a high lift/drag lifting body.[2] However, the successful implementation of crosswind flight requires a robust control framework for optimizing the crosswind flight path in the presence of disturbances. AWE systems executing crosswind flight are one of many control systems that execute repetitive (cyclic) motion, under a varying environmental profile, in order to maximize an economic metric (average net power output for AWE systems). Iterative Learning Control (ILC) presents a foundation for addressing this problem, allowing the controller to draw from previous iterations to inform decisions at the present iteration. However, traditional ILC techniques focus on tracking a prescribed path/trajectory in the absence of an external disturbance that can vary from iteration to iteration, rather than optimizing the path itself to maximize an economic metric in the presence of an iteration-varying disturbance. The proposed research will, for the first time ever, fuse techniques from library-based flexible ILC[5] and optimal control to arrive at a disturbance and performance-weighted ILC (DPW-ILC) framework that:  Bases its learning on an economic performance index, rather than setpoint tracking, and  Biases its learning to emphasize previous iterations whose conditions match the present. Research Plan: The proposed research will focus on a DPW-ILC formulation (Fig. 1), applied to an AWE system executing crosswind flight. This control formulation involves two critical features that occur in the iteration domain, the designs of which will encompass key components of the proposed graduate research: 1. Maintaining and managing a library of previous iterations and results: DPW-ILC is predicated upon maintaining a library of relevant previous iterations’ selected path geometries, corresponding performance (iteration-averaged power output for the AWE application), and associated measured Figure 1. Block diagram of the proposed disturbance (wind speed in the AWE application). As DPW-ILC control scheme, as applied to an iterations progress, a critical task will involve AWE system. managing the library size, maintaining those library entries that maximize a statistical measure of information for future ILC updates. 2. Economic DPW-ILC update: The DPW-ILC framework must identify those previous iterations that are most relevant to the present iteration in terms of having similar disturbance values (wind speed in the AWE application) and favorable performance (iteration-averaged power output in the AWE application). Quantification of a relevance index, which will be informed by related work in higher-order tracking ILC[7][8], will be a significant element in the creation of the DPW-ILC update. Formal stability and convergence analysis will be performed on the resulting DPW-ILC approaches. In particular, the following general questions will be posed:  Defining regret as the difference between the realized and optimal performance, how does long-term expected regret depend on the statistical properties of the external disturbance (e.g., variance, temporal length scale)?  How do the aforementioned regret bounds depend on the library size? These convergence analysis questions also lead to metrics against which designs can be evaluated (for example, a low regret bound that is robust to the library size is highly desirable). The DPW-ILC approaches will also be experimentally validated on a unique lab- scale, water channel-based platform. In particular, the authors of [8] designed a water channel- based setup for closed-loop flight testing of tethered systems, and [2] verified dynamic scaling between lab scale and full scale conditions. This existing platform, which has to-date been used to characterize AWE designs under constant flow profiles, will be extended to characterize real-world wind profiles, which will be dynamically scaled to the water channel level. Specifically, wind data from a Cape Henlopen, DE wind profiler will be scaled down to the water channel level to create low-frequency, iteration-to-iteration variations for initial validation of DPW-ILC algorithms. After the successful performance of this first round of experiments, high frequency, intra-iteration variation will be applied using wake generation devices upstream of the AWE model. Intellectual Merit: The DPW-ILC framework pioneered in this research will be the first to fuse economic ILC with library-based higher-order ILC, creating an entirely new avenue of research within the ILC community. Furthermore, the research will result in the first dynamically scaled experimental validation of AWE flight control strategies on a lab-scale platform, under realistic (and also dynamically scaled) wind profiles. Broader Impacts: The creation of robust, optimized control systems for AWE systems will render wind energy a viable alternative to diesel fuels[4] in remote, off-grid locations and a long-term solution for deep-water offshore locations. Furthermore, the control methodologies created through this research will be applicable to other engineered systems that execute repetitive (cyclic) motion, under a varying environmental profile, to maximize an economic metric. Examples include active exoskeletons, pick-and-place robotic systems operating under variable plant conditions, and even traditional wind turbines. [1] AWEA. https://www.awea.org/wind-101/basics-of-wind-energy/wind-facts-at-a-glance. [2] Cobb, M. et. al., “Lab-Scale Experimental Characterization and Dynamic Scaling Assessment for Closed-Loop Crosswind Flight of Airborne Wind Energy Systems” ASME J. Dyn. Sys., ’17. [3] Fagiano, L. et. al.,“High-Altitude Wind Power Generation,” IEEE T. Egy. Convers., Vol. 25, No. 1, ’10. [4] Altaeros, Inc., http://www.altaeros.com/. [5] Hoelzle, D. et. al., “Flexible iterative learning control using a library based interpolation scheme,” 51st IEEE CDC. [6] DiMarco, C., et. al., “Disturbance & Performance-weighted Iterative Learning Control with Application to Modulated Tool Path-based Manufacturing,” ASME DSCC ’16. [7] Cobb, M., et al, “Iterative Learning-Based Waypoint Optimization for Repetitive Path Planning, with Application to Airborne Wind Energy Systems,” 56th IEEE CDC. [8] Deodhar, N. et. al., “Laboratory-Scale Flight Characterization of a Multitethered Aerstat for Wind Energy Generation,” AIAA Jnl., Vol. 55, No. 6, 2017.	0
Vehicle-Related Hyperthermia Introduction: In the United States alone, nearly 40 young children die every year due to pediatric vehicle-related hyperthermia1,2,3,4,5,8. Young children are more at risk for vehicle hyperthermia because they cannot regulate their core temperature and sweat less than the average adult1. This health issue was relatively prevalent in the news media during the summer of 2014, with many tragic cases occurring across the United States and abroad. Several researchers have examined pediatric vehicle-related hyperthermia, but a gap in the literature exists with respect to communicating this risk to parents2,3,4. Guard & Gallagher (2005) note that addressing vehicle- related hyperthermia is a multifaceted problem that revolves around education and legislation. My primary focus is education and the need for campaigns that deliver more effective messages to the targeted populations, for example, caregivers and parents2,4. While many public service announcements (PSAs) and state campaigns have been undertaken in recent years, rigorous studies have yet to be completed evaluating these campaigns on their ability to both raise awareness and increase risk perception. To effectively target messages to the most relevant parent audience, a focus needs to be placed on parents of children five years of age and younger. A variety of health communication theoretical frameworks can aid in understanding and modifying parents’ behavior when it comes to perceiving vehicle hyperthermia as a potential risk5. A common heath communication framework to predict health-risk behaviors, is the Health Belief Model. The Health Belief Model is known for its use in the adoption and maintenance of behaviors involving both health prevention and promotion6. In order to encourage pediatric injury prevention among parents with young children, it is essential that education and communication increase parents’ awareness of the risks, dangers, and change their perceptions regarding the likelihood and severity of vehicle-related hyperthermia. Research Questions: 1. What current knowledge do parents, who have children 5 years and younger, possess in regards to the danger vehicle hyperthermia poses to children and safety measures for injury prevention? 2. What is the most effective messaging strategy (e.g. scientific vs. emotional) to communicate the risk, alter the risk perception, and change the behaviors of parents (e.g. following recommendations to prevent pediatric vehicle-related hyperthermia)? Methods: To identify potentially effective messages and communication strategies, when it comes to parents of children five years old and younger, a mental model approach to risk communication will be implemented. The mental model approach consists of interviews with experts and parents, a baseline questionnaire, message construction, and a final message experiment7. (1) Initially, I will interview parents from the Athens-Clarke county community to determine their current knowledge, beliefs, and deficiencies in their understanding of vehicle hyperthermia. (2) After conducting several interviews with parents and experts, I will create a questionnaire using the information received from those interviews to sample a wider audience of parents across the southern United States. The questionnaire will be used as the control group during the message experiment and will generally consist of: parents’ risk perceptions, measures of prevention, motivation and intention of adopting specific preventative measures, and demographics of the sample (Question 1). (3) Two experimental groups will be created in order to test the effects and effectiveness of two different approaches to prevention messages on the knowledge, beliefs, and intentions of parents in relation to vehicle-related hyperthermia. Based on preliminary parent and expert interviews, the nature of the messaging could include a scientific approach or an emotional approach, each of which will demonstrate a separate health communication theoretical framework. Prior to the message experiment, a base-knowledge questionnaire will be given to the participants in the experimental groups in order to understand their initial thoughts on the subject matter. Other questions regarding child vaccination will be provided to mask the intent of the study. (4) During the message experiment, individuals will be assigned to one of the two experimental groups, in which the designated message will appear in brochure format and be mixed in with other messages on child vaccination. (5) After reading the materials, the experimental groups will be given a similar questionnaire in order to assess their understanding of the risks and preventive measures of pediatric vehicle-related hyperthermia. The questionnaire results between the experimental and control groups can then be compared in order to determine which message was the most successful in altering the perception of risk and intention of adopting preventive steps on the subject of vehicle-related hyperthermia (Question 2). Some limitations and weaknesses exist throughout this experimentation process. Although I plan to conduct a survey with a random sample of parents of children five years old and younger in selected southern states, a completely generalizable sample will not be met. A sample of the southern states was chosen due to the high volume of pediatric vehicle-related deaths that have occurred in this region8. A better understanding of the current knowledge involving pediatric hyperthermia would be better achieved using a nationwide sample of parents. Broader Impacts/Intellectual Merit: With my interdisciplinary background, involving the fields of risk communication, psychology, and climatology, I can successfully implement this project and impact society by potentially increasing our understanding of how parents’ perceive vehicle hyperthermia. Using the expertise of my well-rounded committee members at the University of Georgia, we developed a multi-method approach that has the potential to create a new subfield examining risk communication efforts in the atmospheric sciences. Developing a sound communication strategy for vehicle hyperthermia is a positive step toward preventing future pediatric injury2,4. It is my hope that this project can develop and implement an evidence-based method for communicating the risk of vehicle hyperthermia to parents. Using the results of the study, I aspire to use this empirically-based method of communication to further develop messaging strategies for other heat-related and natural hazards. 1 Duzinski, S. V., Barczyk, A. N., Wheeler, T. C., Iyer, S. S., & Lawson, K. A. (2013). Threat of paediatric hyperthermia in an enclosed vehicle: a year-round study. Injury prevention, injuryprev-2013 2 Grundstein, A., Null, J., & Meentemeyer, V. (2011). Weather, geography, and vehicle-related hyperthermia in children. Geographical review, 101(3), 353-370. 3 Grundstein, A., Dowd, J., & Meentemeyer, V. (2010). Quantifying the heat-related hazard for children in motor vehicles. Bulletin of the American Meteorological Society, 91(9), 1183-1191. 4 Guard, A., & Gallagher, S. S. (2005). Heat related deaths to young children in parked cars: an analysis of 171 fatalities in the United States, 1995–2002. Injury Prevention, 11(1), 33-37. 5 Toolo, G., FitzGerald, G., Aitken, P., Verrall, K., & Tong, S. (2013). Are heat warning systems effective?. Environmental Health, 12(27) 6 Richard, L., Kosatsky, T., & Renouf, A. (2011). Correlates of hot day air-conditioning use among middle-aged and older adults with chronic heart and lung diseases: the role of health beliefs and cues to action. Health education research, 26(1). 7 Morgan, M. G. (Ed.). (2002). Risk communication: A mental models approach. Cambridge University Press. 8 Null, J. (Updated: 2014, October 16). Heatstroke Deaths of Children in Vehicles. Retrieved September 8, 2014, from http://www.ggweather.com/heat/	0
Center reported a total of 19,105 new cases of spinal cord injuries with some amount of paralysis in 2019. Brain machine interfaces (BMIs) are a burgeoning technological solution to restore quality of life to these people through connecting brain signals to prosthetics. Current brain machine technology collects and transmits neural signals from implantable electrode arrays to be decoded using algorithms which are implemented on cumbersome and power inefficient computers. These systems also require daily calibrations by scientists and clinicians to maintain Figure 1: Proposed Brain Machine Interface their usability. Herein lies an approach to address these Architecture. problems through implementing specially designed algorithms that are memory and computationally efficient onto a low power application specific integrated circuit (ASIC) to decode patient intent using a fully implantable device. The goal is to engineer hardware to implement intelligent decoding algorithms that will increase the reliability of neural decoding systems and decrease the physical size and power requirements by orders of magnitude through removing the need for transmission of unprocessed neural data. I am working with professor Azita Emami at the Caltech Mixed-mode Integrated Circuits and Systems (MICS) laboratory to begin my work as an ASIC and algorithmic designer implementing these systems. Previous Work: The promise of this project to produce reliable power efficient BMIs relies on the development of power efficient algorithms. The MICS lab has developed efficient algorithms that utilize deep multistate dynamic recurrent neural networks [1], as well as done an assay on decoding algorithms [2]. Furthermore, if energy efficient algorithms do not provide the power performance required, in memory processing architectures could be utilized to reduce power lost from transporting weights between memory banks and processing units [3]. Intellectual Merit: Current BMI technology largely utilizes hardware implementations which transmit digitized neural signals back to a compute station. There are few BMI ASICs that decode patient intent directly without off-the-shelf hardware implementations. Custom machine learning ASICs provide an opportunity to optimize for power and area efficient systems. BMI systems have unique signal processing constraints due to relevant information being mixed across time, frequency, and space in highly dimensional, redundant datasets. These constraints often require complex computational algorithms with high internal state complexity, that generally decreases power efficiency [2]. This poses a particularly difficult engineering dilemma which requires a uniquely multidisciplinary approach to leverage knowledge of neuroscience with engineering expertise in ASIC design. This dilemma is to fit the required computationally complex task of decoding patient intent from neural signals into a power and area efficient package. Hypothesis: BMI ASICs implementing computationally, and memory efficient algorithms will be able to efficaciously ascertain patient intent while maintaining robust performance whilst still meeting power and area constraints requisite of fully implantable systems. Research Plan: Several crucial prerequisite steps for this proposal are already underway in Azita Emami’s lab including the evaluation of neural features measured from patient data for stability over time. Aforementioned specialized algorithms have been developed and evaluated for performance. While these prerequisite steps are crucial to the outcome of this project, the scope of this proposal is constrained to the implementation of these algorithms in hardware, optimizing for low power. Therefore, this proposal will only discuss the development, implementation, and testing of the algorithms into CMOS fabric. Stage I- Algorithm CAD Design and Testing: Digital and analog hardware will be described and laid out into Virtuoso Computer Assisted Design (CAD) program. The circuits will be fabricated with general power optimization techniques in mind such as clock and power gating portions of the circuit when they are not in use. Other techniques include using intentional specificity of transistor thresholds throughout the design so that leakage current is minimized, as well as a minimization of supply voltage levels. Furthermore, specific tradeoffs will be made between the bit precision of the algorithms and the resources required to run at those precisions. Significant energy is also lost by moving the algorithm’s weights from memory to the processing hardware, so a layout will be designed such that the algorithm weights are physically closer to where computations are done. While designing the circuits, each component will be characterized at a unit level so that the performance of the entire circuit can be ensured. Stage II-CAD Simulation and FPGA Testing: After designing the circuit in Virtuoso, the entirety of the circuit will be tested with patient neural data collected over several weeks. The circuit simulation will give significant insight into the performance of the decoder system once fabricated. The simulation will also allow for design bugs to be caught and corrected before resources are spent fabrication of the design into silicon. The digital components of the circuit will also be implemented on a commercially available Field Programable Gate Array (FPGA) which will not only give physical proof that the logic and algorithms designed will work, but will give an upper bound on the power and area usage for the ASIC implementation. Stage III-Hardware Fabrication: Several test chips will be fabricated to investigate the performance of the decoding algorithms. This allows for verification and debugging of the major components of the algorithm, with the final system chip produced after all components are aptly constructed and verified. The circuits will be implemented into standard cell libraries for 65 nm CMOS technology using Design Vision. Stage IIII-Hardware Testing: The fabricated chip will be designed for testability using techniques such as built in scan chains to give access to the internal states of the circuit. Custom test systems will be built to feed neural signals to the device under test and validate the performance of the hardware. Stage IV-Going Forward: Once designed, fabricated, and verified, the integrated circuits could be tested in vivo using the same experimental therapy program from which the neural data was harvested. Pitfalls and Alternatives: Due to variation in fabrication processes, the fabricated chips may exhibit characteristics and behavior that were not accounted for during simulation. If experienced, the test hardware designed into the chip will be utilized to identify and debug the flaw. Timeline and Collaboration: The duration of the proposed project is three years and will be conducted under the supervision of Azita Emami. Azita Emami’s MICS lab works in direct collaboration with Richard Anderson’s neuroscience lab. This is a collaboration between two leading scientists in their respective fields. The MICS and Anderson lab collaboration has significant skill and expertise to confidently produce the anticipated results of this proposal. Emami’s lab has adequate facilities and resources to fund the significant capital required to design and develop custom ASIC hardware. This project also has direct access to neural recordings of patients with implanted UTAH neural arrays which provides essential data for training the decoder algorithms. Anticipated Results: Using specially designed decoding algorithms, significant reductions in power and area will be observed in the resultant neuro decoding system. It is important to note this project aims to maintain decoding accuracy and stability, despite using orders of magnitude less power and area. Broader Impacts: Fully implantable neural intent decoders will not only greatly improve the quality of life of patients with paralysis, but also provide the basis for fully implantable ASIC chips designed for direct study of neural activity without the need to be linked to heavy, memory and power inefficient recording systems. The miniaturization of hardware and computational effort can further be generalized to many IOT or wearable systems which requires robust signal processing algorithms with limited power and area requirements. This will enable a variety of wearable devices to decode bio signals without the need to upload the signal data to an off-chip server, greatly improving security, power, and speed performance. References: (1) B. Haghi, S. Kellis, M. Ashok, S. Shah, L. Bashford, D. Kramer, B. Lee, C. Liu, R. Andersen, A. Emami, “ Deep multi-state dynamic recurrent neural networks for robust brain- machine interfaces”, Program No. 406.04. 2019 Neuroscience Meeting Planner. Chicago, IL: Society for Neuroscience, 2019. Online. (2) Mahsa Shoaran, Benyamin A. Haghi, Milad Taghavi, Masoud Farivar, Azita Emami, “Energy-Efficient Classification for Resource-Constrained Biomedical Applications” IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS), 2018. (3) T.-J. Yang, V. Sze, “Design Considerations for Efficient Deep Neural Networks on Processing-in- Memory Accelerators,” IEEE International Electron Devices Meeting (IEDM), Invited Paper, December 2019.	0
The First LAI-Linked Predictive Model for Below- and Above-Ground Carbon Sequestration in Quaking Aspen Keywords: vegetative regeneration, carbon modeling, climate change, aspen, conifers Background: Forest ecosystem carbon is a key component to global climate initiatives and forest ecosystems are the primary ecotype used for carbon registration protocols. Although methodologies for estimating above-ground carbon sequestered are well established, using leaf area index (LAI) linked to basal area, below-ground carbon estimates are hampered by the paucity and inconsistency of data for both coarse- and fine-root biomass (1). As soil root carbon is a more stable carbon sink than above-ground carbon, accurate below-ground carbon estimates are indispensable for modeling efforts (2). Objectives: I will measure the total biomass of quaking aspen in representative North American stands. LAI has strong positive correlations to coarse- and fine-root aspen biomass (3),(4). Above- and below-ground biomass and LAI will be determined in order to develop the first predictive model that relates LAI to above- and below-ground biomass for aspen. Quaking aspen (Populus tremuloides) is an excellent candidate for necessary root biomass sampling and carbon modeling. Populus is one of the most widely cultivated northern temperate tree genera in the world, and has both ecological and economic significance; in many ways, it is becoming a ‘model tree’ for biomass and carbon sequestration (5). The most widely distributed species in North America, aspen is extremely important ecologically for water quality and habitat. However, it is in decline in the western United States for reasons not yet fully understood. Aspen root systems are unique; this species regenerates vegetatively from shallow lateral roots, forming large clonal stands which can persist indefinitely, given the correct disturbance regime. These coarse roots, then, persist after removal or decomposition of above- ground biomass, and possibly for several such stand-replacing events. This is very different from roots of seed-regenerated trees, in which the root system dies with stem death and is subject to soil microorganism heterotrophy. Expected Results: This study will generate a predictive model that relates quaking aspen LAI to above- and below-ground carbon allocation. It will also advance understanding of the ecology of a vegetatively reproducing forest species, an important but often overlooked niche (6). This research methodology may be applied to other species and used to explore below- ground relationships of other vegetatively regenerated forest ecosystems. Moreover, an accurate model that related above-ground aspen LAI to below-ground biomass and persistence will be useful economically for forest managers and carbon accreditation. With the basal area: LAI relationship we will develop, this model will be economical and practical across scales and for many interested parties, from small landholders to climate modelers. For small landholders with small forests, income from carbon accreditation can be important in deciding whether to invest in afforestation. A persistent soil carbon stock in aspen stands, if present, would create significant financial incentive for afforestation with and preservation of aspen. Benefits would also accrue for global climate and for areas historically forested with aspen. Broader Impacts: Through my ongoing volunteer work at high schools in Oakland Unified School District with minorities and disadvantaged students, I will integrate the project and its activities with educational activities for a variety of students. Interested students will be given Proposed Plan of Research Benjamin Caldwell the opportunity to involve themselves in the research, with special effort made to employ underrepresented groups as field assistants and involve them in the laboratory. Once developed, our LAI-carbon model will be made available for download free of charge. In addition to the peer-reviewed literature, we will make presentations at conferences for forest managers and environmental organizations. Methodology: I will sample aspen in representative stands in western North America. Stands will be selected on the basis of prior disturbance regime, stand health, and ecotype. I will use several complementary methods to obtain a complete picture of leaf area, course root and above-ground biomass, and fine-root flux. Leaf area: We will determine leaf area relationships by destructive sampling of approximately 30 trees. A random subset of leaves from each tree will be collected and weighed in the field, and subsequently scanned in the laboratory to determine leaf surface area. This will allow the prediction of leaf area at the stand level from basal area. Above-ground biomass: We will use fixed-area inventories from plots we installed to estimate above-ground biomass. Canopy leaf weights and wood density will be used to predict carbon content with allometric equations. Course root biomass: Since aspen roots are typically superficial, they are relatively easy to map. We will map root systems using ground-penetrating radar. This methodology is much less invasive and labor-intensive than excavation, and can be used to map changes in coarse-root biomass over time. Soil cores will be taken to standardize data sets and extrapolate root maps to biomass and carbon (7). Fine root biomass: Fine-root biomass annual flux is best estimated over the course of one year using minirhizotrons (8). We will install minirhizotrons, capped plastic pipes measuring approximately 180 by 5 cm, at a 45 degree angle from the soil surface. We will then take photographs of roots which infiltrate the pipe throughout the course of the year, and determine root length and biomass by analysis with dedicated software. Root biomass, determined from soil cores at the beginning of the sampling period, will be extrapolated to the stand level. Using the initial root biomass and the initial minirhizotron values, we will find fine-root flux for the stand. Analysis: I will use regression analysis to develop leaf area prediction models from tree basal area and height of the crown base in aspen. Tree growth will be predicted from tree leaf area. Using data on root biomass and stand LAI, we will develop predictive equations that relate LAI to above- and below-ground tree carbon. Results will be integrated with MASAM, an existing forest stand health model, to provide tree restoration guidelines, LAI, and carbon estimates (9). Cited Literature 1. Brown, S. Environmental Pollution. 116, 363-72 (2002) 2. Rasse, D. P., Rumpel, C., Dignac, M. Plant and Soil. 269, 341-356 (2005) 3. DesRochers, A., Lieffers, V. Canadian Journal of Forest Research. 31, 1012–1018 (2001) 4. Davis, J., Haines, B., Coleman, Hendrick, R. Forest Ecology and Management. 187, 19-33 5. Taylor, G. Annals of Botany. 90, 681-689 (2002) 6. Bond, W., Midgley, J. Trends in Ecology & Evolution. 16, 45-51 (2001) 7. Butnor, J. et al. Soil Science Society of America Journal. 67, 1607 (2003) 8. Hendricks, J. J. et al. Journal of Ecology. 94, 40-57 (2006) 9. O'Hara, K. Forest Ecology and Management. 118, 57-71 (1999)	0
Introduction: Grain growth is a critical process to both metals and ceramics processing, as grain size plays a major role in bulk material properties, such as fracture toughness. Abnormal grain growth (AGG) is a process by which the growth of a small fraction of grains is incentivized and they grow faster than their neighbors, resulting in a bimodal grain size distribution and heterogeneous bulk properties. Though work has been conducted in this field for decades1, the cause and mechanism behind AGG are still poorly understood. The process is particularly import to ceramic materials, as the superior thermal resistance of ceramics lends itself to extreme environment applications. At these elevated temperatures kinetics are accelerated, expediting grain growth and AGG – this is a particular challenge in alumina, which is very susceptible to AGG2. Processing of these materials also raises concern, as high sintering temperatures can have the same deleterious effect. As such, it is vital to understand how to control grain growth in ceramics processing and applications. Textured microstructures with enhanced mechanical properties in materials can be designed by controlling their crystallographic orientation during processing. One technique that has been explored is the application of a magnetic field during processing3,4. This project will investigate how texturing by applied magnetic field in alumina ceramics impacts grain growth through the use of electron and synchrotron X-ray based techniques, which allow us to track individual grains and grain boundaries. I hypothesize that a strong applied magnetic field during processing will reduce grain growth and mitigate AGG due the formation of low-angle and, thus, low-energy grain boundaries that have a low driving force to move. This will be validated by 3D microstructural characterization, allowing the measurement of grain boundary orientation and character for textured samples. These results will be relevant to industrial applications of alumina – including the manufacture of automotive parts and ballistic armor – as microstructure engineering can Figure 1. Preferential crystallographic orientation (ratio of 006 signal intensity to total signal intensity) improve the mechanical properties of ceramic materials as a function of applied magnetic field strength in and improve stability and lifetime. alumina with different slip solid loading4. Objective 1: Preparation of textured Al O samples by thermomagnetic slip-casting process: 2 3 Alumina samples for this experiment will be prepared via slip casting with high-purity α-Al O 2 3 powder. Alumina is chosen as a test material due to its impressive mechanical properties and applicability as a structural ceramic, as well as its susceptibility to texturing by magnetic field. A dispersant will be added to the slip to prevent the agglomeration of particles. The samples will be subjected to an applied magnetic field during casting – this texturing technique has been shown to induce the growth of preferentially oriented grains during annealing3,4. Figure 1 illustrates this effect in alumina. Samples will be cast under applied magnetic fields of 0-8 T with 0 T being a control sample. Once cast, alumina green bodies will be sintered to near theoretical density and annealed at temperatures above 1400 ºC for various times. Objective 2: Electron microscopy characterization of grain growth: From the bulk samples, centimeter-sized sections will be cut and polished for observation under a scanning electron microscope (SEM) to determine the initial average grain size by image analysis software employing the linear intercept method5. The samples will then be further annealed under identical time and temperature conditions, and the same method will be repeated to determine the grain growth in each sample. These results will provide a quantitative measure of grain growth as a function of applied magnetic field during processing and annealing time. Objective 3: 3D characterization of crystallinity and grain size distribution: The novelty of this work lies in the use of high-energy X-ray diffraction microscopy (HEDM) to characterize the samples. In this technique, a sample is placed in the path of an incident X-ray beam and rotated while diffraction patterns are collected. In post-processing, these can be indexed to generate a crystallographic map of the measured volume. From the bulk, millimeter-sized samples will be prepared. From 3D crystallographic maps measured by HEDM, true grain sizes (at a resolution of 1 µm) can be determined and a grain size distribution created. The non-destructive nature of this technique is extremely advantageous, as it will allow tracking of individual grains and boundaries across heat treatments. Thus, the slower movement of individual textured low-angle grain boundaries can be observed and quantified. Via 3D characterization of individual grains and boundaries, these results will verify a) the character and Figure 2. Set-up for HEDM synchrotron motion of the boundaries as a function of applied magnetic measurements, beamline 1-ID at Advanced Photon Source, Argonne field, and b) whether observed low-angle grain boundaries National Lab. Incident X-ray beam induced by texturing reduce AGG. travels along positive z-direction. Research Plan: The timeline for this project is one year. Slip casting will be done at Oak Ridge National Lab, which houses a commercially available thermomagnetic system offering up to 8 T magnetic field. The timeline for this step is one week, accounting for travel time, as slip casting is a well-known process that can be modified as needed. SEM will be conducted at the University of Florida, whose Research Service Centers house a TESCAN SEM. The polishing, sample preparation, and data analysis will take 9-10 months. Lastly, HEDM experiments will be run at the NSF-sponsored CHESS synchrotron’s Structural Materials beamline, at which HEDM will be available beginning in December 2019. The experimental design considers that each HEDM measurement takes hours, and beamtime slots are limited. As such, high-priority samples exhibiting strongly textured microstructure will be selected for synchrotron measurement. Broader Impacts: Beyond structural materials, microstructure engineering is essential to functional materials like oxide fuel cells and laser materials. The results of this project will offer quantitative insight into the use of an applied magnetic field to texture alumina ceramics – this fundamental study will serve as a bridge for future industry-specific studies. As such, I would enjoy sharing these results at conferences and with industrial collaborators directly. 1. Journal of the American Ceramic Society, 80(5), 1149–1156. 2. Scientific Reports, 6(37946), 2–11. 3. Scripta Materialia, 54(6), 977–981. 4. Science and Technology of Advanced Materials, 7(4), 356–364. 5. Journal of the American Ceramic Society, 91(7), 2304–2313.	0
A understand how neural circuits, assembled through genetic programs, can give rise to complex behavior. Through evolution, species display a wide range of behaviors, some of which have been mapped to specific genetic variations1. Genes that mediate complex behavior must act via neural circuits, yet little is known about these intermediate changes. In this proposal, I will bridge this knowledge-gap by investigating neural circuit differences that determine B vocal communication behaviors in two closely-related rodent species. Background and Rationale: Using sounds to communicate is widespread in nature — from croaking frogs, duetting birds, to us, humans, engaged in conversation. Our lab has recently discovered a rodent species (Alston’s singing mice) FIG. 1: (A) Phenotypic variation in that engages in similar fast vocal interactions. Singing mice vocalizations of the lab mouse and Alston’s breed in captivity, can be maintained in a colony, and show singing mouse. (Spectrograms from the stereotyped vocal behaviors even in laboratory settings. Phelps lab, U.T. Austin) (B) Divergence and Additionally, we have already established the use of viral tools duplication model as observed in cerebellar for mapping, manipulating, and measuring neural circuits2. nuclei8. Singing mice (Scotinomys teguina) and lab mice (Mus musculus) are separated by a few million years of evolution (Steve Phelps lab, unpublished), are roughly the same size, and brain slices are largely indistinguishable between the species. Yet, there are key differences in their vocal repertoires; Lab mice produce only short, variable ultrasonic vocalizations (USVs), while S. teguina produce both USVs and human-audible ‘songs’ (Figure 1A). Crucially, unlike singing mice, lab mice do not participate in vocal turn-taking. Thus, we ask: What are the neural circuit differences underlying this behavioral distinction? Though traditionally thought to be unique to the primate lineage, our lab recently demonstrated robust motor cortical control of vocal behavior in the singing mice. Using four complementary lines of evidence (intracortical micro-stimulation, stimulation induced vocal arrest, focal cooling and pharmacological silencing), we defined a region of orofacial motor cortex (OMC) that mediates flexible vocal behaviors in the singing mice2. In contrast, lab mice born without the entire cortex (including OMC) can still produce USVs3. Therefore, we predict that differences in the motor cortical circuitry between the lab mice and singing mice underlie differences in their vocal behaviors. We hypothesize that motor cortical control over vocalization in the singing mice evolved from the ancestral orofacial control neural circuits via a duplication of OMC followed by cell-type divergence (Figure 1B). This duplication- divergence model predicts the existence of a dedicated group of song-specific neurons in the singing mouse OMC with specific projection patterns to downstream vocal pattern generators in the midbrain and the brainstem. Using novel spatial transcriptomics and barcoded projection mapping methods developed in Tony Zador’s (my co-advisor) lab, I will determine the diversity of cell-types in the motor cortex and their downstream projection patterns in both the singing mice and the lab mice. Aim 1: Do motor cortical cell types differ between lab mice and singing mice? The duplication and divergence model suggests that neural cell types in the OMC of singing mice evolved in a spatially segregated manner. First, to determine differences in cell types, I will perform single cell RNA sequencing (scRNAseq) in the OMC of lab and singing mice. Analysis of scRNAseq data requires aligning sequenced reads to a genome, publicly available for the lab mouse and recently generated by our collaborators for the singing mouse (unpublished, Steve Phelps). Cell types will be identified using known marker genes found in the literature. We will identify potentially novel cell types as those which have no assigned identities based on canonical marker genes. While scRNAseq will allow us to quantify differences in neural cell types through in-depth transcriptomics, we lose spatial information. To determine spatial location of neuronal cell types, we will use a spatial transcriptomic method, BARseq, developed in the Zador lab4. This technique uses hybridized probes and in situ sequencing to determine spatially resolved expression data for hundreds of genes in parallel4. I am confident that I can perform this experiment as the Zador lab has a dedicated pipeline to complete this experiment and regularly performs spatial transcriptomic experiments. Aim 2: Do projection patterns of motor cortical neurons differ between lab and singing mice? To determine OMC projection patterns, I will first perform viral tracing experiments. I will inject AAV vector that expresses GFP into the OMC of both species and image the brains using confocal FIG. 2: MAPseq protocol involves injecting barcodes into target microscopy. While viral tracing can detect area and sequencing barcodes expressed in neural projections in bulk anatomical differences, this method downstream areas5. lacks accurate quantification of projections and cannot distinguish changes that occur on the single cell level. To address these inadequacies, we will also be performing MAPseq, a method for single cell tracing developed by the Zador lab. MAPseq is a method that uses virus to infect neurons with a DNA barcode that is expressed in the cell body and axon of the neuron5,6. Through dissection and sequencing, we can recover the projection patterns of thousands of individual cells (Figure 2). I am confident that I can perform these experiments as I have already generated preliminary results for the lab mouse OMC. Furthermore, we can combine MAPseq with our spatial transcriptomic method to correlate projection patterns and cell types4,7. Anticipated results: If the duplication-divergence model of the singing mouse OMC holds true, I would expect to observe the following results: (1) novel cell types in the OMC of singing mouse (2) the spatial location of these novel cell types to be located in a spatially distinct area, and (3) novel projection patterns, perhaps to brainstem pattern generators, correlated with these novel cell types. In summary, the duplication-divergence model predicts correlated changes in cell transcriptomes and their projection patterns. Of course, another possibility is that cell type and projection pattern differences occur independently. Even so, I will be able to distinguish independent changes due to the resolution of the outlined experimental design. Thus, we have designed experiments that will produce results whether or not our expected model (duplication and divergence) is true. Intellectual Merit: I anticipate three major contributions to neurobiological methods, as well as our understanding of the evolution of neural circuits. First, this study can identify distinct neural populations based on projection patterns and/or genetic markers. Identifying neural populations in this manner allows scientists to target these neural population for further functional validation and experimentation. Second, our results could identify genes underlying neural circuits in vocal communication, findings which could contribute to the development of better molecular tools for manipulating vocal circuits. Lastly, this study would provide insight into the evolutionary underpinnings and biological basis of vocal communication. Broader Impact: I plan to make code and data available on open-source websites including GitHub. During my time at NIH, I created a RNAseq tutorial and shared resources on my GitHub page in addition to uploading code I wrote for analyzing RNAseq data9. I plan to maintain my GitHub page and upload code developed for analyzing data collected through this project for other scientists to consult and use. In addition, I plan to create an online tutorial geared toward high school and/or college students that have little coding experience or exposure to bioinformatics. I also plan to publish our results in open access journals including uploading early drafts of the manuscript to bioRxiv to facilitate timely advancement of scientific knowledge. References: [1] Metz et al. 2017. Current Biology. [2] Okobi, Banerjee et al. 2019. Science. [3] Hammerschmidt et al., 2015. Scientific Reports. [4] Chen et al., 2019. Cell. [5] Kebschull et al. 2016. Neuron. [6] Han, Kebschull, Campbell et al., 2018. Nature. [7] Sun, Chen et al., 2021. Nature Neuroscience. [8] Kebschull et al. 2020. Science. [9] https://github.com/eisko/RNAseq/	0
studies of the RAG complex led to my interest in the evolutionary origins of adaptive immunity. V(D)J recombination is the process responsible for generating the massive diversity of antigen receptors that characterizes the vertebrate immune system. RAG1 and RAG2, the protein prod- ucts of recombination activating genes 1 and 2, cooperate to initiate V(D)J recombination in lymphoid cells by making double-stranded breaks at recombination signal sequences (RSSs)(1). The recombinational DNA rearrangements catalyzed by RAG have long been biochemically lik- ened to the cleavage reactions effected by transposases (TPs)(1). In 1998, the demonstration that RAG displays transposition activity in vitro strongly suggested that this likeness can be ex- plained by homology and that RAG is a descendant of an ancient transposable element(1). Due to extensive sequence divergence, a close homolog of RAG within the modern diversity of TPs evaded detection until targeted PSI-BLAST searches linked the RAG1 core to the Transib family of TPs(2). Subsequent biochemical analysis of Hztransib, a Transib transposon active in the ge- nome of the corn earworm, revealed that, like RAG, Hztransib TP cleaves DNA through nicking and hairpinning steps that produce blunt transposon ends and hairpinned flanking ends(3) (Fig. 1). Additionally, insertion events create CG-rich 5-bp target-site duplications, as is typical for RAG(3). These results conform to expectations of a RAG-like TP, but it is reasonable to suppose that some of RAG’s properties are specific to V(D)J recombination and do not describe an ances- tral TP. I propose to conduct an exhaustive biochemical analysis of the Transib transposon in the Yale Department of Molecular Biophysics & Biochemistry, in the laboratory of David Schatz, who discovered and biochemically characterized RAG1 and RAG2. Biochemical similarities be- tween RAG and Transib can lend further support to their homology. Biochemical differences can suggest which functional aspects of RAG are evolutionarily recent recombinase-specific innova- tions, perhaps due to association of RAG1 with other factors (e.g. RAG2) or perhaps due to structural changes within the endonuclease itself. Aim 1: Determine the substrate requirements for Hztransib TP activity. Each RSS comprises a conserved heptamer and nonamer separated by a nonconserved spacer of either 12 or 23 bp(1). RAG’s activity is governed by the 12/23 rule: cleavage can only occur if both a 12- and a 23-RSS are present(1). Hztransib TP has already been demonstrated to cleave at paired 12/23 RSSs (unpublished data in the Schatz lab), but other RSS combinations have not been tested. To evaluate Hztransib TP’s adherence to the 12/23 rule, I will incubate purified Hztransib TP pro- tein with DNA substrates containing various combinations of 12- and 23-RSSs, and I will deter- mine the efficiency of cleavage by visualizing and characterizing radiolabeled DNA products on a denaturing polyacrylamide gel. In this and all other described experiments, a negative control reaction will contain no endonuclease, and a positive control reaction will use RAG as the endo- nuclease. RAG activity is highly dependent on conservation of the first 3 bp of the heptamer (CAC), while flanking sequences have little effect on cleavage efficiency(1). I will investigate the precise sequence requirements of Hztransib TP by quantifying cleavage of DNA substrates with various point mutations in the RSSs and their flanking DNA. The sequences with greatest cleav- age efficiency will likely approximate Hztransib’s own terminal inverted repeats (TIRs), which resemble RSSs and begin with the same CAC sequence. Accordingly, for all described experiments, I will compare reactions that use RSS-containing substrates to reactions using TIR- containing substrates to determine the sequence dependence of any effects I observe. Aim 2: Determine structural characteristics of Hztransib TP’s catalytic state. RAG can nick individual RSSs, but completion of cleavage via hairpin formation can only occur in a synaptic complex containing a 12- and a 23-RSS(1). To assay Hztransib TP for nicking and hairpinning activity in the absence of synapsis, I will immobilize low concentrations of biotinylated DNA substrates containing a single 12- or 23-RSS on streptavidin agarose beads, and I will character- ize products after addition of TP. I will then add free DNA substrates to the slurry to assay for cleavage activity with specific synaptic pairings. RAG’s cleavage efficiency is greatly enhanced by the DNA-bending high-mobility-group protein HMGB1 because cleavage requires DNA dis- tortion(1). I will add HMGB1 to standard Hztransib TP cleavage reactions and observe its effect on cleavage efficiency. Following RAG cleavage, the four newly created DNA ends remain syn- apsed in a postcleavage complex(1). To probe for an Hztransib postcleavage complex, I will bi- otinylate specific DNA ends, pull down biotinylated cleavage products with streptavidin agarose beads, and characterize any unbiotinylated DNA species that are also pulled down. Aim 3: Determine secondary nuclease activities of Hztransib TP. In vitro, RAG exhibits vari- ous nuclease activities besides cleavage at RSSs: it cleaves single-stranded heptamers, it cuts off 5’-ended overhangs on duplex DNA, and it removes 3’-terminated single-stranded flaps(1). By incubating Hztransib TP with representative radiolabeled substrates and characterizing products, I can determine whether Hztransib TP also exhibits these activities. Aim 4: Suggest catalytic and regulatory roles for RAG2. While RAG1 requires RAG2 for activity(1), Hztransib TP bears sequence similarity only to RAG1(2) and is able to effect cleavage without supplementary protein factors(3). To elucidate RAG2’s role in V(D)J recombination, I will include in each of the previously described experiments an additional reaction containing both Hztransib TP and RAG2. If RAG2 enhances a RAG-like biochemical property of Hztransib TP, that property may have evolved due to recombinase-specific selection pressures. Challenges: As of yet, cleavage activity in low-purity Hztransib TP preparations from another lab has been observed only after addition of Mn2+ (2), which deregulates RAG endonuclease ac- tivity when substituted for the physiological electrophile Mg2+ (1). The Schatz lab has ample ex- perience developing expression constructs and purification/reaction protocols for RAG, expertise that can now be applied to Hztransib TP to increase purity and, I predict, allow cleavage with Mg2+. Hztransib may not represent the entire Transib family in all details; whereas several other Transib transposons contain V(D)J-like asymmetric TIRs(2), Hztransib has symmetric TIRs. I will use my background in computational sequence analysis to identify and conduct experiments with an active Transib transposon bearing asymmetric TIRs, allowing requirements of asymmet- ric synapsis to be evaluated both with RSSs and with the transposon’s own TIRs. Broader Impacts: I will recruit undergraduate mentees from my classes and from oSTEM to get involved in this work, capitalizing on the multifaceted nature of the project to teach them to ap- proach problems from various angles. Additionally, because the USA currently falls far behind other scientifically advanced nations in popular acceptance of evolutionary theory, I will present the exciting history of this tamed transposon at high school teacher conferences to encourage DNA-level approaches to evolution pedagogy, obviating higher-order misinterpretations. Intellectual Merit: Because V(D)J recombination is an essential step in the development of an- tigen-specific lymphocytes, a complete functional comparison between Transib TP and the RAG complex would strengthen the current model for adaptive immune system development. It would also offer clues as to how early organisms acquired pathogen defense capabilities, a significant evolutionary hurdle that, once cleared, initiated a dramatic increase in organismal complexity. References: (1) Gellert M. 2002. Annu Rev Biochem 71: 101-32. (2) Kapitonov VV, Jurka J. 2005. PloS Biol 3: e181. (3) Hencken CG, Li X, Craig NL. 2012. Nat Struct Mol Biol 19: 834-6.	0
An investigation of thermal effects on Anax junius nymph growth Background: Earth’s climate is changing, and scientists are already observing impacts on biota across many taxonomic groups.1 Odonata are known to be useful biological indicators of environmental change at a macro-scale, including climate change.2 Odonata are highly temperature-sensitive, with direct effects on their physiology (e.g., developmental rate) and other life-history traits (e.g., phenology).1 Additionally, distributional and phenological records for Odonata are extensive, so they are excellent model organisms for studying the impacts of climate change on animal distributions, life history strategies, and development. Finally, the fossil record and historic data show that Odonata have survived rapid and dramatic climatic transitions in the past. However, present-day rates of climate change are substantially greater due to anthropogenic causes.3 Historically, Odonata have proven to be resilient and adaptable, but their current response is unknown. In sum, Odonata are considered sentinels of climate change, and there is growing interest in examining changes in their phenology and physiology as the climate warms. Odonata have been closely researched in the field. Studies using Odonates in applied research areas, such as climate change, are beginning to gain attention, but overall, research in this area is lacking.4 I believe that Odonata would be an exceptional research subject to help us understand how freshwater organisms are responding to warming temperatures. Shifts in air temperatures will influence lentic water temperatures through convection and by changing evaporation rates.5 Odonata are likely to reflect the mismatches between water and air temperatures due to climate change, demonstrating a potential temporal decoupling between aquatic and terrestrial species.5 Understanding this response in Odonates is particularly important, because they play an important role in structuring food webs, especially in fishless ponds which harbor unique biodiversity among macroinvertebrates, and are quite numerous across the landscape in many glaciated regions.6 Research Proposal: I propose an investigation to examine the effects of warming temperatures on larval dragonflies using the species Anax junius (the common green darner dragonfly, Order: Odonata, Family: Aeshnidae), in laboratory experiments. Although lab and field observations of temperature effects on larval Odonate development have been done independently, this proposed study will allow for a comparison between the lab experiment and data collected in the field in order to see if the lab findings in the lab hold up in real ecosystems. This proposal aims to 1) determine a range of temperatures that allow for optimal growth conditions for A. junius nymphs, and 2) compare the laboratory results to water temperature and A. junius emergence timing observed in the field by students and citizen scientists in order to better understand the effects that climate change has on aquatic ecosystems. Preliminary Work: For the past three summers, I have worked with my mentor Dr. Emily Schilling at Augsburg University on projects studying dragonflies from the family Aeshnidae. Our studies have focused on species showing evidence of modified life history strategies as an adaptive response to climate change. Through this research, we have developed relatively simple, cost effective and trustworthy sampling methods for nymphs, exuviae, and adults, that can easily be replicated by others. Methods and Materials: Aim 1) For this study, I have selected Anax junius, because this species is common throughout North America and adults are easily identified in the field (as opposed to other Aeshnids), which need to be observed in hand for species identification. Additionally, A. junius is known to be migratory, meaning that this species’ distribution covers a large geographic area. For the laboratory component of my study, I will set up fifteen 20-gallon tanks (30”x12”x12”), each containing ten A. junius nymphs. Each tank will have a heater to regulate the temperature and a HOBO Dissolved Oxygen Data Logger to monitor the dissolved oxygen differences amongst the temperature treatments. Tanks will be supplied with emergence supports and covered with mesh to capture emerging adults. There will be five temperature treatments (10°C, 15°C, 20°C, 25°C, and 30°C), with three replicates of each. Nymphs will be measured for their head-width-to-wing-sheath ratio three times per week in order to monitor growth, the number of days it takes each nymph to emerge will also be recorded. All molts, deaths, and emergences will be documented each measuring period. Aim 2) In order to get students and the general public more involved in STEM, I am going to enlist the help of volunteer scientists to broaden the scope of my data set. I will do this by contacting high schools and universities with NSF grants, and by posting ads on social media. I will also contact the Dragonfly Society of the Americas to enroll citizen scientists. For the recruited volunteers, I will let them choose a pond to sample and send them a temperature logger that continuously records water temperature, dip-nets for sampling dragonfly nymphs, and a guide on nymph and adult identification for A. junius. Lastly, I will create a web page where volunteers can easily upload their data and observations from their field sites. By using citizen scientists, I will be able to sample a larger geographic area than would be possible on my own, and collect data from multiple regions simultaneously. Broader Impact: By examining the data I receive from citizen scientists around the country, I will be able to gain insight as to which regions in North America are seeing the most dramatic changes in water temperature and gain a better understanding of the biological response to this environmental change. It is important to determine regions of concern so conservation planning can be prioritized in those areas. All humans and a large proportion of earth’s biodiversity require fresh water to survive. That is why research focusing on freshwater ecosystems is essential. Since climate change is a global issue, it is important to involve people in climate- related research that can ultimately inform how we protect freshwater ecosystems, arguably our most precious ecological resource. By engaging students and citizens in science, by allowing them to be a part of the data collection process, I hope to get more individuals interested in helping preserve and conserve the limited resources we have on Earth for generations to come. References: [1] Hassall, C., D. J. Thompson. 2008. The impacts of environmental warming on Odonata: a review. International Journal of Odonatology 11(2): 131-153. [2] Bried, J. T., C. Hassall, J. P. Simaika, J. D. Corser, J. Ware. 2015. Directions in dragonfly applied ecology and conservation science. Freshwater Science 34(3): 1020-1022. [3] Pritchard, G. & M. Leggott. 1987. Temperature, incubation rates and the origins of dragonflies. Advances in Odonatology 3: 121- 126. [4] Bried, J. T., M. J. Samways. 2015. A review of odonatology in freshwater applied ecology and conservation science. Freshwater Science 34(3):1023-1031. [5] Matthews, J. H. 2010. Anthropogenic climate change impacts on ponds: a thermal mass perspective. BioRisk 5:193-209. [6] Schilling, E.G, C. S. Loftin, A. D. Huryn. 2009. Macroinvertebrates as indicators of fish absence in naturally fishless lakes. Freshwater Biology 54(1):181-202.	0
Introduction and Preliminary Results: Discovered at Drexel University in 2011, MXenes are a novel class of 2D materials that comprise metal carbides and nitrides. Due to their excellent electronic, optical, thermal, and mechanical properties, MXenes have great promise for applications in several technologies including additives in solar cells and electronic contacts for semiconductors [1]. Compared to other 2D materials, MXenes offer an optimal combination of high electronic conductivity, low cost, and facile synthesis methods. Furthermore, their tunable optoelectronic properties, such as work function and optical absorption, enable MXenes to improve emerging photovoltaic materials such as perovskites and inorganic semiconductors. To realize the full potential of MXene photodetectors, an improved fundamental understanding of their electronic and optical properties is needed. During my master’s thesis, I refined methods for photodetection analysis of MXene thin films. While it laid the groundwork for the optoelectronic study of MXenes, the underlying factors that drive MXene response to light (photoresponse), such as the impacts of the electrode and substrate type, are not well- understood. While MXenes have proven successful as additives and electrodes, I aimed to bring them to the mainstream as active materials. My work focused on the photoactive capabilities of Ti C , which has already 3 2 succeeded as a transparent photodetector electrode [2]. Although Ti C is the most commonly studied MXene, 3 2 its response to chopped illumination with visible light Figure 1: Ti C films exhibit consistent negative had not yet been reported. 3 2 photoconductivity when deposited on patterned Figure 1 compares the average change in fluorine-doped tin oxide (FTO) substrates without resistance (R) upon illumination for films with the use of silver paste contacts (black). However, different initial resistance values (corresponding to thin films with high R switch from negative to thickness) and with different contact methods. Here, 0 positive photoconductivity upon illumination with the application of silver paste as an electrical contact the application of metal contacts (red) and causes Ti C to deviate from innate behavior upon 3 2 experience suppressed photoresponse when illumination, while a change to a thinner substrate deposited on glass slides (green). Schematic (glass slides) suppresses the magnitude of the of MXenes shown in inset [2]. photoresponse. Ti C is a well-known metallic and 3 2 photothermal material with innate negative photoconductivity, leading to an expected increase in R upon illumination or heating; these observations that contradict expected behavior call into question the role of silver-MXene interactions, carrier dynamics, and heat transfer in determining the material’s photoresponse. I hypothesize that the photovoltaic (PVE) and the photothermoelectric (PTE) effect each play into the photocurrent generated by MXenes, giving them the power to serve multiple applications, from thermal imaging to photovoltaic electrodes. Research Plan: I aim to both experimentally and computationally study heat and charge transport in MXene photodetectors to guide their design in imaging and energy generation applications. I will begin my examination with Mo-based MXenes, a lesser-studied subset of MXenes with potential as a photoactive material. Previous empirical studies question computational results showing Mo TiC has semiconductor- 2 2 like properties [3]. This work will seek to confirm these analyses by isolating contributions to light-matter interactions for Mo-based MXenes. Furthermore, over 30 different MXenes have been reported to date [1]. This proposal outlines just the beginning of our exploration into MXene photodetection capabilities in response to visible light, as the methods listed can be applied to other photoactive MXenes as well. Objective 1 – Understand the impacts of device architecture: In varying the deposited film thickness, contact geometry, and substrate type, I will evaluate their individual influences on photodetector properties (responsivity, noise, stability) in response to chopped illumination with visible light. Using thermally conductive substrates, such as sapphire, the impact of thermal effects can be mitigated. I expect strongly absorbing films, non-metal electrical contacts, and thin, thermally conductive substrates to produce the strongest photoresponse for Mo-based MXenes. Upon gaining this phenomenological data, I will then study charge carrier dynamics to understand the light-matter interactions for each MXene device. Under the guidance and expertise of Prof. R.J. Holmes, I will probe exciton diffusion at the interface of the photoabsorbing MXene and the electrical contact via an external quantum efficiency measurement method curated in his group [4]. This broadly applicable method provides additional understanding of carrier dynamics upon photoexcitation and will guide selection of device architecture for improved performance. Objective 2 – Model optical and thermal transport kinetics: Through computational efforts to model contributions from the PTE and the PVE, I will confirm the dominant effect that determines the photoresponse. Should combined contributions dictate the photoresponse, I aim to create a secondary model system specific to MXenes. By compiling a model from literature for both heat and carrier transport, I will determine optimal film thicknesses, contact geometry, and substrate types for devices that rely on either the PVE or the PTE, creating two reliable device architectures with improved responsivity. Moreover, simulating the photodetector architectures created in Objective 1 using COMSOL will push them to their thermal and electronic limits, granting insight into widespread implementation of MXene photodetectors. Objective 3 – Create devices and optimize performance: Equipped with the knowledge of optimal device architecture and film deposition, I will build Mo-based photodetectors and investigate industrially relevant issues, such as stability, lifetime, and performance of larger area devices. Given the inevitable obstacles in scaling up a device, I must tailor the device parameters found in Objective 1 to suit applications that would benefit from either the PVE or the PTE, such as energy generation or thermal imaging, respectively. I envision my contributions will spur the development of MXene-based photodetectors that suit multiple purposes simply by changing the device architecture. Intellectual Merit: My well-rounded background in chemical engineering and materials science and engineering allows me to understand not only why MXenes behave the way they do, but also how we can implement these materials in devices. I will utilize the wealth of knowledge from multiple energy transport experts, including Prof. Holmes, as well as state-of-the-art facilities for nanotechnology research at UMN to ensure the success of this project. The proposed research will provide an improved fundamental understanding of the factors that influence MXene photodetection, an emerging field of interest with limited literature available. Upon gaining this understanding, we can continue to use MXenes in optoelectronic applications, reducing the cost to produce photodetectors and allowing for widespread implementation of more conductive, easily synthesized materials. Broader Impacts: My work aims to inspire other researchers to consider implementing MXenes in their devices, bringing the field closer to a reliable, reproducible method for renewable energy generation. Through my research on nanomaterials in energy applications, I aspire to make clean energy commonplace, expanding on my dreams of a sustainable future arising from wanting to develop accessible biodegradable plastics in high school. I also aim to continue my impactful record of mentorship and community outreach. Leaning on my extensive outreach experience described in the accompanying personal statement, I plan to create an interactive lesson on current and novel photodetectors and sensors through Science for All, a student-run group created to support and promote STEM fields to local, underserved middle schools in the urban Twin Cities. Prof. Holmes also has the laboratory facilities to package photodetectors, allowing me to bring samples to the classroom. Lastly, through the Undergraduate Research Mentorship Program (UROP), I will seek and recruit undergraduate students from underrepresented groups for this project, serving as a research and personal mentor to guide them through their technical careers and encourage them to continue their STEM education. References: [1] L. Zhao, et al. Tungsten, 2 (2020): 176 – 193 [2] K. Montazeri, et al. Adv. Mater., 31.43 (2019): 1 – 9. [3] G. Li, et al. Proc. SPIE 11279, 112791U (2020): 66 – 84. [4] T. Zhang, et al. Nat. Commun., 10.1 (2019): 3489 – 3495.	0
"Introduction. The question of how a child learns her native language remains highly debated in linguistic research. Cross-linguistically, children learn much of the morphology (word structure) of their native language by age three, when their vocabulary is approximately 1000 words.1 Yet the frequencies of words in child-directed speech follow a skewed, roughly Zipfian distribution, with the frequency of a word being proportional to the inverse of its rank.2 This means that a few words may occur hundreds of times in the child's linguistic input, but most occur only a few times. Similarly, most words only appear in a few of their possible inflected forms (e.g. the child may hear fall and fell, but not falls or fallen).3 Such input contrasts with the larger, more saturated data used by most machine learning systems today. How, then, does a child learn the morphology of her native language from such a skewed, sparse input? During my PhD, I seek to answer this question via computational modeling of morphological acquisition. A plausible model of morphological acquisition should follow the developmental and behavioral patterns of children, which may be studied experimentally and through analysis of errors in children's speech. Productivity of a linguistic process is marked by its ability to generalize to novel contexts, and is a foundational component of language. In the famous wug study, for example, most English-learning children generalized -ed, -ing and -s to novel verbs (e.g. gling) by age three, demonstrating that they had learned the productivity of these suffixes.4 Further, most errors in children's speech are caused by over- application of productive processes: English verb acquisition is known to follow a ""U-shaped"" curve, where a sudden dip in performance is caused by the overapplication of -ed to irregular verbs (e.g. goed, feeled) when the child discovers its productivity.5 Several promising computational models that account for these facts utilize the Tolerance Principle (TP), a threshold of productivity which posits that a child generalizes a process when it is more computationally efficient to do so under a Zipfian distribution.2 Such distributional learning models have been the focus of my undergraduate research: working with Dr. Charles Yang, I developed a model that acquires meaning-form mappings (e.g. PAST = -ed) between suffixes and their corresponding semantic features, such as person (first, second or third), number (e.g. singular, plural), and tense (e.g. past, present, future).6 This model follows developmental patterns and correctly acquires morphological rules on small vocabularies of Spanish and English verbs. I also created a model that acquires such mappings for German plural nouns and English verbs, even displaying U- shaped regression in English, and contributed to a third model with comparable results.7 It is my goal to build on these models to create an integrated, incremental, and cognitively-plausible model of morphological acquisition that succeeds on a wide array of languages. Aim 1: To create a model of incremental morphological acquisition that succeeds on a typologically diverse set of languages. While the models outlined above provide promising results, no single model is able to account for all languages: the latter two succeed on concatenative, non-agglutinative languages (e.g. English, German), but fail to model non-concatenative (e.g. Hebrew, Arabic) and agglutinative (e.g. Spanish, Swahili, Japanese) languages. Segmenting a word into morphemes is more challenging in such languages: in Spanish, for example, a verb may take multiple suffixes (e.g. ama-ba-s = love-past-2nd+ singular = ""you loved""). These models are also not incremental learners, but extract morphological rules from fixed-size vocabularies; this contrasts with the incremental nature of language acquisition. In my first aim, I thus plan to build on the models above to design a novel algorithm that incrementally acquires morphological rules across agglutinative, non-agglutinative, and non-concatenative languages. While current models take in each item as a lemma, inflected form, and semantic feature set (e.g. walk, walked, {PAST}), the child may be able to group each of the inflected forms in which she has seen a word (e.g. get, gets, gotten). I hypothesize that doing so will allow for cleaner segmentation and identification of morphemes, helping the learner to succeed on a wider array of languages. To test this, I will create such a model and compare it with experimental findings on the aforementioned languages and others. I will work closely with linguists and cognitive scientists from differing subfields to ensure that this work benefits from both theoretical and experimental insights and provides a plausible account of acquisition. Aim 2: To extend this model by incorporating models of other portions of language acquisition. Morphological acquisition does not happen in isolation, and morphology is known to interact with other levels of linguistic representation, particularly phonology (e.g. -s is pronounced /s/ in cats but /z/ in dogs). The nature of the input to the models discussed above assumes that the child has already learned much of the phonology of their native language and extracted the relevant semantic features such as person, number and tense onto which they will map the segmented input. The grouping of forms as discussed in Aim 1 makes the additional assumption that the child is able to form these groups. To create a more holistic account of acquisition, I will thus integrate models of morphological learning like the above with models of acquisition of other levels of linguistic representation, particularly those on which Aim 1 relies. It is highly plausible that similar learning algorithms are used for each level of representation, so I will begin by testing the ability of the algorithm developed above to account for these other levels. I will also test integration of the model developed under Aim 1 with existing models in the literature. With the end goal of an algorithmic hypothesis about how children acquire their native language, I will collaborate with experts on each of the levels of representation I will consider. I will compare the integrated model's predictions with experimental findings on a typologically diverse set of languages to ensure that this algorithmic account of learning is a plausible and generalizable one. Intellectual Merit. The end goal of this work, an input-to-grammar model of morphological acquisition, will provide insight into linguistic theory and the learning mechanisms employed by children. The model developed under Aim 1 will provide an algorithmic hypothesis regarding how children learn from skewed, sparse data, and structural hypotheses regarding morphological knowledge in the mind as the end result of acquisition. Both will be valuable in answering questions of learnability and the structure of linguistic knowledge, and may also provide insight into atypical language development. The models developed under Aim 2 will yield hypotheses about the interactions between linguistic levels of representation, which may be compared with theoretical hypotheses to provide new insight into linguistic structure. Further, these models will give a bottom-up account of language acquisition, and thus yield testable hypotheses regarding the innate factors that may enable it, a highly-debated topic. This model will, to my knowledge, be the first to model morphological acquisition from phonological input to a structured grammar, and it will thus provide a basis for further integrated models of language acquisition. Broader Impacts. This work has applications to Natural Language Processing (NLP), which focuses on the creation of language technologies. Models used in NLP are typically trained on data several orders of magnitude larger than that to which the child is exposed. This can lead to biases and makes models inaccessible for “low-resource” languages for which large corpora do not exist, such as Indigenous languages and languages of Africa and Asia.7 Cognitively-motivated approaches already show promising results,8 and since the algorithms I will develop are designed to succeed on small, sparse input, they will be strong candidates for use with low-resource languages and for testing bias mitigation strategies. This, in turn, will allow for the creation of more accessible, equitable language technologies for all. References. [1] Brown, R. 1973. A first language: The early stages. Harvard University Press. [2] Yang, C. 2016. The price of linguistic productivity: How children learn to break the rules of language. MIT Press. [3] Chan, E. 2008. Structures and distributions in morphological learning. UPenn Dissertation. [4] Berko, J. 1958. “The child’s learning of English morphology.” Word. [5] Pinker, S. and Prince, A. 1988. “On language and connectionism: Analysis of a parallel distributed processing model of language acquisition.” Cognition. [6] Payne, S, et al. 2021. “Learning Morphological Productivity as Meaning- Form Mappings.” Proceedings of the Society for Computation in Linguistics. [7] Belth, C, Payne S, et al. 2021. “The Greedy and Recursive Search for Morphological Productivity.” Proceedings of the Cognitive Science Society. [8] Bender, E, et al. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” Proceedings of the ACM Conference on Fairness, Accountability, and Transparency. [9] Xu, Chao et al. 2020. “A Cognitively Motivated Approach to Spatial Information Extraction.” Proceedings of the Third International Workshop on Spatial Language Understanding."	0
permission. CAUSES AND CONSEQUENCES OF BIOCOMPLEXITY Keywords: Adaptation, biocomplexity, biodiversity, natural selection, resilience Conspecific populations often differ in important fitness-related traits. Why might this be? One mechanism driving population differentiation is divergent natural selection, wherein spatial variation in selection drives divergence of populations. Such phenotypic diversity among proximate populations [hereafter “biocomplexity” (1)] is important for the long-term sustainability of the larger population complex because the relative contribution to total production may shift among different life histories depending on the prevailing environmental conditions (1). Despite the intuitive appeal of these ideas, the causes and consequences of biocomplexity are rarely studied. Variation in natural selection is the presumed mechanism generating biocomplexity. For instance, spatial variation in selection has been shown to drive divergence in age-at-maturity of Trinidadian guppies (2). Similarly, temporal variation can drive evolution of phenotypic traits, as evidenced by microevolution of beak size in Darwin’s finches in response to short-term climate perturbations (3). Yet, studies quantifying spatio-temporal variation in selection are exceedingly rare and the ecological circumstances driving selection are rarely understood. The overarching goal of my study is to examine the causes and consequences of biocomplexity among proximate salmon populations. Pacific salmon are an excellent system in which to test these ideas because they form discrete breeding populations, which are then subject to local selection pressures. Reproductive isolation combined with spatially varying selection pressures result in adaptation to local conditions and, ultimately, a diversity of phenotypes among populations. Furthermore, fine-scale variation in environmental conditions also leads to ecologically-driven variation among proximate populations (i.e., phenotypic plasticity). Herein, I propose to study coho salmon (Oncorhynchus kisutch) during their freshwater-rearing (juvenile) stage across multiple sites within the Lagunitas Creek watershed (California). Goal 1: Determine the causes of biocomplexity Hypothesis 1: Physical habitat attributes drive variation in fitness-related traits. Variation in local conditions can lead to phenotypic variation among salmon populations (4). Indeed, recent work has demonstrated that when habitat diversity is lost, specific salmon life history components are also lost (5). I will quantify seasonal variation in stream temperature and flow among ten tributaries of Lagunitas Creek across 3 years using standard methods (6). At each site, I will mark individual fish to monitor their size and growth through time. I will use a formal model comparison approach (7) to determine the physical habitat attributes contributing to phenotypic diversity among these proximate salmon populations. Hypothesis 2: Food web structure drives variation in fitness-related traits. Variation in stream flows has been shown to have a strong impact on food web structure among years (8). Moreover, variation in food-web structure can influence fitness-related traits in fish consumers (9). To investigate the role of food web structure as a driver of biocomplexity among salmon populations, I will characterize variation in food web structure among three tributaries spanning a gradient of stream size. Specifically, I will sample tissues from multiple trophic levels and quantify stable isotopes of carbon and nitrogen to illuminate food web structure (10). I will again use a formal model comparison approach to test the drivers of variation in food web structure among sites using data collected as part of Hypothesis 1, as well as the consequences of variation in food web structure to size and growth of salmon across sites. Hypothesis 3: Spatio-temporal variation in natural selection drives variation in body size. After determining the drivers of variation in size and growth, I plan to quantify natural selection acting Copyright 2008 All Rights Reserved to Original Author. Do not duplicate or use without permission. on these traits. I will use data collected on individual fish across multiple seasons to relate phenotypic traits to survival across focal intervals (e.g., winter). I will use standard approaches (11) for quantifying natural selection across sites and years. Goal 2: Determine the consequences of biocomplexity Hypothesis 4: Variation in smolt size and production differs among sites and years. Many studies have demonstrated the importance of size-at-smolt migration to ocean survival, with larger smolts presumed to survive at higher rates than relatively smaller smolts (12). I propose to determine the consequences of biocomplexity in juvenile growth rates and survival by quantifying variation in smolt size and production among sites by trapping out-migrating smolts from each site, measuring body size, and estimating site-specific smolt production. Analysis of these data will allow me to determine the consequences of variation in the ecological circumstances experienced during the juvenile-rearing stage to smolt size and production, which then influence adult production. This will then allow me to identify production “hotspots” within the system and to distinguish factors that lead to high smolt production in those areas. Anticipated Results: I anticipate that variation among sites of abiotic conditions and food web structure drives biocomplexity among proximate salmon populations. I also expect to find evidence of spatio-temporal variation in the strength and form of selection acting on body size. With the above factors potentially driving biocomplexity, I expect smolt size and production to vary among sites. This underscores the importance of maintaining a diversity of freshwater habitats to maintain biocomplexity among salmon populations as a buffer for future changes. Intellectual Merit and Broader Impacts: With the recent collapse of California salmon stocks, investigations regarding the link between biocomplexity and sustainability are becoming increasingly important. My research seeks to provide insight into the causes and consequences of biocomplexity and may help create more robust management practices that maintain the full diversity of phenotypes in proximate populations, thus ensuring some resilience to future perturbations. To communicate the significance of my original research, I plan to disseminate the findings of this study through various modes including via 1) peer-reviewed, published literature; 2) the Ecological Society of America conference (where I am a student member); and 3) to interested citizens via, for instance, the Point Reyes National Parks Service Podcast. Additionally, I plan to work closely with a local community-based group, the Salmon Protection and Watershed Network (SPAWN), in order to convey my findings to the local community. Finally, as a Burmese-American student in the environmental sciences, I am aware of the lack of ethnic diversity in my field. U.C. Berkeley has an incredibly diverse undergraduate body and I will strive to incorporate students from diverse backgrounds into all aspects of my research. References: 1. R. Hilborn, T. P. Quinn, D. E. Schindler, D. E. Rogers, Proc. Natl. Acad. Sci. U. S. A. 100, 6564 (May, 2003). 2. D. A. Reznick, H. Bryga, J. A. Endler, Nature 346, 357 (Jul, 1990). 3. P. R. Grant, B. R. Grant, Science 296, 707 (Apr, 2002). 4. P. J. Wigington et al., Frontiers in Ecology and the Environment 10, 513 (Dec, 2006). 5. M. M. McClure et al., Evolutionary Applications 1, 300 (2008). 6. F. R. Hauer, G. A. Lamberti, Methods in Stream Ecology. (Academic Press, San Diego, CA, ed. 2nd, 2007). 7. K. P. Burnham, D. R. Anderson, Model Selection and Multimodel Inference: A Practical Information-Theoretical Approach. (Springer, New York, ed. 2nd ed., 2002). 8. M. E. Power, M. S. Parker, W. E. Dietrich, Ecological Monographs 78, 263 (May, 2008). 9. K. B. Suttle, M. E. Power, J. M. Levine, C. McNeely, Ecological Applications 14, 969 (Aug, 2004). 10. J. C. Finlay, S. Khandwala, M. E. Power, Ecology 83, 1845 (Jul, 2002). 11. R. Lande, S. J. Arnold, Evolution 37, 1210 (1983). 12. T. P. Quinn, The Behavior and Ecology of Pacific Salmon and Trout. (University of Washington, Press, Seattle, 2005).	0
Nonlinear System Identification, Reduced Order Modeling, and Model Updating of the Effects of Mechanical Joints on Structural Dynamics Keywords: mechanical joints, nonlinear system identification, finite element model updating Summary: Mechanical joints are present in nearly every structure, device, or vehicle in operation today. As these become ever more complicated the need for the classification and understanding of the nonlinear effects on structural dynamics grows ever more critical. I propose to apply recently developed nonlinear system identification methods, reduced order modeling and model updating techniques to characterize and model these nonlinear effects. The outcome of this research will be the development of models for use in standard finite element (FE) methods that capture the nonlinear effects of mechanical joints. Literature Review: Several techniques exist for the identification of joint parameters, but these methods require extensive instrumentation and measurements that may not be practical and rely on frequency response functions that are assumed to be linear [1]. Yet the current FE model updating techniques necessitate accurate modal parameters and often produce results that differ greatly from experimental results [1]. A recently developed nonlinear analysis methodology with broad applicability can be applied to alleviate these issues by characterizing the nonlinearities and developing reduced order models for use in standard FE codes. The proposed method relies on the assumption that the application of Empirical Mode Decomposition [2], a time-domain based signal decomposition method, results in nearly orthogonal components, called Intrinsic Modal Functions (IMF), characterized by ‘fast’ oscillations controlled by ‘slow’ changing amplitudes [3-5]. The IMFs result in local nonlinear interaction models [6] that portray the local dynamics through sets of intrinsic modal oscillators (IMO). The IMOs are able to reproduce the measured times series while completely capturing the effects of the nonlinearities. The global dynamics are determined by superimposing the wavelet transform (WT) of the original time series in the energy-frequency domain with the frequency-energy plot (FEP) [7] of the representative Hamiltonian system. By assessing the global dynamics an understanding of the energy dependence of the nonlinear normal modes [7] of the system can be developed. This method was recently applied to a beam with a bolted lap joint to identify the damping nonlinearities and the effects on the structural dynamics [8]. This research aims to continue that study and extend the results into FE model updating. Hypothesis: Through the application of recently developed nonlinear system identification, reduced order modeling, and model updating techniques the nonlinear effects of mechanical joints on structural dynamics can be classified and incorporated into standard FE models. Research Method: In order to study the effects of mechanical joints, steel beams will be constructed that incorporate bolted, riveted, and welded connections. “Monolithic” steel beams without any connections, but with holes, bolts, rivets, etc. will be used as experimental control beams for the analysis. The beams will be constrained in various positions to model the most common structures: cantilever, fixed-fixed, fixed-pinned, and similar configurations. Accelerometers will be attached using adhesives at evenly spaced points across the beams. Two cases will be studied: 1) free vibration characteristics induced by impact forces and 2) forced vibration characteristics induced by an electrodynamic shaker. Keegan James Moore Page 1 of 2 Keegan James Moore Graduate Research Proposal Linear modal analysis in addition to the proposed nonlinear analysis will be applied to both the “monolithic” systems and the systems comprised of mechanical joints. This will allow me to verify that the nonlinear analysis is able to reproduce both the linear and nonlinear effects as well as e the deficiencies of the linear modal analysis. EMD will be applied to the measured time series to decompose them into nearly orthogonal IMFs. The extracted IMFs will be used to develop IMOs that capture the local dynamics. The global dynamics will be characterized by superimposing the Hamiltonian FEP with the WT of the measured time series in the energy- frequency domain. A FE model consisting of two linear beams connected by a nonlinear element will be used to compute the Hamiltonian FEP and will serve as the basis for the model updating. By characterizing both the local and global dynamics, I will be able to develop a reduced order model of the nonlinearity for each particular configuration. These reduced order models will be used to reproduce the dynamics of the measured systems and predict the dynamics of unmeasured systems. Finally, the models will be incorporated into standard FE methods for use in a broad range of applications. Anticipated Results: 1) The application of the proposed nonlinear analysis methodology will fully capture the linear and nonlinear effects of mechanical joints on the structural dynamics. 2) Reduced order modeling techniques will be developed that can be incorporated into standard FE methods that account for the nonlinear modal interactions produced by mechanical joints. Broader Impacts: This research aims to apply recently developed techniques to characterize the nonlinear effects of mechanical joints and to incorporate these effects into standard FE models. These models will made available for use in a broad range of applications in fields such as aerospace, automotive, heavy industrial equipment, turbo machinery and structural engineering. As this research progresses, I will present my findings at technical conferences such as the International Modal Analysis Conference and in technical journals such as the Journal of Sound and Vibration and Mechanical Systems and Signal Processing. Furthermore, this research will lay the foundations for developing further methods aimed at understanding nonlinear effects and incorporating them into standard FE analysis methods. Literature Cited 1. R. Ibrahim, C. Pettit, Uncertainties and dynamic problems of bolted joints and other fasteners, Journal of Sound and Vibration 279 (2005) 857-936. 2. N.E. Huang, Z. Shen, S.R. Long, M.C. Wu, H.H. Shih, Q. Zheng, N.C. Yen, C.C. Tung, H.H. Liu, The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis, Proceedings of the Royal Society A 454 (1998) 903-995. 3. Y.S. Lee, A.F. Vakakis, D.M. McFarland, L.A. Bergman, A global-local approach to nonlinear system identification: a review, Structural Control and Health Monitoring 17 (2010) 742-760. 4. A. Vakakis, L.A. Bergman, D.M. McFarland, Y.S. Lee, M. Kurt, Current efforts towards a non-linear system identification methodology of board applicability, Journal of Mechanical Engineering Science 225 (2011) 2497-2515. 5. Y.S. Lee, S. Tsakirtzis, A.F. Vakakis, L.A. Bergman, D.M. McFarland, Physics-based foundation for empirical mode decomposition, AIAA Journal 47 (2009) 2938-2963. 6. Y.S. Lee, S. Tsakirtzis, A.F. Vakakis, L.A. Bergman, D.M. McFarland, A time-domain nonlinear system identification method based on multiscale dynamic partitions, Meccanica 46 (2010) 625-649. 7. A.F. Vakakis, O. Gendelman, L.A. Bergman, D.M. McFarland, G. Kerschen, Y.S. Lee, Passive Nonlinear Targeted Energy Transfer in Mechanical and Structural Systems, Springer Verlag, Berlin and New York, 2008. 8. M. Eriten, M. Kurt, G. Luo, D.M. McFarland, L.A. Bergman, A.F. Vakakis, Nonlinear system identification of frictional effects in a beam with a bolted joint connection, Mechanical Systems and Signal Processing 39 (2013) 245-264. Keegan James Moore Page 2 of 2	0
Hypothesis: Ocean wave energy has vast potential as a renewable power source, but traditional sequential design methods perpetuate prohibitively high device costs. Applying systems optimization techniques to wave energy would unlock new architectures that are cost-competitive at utility scale. Introduction: Climate change is the most critical problem facing humanity. It threatens warming, flooding, erosion, and storms that damage infrastructure, agriculture, health, and biodiversity. The fatal potential for 4oC of warming can be limited to 1.4oC if complete decarbonization occurs by 2055 [1]. The electricity sector contributes 40% of global CO emissions, representing perhaps the largest challenge and 2 opportunity for decarbonization [2]. Existing renewables have limitations: wind and solar are intermittent and unpredictable; hydropower and geothermal have few suitable sites. To reach 100% clean electricity, we must supplement these sources by developing diverse renewables to technical and economic maturity. Ocean wave energy is uniquely attractive due to its predictability, geographic abundance, and continuous availability [3]. Despite an ability to fulfill 34% of US electricity demand [4], wave energy technology’s high cost, long design cycles, and risk-intensive investment have prevented full-scale deployment [3]. The wave energy converter (WEC) industry has so far followed a trajectory similar to aerospace, focusing on technical risk reduction with an expectation that costs will fall after the technology matures. However, recent analysis indicates that this path is infeasible, and success hinges on a reinvention of the typical design cycle to emphasize early cost and performance innovation before deploying expensive prototypes [5]. Multidisciplinary Design Optimization (MDO) and Control Co-Design (CCD) techniques can provide the design paradigm shift that is required for dramatic cost reduction. MDO and CCD are emerging techniques that depart from the standard sequential design process by considering subsystem interactions early on. MDO is an optimization framework for interdisciplinary design problems. MDO has been used successfully in the automotive, energy, and aerospace sectors but never attempted for wave energy due to novelty and computational costs [6]. CCD, the use of control principles to inspire device design, has never been applied to a utility-scale WEC for similar reasons. CCD offers significant benefits: in offshore wind turbines, CCD decreased structural loading by 99% [7], and one estimate predicts 30% cost reduction potential for wave energy [8]. Overall, MDO and CCD are promising methods to advance wave energy design towards full decarbonization of the electricity sector. Research Plan: This project will be completed over the course of my PhD studies at Cornell University in the Symbiotic Engineering Analysis (SEA) Lab, led by Professor Maha Haji. Objective 1: Develop a novel WEC design framework using principles of MDO and CCD. I will create a multidisciplinary model of WEC dynamics and performance. As shown in Fig. 1, the model will bridge previously disparate simulations in controls, structures, powertrain, and hydrodynamics, which are recognized as the four most impactful subsystems to drive down WEC costs [9]. I will prioritize appropriate model fidelity, simplifying wherever feasible while still capturing important subsystem interactions. One key tradeoff is the decision to model in the stochastic, frequency, or time domain. Degrees of freedom will be selected strategically, balancing computational tractability against flexibility to describe diverse architectures. Objective 1 is attained when a design optimization process can be clearly articulated and validation efforts show that the model can correctly predict performance of existing WECs. This framework forms the foundation for the second research stage. Objective 2: Utilize the design framework to obtain a novel WEC architecture that is cost- competitive at utility-scale. The MDO-CCD model and process developed in Objective 1 will be implemented and iterated upon, focusing on interactions between device controls and structures. Initially, optimization will identify the best combination of existing configurations; ultimately, model degrees of freedom will be extended, allowing optimization to yield new architectures altogether. Sensitivities will quantify impacts to inform further innovation. Objective 2 is complete when a WEC design with levelized energy cost below $0.30/kWh is found, representing a 75% cost reduction from current technology [10]. Objective 3: Validate key features of the optimal design through wave tank testing. The final stage of research involves the detail-design of a scaled prototype of the optimal solution, followed by the manufacturing, testing, and data analysis of the prototype. The testing investigates real-time control implementation, validates model robustness, and provides a practical industry-relevant realization of the new design process and design. Cornell University is equipped with the facilities to enable this testing through the DeFrees Hydraulics Lab wave tank, and collaborations with nearby institutions including the University of New Hampshire or the University of Maine could be leveraged for larger wave tank access. Objective 3 is complete when any major differences between model and test are identified and explained. Intellectual Merit: Wave energy is an under-utilized and under-researched renewable source with great potential. Multidisciplinary integration is expected to be a key enabler of future cost- competitive wave energy. The proposed work will be the first to apply MDO and CCD techniques to a utility-scale wave energy converter, potentially unlocking radical cost savings. My design process will be the first to unify disparate WEC domains; my optimization will provide new insights to advance the field toward design convergence; and my test data will confirm understanding and applicability of these novel contributions. My undergraduate background provides relevant technical depth in mechanical design, power electronics, hydrodynamics, and controls, and above all I have the systems mindset to effectively unify these fields. This experience, coupled with the SEA Lab’s expertise exploiting synergies in marine technologies, uniquely qualifies me to succeed at the proposed research. Broader Impacts: Wave energy has the potential to meet 34% of national electricity demand [4]. My research will enable cost-competitive wave energy, adding to the renewables mix to combat climate change. The integrated MDO-CCD design process applies widely to any system with multidisciplinary interactions and embedded dynamic controllers. Thus, the proposed research advances knowledge in both renewable energy alternatives and systems optimization methods, both of which broadly benefit society. To circulate my research to the academic community, I will publish in journals such as Ocean Engineering and Renewable Energy and present at diverse venues such as IEEE OCEANS and CESUN. Leveraging my advisor’s connections, I will initiate collaborations with the National Renewable Energy Lab and companies such as CalWave. These partnerships will amplify the impact of my research by ensuring its alignment with current industry goals and its rapid dissemination to others upon completion. Finally, I will engage in outreach to both the general public and students of all backgrounds. Societal acceptance is key for widespread adoption of wave energy technology, and education about the opportunities and challenges of renewable energy encourages sustainable habits as well as an interest in engineering. My contributions to STEM outreach and curriculum development through SWE, Splash, and GRASSHOPR are detailed in my personal statement. In sum, my work as an NSF fellow would promote STEM engagement, advance systems design techniques, and enable a carbon-free future. References: [1] “Climate change 2021,” IPCC, 2021. [2] “Net zero by 2050,” IEA, 2021. [3] F. Mwasilu and J. Jung, IET Renewable Power Gener, 2019. [4] L. Kilcher, et al., NREL, TP-5700-78773, 2021. [5] D. Bull et al., Sandia, SAND2017-4507, 2017. [6] J. Sobieszczanski-Sobieski and R. Haftka, Struct. Optim., 1997. [7] X. Du, et al., ASME 2020 Int. Mech. Eng. Congr. and Expo., 2021. [8] M. Garcia-Sanz, ARPA-E, 2018. [9] D. Bull, et al., Sandia, SAND2013-7204, 2013. [10] G. Chang, et al., Renewable Energy, 2018.	0
Background: Martian ice likely holds the key to interpreting Mars’ past climate, but much is still unknown regarding the distribution and properties of Mars’ ice deposits. It is well known that Mars has extensive polar ice caps the size of Greenland. Included in these large polar caps are the north and south polar layered deposits (NPLD and SPLD, respectively), that are comprised of kilometers-thick deposits of water ice. In addition, surveys by Conway et al. (2012) and Sori et al. (2019) have identified craters in the surrounding terrains which contain “outlying” deposits of ice (Figure 1), which may or may not have formed at the same time as the deposition of the polar caps. These, as well as other efforts to identify and characterize ice, have used radargrams from the SHARAD (SHAllow RADar) sounding radar onboard NASA’s Mars Reconnaissance Orbiter (MRO) to analyze the subsurface in these icy areas, as radar images can essentially act as large-scale ice cores. However, additional data such as roughness and Figure 1. From Sori et al. (2019) dielectric constant of the shallow (<5 m) subsurface can be Locations of icy crater deposits near extracted from these radargrams by analyzing the reflectivity of the southern polar cap. All three the surface echo (Campbell et al., 2013; Castaldo et al., 2017; colors of points indicate outlying Grima et al., 2012). crater deposits that will be considered Research Objectives and Motivation: I propose to in this work. leverage this technique to analyze the physical properties of northern and southern outlying polar ice deposits. From my analyses, I will be able to draw conclusions about the purity, composition, and surface roughness of these ice deposits. Comparing similarities and differences between the ice deposits in the two hemispheres, as well as how the outlier deposits compare to their corresponding nearby polar ice caps, will allow me to assess how localized or global the climate processes were which led to the formation of polar ice deposits. In addition, I will analyze the subsurface radar echoes of the southern icy outliers and search for the existence of any buried CO deposits that may 2 have been sequestered from the atmosphere in past climate events (Figure 2). The main objective of this project is to identify differences in physical properties between southern and northern icy outlier deposits. Conway et al. (2012) found evidence that supports similarities in composition between the NPLD and outlying ice deposits located in northern craters. While the NPLD has been found to be made up of pure water ice (Grima et al., 2009), large deposits of CO ice have been found 2 sequestered in the SPLD (Phillips et al., 2011). Given the similarity in composition between the NPLD and northern outlying ice deposits, it stands to reason that such a similarity may exist between the SPLD and corresponding southern outlying ice deposits. This motivates my search for sequestered deposits of CO ice 2 within the southern outlying ice deposits. If such CO deposits are found, it would highlight a major 2 difference between the northern and southern outlier ice deposits, and would also place new constraints on the thickness of the atmosphere in Mars’ past. CO sequestered in the ice would have once been in the 2 atmosphere, which would have a significant impact on the Martian atmosphere and climate system. For example, the amount of CO that has been found in the SPLD alone is enough to have doubled the 2 atmospheric pressure on Mars pre-sequestration (Phillips et al., 2011). Even if sequestered CO deposits 2 are not found in these southern outliers, other differences in composition, purity, and/or surface roughness could provide exciting insights on the icy processes at work in each hemisphere. Methods: To determine if a difference in physical properties of ice in the northern and southern outlying crater deposits exits, I will examine the ice at two different spatial and temporal scales. First, I will use the reflectivity of the primary surface echo to infer the roughness and dielectric constant of the younger surface ice. Similar techniques have been used to make global maps of SHARAD parameters (Campbell et al., 2013; Castaldo et al., 2017; Grima et al., 2012), but have not been used to study these outlying icy crater deposits. This will allow me to compare the present-day conditions of ice in the northern and southern hemispheres of Mars. I will also leverage the subsurface capabilities of SHARAD to analyze older layers of ice below the surface in these outliers, with the additional goal of determining the existence of sequestered CO similar to that which Phillips et al. (2011) found within the SPLD. By analyzing the 2 properties of both subsurface and surface reflectors, I will be able to make comparisons of physical properties of northern and southern icy outliers as well as the climate conditions that could have formed them. Figure 2. Example SHARAD radargram from Phillips et al. (2011). Reflection- free subsurface zones (“RFZ”) were found to be pure CO2 deposits. Intellectual Merit: Such a study of the outlying ice deposits has not yet been done. The Martian ice acts as a record of the climate in which it formed, so understanding the ice in both hemispheres is necessary in order to be able to answer the questions of what conditions were present in Mars’ past. I expect that this study will provide new insights on the similarities and differences in composition, purity, and surface conditions of ice in the northern and southern hemispheres. Analyzing these properties can tell us how the ice was deposited; for example, very pure ice would be indicative of a deposition via snowfall, while low ice contents generally imply formation due to condensation of atmospheric water vapor within pore spaces of the regolith. Finding layers of ice with different properties would also provide strong evidence for changes in the climate, which could be linked to orbital/rotational variations analogous to Milankovitch cycles on Earth. This work may also confirm the presence or absence of sequestered CO 2 deposits in the southern outlying ice deposits, which if found would place new constraints on the thickness of Mars’ atmosphere pre-sequestration of the CO . 2 Purdue University is an ideal institution at which to carry out this research. Here, I am advised by Prof. Ali Bramson, who has extensive experience working with SHARAD observations of Martian ice (e.g., Bramson et al., 2015) and is a Co-I on the NASA-funded Subsurface Water Ice Mapping (SWIM) project, which uses multiple types of observations (including radar) to map subsurface ice through the mid-latitudes of Mars. I will also be working with Prof. Mike Sori (also at Purdue University), using his database of southern outlying ice deposits (Sori et al., 2019) as the set of southern deposits I will be characterizing. Broader Impacts: Indiana is home to the newest of our national parks: Indiana Dunes National Park (INDU). Using my experience as an Astronomy Ranger at Bryce Canyon National Park, I will work with the chief rangers of interpretation and education and INDU, Bruce Rowe and Kim Swift, to develop a night sky educational program that connects what we see in the sky to geology here on Earth. Though INDU is near the light pollution of Chicago, major planetary bodies are still visible. This night sky program will be held outdoors on the park’s West Beach weekly during the main summer season (April–November) and focus on water and sand dunes throughout the solar system that are also present in the park. This park is uniquely located near major metropolitan areas, and as such these programs will serve communities that are traditionally underserved by science outreach. More details on this planned project can be found in my Personal Statement. References: Bramson, A. M., Byrne, S., Putzig, N. E., et al. (2015). Geophys. Res. Lett., 42(16), 6566–6574. Campbell, B. A., Putzig, N. E., Carter, L. M., et al. (2013). JGR: Planets, 118, 436–450. Castaldo, L., Mège, D., Gurgurewicz, J., Orosei, R., & Alberti, G. (2017). EPSL, 462, 55–65. Conway, S. J., Hovius, N., Barnie, T., et al. (2012). Icarus, 220(1), 174–193. Grima, C., Kofman, W., Mouginot, J., et al. (2009). Geophys. Res. Lett., 36(3), 2–5. Grima, C., Kofman, W., Herique, A., et al. (2012). Icarus, 220(1), 84–99. Phillips, R. J., Davis, B. J., Tanaka, K. L., et al. (2011). Science, 332, 838–841. Sori, M. M., Bapst, J., Becerra, P., & Byrne, S. (2019). JGR: Planets, 1–21.	1
"ranging applications. Robotic systems are perfectly suited for repetitive or dangerous construction tasks, from brick laying and creating city infrastructure, to building extraterrestrial or disaster- relief structures. This latter application presents a formidable challenge in robotics: to enter an unknown and uneven environment and autonomously build pre-designed or adaptively designed structures to stabilize the site and prepare it for human presence. Already, there are many designs for emergency relief structures and space infrastructure that are well-suited for this type of autonomous construction, but existing robotic systems are not able to build them. There are two primary approaches to improving construction capabilities. The first approach is to create complex systems that rely on high-accuracy perception, complete representation of surroundings, and precise movement and control. While this method could be capable of building a greater variety of structures, it has problems with cost, computation, and robustness that current technology cannot address. The second approach relies on distributed systems that absorb high levels of error through mechanisms and control, rather than trying to perform perfect actions. This second approach can take great inspiration from biology. Termites, which build mounds millions of times their own size to house their colonies, and beavers, which cobble together dams to adapt their environment for their habitation, demonstrate the extraordinary potential of these systems. Additionally, the compliant or underactuated mechanisms that compose many animals show the potential for error-absorption and robustness in material choice and mechanism design. In this vein, some recent work has explored the use of robot collectives for construction. These systems take control and communication methods from biology, such as distributed decision-making based on the state of the built structure, called stigmergy, and communication through pheromones. In [1], a multi-robot system built simple structures with blocks using stigmergy, and mimicked ""tagging"" the structure with pheromones by using color-coded LED's. In [2], a multi-robot system relied on a stored blueprint to build complex structures much larger than an individual agent, and relied completely upon stigmergy for representation of the current built structure. Even more similar to termite construction, [3] presents an algorithm for construction of a ramp using amorphous material similar to the soil used by termites. These robotic systems rely upon simple agents and control policies to absorb errors and robustly and efficiently build. Other work takes inspiration from the underactuated and soft mechanisms present in nature to make compliant robots. These soft robots are extremely effective at absorbing error; their flexibility simplifies control and damps out unwanted perturbations. However, soft robots are presently limited by difficulty of manufacture and poor performance of soft actuators. There exists a large body of work developing soft robots, but these concepts have not been applied to construction tasks. Research Plan: My aim is to enable collective construction that is robust and effective on uneven terrain. It will consist of a mechanical design phase and an algorithm design phase, supported by a foundation of tools to evaluate system success objectively. I. Metrics: First, I aim to develop a novel metric to quantify error tolerance in a multi-robot construction system. Presently, most metrics that would apply are within the realm of control theory: metrics such as the size of zone of attraction, or the degree to which an action in the system is passively stable. These are effective in characterizing aspects of a collective construction system on the micro-level, in terms of individual actions, but fail to characterize the system on a macroscopic scale. I will create and validate a metric that effectively characterizes the response to disturbances and errors of a collective construction system. This will allow comparison of my future work with its biological counterparts, and will give the necessary basis for verifying a collective construction system. II. Physical System: I plan to create a physical robotic system capable of robust collective construction on uneven terrain. My research group at University at Buffalo, under Nils Napp, is the foremost group exploring autonomous construction on uneven terrain, a broad and applicable subset of the autonomous construction field. One approach that is almost untouched within the field is the use of soft, compliant, or underactuated robots and robot components to increase error tolerance. In the field of behavioral robotics, soft robots are ideal: their simplified control and adaptable structures make them perfectly suited. However, the lack of effective untethered actuation systems make them underutilized. I intend to use soft components in my robotic system, focusing on passive mechanisms and flexible-rigid interfaces to increase error tolerance without relying upon soft actuation. Additionally, I will extend research I performed in the past year, developing a robotic platform for construction over uneven terrain. I will modify it to be highly modular, allowing soft components to be swapped in and out easily, and enabling new directions of research involving self-charging and even self-healing agents. III. Control System: I will develop new control methods to improve coordination and efficiency in collective construction applications. Presently, there exist few control systems capable of controlling multiple robots towards a construction objective over uneven terrain. One of the only examples, presented in [3], makes non-navigable terrains navigable through deposition of expanding foam. With the Napp group at UB, I extended this in a paper submitted to ICRA this year, which modified the algorithm to work with a single-agent system and discrete objects: filled bags. I intend to generalize this work to construct any arbitrary structure, by developing new planning policies for the order of voxel construction based upon the structure shell and other parameters. Additionally, I will incorporate policies for a range of materials: continuous materials such as foam, and discrete amorphous objects such as filled bags. Broader Impact: The broader impact of the project lies in its motivation: autonomous construction can be used to save lives in disaster areas, and to build structures on different planets. Robots can stabilize buildings and make rubble navigable in areas of disaster, build levee walls to prevent flooding, build emergency shelters, and build structures on the moon and mars. Additionally, the project could have a parallel initiative in educational outreach, by incorporating high school and undergraduate students in important roles. Because the core of the project is small, highly modular robots whose abilities are augmented through collaboration, undergraduates and even high school students can take part in construction of the robots while learning about robotics and experiencing the environment of a research lab. Further, these modular robots can be easily integrated into demonstrations in schools to foster interest in robots and STEM in general. The coordinated robots could inspire elementary, middle, and high school students to become involved with robotics, and could form the foundation for a program to increase college attendance in STEM fields. [1] M. Allwright, N. Bhalla, C. Pinciroli, M. Dorigo, M. All- wright, N. Bhalla, C. Pinciroli, and M. Dorigo, “Towards Autonomous Construction using Stigmergic Blocks,” 2017. [2] J. Werfel, K. Peterson, and R. Nagpal, “Designing Col- lective Behavior in a Termite-Inspired Robot Construction Team,” Science, vol. 343, no. February, pp. 754–758, 2014. [3] N. Napp and R. Nagpal, “Distributed amorphous ramp construction in unstructured environments,” Springer Tracts in Advanced Robotics, vol. 104, pp. 105–119, 2014."	0
Introduction Many metabolites are reactive and unstable, making them prone to undesired chemical modification outside of the intended pathways1. While metabolism as a whole is well-studied, the biochemical mechanisms of managing such reactive metabolites are not. Proteins which are closely metabolically involved with each other can frequently be found in protein-protein interactions which are essential for nearly all cellular function. It is currently believed that most, if not all, proteins participate in protein-protein interaction networks2. Such a network suggests the possibility of metabolic substrate channeling, in which a metabolite travels from one enzymatic active site to another without freely diffusing into the surrounding medium. Evidence for substrate channeling has been observed in enzymes of many major biochemical pathways and is being increasingly recognized as foundational to metabolic regulation3. Through substrate channeling, an intermediate metabolite may be retained for use in a specific pathway, protected from degradation, or prevented from causing damage to the cell4. The goal of my proposed research is to discover mechanisms by which unstable metabolites are managed in biochemical systems. This research will provide insight into cellular control over reactive metabolites, protein-protein interactions, and substrate channeling. To achieve this goal, I will uncover mechanisms of substrate channeling for the reactive metabolite Δ1-pyrroline-5-carboxylate (P5C). P5C is an unstable intermediate at the intersection of proline, glutamate, and ornithine metabolic pathways (Figure 1). P5C has been shown to react with other metabolites and inhibit enzymes and is linked with human disease including hyperprolinemia type II5. Under physiological conditions, P5C exists in dynamic equilibrium with glutamic semialdehyde (GSA) via spontaneous nonenzymatic hydrolysis and condensation reactions. In the proline biosynthetic pathway, P5C serves as an intermediate for production of proline from glutamate. Glutamate is reduced to GSA by P5C synthetase (P5CS), then P5C is reduced to proline by P5C reductase (P5CR). Similarly, the glutamate biosynthetic pathway uses P5C as an intermediate in the production of Figure 1. Involvement of P5C with proline, glutamate from proline. Proline is oxidized to P5C by glutamate, and ornithine metabolism. proline dehydrogenase (PRODH), then GSA is Adapted from Stránská et al6. oxidized to glutamate by P5C dehydrogenase (P5CDH). In an alternative pathway, P5C can be produced or consumed by ornithine aminotransferase (OAT), which uses ornithine as a reactant or product for P5C production or consumption. Under normal physiological conditions, OAT functions in the “forward” direction from ornithine to P5C, however, under extreme dysregulation, OAT may catalyze the opposite “reverse” reaction from P5C to ornithine6. Aim 1: Protein-protein interactions among enzymes of the proline/glutamate/ornithine pathways. Existing research already supports protein-protein interactions between PRODH and P5CDH7. However, the involvement of other enzymes in the proline, glutamate, and ornithine pathways is unknown. This aim will focus on potential protein-protein interactions between P5CS–P5CR, OAT–PRODH, and OAT–P5CDH. Because of the current discord surrounding the cellular locations of P5CS and P5CR (e.g., mitochondrial vs. cytosolic), OAT–P5CS and OAT–P5CR complexes will also be considered. In fact, the expression in some organisms of ornithine cyclodeaminase, which directly catalyzes ornithine to proline, supports potential protein-protein interactions between OAT and P5CR8. Enzymes used in these studies will be expressed as His-tagged proteins, as done previously with PRODH and P5CDH7. Stable protein-protein interactions will be examined through coimmunoprecipitation with anti-His-tag antibodies and supplemented with pull-down assays via Ni-NTA chromatography. In each experiment, the identification of specific proteins will be accomplished through Western blot. Transient protein-protein interactions will be observed through surface plasmon resonance (SPR) at the BIAcore 3000 instrument maintained by the Nanomaterials Characterization Core Facility at the University of Nebraska Medical Center. For SPR, an enzyme’s His-tag will be used to anchor the protein to a Ni-NTA sensor chip. SPR will allow the characterization of protein-protein interaction association and dissociation constants. For cellular characterization of protein-protein interactions, fluorescence resonance energy transfer and yeast two-hybrid system experiments will be carried out2,7. Aim 2: Kinetics of P5C channeling among enzymes of the proline/glutamate/ornithine pathways. Even in the case of weak, transient protein-protein interactions, substrate channeling may occur. Substrate channeling of P5C has been identified in the glutamate biosynthetic pathway but has yet to be characterized among other protein pairs of the proline, glutamate, and ornithine pathways7. Previous studies with PRODH and P5CDH have used a free diffusion two-enzyme model to simulate reaction progress curves. In these models, the presence of substrate channeling can be inferred by comparing theoretical non-channeling versus experimental differences in transient time7. In addition, P5C trapping studies will be carried out to support results from the simulated progress curves. In these experiments, ortho-aminobenzaldehyde (oAB) is conveniently used to “trap” P5C in a spectrophotometrically-monitored oAB–P5C complex. In the presence of P5C channeling, less P5C will freely diffuse to be complexed with oAB as compared to a negative non-channeling control. In addition, I will use stopped-flow kinetics to measure the transient time of P5C channeling between the enzymes. Intellectual Merit My proposed graduate research plan to examine unstable metabolite management is a natural continuation of research I have been involved with in the Becker lab at the University of Nebraska-Lincoln. Previously, I conducted research investigating P5C channeling in the glutamate biosynthetic pathway, so I am well-prepared to expand into protein-protein interactions and broader aspects of substrate channeling. I have experience with fundamental methods used for protein and metabolic research, including protein overexpression, purification, UV-visible spectroscopy, stopped-flow, and steady state enzyme kinetics. This research will be well-supported in the Becker lab at the University of Nebraska-Lincoln, home of the Center for Biological Chemistry and the Redox Biology Center. My projects will be pursued in collaboration with existing structural biology partners in the Tanner lab at the University of Missouri. Broader Impacts It is estimated that over 80% of proteins rely on protein-protein interactions2, and substrate channeling has been identified in many major metabolic pathways3. My research will provide specific knowledge in unstudied interactions and channeling between proteins of the proline, glutamate, and ornithine pathways, offering insight into the biochemical methods of unstable and reactive metabolite management. Additionally, this research will serve as a case study to lay the foundation for metabolite management experiments in other major pathways. Research surrounding P5C has implications for biological mechanisms of metabolism relevant to all life. Outside of strict academics, I will leverage this graduate research in a way that benefits the scientific community and the general public. I will present results at regional and national conferences and publish using accessible language in open-access journals. Through this research, I will take advantage of important mentorship opportunities. Mentoring undergraduate or beginning graduate students in the lab will allow me to help develop the next generation of scientists as I provide a rigorous yet supportive environment to foster academic, professional, and personal growth. References 1Lerma-Ortiz et al. Biochem. Soc. Trans. 2016. 2Berggård et al. Proteomics. 2007. 3Sweetlove et al. Nat. Commun. 2018. 4Liu et al. Arch. of Biochem. & Biophys. 2017. 5Farrant et al. J. Biol. Chem. 2001. 6Stránská et al. Plant Sig. & Behav. 2008. 7Sanyal et al. J. Biol. Chem. 2015. 8Goodman et al. Biochem. 2004.	0
Title:Investigating Maine’s Indigenous Fire Prehistoryto Inform Forest Management Under Global Change With climate change amplifying underlying environmental issues, modern wildfires have become enormous devastating forces, costing lives, our natural resources, and billions of dollars. Early US Forest Service practices focused on fire suppression as a management tool, which increased the presence of underbrush, snags, and flammable material in forests1.Though these practices have changed, those initial management plans coupled with drought, eco-tourism, and rising temperatures have led to the large-scale, uncontrollable, high-intensity fires in the West that, as of October 1, 2020, have burned nearly 7.7 million acres this year2. In contrast, indigenous fire management has played an important role in the ecology of many North American landscapes for thousands of years. Native peoples used fire to clear the land for cultivation, promote healthy and diverse food-rich forests, and facilitate diverse wildlife habitat3. These practices have largely been excluded in modern forest management plans. By the 1970’s the Forest Service began utilizing indigenous knowledge to set controlled burns in the West, Southwest, and Southeast, but not in the mixed hardwood forests of the Northeast1, wherefire is not widely considered to be an important process4. However, there have been large-scale destructivefires in the last century, like the Great Fire of 1947 in Maine. This drought-intensified fire consumed 17,188 acres, destroyed 240 buildings, and cost over $23 million in property damages5. As climate change is causing warmer temperatures and droughts in the Northeast6, there remains a critical need tounderstand the long-term history of fire (both natural and anthropogenic) in this region. Most of our academic knowledge about indigenous fire use in New England is largely based on journals and other written accounts by European settlers. However, those records lack the perspectives of indigenous people, and only explain fire use post-contact7.Historical observations and Wabanaki oral knowledge indicate that, within the last 500 years, Native peoples used fire to clear land for agriculture, and to improve hunting grounds south of the Kennebec River in Maine. Penobscot place names describe areas that experienced regular burning. For example, Schoodic (skudek) Peninsula, a part of Acadia National Park, means “burnt-land”8. Long-term fire records from lake sediment cores and tree rings have provided another valuable source of information about the relationships between fire, climate, vegetation, and people in the American West, Midwest, and Southeast, but are still lacking for the Northeast, including New England. A recent study synthesizing charcoal patterns across New England found no evidence of pre-European anthropogenic fire use, but this reflected a regional fire record that would not highlight the more localized scale at which indigenous peoples would have been burning4. Charcoalrecords in the Northeast have primarily been taken from large bodies of water9, which are biasedtowards large regional fires, instead of local, low intensity fires, which would have been the types of fires Native peoples used for land management4. Therefore, while previous paleoecological studies have been important for understanding the large-scale fire prehistory of New England and its relationship to climate, they are poorly suited to the study of anthropogenic fires. And, by failing to partner with Native scholars and incorporating oral knowledge of past land use, such studies mask indigenous peoples’ expertise and contributions to the health of the landscape10. Intellectual Merit My research goal is to reconstruct localized fire records in Maine to better understand fire as a prehistoric land management tool in New England. I will take a multi-pronged approach to this work: 1.) I will conduct an actualistic study to identify the signals of localized understory burns and small patch clearings in the charcoal and pollen records of forest hollows and small ponds. 2.) In collaboration with members of the Penobscot Tribe, I will collect sediment cores from small ponds and forest hollows near settlements and prehistoric hunting grounds to examine whether small-hollow cores can identify local, small-scale burning. 3.) I will then synthesize these findings with existing geoarchaeological, climate, and pollen records to assess the relationships between population and cultural shifts, climate, vegetation, and fire histories across scales. All of the necessary equipment and facilities to carry out this project are available at the University of Maine Climate Change Institute, and Dr. Gill is building tribal partnerships via collaborations with Penobscot faculty at UMaine: Dr. Darren Ranco, director of the WaYS program, and Dr. Bonnie Newsom, archaeologist. Partnership opportunities are also available at Acadia National Park through the National Park Service. Many fire-use studies focus on written accounts, the charcoal record, and tree scaring to reconstruct past fire regimes, but researchers have historically excluded indigenous communities when studying past human land use. My project will contribute to a more accurate historical record of prehistoric land use by better matching the tools to the questions to characterize anthropogenic fire histories and impacts. This project will also add to our understanding of charcoal records taken from small hollows. In contrast with the pollen record, hollow-based fire records are lacking, which limits our ability to interpret stand-scale fire impacts11. Broader Impacts Though fire is not considered to be an important process in the Northeast, with climate change exacerbating existing environmental issues, it is becoming an increasingly dangerous threat. This past summer, drought conditions and increased eco-tourism due to COVID-19 resulted in a summer of over 900 high-intensity, destructive fires in Maine6. Maine’seconomy depends on logging and tourism12and drought-induced fires put both of those industries at great risk. This study seeks to understand low intensity fires and will inform conservation and management practices. Such fires clear underbrush and snags, reducing the fuel load for uncontrolled fires. This would make Maine’s forests safer while also reducing tick populations by burning shrub species that foster these disease vectors13. Cleared underbrush would improve forest health by reducing canopy competition and eliminating weaker diseased trees. All of these benefits could increase timber quality and forest health, boosting two of the state’s major industries during a time of economic uncertainty. The Wabanaki Confederacy is a collection of Eastern Algonquin tribes including the Penobscot, Passamaquoddy, Mi'kmaq, and Maliseet people. I plan to use my research to contribute to the Native American Graves Protection and Repatriation Act (NAGPRA) by providing supporting evidence of long term tribal habitation. This project will contribute to a long-term collaborative relationship with local tribes and will provide critically needed information in support indigenous sovereignty claims. I also intend to collaborate with the NSF-funded Wabanaki Youth in Science (WaYS) program at UMaine. WaYS trains Wabanaki youth in both tribal knowledge and scientific approaches through summer camps and internships. I intend to include Wabanaki students in my project by bringing groups of students out into the field and mentoring students in the lab to learn sediment coring and paleoecological techniques. References. [1]Forest History Society.US Forest Service FireSuppression. [2]Congressional Research Service. 2020. 43. [3]Ryan K.C. 2013.Frontiers in Ecology and the Environment.[4]Oswald, W.W. 2020.Nature 3,241–246. [5]National Park Service, Acadia. 2020. [6]The Maine Monitor. 2020.Bangor Daily News. [7]Ruffner, C. M. 2005.USDA: Proceedings 16th CentralHardwood Forest Conference.[8]Francis, J.E. 2008.Farms, Forest, and Fire44(1): 4-18.[9]Patterson.1988.Holocene Human Ecology in Northeastern North America.[10]Kimmerer, R.W. and Lake, F. 2001.Journal of Forestry99(11):36-41. [11]Higuera P.E. 2005.The Holocene15(2): 238-251.[12]US Newsand World Report. 2020.Best States: Maine. [13]Gleim E.R. 2019.Nature.	0
Motivation and Background: Mammalian white adipose tissue (WAT) distribution and expansion is sex-dependent, with males preferentially accumulating visceral WAT (VWAT) and females exhibiting a subcutaneous WAT (SWAT) accumulation bias. Interestingly, females switch to a male-like pattern of WAT distribution after menopause when estrogen levels decline, indicating sex hormones play a role in the distribution of subcutaneous and visceral WAT mass, yet the molecular mechanisms governing these processes in vivo are not well understood. WAT distribution is strongly correlated with the development of pathologies related to obesity, with accumulation of VWAT being more detrimental for metabolic health than accumulation of SWAT, which may confer protection against these pathologies. Our lab has shown that there is a sexually dimorphic pattern of adipocyte precursor (AP) activation in mice in response to high fat diet, with males having robust AP activation in the VWAT but not SWAT and females having activation in both VWAT and SWAT.1 Once APs are activated they commit to differentiating into mature adipocytes and thus contribute to WAT mass. Interestingly, the sex-specific AP activation pattern observed occurs in an estrogen-dependent manner. Therefore, estrogen levels appear to be crucial for AP activation and expansion of SWAT but not VWAT. Herein I propose to identify the role of estrogen signaling in sexually dimorphic WAT expansion and elucidate the mechanisms controlling differential AP activation in male and female mice. Hypothesis: Estrogen receptor alpha (ERα) is required for AP activation and expansion of SWAT and there are distinct molecular mechanisms driving WAT expansion in VWAT and SWAT, with VWAT expansion being independent of ERα activity. Aim1: Characterize the requirement of ERα in the activation of adipocyte precursors in SWAT. For this aim, we will knockout the Esr1 gene in APs using an inducible Cre-recombinase system driven by the AP-specific promoter PdgfRα (Figure 1).2 This will enable us to ablate ERα expression postnatally to avoid any developmental phenotypes. We will then test the proliferation of APs via incorporation of BrdU, a nucleoside analog, in these ERα-APKO mice upon high-fat diet (HFD) or standard diet feeding (SD). After the HFD-induced AP proliferation phase, incorporation of BrdU will be assessed in APs via flow cytometry. If ERα is required for AP proliferation in female SWAT, we expect to see a decrease in BrdU positive cells when challenged with HFD only in this depot. If ERα is also important for AP proliferation in male SWAT, we expect to see an even lower percentage of BrdU+ cells than wildtype (WT) littermates. We do not expect to see an impairment in AP proliferation in visceral fat in males or females. Aim2. Identify differences in WAT depot estrogen Figure 1. Adipocytes are derived from PdgfRa+ levels. Even though WAT can produce estrogen locally, our precursor cells. Fat from mouse strain with findings suggest that circulating levels of estrogen are fluorescent-membrane dTomato/ membrane eGFP required for SWAT AP activation but not VWAT AP (mT/mG) Cre reporter. Cre excision is marked by a switch from tdTomato expression to eGFP activation in both males and females.1 Therefore, I expression. PdgfRa-Cre labels all mature hypothesize that circulating levels of estrogen influence adipocytes in WAT but PdgfRa is not expressed in estrogen levels in SWAT but not VWAT to drive mature adipocytes, thus the GFP expression observed in adipocytes of PdgfRa-Cre:mT/mG adipogenesis upon periods of HFD. To test this, I will mice is due to lineage tracing.2 measure estradiol levels in the WAT depots of WT female and male mice on days 1, 3, and 5 of HFD or SD. Our lab has shown that activation of APs initiates on day 1 of HFD, with a peak on day 3, and returns to SD levels by day 5.3 Hormone extraction from WAT will be performed and levels of estradiol and estrone will be quantified by liquid chromatography tandem-mass spectrometry. I expect to see increased levels of estrogen in SWAT of WT female mice on day 3 of HFD compared to VWAT. Because WT males do not have significant circulating levels of estrogen, I do not expect to see a difference in VWAT and SWAT levels. I can also perform the same experiment in ovariectomized (Ovx) females and estrogen-treated males, where circulating levels of estrogen are diminished/increased respectively as compared to WT mice. If I see a decrease in estrogen levels in SWAT of Ovx females and an increase in SWAT of estrogen-treated males on day 3 of HFD as compared to WT, then circulating levels of estrogen influence SWAT levels of estrogen upon HFD and promote WAT expansion through estrogen signaling in this depot. Aim3: Elucidate distinct molecular mechanisms of adipogenesis in VWAT and SWAT. Our lab recently identified FOXM1, a nuclear fork box protein, as an important gene in male visceral AP activation (unpublished). Interestingly, FOXM1 has been shown to work with ERα to promote gene expression in a breast cancer model.5 Furthermore, when in the presence of activated ERα, FOXM1 drives the expression of a different gene program than when in absence of ERα.5 Therefore, we hypothesize that FOXM1 is important in adipocyte hyperplasia in both males and females but it acts through distinct molecular mechanisms depending on the presence of estrogen. To test this, we will perform RNAseq on isolated APs from both fat depots under HFD and SD conditions on WT and Ovx female mice. If FOXM1 is working with ERα to promote AP activation in SWAT, we expect to see an increase in gene expression in FOXM1-ERα targets only in subcutaneous fat of WT females. If we do not see this same pattern in the subcutaneous fat of Ovx females, but we do find increased gene expression of FOXM1 targets in the visceral fat, we can conclude that in the presence of estrogen, FOXM1 and ERα promote the activation of APs and expansion of subcutaneous WAT and in the absence of estrogen, FOXM1 alone promotes activation of APs and expansion of visceral WAT. To further confirm this, I will perform co- immunoprecipitation (co-IP) on isolated APs from both depots from SD and HFD-fed mice to assess if FOXM1 partners with ERα in SWAT but not VWAT of WT females. Intellectual Merit and Broader Impact: This study will clarify for the first time the mechanistic role of estrogen signaling in WAT and will significantly impact the field of adipose tissue biology. We will also set precedent on elucidating distinct sex-dependent molecular pathways governing WAT mass expansion. As a hispanic woman in science, my goal is to inspire others to pursue careers in science and become advocates for minorities in STEM. By sharing my research findings in activities coordinated by Yale organizations and minority-focused science conferences I plan to motivate not only undergraduate women and minorities to pursue careers in science, but I will also educate the greater community and general public about the importance of science education in order to advance knowledge beyond an academic environment. IACUC Approval: We have clearances and training for all handling and proposed mouse procedure (Yale IACUC protocol 2012-11249). The University’s Assurance number with the Office of Laboratory Animal Welfare is #A3230-01, approval through 5/31/19. IACUC oversees the University’s centralized, AAALAC-accredited animal resource, the Yale Animal Resources Center (YARC). References: 1Jeffery, E., et. al. (2016). Cell Metabolism, 24(1):142-50. 2Berry, R., Rodeheffer, M. (2013). Nature Cell Biology, 15(3): 302-308. 3Jeffery, E., et. al. (2015). Nature Cell Biology, 17(4): 376-385. 4Falk, R. T., et. al. (2008). Cancer Epidemiology, Biomarkers, and Prevention, 17(8): 1891–1895. 5Sanders, D., et. al. (2013). Genome Biology, 14:R6.	0
extreme hydrodynamic and aerodynamic loads on offshore wind turbines (OWTs), specifically wave and wind loads during hurricanes. To this end, I propose to numerically simulate OWTs subjected to extreme wind and waves using computational fluid dynamics (CFD). This research aims to advance basic understanding of OWT loads by answering the questions: 1. How do extreme hydrodynamic and aerodynamic loads on OWTs vary for different support structures and hurricane characteristics? 2. How do OWTs (especially floating OWTs) respond to hurricane wind and waves? Motivation: To meet the federal goal of 20% electricity from wind energy by 2030, the U.S. wind industry must expand to include offshore wind development. Offshore wind offers several advantages over onshore wind, including the mitigation of aesthetic and land use issues, as well as the utilization of abundant, high-quality offshore wind resources in proximity to population centers [1]. However, wind farms off the eastern and southern U.S. coast could be destroyed by hurricanes, unless their support structures are designed with such extreme loads in mind [2]. These designs require accurate load data, but experimental data is mostly unavailable due to the lack of OWTs in hurricane-prone areas. So, current OWT simulations find hydrodynamic and aerodynamic loads using simple empirical models, which are much less accurate than CFD [3]. This inaccuracy is catastrophic when designing OWTs to withstand hurricanes, so using CFD to better predict extreme loads will inform more robust OWT designs. Methods: To generate hydrodynamic and aerodynamic loads typical of hurricanes, I will numerically simulate OWTs subjected to extreme, hurricane-like wind and waves. These simulations will be done in the CFD software Converge from Convergent Science. Unlike most CFD software, Converge doesn’t require user-made meshes, which means researchers can complete simulations faster. Converge’s adaptive automatic meshing also improves solution accuracy for moving objects like floating OWTs, since the grid resolution adapts where necessary. Converge could also fix stability issues found when modeling floating OWTs in other CFD software like OpenFOAM [4]. I will first identify hurricane parameters characteristic of the U.S. coast, focusing on areas where OWT development is likely. I will then validate my predicted hydrodynamic and aerodynamic loads against experimental and numerical data: CFD-based numerical data for non-extreme waves is available from Benitz [4], while proprietary experimental data for loads from Hurricane Irene is available from industry collaborators. Aerodynamic loads will be validated against the open literature for onshore wind turbines. Finally, I will create databases of hydrodynamic and aerodynamic loads corresponding to various hurricane parameters for several support structure types. The predicted non-hurricane hydrodynamic loads will be validated for some OWT structures prior to the beginning of the proposed project: the proposed team (detailed below) is currently collaborating on a 1-year project on OWTs in breaking waves, which includes validating the predicted hydrodynamic loads on non-floating structures against numerical data and experimental data from industry partners. Deliverables: The main deliverables of the project and their estimated times of completion are: 1. Identify representative hurricane characteristics for the U.S. coast (2 months), 2. Validate aerodynamic loads against experimental data (3 months), 3. Validate hydrodynamic loads against experimental and numerical data (5 months), 4. Generate hydrodynamic and aerodynamic load databases for various hurricane parameters for non-floating structures (12 months) and floating structures (14 months). Collaborations: This work will involve collaboration between University of Massachusetts faculty from two departments, as well as collaboration with industry partners (Convergent Science and others in development). Dr. David Schmidt and Dr. Matt Lackner (Mechanical Engineering) have previously studied hydrodynamic loads on OWTs [3,4], and have access to the computing resources necessary for CFD. Dr. Schmidt also worked in Converge on other applications, and his long relationship with Convergent Science enables their collaboration and assistance in introducing hydrodynamics and aerodynamics as new applications. Dr. Sanjay Arwade (Civil Engineering) brings expertise in OWT support structures and hurricanes’ impacts on OWTs. Intellectual merit: The proposed project furthers basic scientific understanding of OWT loads, introduces better software for OWT modeling, and provides better data for structural models of OWTs. First, using CFD to study extreme hydrodynamic and aerodynamic loads on OWTs will improve fundamental understanding of how OWT support structures behave during hurricanes, which is currently hindered by overly simple models and a lack of experimental data. Second, this project will validate a faster, more accurate CFD software for OWTs and other ocean engineering applications. Third, this research will provide more accurate loads used in OWT structural analysis, like that done by civil engineers. These results will be distributed at wind energy and CFD conferences, in journal articles, and in my PhD dissertation. Broader impacts: By providing more accurate hydrodynamic and aerodynamic hurricane loads for use in structural OWT models, the proposed project allows for better designs of OWTs that can withstand hurricanes. Hurricane-resistant OWTs lower the risks of offshore wind, encouraging the widespread development of offshore wind energy in the U.S. and other hurricane-prone countries. In this way, the proposed research contributes to the growth of renewable energy on the national and global scale. References 1 Musial, W., and Ram, B. 2010. Large-scale offshore wind power in the United States: Assessment of opportunities and barriers. Technical Report NREL/TP-500-40745, U.S. Dept. of Energy, 1–221. 2 Wei, K., Arwade, S.R., Myers, and A.T. 2014. Incremental wind-wave analysis of the structural capacity of offshore wind turbine support structures under extreme loading. Engineering Structures 79, 58-69. 3 Benitz, M.A., Lackner, M.A., and Schmidt, D.P. 2015. Hydrodynamics of offshore structures with specific focus on wind energy applications. Renewable and Sustainable Energy Reviews 44, 692-716. 4 Benitz, M.A. 2016. Simulating the hydrodynamics of offshore floating wind turbine platforms in a finite volume framework. PhD thesis, University of Massachusetts - Amherst.	0
Zack Morrow Introduction and Previous Work At the molecular level, density functional theory (DFT) describes the electronic state of a system through functionals, which are mappings whose domain is a function space [1]. Existing chemistry software packages (e.g., Gaussian) use DFT to provide evaluations of a potential energy surface (PES) at molecular coordinate locations. In order to drive simulations of molecular dynamics, one needs potential energy in order to solve for the reaction path. However, the evaluation of the true PES at the internal coordinates is far too expensive to carry on a dense grid over the full domain. Sparse grids provide a means to keep the computational cost in check. We compute and store evaluations of the true PES only at the sparse points. We then build a sparse interpolating polynomial of the PES as a surrogate, which we evaluate on the full grid over the domain. The evaluations of the interpolant are significantly less expensive than the evaluations of the true PES; the main expense occurs in evaluating the true PES at the sparse interpolation points, which is done at the front end and stored for later use. Previous work in our group by James Nance investigated the Smolyak construction of sparse grids and its application to molecular dynamics and surface-hopping problems [2, 3]. Nance worked in collaboration with the research group of Prof. Elena Jakubikova in NC State’s Department of Chemistry, our current collaborators. The sparse grid code that Jakubikova’s group utilizes today is the code written by Nance as part of his thesis. Proposal: Intellectual Merit Nature tends to an energy-minimizing state, and accordingly, stable molecular geome- triescorrespondtolocalminimaofthepotentialenergysurface(PES).Inordertoconstruct the PES itself, we first take q to be a fixed geometry and find the electron energy levels E i by solving (e.g., with Gaussian) the time-independent Schr¨odinger equation, a well-known quantum mechanical relationship encoding the energy of a system: Hˆ Ψq = E Ψq, i ∈ {0,1,2,...}. (1) i i i ˆ Here, H is the molecular Hamiltonian operator (connected to the total energy of the sys- tem), the eigenvalue E is the energy of electronic state i under geometry q, and Ψq is the i i wavefunction corresponding to E (connected to the probability that a given electron in a i molecule under geometry q has energy E ). i Now, as a function of all admissible geometries q, we denote E (q) to be the PES i corresponding to energy state i. Again, stable molecular geometries correspond to local minima on the PES; moreover, transitions of a molecule from energy state i + 1 to state i occur when the molecular geometry corresponds to a local minimum on E (q). The i+1 reaction path to an energy-minimizing configuration is the solution to q˙ = −∇E (q), (2) i which, pictorially, is the path of steepest descent on the PES from the initial geometry to a local minimum. Using Equation (2), we can simulate numerically how molecular geometry evolves in time after excitation to higher energy states and subsequent relaxation. Since the true PES E (q) is far too expensive to compute on the full grid, we generate a i set of sparse grid points and utilize the sparse interpolating polynomial Es(q) as a surrogate i for the true PES in our simulations. Currently, we use a MATLAB code to manage the generation and evaluation of the sparse interpolant Es(q), and the dynamical simulation i routines run on university desktops and laptops. However, the simulation code will soon be migrating to the XSEDE platform, a geographically distributed computing cluster on which it is infeasible to use MATLAB. We are therefore replacing the MATLAB sparse-grid manager with an open-source C++ sparse-grid manager called Tasmanian, developed at Oak Ridge National Laboratory, which additionally comes with a Python wrapper [5]. I have replicated previous PES results using the new Tasmanian package and am cur- rently working to integrate it fully into our dynamical simulation codes. One difficulty to resolve is that Tasmanian currently does not compute gradients internally—a problem when using Es(q) in Equation (2). To boost accuracy and exploit parallelizability, gradi- i ents are best evaluated inside the sparse-grid manager itself. I will spend next summer at Oak Ridge working to add gradient-computation routines to the Tasmanian package, which must meet rigorous Department of Energy software standards. In the meantime, I am familiarizing myself with the high-performance computing environment at NC State, in addition to becoming as comfortable with C++ as I am with Python and MATLAB. Beyond next summer, I will very likely need to refine the gradient routines, and I will also continue my literature review on quantum chemistry and the analysis of sparse grids. Proposal: Broader Impacts Sparse grids as a general category have ready applications in any field where compu- tational cost is a major concern. In the realm of chemistry, computationally tractable methods of handling high-dimensional DFT-driven dynamical simulations have a wide ar- ray of benefits to society, including public health, renewable energy, and national security. The research I propose to undertake can advance the state of the art of simulations in pharmacological modeling, photochemistry, conversion in solar cells, nuclear chemistry, and nuclear power [3, 4], to name a few. Additionally, the gradient routines that I will add to the Tasmanian code will be useful to all who use the publicly available Tasmanian package to manage sparse grids in their research. Conclusion Sparse grids are powerful tools, making simulations that were once intractable become feasible. I possess the drive, prior computational laboratory experience, and proven aca- demic background to carry this project through to completion and to communicate my results to both specialist and non-specialist audiences. [1] Hohenberg,P.andKohn,W.“InhomogeneousElectronGas”.Phys. Rev.136.3B(1964),B864–B871. [2] Nance, J. and Kelley, C. T. “A Sparse Interpolation Algorithm for Dynamical Simulations in Com- putational Chemistry”. SIAM J. Sci. Comput. 37.5 (2015), S137–S156. [3] Nance,JamesD.“InvestigatingMolecularDynamicswithSparseGridSurrogateModels”.PhDthesis. North Carolina State University, 2015. [4] Peherstorfer,Benjaminetal.“SelectedRecentApplicationsofSparseGrids”.NumericalMathematics: Theory, Methods, & Applications 8.1 (2015), pp. 47–77. [5] Stoyanov, M. TASMANIAN Sparse Grids. Tech. rep. ORNL/TM-2015/596. ORNL, 2015.	0
global coral reef mortality. These stressors may reduce reef resilience by stimulating macroalgal growth and competition [1]. Parrotfish control macroalgae through herbivory but often predate corals to supplement their diets. This causes some coral tissue damage in the form of individual lesions, but rarely causes total colony mortality [2]. However, Zaneveld [1] surprisingly found that in waters enriched with nitrogen and phosphorous, colony mortality increased from zero to ~65% in Porites colonies after parrotfish predation, but why this occurred was unknown. For my dissertation project I aim to study if and how the combined stressors of predation and nutrient enrichment disrupt coral physiology and/or their microbiomes to cause this increase in mortality. The coral holobiont is a dynamic assemblage of the coral animal and its associated microorganisms such as bacteria and algae which collectively make up the microbiome. Elevated nutrients can alter the abundance and types of coral-associated mutualistic algae in the genus Symbiodinium [3]. Environmental stressors cause shifts in healthy coral-associated bacteria [4], that may provide antibiotic activity against invasive microbes and pathogens [5]. Zaneveld [1] found that the combination of predation and nutrient enrichment increased the amount of potentially opportunistic bacteria when compared to proposed coral mutualists. My project will determine if parrotfish are a vector for bacterial opportunism and/or if nutrients drive an increase in host susceptibility to infection following wounding by predation. Hypothesis: I hypothesize that nutrient enrichment and predation interact to cause two major changes to the coral holobiont that result in coral death: reduced host immunity and the proliferation of pathogens. My work will focus on the following questions: Q1. How do bacteria in the coral mucus protect against predation-mediated mortality in water with ambient nutrient levels? I hypothesize that coral mucus already possess specialized microbiota that protect corals from pathogens via several testable mechanisms such as antibiotic production, competitive exclusion, or predation. Q2. How do nitrogen and phosphorous alter coral and algal symbiont physiology and the microbiome? I hypothesize that a decrease in host immunity and destabilization of the Symbiodinium community will combine to reduce the holobiont’s ability to regulate its microbiota. I also predict that the microbiota with anti-pathogen activity identified in Q1 will be reduced and opportunistic bacteria will increase in nutrient enriched treatments. Q3. Is the microbiome-dependent route to coral death driven by physical wounding or predator specific corallivory in nutrient enriched waters? I hypothesize that parrotfish serve as vectors for the proliferation of pathogens in the mucus around the wound site. Alternatively, I hypothesize that any wounding in the presence of elevated nitrogen and phosphorus provides a route to enhanced bacterial infection. Research Plan: These questions will be addressed at the Gump South Pacific Research Station on Moorea, French Polynesia, through two complementary experiments: on the reef and in controlled tanks. While I expect to see similar changes in microbial communities and host health between the two experiments, each will provide a specific component to my investigation. Using SCUBA, individual Pocillopora colonies will be transplanted to saltwater mesocosm tanks. In the tanks, I will pre-expose corals to an antibiotic mix [6] to deplete the bacterial community. The types, concentrations, and exposure length will be determined the year prior to the experiment. Then I will move the treated and untreated corals to new tanks with natural seawater or to the field for monitoring. I will simulate predation in the tanks by physically wounding the coral and track host immunity to determine if the host alone is capable of preventing mortality or if associated microbiota are necessary for defense and recovery (Q1). On the reef, a subset of corals will be exposed to parrotfish predation while others will be shielded from predation with herbivore exclosures (Q3). A subset of the colonies in both experiments will be maintained at ambient nutrient levels while others will be enriched using slow-release fertilizer diffusers (Q2). N and P concentrations will be comparable to those on reefs impacted by nutrient pollution [1]. Phase 1. Simulate the effects of predation, nutrient loading, or a combination of these stressors with manipulative experiments on the reef and in tanks. At regular intervals, 1) photographically monitor coral tissue growth/loss and coral mortality, 2) record dissolved organic nitrogen and soluble reactive phosphorus concentrations via autoanalyzer, 3) measure Symbiodinium density with Pulse Amplitude Modulation, 4) measure bacterial respiration with oxygen probes, 5) count mucus associated bacteria with epifluorescence microscopy, and 6) sample coral tissues for DNA/RNA, taking care to minimize any serious damage to the coral. Phase 2. Track changes in the holobiont. For bacterial community dynamics, extract DNA from mucus to generate microbial 16S amplicon libraries [1] and metagenomes [4] for bacterial functional analysis. For Symbiodinium and host gene expression changes, extract RNA and DNA from tissue for RNAseq as well as for ITS-2 amplicon libraries [3]. Phase 3. Sequence the libraries on Illumina platforms at OSU’s Center for Genome Research. Phase 4. Use bioinformatics (e.g. QIIME [7], Shotmap [8]) and statistical pipelines (e.g. STAMP [9]) to analyze changes in structure and function of microbial communities and in gene expression patterns of innate holobiont immune responses. Predictions: Antibiotic producing bacteria, not host immunity, will be the primary defense against coral tissue loss or mortality from predation or wounding. Nutrient enrichment in combination with predation or wounding will lead to coral mortality. Coral mucus will exhibit an increase of one or more pathogenic strains, either found in low abundance in the communities of control colonies or absent from control colonies and therefore introduced by parrotfish predation. Coral mucus will also exhibit a decrease of one or more strains with antibiotic capabilities. I will identify the proliferated pathogenic strain(s) and the reduced defensive strain(s) thereby identifying the microbial route to colony mortality. Intellectual Merit: The Zaneveld study [1] is the first to document increases in predator- mediated mortality in the presence of elevated nutrients. Parrotfish herbivory is accepted as beneficial to coral reefs, and parrotfish predation is accepted as normally benign. My study will pin down the mechanism(s) in which these herbivores become agents of mortality and will transform how we approach the conservation of coral reefs, and more specifically, trophic interactions on a reef. Restoration of parrotfish populations may have negative consequences for coral health if efforts are not simultaneously made to combat water quality issues. Broader Impacts: During the field season in Moorea, I will design and conduct interactive teaching workshops for the local community similar to my outreach as an undergraduate. Using resources at the Gump Station and connections with the Atitia Center for outreach, I will print 2- D reef replicas of my nutrient-enriched and ambient level in situ corals over time for use in citizen science training. Local schoolchildren and adults will use the photos along with quadrats, transect tape, identification guides, and whiteboards to ‘become’ a marine biologist for a day. I will guide participants in using quadrats to quantify metrics of reef change, such as percent live coral cover. Children and adults will experientially observe how nutrients such as fertilizers affect the health of marine species. Through this citizen science initiative, I hope to transform how local communities understand and interact with their coral reefs. Citations [1] Zaneveld et al. (2016) Nat Commun, [2] Rotjan & Lewis (2008) Mar Ecol Prog Ser, [3] Correa et al. (2009) Coral Reefs, [4] Vega Thurber et al. (2009) Environ Microbiol, [5] Ritchie (2006) Mar Ecol Prog Ser, [6] Glasl et al. (2016) The ISME Journal, [7] Caporaso et al. (2010) Nat Methods, [8] Nayfatch et al. (2015) PLoS Comput Biol, [9] Parks et al. (2014) Bioinformatics.	0
"language pairs and translation tasks. However, two of the most pressingopenproblems inNMT are domain adaptation and low-resource scenarios [1] (i.e., language pairs for which large parallel corpora do not exist). This project will address both issues via cross-lingual domain adaptation in an extremely low-resource setting. The work is motivated by an urgent humanitarian crisis: refugees seeking asylum in the US who speak only Central American indigenous languages like K’iche’, Mam, Kanjobal, and Mixtec [2]. The scarcity of interpreters in these languages makes itdifficult for speakers toaccess legalservices,andexistingnonprofits and NGOs providing legal aidto refugeesdonothaveadequate resourcestoprovideinterpreters. Additionally, existing methods for low-resource NMT do not scale down to the extremely low-resource situation for these Central American languages. Improving both extremely low-resource applicability anddomain adaptationfor NMT will increasethenumberof scenarios in which NMT is a viable solution. Additionally, domain adaptation techniques that workinthe challenging low-resource setting will also improve NMT domain adaptation in higher-resource settings. Thus, this project addressesachallenging andwidelyapplicable scientific problemwith immediate humanitarian impacts. Objectives and Hypothesis: The objective of my proposed research is atwo-prongedapproach to domain adaptation in low-resource NMT. I will address the domain issue by fine-tuning a pretrained NMT model on synthetic parallel data generated via backtranslation [3] of monolingual in-domain data in the target language. I will first validate my approach in a high-resource setting by fine-tuning a baseline Spanish-English model using backtranslated in-domain English data. Independent of the performance on low-resource languages, a Spanish-English translation system that is well-adapted for asylum testimonials will be of great use to organizations providing legal aid to refugees. Then, I will apply the fine-tuning via backtranslation approach to the low-resource case. In order to successfully backtranslate the English finetuning data, a decent English-LR model is needed. Thus, I plan to apply a combination of unsupervised NMT, transfer learning, multilingual translation models, and various data augmentation approaches totheproblem ofextremelylow-resource NMT.Although various methods to augment parallel corpora have been proposed [4-7], they typically augment corpus size from a few hundred thousand sentences to a few million . They do not adequately address a context in which only thousands or tens of thousands of sentences are available, as is the case for K’iche’ and other indigenous languages. This project will develop a linguistically-informed method to generate enough plausible, syntactically correctsyntheticdata that existing corpus augmentation techniques like backtranslation can be applied, bridging the chasm between researchers’ assumptionsabout theamount of availablemonolingual dataand the reality for many low-resource languages. Method and Research Plan: In this research, I will use K'iche as a case study language to develop methods, then I will apply those methods to Kanjobal, Mam, and Mixtec. BLEU score, the standard evaluation metric for machine translation [8], will be the primary metric for evaluation of this proposed research. Possible BLEU scores range from 0 to 100, with higher scores indicating a closer match to the reference translation. In my exploratory work on the Magdalena Peñasco Mixtec dialect,achievinga maximumBLEU scoreof7with amodel trained from scratch on 8000 Mixtec-English parallel sentences. After this preliminary work, I decided to use K’iche’ as the test language in further study for two reasons: first, monolingual K’iche’ speakers are more common among migrants to the US than monolingual Mixtec speakers; second, there are more than fiftydistinct dialectsof Mixtec,each withvaryingdegrees ofmutual intelligibility, which makes collection and cleaning of data intractably difficult. The first year of my work will focus on domain adaptation to asylum testimonialsin the high-resource Spanish-English setting. High-resource NMT models are primarily trained on parallel data extracted from news articles, while LR models are generally trained on whatever portions of the Bible are available for a givenlanguage.Because theultimate goal ofmyproject is to translate first-person testimonials ofasylees aboutthe violencefromwhich theyarefleeing, I needto adapttheNMT modelsto fluentlytranslatefirst-person narrativesandto understandthe specific vocabulary relevant to asylees. One way to accomplish this adaptation is to fine-tune a pretrained translation model on in-domain data. While parallel data is ideal,itis also possibleto use monolingual target-language data via backtranslation. I plan to use the transcripts from the MALACH dataset [9], compiled from the USC Shoah Foundation’s Video History Archive, which collects testimonials from Holocaust survivors. I hypothesize that these testimonials are similar enough to the stories of modern asylees to be useful as in-domain training data. In the subsequent years of the project, I will apply the domain adaptation technique developedin thefirstyear totheextremelylow-resource setting.Thefirst priority for Year2is to gather and clean a K’iche’-English parallel corpus (at least the New Testament and some Jehovah’s Witness literature is readily available). Next, I will establish a baseline machine translation result for K’iche’-Englishusing available parallelcorporawithnodata augmentation. Then, I will apply the same backtranslation for domain adaptation technique used for Spanish-English. However, I expect that the backtranslation results will be unsatisfactory because of the limitations of the English-K’iche’ model used for backtranslation. Thus,I planto develop a data augmentation scheme syntax-aware compositional data augmentation methods from [10], withsome simplifications necessaryduetodata scarcity. Sentencesproducedwiththis method are guaranteed to be grammatically correct, but may not make sense semantically; however, they are much easier to produce than semantically correct sentences, especially in a low-resource setting. This research will investigate whether NMT systems can learn useful information from synthetic data that is syntactically, but not semantically, correct. In additionto data augmentation, I plan to explore whether the K’iche’-English and English-K’iche’ models can be further improved by transfer learning and unsupervised NMT techniques. Intellectual Merit:Thiswork addressesacrucial gapincurrent methodsfor low-resource NMT: cases where there are fewer than 10,000 sentences of monolingual data available. My proposed data augmentation method would allow researchers to generate enough synthetic monolingual data to apply state-of-the-art methods in low-resource NMT to languages that would otherwise be intractable due to lack of data. If this data augmentation is successful,it willshowthat NMT systems can learn grammar and syntax from synthetic training data that is semantically very noisy, opening up future research on how exactly NMT systems learn grammar and how this learning differs from learning vocabulary. Broader Impacts: The human rights impact of my proposed research is immediate and transformative. Interpreters for K’iche’ and related languages are hard to find and often prohibitively expensive. Translation systems, even imperfect ones, would allow non-governmental organizations and pro bono immigration lawyers to help asylees they were previously unable to serve. Widespread access to legal assistance would speed up backlogged immigration courts and give thousands of asylum seekers per year a chance to enter the US legally. This work could save lives, because many of these asylees are children and teenagers fleeing from horrific gang violence. References [1] Koehn, Philipp, and Rebecca Knowles. ""Six Challenges for Neural Machine Translation."" Proceedings of the First Workshop on Neural Machine Translation. 2017. [2] Medina, Jennifer. “Anyone Speak K'iche' or Mam? Immigration Courts Overwhelmed by Indigenous Languages” New York Times (2019). [3] Sennrich, R., Haddow, B., & Birch, A. (2015). Improving neuralmachinetranslationmodels with monolingual data. arXiv preprint arXiv:1511.06709. [4] Zhang,Jinyi, andTadahiroMatsumoto. ""CorpusAugmentationbySentenceSegmentation for Low-Resource Neural Machine Translation."" arXiv preprint arXiv:1905.08945(2019). [5] Li, Hongheng, & Heyan Huang. “Evaluating Low-Resource Machine Translation between Chinese and Vietnamese with Back-Translation”. arXivpreprint arXiv:2003.02197(2020). [6] Currey, Anna, Antonio Valerio Miceli-Barone, and Kenneth Heafield. ""Copied Monolingual Data Improves Low-Resource Neural Machine Translation."" WMT 2017. [7] Fadaee, Marzieh et al. ""Data Augmentation for Low-ResourceNeuralMachine Translation."" ACL 2017. [8] Papineni, Kishore, et al. ""BLEU: a Method for Automatic Evaluation of Machine Translation."" ACL 2002. [9] Ramabhadran, Bhuvana, et al. USC-SFI MALACH Interviews and Transcripts English – Speech Recognition Edition LDC2019S11. [10] Andreas, Jacob. ""Good-enough compositional data augmentation."" arXiv preprint arXiv:1904.09545(2019)."	0
Background and Rationale: Cerebral Arteriovenous Malformations (cAVMs) are congenital vascular lesions that affect 0.01-0.50% of the population. The annual risk of hemorrhage in the cAVM nidus is on average 3%, but the risk can be as low as 1% to as high as 33% depending on patient-specific anatomies. Since the cAVM nidus has low resistance, it allows blood to shunt directly from arterial feeders (AFs) to draining veins (DVs) at high flow rates. As a result, if the hemodynamic stresses exceed the elastic modulus of the vessel wall, then a rupture may form and cause a hemorrhage. Embolization – the intravascular injection of embolic materials to AFs, is one of the most common interventional therapies to prevent hemorrhages by diverting blood flow away from the nidus. However, current methods to visualize blood flow patterns for embolization treatment planning are limited. Using the standard-of-care – 2D superselective angiogram sequences (2D+τ) and 3D rotational angiography (3DRA) – is challenging because the contrast agent reaches multiple regions of the nidus simultaneously, which prevents the identification of cAVM compartments and fistulae. Therefore, there is an urgent need to elucidate blood flow patterns in the cAVM to improve on treatment planning for embolization. One method to address this problem involves computational fluid dynamics (CFD) to simulate cAVM hemodynamics on patient-derived models. However, the length-scale of the nidus prevents CFD from being applied because the spatial resolution of 0.6mm in 3DRA is insufficient to capture the 0.1mm or smaller diameters of intranidal vessels. As an alternative, some studies have resorted to using electric network models, but these models are typically not based on clinical data. Instead of modeling every intranidal vessel in the nidus, I could model all the intranidal vessels collectively through a porous volume [1], which has been shown to be a good approximation of the nidus [2]. Simulating the density of intranidal vessels beyond the spatial resolution limitations of 3DRA is possible because voxel intensity is proportional to the amount of contrast agent in a vessel [1], which provides information on the internal geometry of the nidus. Based on Darcy’s law and mass conservation, there are seven parameters that characterize blood flow through the porous volume: porosity, permeability, fluid viscosity, fluid density, quadratic drag factor, and the velocity and pressure boundary conditions [1]. These parameters can be inferred from 3DRA images. However, existing models [1], [3] lack validation with other paradigms, such as in vitro testing, and the models are limited to Types IIa, IIb, and IV in the Yakes classification of cAVMs, which are not representative of the population since there are a total of six types. My research objective is to investigate cAVM hemodynamics for embolization planning through generalized patient-specific CFD models for each angioarchitecture type in the Yakes classification, with a porous volume representing the nidus, and validate in silico results using an in vitro flow loop. Specific Aim 1: Develop simplified cAVM digital phantoms for validation with varying spatial porosity distributions. I will design digital phantoms on SolidWorks (Dassault Systèmes, Vélizy- Villacoublay, FR) with one tubular AF and DV connected to a rectangular porous volume. The phantom will be meshed using a triangular grid of approximately 104-105 elements for a cell density of around 16 cells per voxel [1] using Gambit (Ansys, Canonsburg, PA). The velocity boundary condition at the AF inlet will be set according to phase-contrast magnetic resonance angiography (PC-MRA) images, while a zero-pressure boundary condition will be used at the DV outlet. Blood viscosity will be set to 4.00cP, density to 1060 kg/m3, and Re to 265. Fluent (Ansys, Canonsburg, PA) will be used to run the CFD simulation. To validate, I will confirm that the fluid behaves according to expectations where flow paths will mostly circulate in nonporous regions and that flow through porous regions will obey theoretical expectations from Darcy’s law. Specific Aim 2: Construct generalized patient-specific cAVM models. I will construct the boundaries of the cAVM and the AFs and DVs from 3DRA images. This will be done through segmentation (3D Slicer, Boston, MA), and I will obtain morphometric measurements using Analyze 12.0 (AnalyzeDirect, Overland Park, KS). Based on the segmented anatomies and patient- specific measurements, I will create generalized models for each cAVM angioarchitecture type on SolidWorks. Afterwards, I will mesh the models using a tetrahedral grid on Gambit. The porous volume parameters will be inferred from 3DRA images and then averaged. As before, I will set the fluid properties of blood based on nominal values. I will validate mesh convergence through a mesh independence study on Fluent by comparing coarse vs. medium, medium vs. fine, and coarse vs. fine meshes. Moreover, I will compare porosity distributions through CFD between the generalized models and five different patient-specific anatomies for each angioarchitecture type. Specific Aim 3: Develop a flow loop for in vitro validation. I plan to modify an existing flow loop in Prof. Ajit Yoganathan’s lab to simulate cAVM hemodynamics. For example, the flow loop has two pressure measurement probes, which is insufficient for cAVM simulation. I will make modifications to incorporate more probes to accommodate for all the AFs and DVs. From the models used in the in silico study, I will manufacture transparent rigid physical phantoms using an MR-compatible resin (Watershed 11122, DSM Somos, Elgin, IL). I will then implement the same inflow conditions from the computational study to the flow loop and compare flow field data through PC-MRA and digital particle image velocimetry for validation. Furthermore, I will also manufacture the five patient-specific anatomies for each angioarchitecture type in the Yakes classification from Specific Aim 2 and compare in vitro flow field results to in silico results. Timeline and proposed laboratory: I would like to work with Prof. Ajit Yoganathan (Georgia Tech) because of the close alignment of research interests. I anticipate that this study will take five years: one for Specific Aim 1, and two each for Specific Aim 2 and Specific Aim 3. Intellectual Merit: Currently, only Orlowski et al. [1], [3] have used a porous volume to simulate the cAVM nidus. My project expands on this model, in that there have been no published papers that uses a generalized CFD model based on patient-averaged data, an in vitro flow loop to validate and compare with in silico findings, and a porous model to simulate the cAVM nidus. As post-embolization complications are a major concern, my computational model can serve as the first step to a surgical planning software that meets this need. Broader Impact: With further exploration, the computational model proposed can be expanded to a two-fluid model for simulating the propagation and solidification of embolic therapies, which can provide pre-operative outcome prediction, potential increase in embolization session efficiency, and optimize interventional strategies. My project may also be scaled-up for interventional planning in other AVMs situated in the lung, muscle or bone, prognosis evaluation, and optimization of therapies. Finally, I will collaborate with clinicians at Emory University and University College London to ensure clinical utility and present my work at conferences. Feasibility: From my master’s thesis, I will be supported by University College London, University College Hospital, and Kings College London to obtain 3DRA and PC-MRA patient data. I will seek Institutional Review Board approval under Prof. Yoganathan’s support. [1] P. Orlowski, F. Al-Senani, P. Summers, J. Byrne, J. A. Noble, and Y. Ventikos, “Towards Treatment Planning for the Embolization of Arteriovenous Malformations of the Brain: Intranidal Hemodynamics Modeling,” IEEE Trans. Biomed. Eng., vol. 58, no. 7, pp. 1994–2001, Jul. 2011. [2] C. W. Kerber, S. T. Hecht, and K. Knox, “Arteriovenous malformation model for training and research,” AJNR Am. J. Neuroradiol., vol. 18, no. 7, pp. 1229–1232, Aug. 1997. [3] P. Orlowski, P. Summers, J. A. Noble, J. Byrne, and Y. Ventikos, “Computational modelling for the embolization of brain arteriovenous malformations,” Med. Eng. Phys., vol. 34, no. 7, pp. 873–881, Sep. 2012.	0
INTEGRATING CONNECTED PEDESTRIANS IN INTELLIGENT INTERSECTION CONTROL SYSTEMS (IICS) Keywords: Automated Vehicle, Pedestrians, V2X Connectivity INTRODUCTION Broader Impact: With approximately 1.3 million road traffic deaths worldwide, roadway safety is a major health problem that affects humans around the globe. Nine out of ten of those serious roadway crashes are due to human behavior. Emerging automated and connected vehicle technologies have potential to transform the transportation system by removing fatal human errors. Using a variety of sensors and terrain information, automated vehicles (AV) and connected vehicles (CV) will have the ability to communicate with infrastructure (V2I), surrounding vehicles (V2V), and all roadway users (V2X). While embracing technology, engineers must consider all road users; not just automobiles. Complete Street policies adopted across the country and require streets to be planned, designed, and maintained to enable access for users of all ages and modes of transportation. Advances in AV and CV technology must integrate all modes transportation to achieve a safer and more efficient transportation system. Intellectual Merit: AV technology removes the ability for pedestrians to visibly communicate (i.e. eye contact) with a driver. One possibility of keeping pedestrians connected, proposed in the project, is based on connected pedestrian detection. With a large portion of the US population in possession of a “smart” or connected device, pedestrians may have the ability to connect with the infrastructure and vehicles. Pedestrians would eventually be to indicate which road they wish to cross to get the right-of-way. AV and CV vehicles in possession of pedestrian data can make safe and informed decisions to improve efficiency at an intersection. Although it is not feasible to have 100% saturation of connected pedestrians, this project develops the framework for connecting pedestrians into an intelligent intersection control systems. RESEARCH PLAN The goal of my Ph.D. research is to optimize intersections with AVs, CVs, and pedestrians. With the resources and support from the University of Florida (UF), I will develop, test, deploy, and analyze a pedestrian-integrated intelligent intersection control systems (IICS). My research expands ideas from my current undergraduate collaborations with Dr. Lily Elefteriadou, director of the UF Transportation Institute, Dr. Ruth Steiner, an Associate Professor at the UF Department of Urban and Regional Planning. Broader Impact: During my research process, I will include undergraduates in my team, mentor engineering students, and present the project to multiple audiences. A diverse undergraduate research team will assist my project with accessible tasks, exposing them to the research setting, and facilitating a passion for transportation engineering by working with cutting-edge technologies. I will continue mentorship in established programs at the UF ASCE student chapter and College of Engineering. I will disseminate my work through multiple publications in Transportation Research Board and other scientific journals. I will present my work to undergraduates at their club meetings and research fairs. Practicing engineers will gain access to my work through my involvement in professional societies (ASCE) and start to integrate AV and CV technologies into their long-range plans. Intellectual Merit: Recent publications developing system control algorithms incorporating AV and CV technologies intelligent intersection control systems (IICS) assume a mixed-traffic of conventional, connected, and autonomous vehicles in undersaturated conditions [1,2]. With my Rebecca Kiriazes Graduate Research Proposal established connections at the UF, I can procure all necessary equipment including autonomous vehicle, detectors, IICS, and a testing location. OBJECTIVE 1: INTEGRATING PEDESTRAINS IN ALGORITHM (FIGURE 1) The central computer will receive each vehicles and pedestrian arrival information. An algorithm will compute the parameters of the AV or CV to minimize travel time delay while considering the estimated movement of the connected pedestrian and conventional vehicles. The optimized AV or CV parameters determine the intersection signalization and the AV or CV trajectory [1]. Challenges in developing the algorithm include predicting pedestrian movement with the presence of AV, CV, or Figure 1: Intelligent Intersection Control System (IICS) with conventional vehicles [3]. Connected Pedestrians OBJECTIVE 2: PEDESTRAIN AND IICS COMMUNICATION Previous research in smartphone-based detection and localization has been developed for visually impaired pedestrians [4]. Smartphone-based connectivity will be established in the IICS. OBJECTIVE 3: TESTING AND EVAULATION Full scale testing will take place to establish working connection between pedestrian, vehicles, and infrastructure [2]. Multiple scenarios with varying frequency and volumes will be tested and the results will be recorded and established performance measures will be analyzed. OBJECTIVE 4: ROADWAY SAFETY EVAULATION Additional testing will determine the intersection effectiveness of the perceived pedestrian safety. Response time and post-crossing participant survey will be analyzed to increase the accuracy of the IICS pedestrian prediction and inform future designs of vehicle-to-pedestrian communication [5]. From this testing, a series of pedestrian safety workshops will be presented to inform the public on safe interactions with AVs and CVs. CONCLUSION Broader Impact: This proposal strengthens the connection between transportation engineering and urban planning in development of emerging technologies. It is expected that results of the proposed project will facilitate the adoption of connected and autonomous vehicle technologies. Adopting these technologies will result in a safer and more efficient transportation system. Intellectual Merit: This project will encourage the academic community to investigate vehicle- to-pedestrian issues including non-connected pedestrians, data security, moral dilemmas, and the effect AV will have on mode choice. REFERENCES [1] Pourmehrab, M. et al. 2017 Unpublished. [2] Omidvar, A. et al. 2017. Unpublished. [3] Marisamynathan, S. et al. 2014. Journal of Traffic and Transportation Engineering. (103-110) [4] Murali, V. et al. 2013. IEEE Int Conf Multimed Expo Workshops. July 2013. (1–7) [5] Clamann, M. et al. 2017. Transportation Research Board. (AND10).	0
Starsexplode. Supernovae(SNe), orstellarexplosions, canoccurthroughtheignitionofa degenerate white dwarf (WD) star, a star that is supported by quantum electron degeneracy pressure, orbythecorecollapseofamassivestar. Recenttransientsurveys, suchasTheDark Energy Survey have discovered and imaged thousands of supernovae since 2013 including the anomalous SNe DES13S2cmm. The forthcoming Large Synoptic Survey Telescope will further these efforts utilizing a three billion pixel digital camera to cover more than 20,000 deg2 of the night sky. However, even with this wealth of observational data, many aspects of the evolution and subsequent explosion of massive stars remain unknown. My background in theoretical astrophysics has prepared me to aide in the advancement of theseefforts. I propose to investigate the stellar structure and evolution of massive stars, core collapse supernovae explosion (CCSNe) mechanisms, and implications for cosmic chemical evolution and gravitational wave radiation. The confluence of advancements in multiple fields will provide the empirical basis needed to accomplish these goals. The focused efforts proposed are summarized as follows: (i) getting the progenitor right, (ii) supernova explosion mechanisms, and (iii) nucleosynthetic yields and gravitational wave bursts. Advancement in our understanding of massive stars can lead to furthering our knowledge of the cosmic chemical evolution of the Universe and provide direct tests of Einstein’s General Theory of Relativity (GR). Getting the progenitor right. The star that will eventually explode as a CCSNe is often referred to as the progenitor. Computational modeling of the progenitor star can lead to the insight of how a star will end its life, or allow one to infer the initial progenitor of an observed supernova. Recent 3D hydrodynamic simulations of radiation dominated envelopes in massive stars and internal magnetic field strengths of order ∼105 Gauss, suggest the need for further investigation [4]. I will investigate the uncertainties associated with the structure and evolutionary properties of massive stars that will end their lives as CCSNe explosions. Using a state of the art stellar evolution code, Modules for Experiments in Stellar Astrophysics, I will focus on uncertainties due to the nuclear reaction rates, compositional mixing, and the effects of rotation and induced magnetic dynamos. Specific steps include the sampling of new Monte Carlo nuclear reaction rate distributions for key nuclear reactions using the recently constructed rate library, STARLIB [5], and performing a quantitative assessment of the effect of varying strengths of compositional mixing and rotational values. The utilization of new measurements of nuclear reaction rates at astrophysically relevant energies forthcoming from the Facility for Rare Isotopes Beams willalsobeparamountinthiseffort. My background in stellar astrophysics, especially my past published work on modeling super asymptotic giant branch stars [2], will allow me to play a productive role towards modeling more physically accurate stellar models that can address fundamental questions in stellar and galactic evolution. Supernova explosion mechanisms. Collapse of the iron core within a massive star initiates the CCSNe explosion. The inner core is then halted once densities exceed that of nuclear matter, resulting in core bounce launching a shock towards the still collapsing outer core. However,theshockisnotstrongenoughtoblowupthestarandisusuallyhalted. Thisstalled shock has led to the so-called ‘failed supernovae’ problem and has left many scientist trying to determine the mechanism which allows for the efficient explosion of CCSNe observed. Contemporary approaches favor neutrino transport as an efficient means of reheating, or addingenergyto, thestalledshockallowingforasuccessfulexplosion. Recentstudiessuggest a correlation between the local neutrino heating rate and successful explosion [1]. I propose to continue this effort by investigating various explosion mechanisms of CCSNe and addressing uncertainties therein. My primary numerical instrument will be the 3D adaptive mesh hydrodynamic code, FLASH. The computational resources available at my proposed graduate institution, California Institute for Technology, will make theseeffortsfeasible,whiletheexpertiseofthegroupIwishtojoinwillprovidetheneccessary support to successfully address these scientific questions. Nucleosynthetic yields and gravitational wave bursts. Successful CCSNe explosions are also known to produce iron-group elements and experience bursts of gravitational wave radiation. I propose to investigate gravitational wave bursts caused by CCSNe as well as the associated nucleosynthetic yields. Minutes after the Big Bang, the Universe began to synthesize light isotopes such as 1H and 4He. However, uncertainties still lie within the steps taken to arrive at the m´elange of isotopes in our interstellar medium today. The next step towards understanding the cosmic chemical evolution of our Universe is to move towards a deeper understanding of the nucleosynthetic yields of CCSNe. My current NSF-supported work with Dr. Frank Timmes at Arizona State University on nucleosynthetic yields in WDs [3] is preparing me to address aspects of forging the elements during my graduate work. Furthermore, with the recent upgrade of The Laser Interferometer Gravitational-Wave Observatory (LIGO) complete, direct detection of gravitational waves (GWs) is imminent. These ripples in spacetime can occur during asymmetric collapse to a black hole of CCSNe and provide direct tests of GR. A new field of astrophysics is upon us and requires necessary interplay between astronomy and theoretical physics. While participating in the NSF LIGO summer research program I simulated GWs emitted by compact binary systems in an effort to test the strong-field dynamics of General Relativity and this experience has prepared me to play a large role in this effort. Here I present a framework for maintaining successful completion of these efforts. In years 1-2 of my graduate studies, I will work on focused effort, Getting the progenitor right, with successful completion corresponding to a peer-reviewed journal publication. Years 3-4 will focus on Supernova explosion mechanisms, again with successful completion corresponding to a peer-reviewed journal publication. Lastly, I will spend my final year considering Nucle- osynthetic yields and gravitational wave bursts, with the culmination of this project resulting in a publication and successful completion of my Ph.D. The focused efforts presented here would result in the advancement of our understanding of the evolution and subsequent explosion of massive stars, leading to advancements in the fields of cosmology, astronomy, and theoretical physics. These are immense, broad questions that require expertise in multiple backgrounds as well as interdisciplinary collaborative ef- forts. Being supported by the NSF through the GRFP would accelerate my goals by allowing me to begin research my first year and be invaluable in preparing me for a successful career. [1] Couch, S. M., & Ott, C. D. 2015, The Astrophysical Journal, 799, 5 [2] Farmer, R., Fields, C. E., & Timmes, F. X. 2015, The Astrophysical Journal, 807, 184 [3] Fields et al. 2016, The Astrophysical Journal, in prep. [4] Fuller, J., Cantiello, M., Stello, D., Garcia, R. A., & Bildsten, L. 2015, Science, 350, 423 [5] Sallaska, A. L., Iliadis, C., Champange, A. E., et al. 2013, ApJS, 207, 18	0
2 fired power plants1. Due to the role of CO in global climate change, reducing emissions is 2 imperative, and its capture and sequestration at the source is highly attractive. While scrubbing technologies are currently employed as the industrial standard, they are inefficient and regeneration of the amine reagent used for CO capture is expensive.1 2 Several classes of adsorbent materials have been proposed as alternatives for CO capture, 2 but metal-organic frameworks (MOFs) are one of the most promising. As porous, crystalline solids, their robustness, chemical tunability, and void space for guest occlusion, i.e. a gas or solvent molecule within the pore, make MOFs highly attractive for such applications. Designing a porous material such as a MOF to replace reagent-based methods is a challenge, as it must exhibit superior selectivity for CO at elevated temperatures and low partial pressures, from a mostly nitrogen-rich 2 atmosphere. In addition to excellent CO adsorption under challenging conditions, the MOF must 2 possess a high tolerance to water, straightforward regeneration, and robustness over thousands of cycles.2 Selective gas or solvent adsorption in a static MOFs is attributed to size exclusion and/or favorable host-guest interactions.2 MOFs that exhibit framework distortion upon guest addition or removal offer an additional route for tuning selectivity. Termed “breathing MOFs”, they were pioneered by Ferey and Kitagawa in the early 2000s.3 Though their body of literature has grown significantly, the ability to rationally design new breathing MOFs has yet to be demonstrated, contrary to their static counterparts.4 Selectivity in breathing MOFs is attributed to a “gate- opening” pressure in which the pore expands, allowing the guest to enter. This unique property could offer a superior method for designing new, more efficient materials for CO capture. 2 Thoroughly understanding how chemical environment, pore aperture, and breathing ability affect selectivity for CO could yield a great leap toward designing an industrially viable material. 2 Objective: Utilizing isoreticular synthesis5, I will develop a series of porous, breathing MOFs tailored for CO capture. Exceptional guest selectivity will be achieved through optimizing a 2 synergistic relationship between functionality, pore availability (i.e. aperture), and breathing motif. By tuning the pore environment in such a way, it will be possible to enhance the initial framework- CO attraction and reduce the available “gate-opening” pressure to that of CO . 2 2 Methodology: Using a set of semi-rigid, organic linkers where size, shape, and functional groups have been altered, I will synthesize 2-pillared, 2D sheet MOFs or 1D tube MOFs, which are also referred to as metal-organic nanotubes (MONTs). Continuing work started in the blank group6, these breathing MOFs will exhibit guest-dependent rotation of the ligand (Figure 1). Characterization using X-ray diffraction methods will be employed. For MOFs in which breathing is a result of rotation of phenyl rings in the ligand, breathing can be verified by 13C CP MAS NMR, as demonstrated in a publication currently under revision by the blank group. Tuning pore size and functionality within a framework Figure 1: Illustration depicting has been well documented5. Triazole ligands of increasing ligand rotation in breathing MOFs. size that adopt a syn-geometry will be incorporated into the frameworks, and isoreticular synthesis will be exploited by adorning the non-triazole moiety with amino, nitrile, hydroxyl, methyl, halide, and ester groups, similar to previous reports (Figure 2).5,7 Experimental evidence show that these alterations can affect a framework’s selectivity for CO 2 over CH , N , and O ; moreover, nitrogen-containing functional groups tend to enhance the 4 2 2 framework-gas attraction via a Lewis acid-base interaction.8 My first aim is to identify the functional groups that enhance framework interaction with CO in breathing MOFs. From there, I 2 will determine the role of pore aperture by systematically increasing the ligand size. The final variable to evaluate if breathing mechanism influences selective CO adsorption. The effects of 2 these alterations can be understood by single and multi-component gas adsorption studies. Selectivity will be evaluated according to previously-published methods that evaluate the role of kinetic favorability in mixed-gas systems.7 Anticipated Results: My work will result in the development of a porous, breathing MOF that will outperform the most highly-selective materials to date for selective CO adsorption in mixed- 2 gas systems.7 Preparation of such a MOF could form a platform for newer, more efficient materials to be developed. My Figure 2: Representative ligands for MOF synthesis. findings will be presented at regional and national conferences, and published in relevant peer-reviewed journal articles. Broader Impacts: Because of the correlation between increased atmospheric CO levels and 2 rising global temperatures, curbing anthropogenic CO emissions is a goal of utmost importance 2 for our society; capturing these emissions at the source is highly attractive. In addition, synthetic chemists are seeking routes to convert CO to valuable commercial commodities like plastics9 and 2 synthetic fuels.10 Efficient capture of CO will help advance these processes, as CO is an abundant 2 2 C -feedstock. Development of a highly selective and efficient porous, adsorbent material for CO 1 2 capture from coal-fired power plants would be invaluable for its environmental and economic benefits. Intellectual Merit: My relevant experience working with gas storage materials at both blank and blank has prepared me well for graduate work at the University of blank, where I will work under Dr. blank blank. I possess strong synthetic skills and a familiarity with an array of characterization techniques (see personal statement). I have presented posters and oral presentations at several regional and national conferences, and was recently published11 as a co-author for my work at blank. As a Latino in science, I hope to mentor young scientists from underrepresented backgrounds in STEM fields through tutoring and educational outreach initiatives. My leadership role at a non-profit organization (see personal statement) has left me with the experience to engage the general public in educational discourse, and I hope to foster an understanding of basic science in our society. Whether I pursue academia, industry, or government-employment, I look forward to joining the generations of scientists who will solve the greatest issues of our age. References [1] Science 2007, 317 (5835), 184-186. [2] Coord. Chem. Rev. 2011, 255 (15–16), 1791-1823. [3] Chem. Soc. Rev. 2009, 38 (5), 1380-1399. [4] Coord. Chem. Rev. 2014, 258–259 (0), 119-136. [5] Science 2002, 295 (5554), 469-472. [6] blank citation blank citation. [7] ProC. Natl. Acad. Sci. USA 2009, 106 (49), 20637-20640. [8] J. Am. Chem. Soc. 2009, 131 (11), 3875-3877. [9]http://www.research.bayer.com/en/CO2.aspx, 2012. [10] Chem. Ing. Tech. 2013, 85 (4), 489-499. [11] blank citation blank citation.	0
"Background:Children who are victims of interpersonalviolence have an elevated risk of engaging in aggressive behavior and perpetrating violence in adolescence and adulthood1. Youth currently in the foster care system are particularly vulnerable to this cycle2,and are considerably more likely than their counterparts to have contact with the criminal justice system. This has created substantial public costs for the United States3. However, while research has shownhigh rates of criminal involvement within foster populations4, not all victimized children engage inviolent behavior. In fact, a significant portion of this population demonstrates relatively uncompromised, or “resilient” functioning5. Factors such as presence of supportive adults, satisfaction in school, and participation in extracurricular activities were found to be protective factors in high-risk populations6. However,access to and enhancement of these resources due to foster placement and stability has been less well studied. Similarly, while risks associated with negative outcomes have been identified, such as previous physical abuse and delinquency in youths’ original families7, an examination of how these factors areamplified and affected by the foster care system has not taken place. So while adverse childhood experiences often predict aggressive behavior4, there is variability in delinquency rates within this population and little agreement about what is responsible for increased or decreased risk of violent behavior. The lack of understanding of contributions to this variability complicates the development of effective social interventions within the foster care system and adds greatly to economic burden in the United States3. Proposed Research:To better understand this variability,I aim to examine the relationship between history of violence and rates of delinquency, focusing on the factors that potentially moderate this association. The proposed study will focus on factors which mayplace children at increasedrisk, but alsofactors that may explain resiliencein this population.My analytic strategy will incorporateboth quantitative and qualitative techniques. Combiningthese approaches will provide multidimensional understanding of complex issues that cannot be obtained through one method alone8. Large scale quantitative research will provide precise foundational information used to conduct the study, analyze the data, identify factors that can be intervened on, and verify the findings. Smaller scale qualitative analyses will expand understanding by exploring subjective factors, identifying other factors not captured in the quantitative data, gaining insight into social processes, and giving voice to participants in the study. First, I will analyze the Midwest Evaluation of the Adult Functioning of Former Foster Youth at Chapin Hall9, self-reported survey data collectedfrom 732 study participants when they were 17 or 18 years old. Within this data set, I will examinehistoryof maltreatment, delinquency, foster care service factors, access to protective factors, and placement satisfaction.History of maltreatmentwill be assessed by The Lifetime Experiences Questionnaire.Delinquencywill be measured by surveys regarding history of arrest, conviction for committing a crime, and overnight stay in a correctional facility. This analysis will also include surveys collected regarding victimization in the past 12 months, as well as perpetrator status in the past 12 months.Foster servicefactorswill be measured by data regarding age at entry into foster care system, number of placements, type of placements, and total length of stay in foster care system.Access to protective factorswill beassessed by The MOS Social Support Survey, surveys regarding the impact of foster care on ability to attend school, and information regarding educational attainment, such as last grade level completed. Lastly,placement satisfactionwill be analyzed by a survey concerning attitudes and satisfaction with most recent placement situations, and a survey regarding the likelihood of turning to the child welfare system for support in the future. A series of linear regression models will explore the effects of maltreatment, foster care service factors, protective access, and satisfaction on delinquency.I aim to(1) comparedelinquency rates for maltreated and non-maltreated foster youth, (2) within maltreated foster youth, identify which factors are associated with increased or decreased delinquency,and(3) determine whether foster care service factors, access to protective factors, and placement satisfaction moderate the relationship between violence exposure and violent behavior within the foster care system. Second, I will conduct qualitative analysis using a smaller sample of foster youth, recruited at University of Illinois at Chicago. I will interview foster care youth (n=50), ages 16 to 18 with a history of maltreatment and delinquency history tounderstandself-reported protective and risk factors in the current operation of foster care.Example questionsare ""What do you think could be improved in your experience with foster care?"", and ""what do you feel has been useful to you during your foster care experience?"". After transcription of these interviews, a code book will then be developed for the identification and interpretation of patterns and themes in the textual data. This qualitative research will allow for better insight into social interactions, foster care delivery processes, and subjective factors that may not have been included in the surveys of the Midwest Evaluation. This qualitative research will generate explanatory models and theories, which will also be useful in the creation of interventions. Working collaboratively with the quantitative data, these interviews will provide valuable insight, allow foster youth to have a voice, and add a more comprehensive analysis to numeric methods. HypothesesI hypothesize thatHypothesis (1)Maltreatedyouth will experience higher rates of delinquency.Hypothesis (2)Within the populationof maltreated youth, higher violence exposure will be associated with lower rates of foster care service factors, access to protective factors, and placement satisfaction.Hypothesis (3)Differences in thesefactors will identify which victims of abuse are more likely to engage in delinquency, and thus strengthen or weaken the association between violence exposure and future violence.Hypothesis (4)The qualitativeinterviews will provide unique insight on complex social issues that will give voice to children in foster care and generate explanatory models that can serve to devise new kinds of interventions. Intellectual MeritThis study will address the relationshipbetween maltreatment and incarceration in a novel way that will advance understanding of variabilities in violent behavior among maltreated foster youth. By interpreting these factors that contribute to variabilities, I will add to research that will determine what is responsible for increased or decreased risk of delinquency and incarceration. As this topic has never been addressed from both qualitative and quantitative perspectives, this study will offer comprehensive and unprecedented data that will begin to address the enormity of cycles of violence among foster youth. FeasibilityThe Midwest Evaluation is a longitudinaldataset that has already been collected and is publicly available with data use agreements. My proposed qualitative interviews will add an unexplored construct to ongoing studies with minimal burden to participants. Preparation and analysis of this data will be conducted with support from Dr. Kathryn Grant, an expert in maltreatment, or with Dr. Elizabeth Raposa, who has worked extensively with foster youth and juvenile delinquents. This NSF award will permit me to pursue coursework that will be critical to the success of my project and allow for dedicated mentored training. The success of the research will be assessed via communication of these results by publications of peer-reviewed first-authored manuscripts, presentations at research conferences such as the International Association for Child and Adolescent Psychology and the International Society for Traumatic Stress Studies, and the development of interventions based on this research. Broader ImpactsThe estimated economic burden resultingfrom cases of child maltreatment in the United States is approximately $124 billion3. An additional $5.1 billion is used annually to incarcerate former foster youth in State and Federal prisons10.Not only are we ethically bound to serve and provide for these underrepresented children, but we are also bound to advance the progress of science and reduce economic burdens for our nation. This research will identify which factors strengthen or weaken cycles of violence, and outline how to specifically support children within the system who have been exposed to violence. This work will enhance and improve foster care procedures that specifically decrease rates of violent behavior and incarceration. My goal is to ultimately improve foster care service factors, access to protective factors, and placement satisfaction. This research will lead to better interventions, decreased rates of incarceration, and therefore a decreased economic burden for maltreatment in the United States. References1. Widom,Science1989 2. McMillen et al.Child Psychiatry2005 3. Fang et al.,Child Abuse Negl. 2012 4. Font et al.Crime Delinquency2021 5.Jones,Soc. Work J.2012 6. Luthar,Developmental psychopathology2006 7. Datta et al. 2019 8. Bartunek& Seo,Journal of Organizational Behavior2002 9. Courtney, Terao, Bost,Midwest Evaluation of theAdult Functioning of Former Foster Youth: Conditions of Youth Preparing to Leave State Care2011 10.Administration for Children and Families, Preliminary Fiscal Year 2009U.S. Department of Healthand Human Services 2010"	0
Improve Surface Age Determination Ali Bramson Keywords: primary and secondary craters, hierarchical clustering, networks, dendrogram Background: Crater statistics are a fundamental tool for learning about the geology and surfaces of planets, as well as the population of bodies in the solar system doing the impacting. Without physical samples, crater densities remain the only way we can estimate absolute ages of planetary surfaces [1]. Impacts generally are considered to happen randomly in time and space, so the more craters a surface has, the older it is. The size-frequency distribution (SFD) of craters follows an inverse power law [2], so the number of craters increases with decreasing crater diameter. This means smaller craters are more statistically significant when used for calculating surface unit ages [3]; however doubts have recently emerged about how these smaller craters should be interpreted. Secondary craters form when the ejecta from a large crater re-impacts the surface. These secondary craters lead to higher than expected crater counts within a geologic instant [2], which presents a problem for age determination because it makes a surface appear artificially older. Most of these secondaries appear close to their primary in clustered patterns such as rays and so can be excluded from crater counts. However, they often can be hard to distinguish from background small primary craters. This is especially true if they re-impact far from their source (at high velocities), in which case they match the circular morphology and depth to diameter ratios of primary craters. A detailed study of the Europan surface by Bierhaus et al. 2005 shows that secondary craters comprise up to 95% of craters with diameters < 1 km. The ability to extract these secondary craters from the background primary crater density is essential if we wish to accurately constrain the ages of surfaces. The amount and quality of imaging data from recent planetary missions (such as the Mars Reconnaissance Orbiter, Mercury Messenger, and Lunar Reconnaissance Orbiter) drive the need for a technique that can not only identify secondary craters, but also work on the very large data sets we are currently receiving. Fortunately, there is a growing body of work in computer science, devoted to understanding networks and clustering, which can assist in the analysis of these data. In particular, social networking algorithms have been developed to better understand how people in different groups are connected [4], and I have experience from my undergraduate senior thesis in applying such techniques to astronomy (galaxy groups). I propose to combine my interest in computer science and passion for planetary science by applying hierarchical clustering techniques common to network science to find patterns in spatial locations of craters. Applying these techniques to analyze the patterns of crater locations is a logical project that could prove to be very insightful, helping us to distinguish primary craters from secondary. The Algorithm: The hierarchical clustering technique [5] iteratively builds a hierarchy of clusters by starting with all nodes (the craters in question, in my case, or people in the social science setting) as separate groups and connecting them until they are all in the same group (agglomerative). To connect these features, it creates a distance matrix, a matrix of “similarity”, where each element will be the distance between two craters. In a social setting, the distance matrix could use number of friends in common as a measure of similarity. The agglomerative technique then merges the two nodes that are the closest into one cluster. These nodes are removed from the distance matrix while our new cluster is added in, and the process is iterated until all objects have been merged into one cluster. Bierhaus et al. 2005 utilized aspects of this algorithm, combining them with other clustering and statistical techniques in his study of Europa’s surface. However, Europa is a simpler case because it is sparsely cratered compared to other surfaces, making it easier to disentangle primary from secondary craters [3]. I propose to apply this algorithm to the more complicated Martian surface, using the latest database of impact craters on Mars from Robbins & Hynek 2012 [6,7]. Data: I will use a new global Martian (covering >99% of the surface area of the planet) crater database from which is freely available via the U.S. Geological Survey’s (USGS) Mars Crater Consortium, for my study [6,7]. This database contains 384,343 craters and is statistically complete down to diameters ≥ 1km. The database does have secondary crater classification for a select section of the database [1,8]. This will allow me to compare my technique to the manual classification of Robbins and Hynek. This comparison can be used to assess the accuracy of my automatic methods. Expected Results: I will use the output of the hierarchical clustering to yield a tree-like “dendrogram” to show the order in which the craters have been connected. Rosolowsky et al. (2008) have used dendrograms to determine the structure of molecular clouds. Where the branches of their dendrograms correspond to self-gravitating molecular cloud structures, the branches of my dendrograms would correspond to larger networks of craters. The shape of these branches can tell us about the spatial patterns of the craters; if the dendrograms are flat, with many connections at equal distances, the features are distributed homogenously. Long connections high in the dendrogram connecting groups of low-level links indicate the features spot the surface of the planet in a much less uniform manner (i.e. we can automatically identify clusters). I will also create dendrograms for a random distribution of objects of equal density as a comparison. I predict that the comparison to random will extract the primary craters that dot the surface randomly, leaving behind the probable secondaries that will appear in groups of low- level links that deviate from random. The advantage of this technique is that it could be used in any field that involves clustering and connections e.g. creating dendrograms of asteroid proper orbital elements could lead to new asteroid groupings. We might also expect the clustering of surface features on Io to be connected with volcanic events or other asymmetries between the leading and trailing hemispheres of tidally locked moons. The broad applicability across many disciplines is a key part of the broader impact and intellectual merit of this research. While I am passionate about planetary science and this research’s applicability within the field, the algorithms and techniques I use will be valuable to many fields. References: [1] Robbins, S.J. & Hynek, B.M. 2011, GRL, 38, L05201. [2] McEwen, A.S. & Bierhaus, E.B. 2006, Annu. Rev. Earth Planet. Sci., 34, 535. [3] Bierhaus, E.B., Chapman, C.R. & Merline, W.J. 2005, Nature, 437, 1125. [4] Mucha, P.J., et al. 2010, Science, 328, 876. [5] Newman, M.E.J. & Girvan, M. 2004, Phys. Rev. E, 69, 026113. [6] Robbins, S.J. & Hynek, B.M. 2012, JGR, 117, E05004. [7] Robbins, S.J. & Hynek, B.M. 2012, JGR, 117, E06001. [8] Robbins, S.J. & Hynek, B.M. 2011, JGR, 116, E10003. [9] Rosolowsky, E.W. et al. 2008, ApJ, 679, 1338.	0
in condensed matter physics. In particular, topological defects have underlied diverse phenomena fromfractionalelectriccharge1tothesuperfluidtransitioninliquidhelium-42. Evenmorerecently, theimportationofideasfromtopologyinelectronicsystemstophotonicsystemshasledtoaflood ofnewdiscoveriesandpotentialdevices3. MyPh.D.researchwillfocusonthepredictionofnovel propertiesoftopologicaldefectsinphotonicandelectronicsystems,withanemphasisonboththe fundamentalphysicsanddeviceapplicationsofthesedefects. Intellectual Merit: I have past experience researching a particularly striking property of topological defects. Since July I have been doing research full-time under Professor Claudio Cha- monatBostonUniversity,whereinarecentpublication(Science,inreview)wehaveproposed to use topological defects to realize non-Abelian exchange statistics in photonic systems4. The defect occurs as a vortex in the order parameter describing a certain lattice distortion in two- dimensional honeycomb lattices, and leads to the formation of localized states which gain non- Abelian geometric phases upon their exchange. Our proposal is the first to predict non-Abelian statisticsinphotonicsystems,asopposedtothedelicateelectronicsystemstheirsearchwasprevi- ouslyconfinedto,andcouldthusleadtotheirfirstconclusiveexperimentaldetection. MypastresearchexemplifieswhatIfindfascinatingabouttopologicaldefectsinelectronic and photonic systems - exotic phenomena, with the potential for real-world realization and appli- cation. My future work willcontinue to espouse these traits. I propose two projects. In project (1) Iwillextendthedefectstatesstudiedinmypreviousworktoafamilyofstatesobeyingabroader class of non-Abelian exchange statistics, and use these states to propose a novel topological nonreciprocal optical device. In project (2) I will separately extend these defect states to Weyl semimetalsandtheirphotonicanalogues,andshowthattheycansustaindissipationlesstransport in both systems. Further, I will address how these topological defects could naturally arise in Weylsemimetalsusingafieldtheoreticapproach. Idescribeeachprojectinmoredetailbelow. Project (1) will seek to realize novel forms of non-Abelian statistics, which when imple- mented in photonics could lead to a strongly nonreciprocal optical device. While the statistics I previouslystudiedrespectreciprocity(lightpassedthroughanexchangeinonedirectionwillgain thesamephaseshiftasintheoppositedirection),Ihavediscoveredasimple,exactlysolvable,fam- ilyoftopologicaldefectstateswith“nonreciprocal”statistics. Importantly,theHamiltoniangiving rise to these states breaks a particular symmetry present in the previous, “reciprocal”, Hamilto- nian. Unfortunately, these particular defects would be difficult to realize in photonics experiment. My research will thus take clues from this simple model and focus on using symmetry classifi- cation to discover new nonreciprocal extensions of these topological defect states, especially those which are experimentally feasible. Breaking symmetries of the system will bring us into a different symmetry class, in which case I will determine whether nonreciprocity and topological protection of our state can coexist. I will be aided by international collaboration with Professor Chamon’s colleague Doctor Christopher Mudry at the Paul Scherrer Institute in Switzerland, an expert in symmetry classification of topological defects. In determining which photonic systems are feasible, I will benefit from current collaboration with engineering and experimental pho- tonics groups at Boston University and MIT working to realize the experiment proposed in my previous publication. Beyond device applications, providing a physical realization of this class of non-Abelianstatisticswouldconstituteamajoradvancementinfundamentalphysics. In Project (2) I will pursue a separate extension of the same topological defect states into both electronic and photonic systems, in which I propose to use a 3D generalization of these states to achieve topologically-protected dissipationless transport in Weyl semimetals. Weyl semimetals have a low-energy electronic theory similar to the systems I previously researched5, and are thus capable of sustaining 3D analogs of the same defect states. Crucially however, these formerly-2D states will now extend in the third direction, and can carry current in that direction. I have already analytically shown that such states would have a chiral dispersion relation, leadingtoalackofbackscatteringandthusdissipationlesstransport. Tosubstantiatetheseclaims, in the first part of project (2) I will determine the position-space changes in hopping needed to create these topological defects, using common tight-binding models of Weyl semimetals. As in mypreviouswork,Iwillusethesehoppingstoperformexactdiagonalizationsimulationstoquan- titativelyprobetheresultingdefectstates,andverifytheirchiraldispersionrelation. Experimental verification of these properties may be easiest in photonic realizations of Weyl semimetals3, for which I would adapt my model to the photonic setting and find estimates of experimental condi- tionsneededforthestates’realization,similartomypreviouswork. Thesecondpartofmyproject(2)willaddressthefactthat,inelectronics,theconductance ofasingledefectwouldbesmallevenwithdissipationlesstransport. Toremedythis,Iwillsearch for a field theory of the order parameter describing the appropriate hopping distortions, in which the creation of a macroscopic number of topological defects would occur. The starting point must be a nonzero mean field value of this order parameter. To determine the conditions for this to occur, I will introduce the appropriate order parameter couplings into the electronic field theory, and integrate out the electron degrees of freedom to derive an effective action for the or- der parameter. Assuming a nonzero mean field value, one would anticipate a massless Goldstone mode associated with fluctuations in the order parameter phase. It would then be an open prob- lem to determine how topological defects in this Goldstone mode might form, although pursuing connections to vortex formation elsewhere in condensed matter may be fruitful. My extensive graduate-levelcourseworkinfieldtheorywellpreparesmeforthiscomponentoftheproject. BroaderImpacts: Bothofmyproposedprojectsfeaturetopologically-protectedstateswith large potential societal impacts through use in photonic and/or electronic devices. Project (1) proposes to use the geometric phase of a topological defect state to uniquely achieve strongly nonreciprocal photonic devices, usable in essential optical elements such as isolators and circula- tors. This could greatly reduce undesirable optical loss compared to traditional nonreciprocal devices, which rely on weakly (relative to my proposal) nonreciprocal materials such as ferrite or strong permanent magnets. Additionally, my proposal would not require breaking time-reversal symmetry,openingthedoortononreciprocaldevicesusingonlyconventionalmaterials! Project (2) seeks to realize defect states with chiral dispersion in Weyl systems, with po- tential impact in both photonics and electronics. In photonics, these modes could function as losslessopticallines,robustagainstimperfectionsduetotheirtopologicalorigin. Suchtopolog- ical “one-way waveguides” have been realized in photonic quantum hall edge states, and have a plethora of novel device applications3. In electronics, the accumulation of a macroscopic number ofthesemodeswouldleadtomassiveconductance,withclearlytremendousdeviceimplications. Assessing the applications and limitations of such devices would require a theory for how this accumulation of topological defects occurs, highlighting the need for the second component of my project (2). All together, this array of broader impacts makes the investigation of topological defectsinphotonicsandelectronicswellworthpursuing. TheNSFGRFPwillallowmetodoso. [1]R.Jackiw,etal.,Phys. Rev. D(1976). [2]J.M.Kosterlitz,etal., JournalofPhysicsC (1973). [3]L.Lu,etal., NaturePhotonics(2014). [4]T.Iadecola,etal.,(2015). [5]X.Wan,etal.,PhysicalReviewB(2011).	0
Overarching career goal: Push the frontier of multiphase flows in extreme conditions that are subjected to high speed, strong turbulence, and large mass loading. For my graduate study, my aim is to unveil the underlying physical mechanisms of complex couplings between solid and gas phases in compressible particle-laden turbulence, which remains elusive for both industrial and astrophysical applications. The major challenge in this area is the lack of high-quality time- resolved experimental datasets that can illustrate the particle-gas and particle-particle interaction in extreme conditions. To address this issue, I plan to leverage a recently-developed ultra-high- speed diagnostic system to embark on understanding multiphase flows in this exciting new regime. Introduction and Background (Intellectual Merit): From the atomization in internal combustion to collisions and growth of dust particles in protoplanetary disks (Fig. 1(left)), particles in high-speed compressible turbulent environments is ubiquitous in nature and engineering applications. This two-phase interaction produces some key issues: (i) Multi-scale physics: particles interact with compressible turbulence with many length scales and coherent structures. (ii) The coupled interaction can lead to clusters of particles and droplets, resulting in enhanced collision rates and fast growth1. This is important for rain droplet growth in turbulent clouds, a type of incompressible turbulence. In this well-known regime, particles preferentially cluster in regions of low vorticity and high strain rate2, promoting collision but inhibiting mixing. This finding may have severe consequences in combustors, where mixing is desired for perfect combustion2. In compressible turbulence, there is a new structure – shocklet3. Rather than high vorticity or strain, it is a region of high pressure. Particles interacting with this new structure may alter the mean fields and turbulent characteristics of the flow4. A numerical study revealed that light particles cluster in very thin and long filaments on the shocklet surfaces5 (Fig. 1(right)), while larger inertial particles form dense clouds downstream of the shocklet. Hypothesis: A new particle clustering mechanism will become important in compressible turbulence due to the unique coherent structure – shocklet. Since shocklets are intermittent spatially and temporally, particles will interact with other low-speed eddies as they would in incompressible turbulence. The dynamic competition between Fig. 1. (left) Artistic illustration of protoplanetary dust. these two clustering mechanisms will pose an (right) Numerical simulation iso-surface displaying interesting new regime where the particle light particles (green) and eddy shocklets (orange)5. collision rate may be sensitive to small changes of the dimensionless groups, such as the turbulence Mach number, Stokes number, and Reynolds number. Aim 1: Upgrade current 2D experimental facility to a supersonic turbulent environment To generate a turbulent and supersonic environment, I will construct a facility to produce a 2D supersonic mixing layer using two opposing parallel supersonic streams, a method that has been used previously to visualize shocklets3. The two jets are separated by a vertical distance, enabling us to capture the interaction between the jets through the shear layer. Particles will be tracked using an in-house particle tracking code. I have access to different algorithms developed in the lab, which includes both 2D tracking and the 3D Shake-The-Box Method6. We have access to the Shimadzu HPV-X2 camera, which captures images at 10 million frames per second at exposure times 1 Graduate Research Plan Statement Juan Sebastian Rubio of 200 ns. With this camera I will take time-resolved measurements. We also own several Phantom high-speed cameras, which can take 40,000 images during one run. With this camera I can perform multi-scale measurements and conduct extensive statistical analysis of the flow. To obtain gas phase velocities, I will seed the flow with sub-micron sized particles and illuminate them with an in-house burst-mode laser (20 mJ per pulse at 100 kHz) and analyze the results using particle image velocimetry, a method I used previously both during my undergraduate research and at Los Alamos National Laboratory under the supervision of Dr. John J. Charonko. With this innovative experimental setup, I will obtain high-quality experimental images for further analysis. Aim 2: Investigate particle physics and its effect on the flow properties Because eddy shocklets are structures with strong compression Particle bow shocks regions, they can be visualized with either shadowgraph or Schlieren imaging methods. Thanks to the quasi-2D configuration of the proposed setup, shocklets will be visualized by the Schlieren method. I have already begun preliminary work to study particle motion using the underexpanded particle-laden jet facility in our lab. With the current experimental setup, I already visualized oblique and normal shocks using the Schlieren method. I also visualized the bow shocks that form around individual particles Fig. 2. Shadowgraph experiments (like eddy shocklets) at high frame rates (Fig. 2). Finally, the using the compressible particle- statistics of particle clustering will be evaluated using the Voronoi laden jet facility at Johns Hopkins analysis and the correlation between each of the particle’s location. depicting particle bow shocks. Aim 3: Study the turbulence by characterizing coherent structures - shocklets The complex coupling covers a multi-dimensional space. Compressible turbulence can be quantified by both the Reynolds and Mach number, which includes both the free stream and fluctuation velocity. Small particles can be characterized by the Stokes number, particle Reynolds number, and particle Mach number. I hypothesize that the particle Stokes number and particle Mach number are key parameters that may control the couplings between the two phases. A large Stokes number results in a large particle inertia, which may induce a strong particle-based shocklet. In addition, the particle mass loading is important. As in the incompressible case, the low mass loading may be key in particle-turbulence interaction, but the particle-particle interaction or particle-turbulence two-way couplings will not be important when the mass loading becomes large. I will spend part of my thesis to understand the behavior of particles in different parameter regimes. Broader Impacts: Improving our understanding of particle-laden compressible turbulence will have significant impact in numerous areas of science and engineering. It may provide new insights as to how the universe formed. To aid researchers around the world, I will store our experimental data in an open source repository. Outreach: I will mentor underrepresented undergraduate students on a semester basis, provide them with the opportunity of assisting me with experiments, and enable them to co- author in research papers. Through the Johns Hopkins SABES program, I will perform simple experiments to show students the importance of particle behavior in the environment. 1. Pan, Liubin, et al. The Astrophysical Journal (2011). 2. Squires, Kyle D., & John K. Eaton. Physics of Fluids (1991). 3. Papamoschou, Dimitri. Physics of Fluids (1995). 4. Chen, Chang H., & Diego Donzis. Journal of Fluid Mechanics (2019). 5. Yang, Yantao, et al. Physics of fluids (2014). 6. Schanz, Daniel, et al. Experiments in fluids (2016). 2	0
Background literature. Introductory chemistry courses are often delivered lecture-style and assessed with multiple-choice exams in classes of hundreds of students, often lacking opportunity for realistic scientific activity.1 Students often develop the notion that scientific knowledge may be acquired from external authorities rather than viewing science as a sense-making endeavor2 and as such they may default to rote learning, as material seems removed from everyday experiences.3 Often, students’ understandings of the nature of scientific knowledge, that is, their epistemological understandings of science, are not explicitly addressed in traditional curriculum, yielding students, some who complete STEM degrees, who do not understand how science progresses.4 Evidence suggests that undergraduate chemistry students possess significantly less sophisticated epistemologies than practicing chemists but that sophistication of undergraduates’ epistemologies are positively correlated with authentic experience. However, processes by which shifts in student epistemologies occur remain unclear.5 For instance, one authentic scientific activity identified by the National Research Council’s A Framework for K-12 Science Education, is that of scientific modeling.6 However, because students often perceive scientific claims as “proven,” they may not consider the nature and purpose of scientific models or a models’ associated assumptions, known as metamodeling knowledge.7 Students may have different metamodeling beliefs in different contexts.8 For example, students have demonstrated particular difficulty reasoning with mathematical models in meaningful ways.9 In contrast, expert scientists create and use models purposefully, to explain and predict phenomena, and generally possess modeling epistemologies that are far more stable and contextually appropriate.8 Evidence suggests that incorporating authentic modeling activities into introductory courses can support students’ development of expert-like epistemological ideas.10 Little is known about the mechanisms by which students’ epistemologies shift in response to engagement with modeling activities, but understanding these mechanisms could inform practices for instruction on scientific modeling epistemology. I intend to identify the moments in which students’ epistemological ideas are challenged, how and why students’ epistemological ideas change, and the factors that contribute to such realizations in a modeling-focused classroom setting. Research questions. 1) How do students rationalize their epistemological decisions while engaging in collaborative modeling activities in introductory chemistry contexts? 2) How do modeling activities and instructor facilitation contribute to shifts in epistemological ideas? Methods. Because students are often unaware of their own epistemologies, it is not reasonable to ask students to describe their epistemological ideas.11 Rather, it is logical to analyze student discourse to deduce the ways in which students think about the nature of scientific knowledge. Furthermore, as I am interested in the process of students’ shifting epistemologies toward those appropriate in chemistry contexts rather than static knowledge states, microgenetic analysis, a detailed, moment-by-moment analysis of the processes which contribute to learning, is appropriate for developing a nuanced understanding of change processes and the factors which contribute to them, such as design features of classroom activities or instructor-student interaction.12 My current graduate research is aimed at developing and testing modeling-focused collaborative learning activities that support students’ understanding of the nature and purpose of models. As part of this project, I plan to examine pre- and post-intervention survey data to gain a sense of the impact of the activities on students’ metamodeling knowledge (see Personal Statement). Support from the GRFP would allow me to expand upon this work to conduct a microgenetic analysis of classroom discourse as collaborative modeling activities take place, which would provide a finer-grained understanding of the mechanisms by which students’ epistemological ideas shift toward more expert-like conceptions. Such a fine-grained account of Katherine Lazenby | NSF GRFP Graduate Research Plan Statement 2 how collaborative learning activities support students’ epistemologies of modeling would provide insight for educators on how to orchestrate learning environments conducive for student development of robust understanding of the nature of scientific inquiry. While common in the mathematics education research community, few discourse analysis studies and fewer still microgenetic studies in particular, exist in chemistry contexts that would provide the kind of detailed mechanistic understanding that would support instructor facilitation efforts. For this study, I will collect multiple sources of data including video recordings of whole class discussion and small group work during collaborative modeling-focused activities, and written work generated during the modeling activities. The collaborative nature of the modeling activities will necessitate that students express their ideas about models to other group members, providing data about students’ thought processes. Since students will complete three modeling activities over the course of a semester, these data will allow me to observe the changes in the ways students discuss epistemological ideas about models over time. As an initial analytical framework for helping me to identify epistemological shifts in classroom discourse, I will adapt Grünkorn et al.’s developmental progression of epistemological views of modeling on five subscales (nature, purpose, and changeability of models, testing models and multiple models for a single phenomenon). Grünkorn et al.’s progression ranges from naïve- realistic interpretations of models as mere copies of phenomena, to constructivist views of models as the scientific products of developing explanations about the natural world, an idea which students should ideally develop as they mature into individuals capable of contributing to science. This framework would allow for the characterization of the sophistication of student modeling epistemologies and for determining whether engaging in modeling activities supports shifts toward more expert-like views of models.13 In completing this project, I will draw on my experience analyzing qualitative data. Additionally, my adviser has experience in classroom discourse analysis and will serve as a valuable resource during the analysis for this study.14 Summary of Intellectual Merit. The proposed research will provide insight about how students reason about the nature of models in chemistry. Since models are key tools used by scientists to communicate and explain the natural world, it is important that introductory courses can provide students with opportunities to develop scientifically appropriate understanding of models’ nature and purpose. Additionally, microgenetic studies of chemistry learners are few; to my knowledge, this analytical technique has never been used in undergraduate chemistry contexts, but these methods are powerful for investigation of agents that contribute to shifts in student ideas. Summary of Broader Impacts. The proposed research will inform instruction on science epistemologies. Students who understand the nature of scientific inquiry are more likely to engage meaningfully in subsequent coursework and experience academic success.15 Evidence-based instruction on epistemologies can support student success. Furthermore, students will be able to apply knowledge of the nature of scientific knowledge beyond classroom chemistry contexts to understand how scientists model globally-relevant phenomena such as climate change. (1) Smith, A. C.; Stewart, R.; Shields, P.; Hayes-Klosteridis, J.; Robinson, P.; Yuan, R. Cell Biol. Educ. 2005, 4 (2), 143–156. (2) Hofer, B. K. Contemp. Educ. Psychol. 2004, 29 (2), 129–163. (3) Songer, N. B.; Linn, M. C. J. Res. Sci. Teach. 1991, 28 (9), 761–784. (4) Clough, M. P. Sci. Educ. 2006, 15 (5), 463–494. (5) Samarapungavan, A.; Westby, E. L.; Bodner, G. M. Sci. Educ. 2006, 90 (3), 468–495. (6) National Research Council. Washington, DC: The National Academies Press 2012. (7) Schwarz, C. 2002. Proceedings of Int. Conf. of Learning Sci. (8) Hammer, D.; Elby, A.; Scherr, R. E.; Redish, E. F. Transf. Learn. Mod. Multidiscip. Perspect. 2005, 89. (9) Brandriet, A.; Rupp, C.; Lazenby, K.; Becker, N. Chem. Educ. Res. Pract. (under review). (10) Schwartz, R. S.; Lederman, N. G.; Crawford, B. A. Sci. Educ. 2004, 88 (4), 610–645. (11) Berland, L.; Crucet, K. Sci. Educ. 2016, 100 (1), 5–29. (12) Siegler, R. S. Handb. Child Psychol. 2006. (13) Grünkorn, J.; zu Belzen, A. U.; Krüger, D. Int. J. Sci. Educ. 2014, 36 (10), 1651–1684. (14) Becker, N.; Stanford, C.; Towns, M.; Cole, R. Chem. Educ. Res. Pract. 2015, 16 (4), 769–785. (15) Tsai, C.; Liu, S. Int. J. Sci. Educ. 2005, 27 (13), 1621–1638.	0
Introduction: Honey bees (Apis mellifera) are cornerstone pollinators and contribute nearly $20 billion to the U.S. agricultural economy each year1. Honey bee populations have drastically declined by an estimated 30-40% in the past three decades, and 2019 marks the largest winter hive loss ever recorded1. Bee decline threatens the U.S. economy and food supply, which has driven agricultural stakeholders and the scientific community to investigate reasons for honey bee deaths. A number of factors have already been identified, including habitat and foraging space loss, pesticide exposure, and infection by parasites, fungi and viruses. Bees have a commensal community of microbes aside from pathogens that includes bacterial, viral, and eukaryotic species, collectively known as the microbiome. Like in most organisms, the microbiome in bees plays an important role in nutrition and shaping host health through immunity and disease susceptibility. The extent to which the honey bee gut microbiome influences health outcomes remains unclear and for these reasons the scientific community is diligently working toward the characterization of the honey bee microbiota. Intellectual Merit/Background: 16S sequencing has revealed that remarkably simple and spatially organized microbial communities of about 8-10 bacterial phylotypes occupy the honey bee gut, consistent across geographic distributions of bees2. In-depth metagenomic sequencing and single-cell characterization at the strain level revealed that each phylotype spans considerable microbial genomic diversity, leading to substantial polymorphism within and between hives. Such variation between bacterial strains belonging to the same phylotype could result from functional diversification (due to niche partitioning) but also suggests co-divergence and adaptation with host lineages. Bidirectionally, the bee microbiome has been shown to play an important role in modulating the host physiology. Germ-free studies highlighted that native gut microbiota is able to stimulate the immune system in adult worker bees3 and the use of probiotics to effectively mitigate parasite effects shows promise4. However, the contributions of the gut microbiota to host immune pathways and the mechanisms by which the host responds to gut variation has yet to be investigated. Lastly, the genetic architecture of a honeybee colony makes any two daughter worker bees of sister queens mated with a single drone share ¾ of their genes on average5. Taken altogether with the consistent microbial phylotypes observed across colonies, this makes honey bees an ideal model for studying host-microbiome interactions. Using a combination of comparative genomics and field experiments, I aim to identify possible routes of honey- bee/microbiota co-diversification. I hypothesize 1) that the host genotype, diet, and environmental landscape shape the functional capabilities of the honey bee microbial community, and 2) this microbial community can acutely impact the innate immune system of adult bees, and that this community can be modulated by the addition of probiotics. I plan to test these hypotheses with the following aims: Aim 1: Determining the contribution of host genetic and environmental landscape in shaping the functional microbiome of the honey bee microbiota. I will rear several hives derived from single drone-mated sister queens from three different subspecies of A. mellifera: ligustica, carnica, and mellifera. These will be replicated in two geographically separated apiaries (collections of hives). Bees in each location will have access to the same respective landscape of flora to forage from and will also be fed identical nutritional supplements (protein supplementation, sugar syrup). Samples will be collected at the initial hive set-up, then once every 3 months for a year. They will be collected from nurse bees (who stay within the hive) and foraging bees (who leave the hive). Use of apiaries, acquisition of bees, and subspecies identification is enabled by collaboration with Dr. Ramesh Sagili of the Oregon State Honey Bee Lab. High-throughput shotgun metagenomic sequencing will be used to assess and compare the bees’ microbial structure at the phylotype and strain level between and within a) subspecies, b) nurses/foragers of each subspecies and at each location, and c) longitudinally, to assess patterns of functional microbiome congruence between the genetically and geographically distinct subgroups of bees. Additional data including winter survival rates per subspecies will be collected. Microbial samples from the flora the bees forage from in the different apiary locations will be collected in an attempt to ascertain the microbes they are exposed to outside of the hive. Aim 2: Comparative analysis of functional and spatial diversity in the A. mellifera gut microbiome following immune system challenge. Germ-free bee studies have found upregulation of antimicrobial peptides in hemolymph (blood analog in bees) to be associated with inoculation by specific gut microbiota members3. I will determine if altering the microbial community with probiotics can modulate the immune response of the bees and reduce fatalities due to Nosema ceranae (a parasite associated with bee depopulation⁶). To do so I previously collaborated with the Honey Bee Lab and performed a three-week in vivo experiment on the addition of probiotics during Nosema infection. Microbiome samples were harvested from the midgut and hindgut of single bees (Nosema localizes in the midgut). My pilot 16S sequencing confirmed that our methods are sensitive enough to detect distinct spatial microbial compositions (consistent with the literature). Shotgun metagenomics will be performed to compare strain-level functional diversity between experimental groups and gut regions, as well as determine impact of probiotic strains on composition and functional diversity. Functional pathways will be identified in each group and quantitatively compared to ascertain up- or downregulation of antimicrobial peptides and other immune-related genes in the context of infection or probiotic addition. mRNA- seq will be completed to quantify and compare host response to those pathways identified by metagenomics. Pathway predictions will also be confirmed via LC-MS/MS of antimicrobial peptides present in the bee hemolymph. Broader Impacts – Research Dissemination: The sequences and metadata generated by this project will be made publicly available through the online BeeBiome consortium2, where there is a pressing need for comprehensive honey bee microbiome data. Results regarding probiotic treatment of honey bee hives will be communicated to bee keepers through local and nation-wide beekeeping meetings and publications. I anticipate submitting a first author paper detailing my findings in spring 2020, as well as presenting my results at the International Society for Microbial Ecology 2020 meeting in Cape Town, South Africa. At OSU, I will utilize opportunities to share my research, such as the bi-annual Center for Genomic Research & Biocomputing conferences, where I have presented research previously. Broader Impacts – Science Communication: I will share my research with audiences from K- 12 children, community members, and faculty. I plan to involve the community in my research, by expanding this project to include bee gut and local flora samples donated by regional beekeepers. This community-driven, crowd-sourced approach is necessary to characterizing honey bee health outside of the lab setting. I also plan to involve high school AP Biology students during the collection and characterization of apiary flora (Aim 1). I currently serve as the Outreach Coordinator for the Micro Grad Student Assoc. and am in the process of developing several opportunities for local K-12 children to learn about honey bees, utilizing hands-on activities at the weekly Saturday farmer’s market. This combination provides an optimal platform for me to talk about my research and engage with community members on the themes of how humans rely on bees for our food production, and how the health of honey bees impacts us. 1. Pollinator Health Task Force. 2016. Report to Congress. 2. Engel et al. 2016. MBio. 3. Kwong et al. 2017. R. Soc. Open Sci 4. Khoury et al. 2018. Frontiers. 5. Johnstone et al. 2012. R. Soc. Bio Sci 6. Rubanov et al. 2019. Sci. Rep	0
Introduction: Understanding exactly how femalesmake matechoices,andhow thesemaponto male fitness and quality, has important ramifications for understanding trait evolution, and for conservation in species where female choice plays a large role in male success. Leks, where many males display at once hoping to attract a mate make an ideal model because female decision-making pathways should depend only on male display or other on-lek factors, and not on extrinsic factors such as territory quality or parental care. I propose to apply eye-tracking technology to female Greater Sage-Grouse (Centrocercus urophasianus) to determine how they acquire information from visual displays produced by males when selecting mates. Eye movements can be used to understand cognitive processes as animals must focusonaparticular subject to process the information given by it. Limitations on sensory processing determine the amount of information a female can take in about a male and thus limits what she considers when choosing a mate.1 I hypothesize that female choice is drivenby integrationof multiple signal elements. Aim 1: I will track female eye movements to determine which parts of the display females consider as they focus on one element at a time before moving on to another. Females cannot take in the whole of the display at once and should reduce their focus to only what they perceive to be the most important signal elements. Aim 2:Once females’preferredtraits areknown, Iwill assessthe differencesbetween successful and unsuccessful males to determine if there is likely selection pressure onthose traits. Finding those differences will demonstrate that females are intaking sensory information, processing it, and using it to make decisions about mating. Methods: I will travel to lek sites across Montana to conduct this research. Using cameras and GPS units2 placed on males, I will track general positions of males on the lek and estimate territory sizes for each male. I will also track mating success for each male by counting the number of matings attempted and presumably successful matings. For assessing females’ preferred traits, I will use eye-tracking technology3 to determine how females are acquiring information about important traits. Analysis of eye movements will show the specific features females target and use as criteria for mate choice. Using data from eye-tracking, I will measure the male ornaments females focused on, comparing between males with many matings, males that received some female attention butfewmatings, andmalesthat didnotreceive anymatings. I will analyze the differences between each group for each trait to assess whether the trait is under significant selection pressure driven by female choice. 1Dukas R. 2002. Behavioural and ecological consequences of limited attention. Philos. Trans. R. Soc. B Biol. Sci. 357, 1539-1547. 2Wann, GT, Coates, PS, Prochazka, BG, Severson, JP,Monroe, AP, Aldridge, CL. Assessing lek attendance of male greater sage‐grouse using fine‐resolution GPS data: Implications for population monitoring of lek mating grouse. Popul. Ecol. 2019; 61: 183– 197. 3Yorzinski JL, Patricelli GL, Babcock JS, Pearson JM, and Platt ML. 2013. Through their eyes: selective attention in peahens during courtship.J. Exp. Biol 216:3035-3046 Intellectual Merit: Eye-tracking is currently an under-utilized4 but insightful method in understanding visual cognition, especially in sexual selection. My research contributes to expanding use of this technology to understand how sight can affect trait evolution in an organism where visual traits are dramatic and emphasized. It provides other scientists with the understanding that these types of studies are feasible. It can be translated to see how cognition varies across the animal kingdom and increaseknowledgeabout visualcognition’sroleinsexual selection. Further, my research addresses the NSF Big Idea “Understanding the Rules of Life” because the process of decision making in sexual selection and female choice is still notclearly defined. Not only will I address the intraspecific interactions between males and females in a population, but I will assess cognition on an organismal level, to figure out how females intake information and perform a highly complex behavior, choosing a mate, in response. My experience working with autonomous recording units and GPS mapping with my undergraduate advisor and in my honors thesis has prepared me to work with various types of technology in the field. I am comfortable handling and managing equipment and data collected from the units. I am able to translate these experiences to this project to create an efficient workflow tohandle massamounts ofdata. I alsohave experienceanalyzinghighvolumesof data from audio recordings, which is easily transferable to processing video. Broader Impacts: With my proposed research, I will create opportunities for undergraduate research and citizen science. I will have undergraduate research assistants help me with capture, measurements, and placement of GPS receivers. These students will gain valuable training in fieldwork techniques andexperimental design,preparing them toalso goontoexpand scientific knowledgein ecology, animalbehavior, andorganismal biology.Asadisabled andfirst generation student, I will reach out to increase research participation for underrepresented students alongside my plans to expand access to research opportunities for underserved students. I will also recruit citizens from the surrounding areatohelp collectdata about thelek, thereby allowing more citizens to appreciate the habitat and gain an appreciation of science. Special effort will be made to extend this opportunity to local high school students who may not otherwise have the opportunity to conduct research with scientists. Leks areparticularly charismatic and tend to engage a wideaudienceof citizensof all agesinterested inbirds,which gives me the opportunity to teach them about behavior andhow tomake scientificobservations. At the same time as citizen outreach, I will also educate thepublic about thesagebrush habitat, Sage Grouse, and emphasize the importance of conservation science inmaintaining ahealthy environment. I also plan to collaborate with the U.S. Fish and Wildlife Service, local conservation organizations and Native American tribes to preserve habitat and reduce the amount of frackingin thearea,leadingtoimproved environmentalconditions, preservationof culturally important land, and more greenspaces, improvingmentaland physicalwellbeing for those living around sagebrush. Increased contact between the groups will also facilitate better working relationships and improve society for people who use these lands. 4Billington J, Webster RJ, Sherratt TN, Wilkie RM, and Hassall C. 2020. The (Under)Use of Eye-Tracking in Evolutionary Ecology. Trends in Ecology & Evolution.Trends Ecol. Evol.	0
EvaluatingCausalModelswithBiometricallyInformedData The relationship between poverty and health has been widely covered in the popular media; within the last year, the Atlantic, Time Magazine, and the New York Times have all featured the SES-health gradient at length, drawing strongly upon conventional explanations that material dis- advantage directly (e.g, access to medical care)1 or indirectly (e.g, chronic environmental stress)2 causes health inequalities. Such explanations account for differences between those who have resources and who do not. However, Gottfredson3 argued that situational explanations do not ac- count for the finely stratified health differences that exist across the entire range of SES, whereas individual differences in intelligence and personality do. Indeed, Chapman and colleagues4 found thatpersonalitycharacteristicsaccountfor20%oftheSES-healthgradient. Regardless of whether the person or situation is the “elusive fundamental cause”3 of the SES- healthgradient,thedebatecannotbesettledusingexperiments. Althoughrandomizedstudiessup- port inferring causality for most situational explanations, poverty cannot be randomly as-signed. Norcanwerandomlyassignmostpersoncharacteristics5. Instead,longitudinalquasi-experimental studiesareused,andpotentialconfoundsareincludedascovariates. The typical use of covariates does not provide control for systematically confounded genetic and environmental influences. This approach risks misattributions of causality. Indeed, poverty and individual differences covary with genes and environment, to such a degree that the covariate approach is fundamentally biased (Rowe & Rodgers, 1997)6. Yet, the covariate approach has been theprimarymethodusedtoevaluatethecauseoftheSES-healthgradient. Instead, quasi-experimental designs can be used; such designs support causal inference with- out random assignment7. Sibling-based quasi-experimental models are particularly effective at in- corporating genetic and environmental design elements8. However, such models are underused in psychology (Rodgers et al, 2001)9, tend to focus on environmental confounds1,0 and do not nat- urally incorporate varying levels of relatedness8. My research proposal aims to address both the methodological problem of evaluating person-driven hypotheses and the substantive problem of explainingtheSES-healthgradient,usingandextendingthesiblingcomparisonapproach. KinshipDyads Traditionalsiblingcomparisonmodelsoftenrelyonrareevents(i.e,twins)oradvancedmethod- ology (e.g, propensity score matching, multilevel modeling). As an alternative, my advisor and I have adapted Kenny’s reciprocal standard dyad model1.1 Our adaptation controls for gene and shared environmental influences within a simple regression framework, by taking the difference betweenthetwosiblings(Rodgers,Garrisonetal,2014):12 Y =β +β Y +β X +β X diff 0 1 mean 2 mean 3 diff Y +Y X +X 1i 2i 1i 2i where Y =Y −Y ;Y = ;X =X −X ;X = diff 1i 2i mean diff 1i 2i mean 2 2 In this model, the relative difference in outcomes (Y ) is predicted from the mean level of diff the outcome (Y ), the mean level of the predictor (X ), and the between-sibling predictor mean mean difference (X ). The mean levels support causal inference through at least partial control for diff genes and shared environment. Therefore, we simultaneously evaluate the individual difference (X diff) and the joint contribution of genes and shared environment (Y mean&X mean). Preliminary applicationshavegivenestimatesconsistentwiththeliterature(Garrisonetal,2015)1.3 S.MasonGarrison,VanderbiltUniversity S.MasonGarrison,VanderbiltUniversity ProposedStudies Application I will apply the kinship dyad model to two nationally representative household samples: the National Longitudinal Survey of Youth 1979 (NLSY79) and the NLSY97. The two household sampling techniques have resulted in 15,589 families with 19,374 sibling pairs, which ourresearchteamhasidentifiedandvalidated. Bothsurveysincludemeasuresofconscientiousness andintelligence,themostconsistentpredictorsofhealthandSES.TheNLSY97alsoincludesself- reportedBigFivepersonalityindicators. I will directly test the impacts of SES, personality, and intelligence on health, estimating how much of the SES-health gradient is caused by each (including covariance among the predictors). Then, I will evaluate specific causal mechanisms, such as access to health care, neighborhood quality, and education level. Identifying specific mechanisms will facilitate translating findings into interventions. To ensure that these findings are externally valid, I will compare results across samplestotestwhetherthegradienthaschangedacrosscohorts. Extension After validating the model using real-world data, I will extend the model in three ways: 1extendthemodeltoincludeadditionalhighlycorrelatedpredictors,2evaluatethemodel’s robustnessundermeasurementerror;3incorporatemultiplegenerations. First, although the Big Five are orthogonal by design, intelligence is correlated with multi- ple facets. For my 1st year project, I evaluated the separate impact of intelligence and conscien- tiousness by partialing out the common variance, resulting in uncorrelated and uncontaminated measures of each1.3 Generalizing this approach will enable the model to test more complicated re- lationships between predictors. 2 To evaluate the model’s robustness, I will conduct a series of Monte Carlo simulations, using NSF’s high-performance computing service, XSEDE under var- ious levels of measurement error for both the outcome and predictor variables. Moreover, I will evaluatethemodel’sexternalvalidityinrelationtopreviousresearchthathasprovidedparameters estimates for the relationship between individual differences, health, and SES. If the kinship dyad model correctly estimates the parameters, this further supports that the model has performed cor- rectlyunderreal-worldconditions. 3Themodelcanbefurtherextendedbyexaminingthechildren ofthesiblings,inthesamewaythatthechildrenoftwinsdesignworks–byexploitingthecommon genetictraitsanddissimilarenvironmentaleffectswithinstandardbiometricalmodels. Thiswould allow the model to distinguish between genes and shared environmental causes. Moreover, the effectivenessofthisextensioncanbetestedusingthemultigenerationalstructureoftheNLSY79. Merit&Impact ThisresearchhasthepotentialtohelpuntilttheSES-healthgradient. Directly,byapplyingthis modeltotestthecausesoftheSES-healthgradient,Iwilldeterminewhetherthepersonorsituation is driving the gradient. Identifying specific mechanisms will facilitate translating these findings into actionable interventions. Indirectly, my research will support accessible and parsimonious solutiontomakecausalinferencesaboutperson-drivenhypotheses. Otherresearcherswillbeable toemploythismodelintheirownworkontheSES-healthgradient. To enhance the model’s accessibility to other researchers, I will develop R, SAS, and SPSS syntax, and make it available on my website with detailed tutorials. Moreover, as this approach can be used on many datasets, I will identify and link to compatible files from various academic databases,suchasHarvard’sDataverseandUniversityofMichigan’sICPSR. Refs1Adleretal(1994)AmeriPsych. 2Baumetal(1999)NYAcadSci. 3Gottfredson(2004)JPSP.4Chapmanet al(2009)AmeriJofEpi. 5West(2009)CurDirPsychSci. 6Rowe&Rodgers(1997)DevRev. 7Shadishetal(2002) Wadsworth. 8 Rutter (2007) Persp Psych Sci. 9 Rodgers et al (2000) Ameri Psych. 10 Lahey et al (2010) Cur Dir PsychSci. 11Kennyetal(2001)PsychBul. 12Rodgersetal(2014Jun)BGA.13Garrisonetal(2015Feb)SPSP.	0
The sparse and heterogeneous distribution of water in savanna landscapes largely determines herbivore distributions. Thousands of animals gather at watering holes and riverbanks, which become foci for both competition and predation. Species with relatively low water needs can take advantage of large, naturally occurring gaps (“refugia”) between persistent water sources to avoid competition and predation; in fact, these refugia can be critical to maintaining their populations1. Kruger National Park (“Kruger”), South Africa, offers a unique long-term experiment of the effects of surface water augmentation on herbivore distributions. Kruger was fenced in the early 1960s, obstructing historic migrations of grazers to dry- season water sources. In response, management installed hundreds of watering holes (“boreholes”), but these fragmented the dry refugia, allowing the ranges of drought-intolerant species (i.e. Equus quagga, Connochaetes taurinus) to expand into those of drought-tolerant, locally rare antelope1,2 (RA) (i.e. Taurotragus oryx, Hippotragus equinus, H. niger). This attracted predator attention to refugia and created more competition for forage during the severe droughts of 1981, 1985, and 19923. Surface water also attracts elephants, which can drastically alter the surrounding ecosystem and impact forage biodiversity4. This increased predation, competition, and ecosystem engineering all contributed to the alarming population declines in RA in the 1980s3. Realizing this, in 1997 Kruger’s management scheme was re- examined, and two-thirds of boreholes were closed4. The current conditions for Kruger’s RA are still fraught, however. Borehole closures have not returned refugia to their original extents4; many remain open for their touristic value (e.g. guaranteed presence of diverse species). In addition, climate models project that southern Africa will experience more frequent and intense droughts5, especially threatening Kruger’s drier northern areas, where most of the park’s RA refugia are located6. Motivation: Water access drives complex savanna dynamics. Lingering hydro-homogeneity in Kruger, along with projections indicating more frequent and intense droughts, make it imperative to understand how this community is structured, how species interactions shape responses to climate change, and how these might change in the future. I will determine (1) how surface water sources and resulting interspecific interactions affect how large herbivores respond to climate variables, and (2) how this changing community structure impacts resilience to extreme environmental events. I will also create a tool for Kruger park managers to infuse sharper knowledge of ecosystem structure into management aims. Methods: I will analyze herbivores’ interactions and responses using GJAMtime, a Bayesian time- series ‘generalized joint attribute model’ developed by Dr. Jim Clark7. GJAMtime models species distributions through environmental and interspecies interactions, improving upon traditional static species distribution models that fail to address interactions between species and temporal dependence on previous conditions. GJAMtime builds species migration, density-independent (DI) growth, and density- dependent (DD) interaction terms into a familiar Lotka-Volterra model and generates a species-interaction coefficient matrix (“a-matrix”), enabling dynamic community structure analysis8. In preparation, I have divided a map of the park into 710 30km2 grid squares, to which I either aggregated (borehole locations, fires, geology, climate variables) or interpolated (rainfall, grass abundance) annual environmental covariates. I determined Bayesian priors for species interactions and responses to covariates through review of savanna literature and personal communication with established savanna scientists. Aerial annual census data are rich; from 1977-98, park rangers counted all animals in parkwide airplane census; from 1998 onwards, 800-m wide transects replaced the expensive full-park census (providing 15-28% coverage instead)9. For transects, data on individual animals’ distance-from-aircraft were also collected. With knowledge of each species’ detection rate (a factor of coloring, preferred habitat, and size), I will derive species detection functions using the R package ‘DSim’ to estimate true herbivore abundances. Research Plan: This project extends my exploratory analyses, detailed in my personal statement, to include borehole and fire data, larger temporal extents, narrower hypotheses, and novel statistical tools. (1) How does surface water affect community structure, and how can this in turn limit RA population density? Hypothesis: If greater hydro-homogenization permits species invasion of refugia, then negative impacts on RA will be seen through (1) decreased populations and (2) negative a-matrix species interaction coefficients. Equilibrium population sizes are a function of migration, DI, and DD growth; I will compare equilibrium abundances and a-matrix coefficients from greater and lesser borehole concentration years to determine whether these boreholes negatively affect RA populations. I will compare pre-1997 data (high borehole density) to post-1997 data, the latter proxying a time with few boreholes. I will extract how much of each species’ climate responses are coming from migration, DI, and DD growth using GJAMtime estimates. If results do not support my hypothesis (negative effect of boreholes on RA population density), then this indicates the influence of other sources on RA population decline, including an increase in elephant disturbance after culling was ended in 1992; anthrax outbreaks in roan antelope in the early 1980s; or changes in fire management, extent, and intensity over time. (2) How does changing community structure affect the savanna ecosystem’s resilience to extreme events? Hypothesis: If these changing community structures reduce ecosystem resilience, then increasing hydro-homogeneity will (1) increase drought recovery time, (2) increase fire recovery time, and (3) increased community instability. Major droughts occurred in Kruger in 1981-82, 1985-86, 1991-93, 2014- 16, and 2018-20. These droughts straddle the borehole closures of the 1990s. Natural and prescribed fires, with varying intensities and times between burns, also occur regularly across the savanna. For intense local fires and parkwide drought events, I will compare each species’ population the year before the event to its population in subsequent years to determine population recovery time. I will compare these trends to a stability analysis of the a-matrix (with negative eigenvalues indicating stability) for these periods to see if community structure and species interactions are impairing ecosystem recovery after extreme events. If we do not see evidence for this, then the influence of management decisions (e.g. removal of fencing, end of elephant culling) or climatic interactions (e.g. fires during drought years) played a larger role than species interactions, setting the groundwork for future studies in community stability and resilience. Resources: The Kruger GIS Lab provided all data, and South Africa National Parks registered my project this fall. In March 2021, I will share my research at Kruger’s Savanna Science Network Meeting, discussing my assumptions and findings with other savanna scientists. I will also travel to Kruger in 2021 and 2022 to participate in dry season censuses and attend courses on savanna ecosystems. My math degree and two years’ coding experience at IBM prepared me well for the analysis of the complex models proposed in my study. I am advised by statistical and climate-change ecology expert Dr. Jim Clark and Kruger grassland ecologists Dr. Steve Higgins (University of Bayreuth) and Dr. Carla Staver (Yale). Intellectual Merit: This analysis is crucial to maintaining one of the world’s last Pleistocene megafauna savanna ecosystems. My novel statistical analysis will illuminate the relationship between species interactions and community environmental responses, a connection understudied in modern ecological literature. I also propose to determine how a changing community structure affects an ecosystem’s resilience to climate change, a pressing issue to today’s ecologists. Finally, my analysis of Kruger’s extensive, long-term datasets with GJAMtime’s Bayesian Gibbs-sampling techniques certainly meet the 2020 GRFP solicitation goal of supporting “computationally intensive research”. Broader Impacts: Adaptive park management requires use of near real-time animal census and climate data. I will provide park managers with a data workflow to feed each year’s new census data into my GJAMtime model. I will (1) convert GJAMtime’s outputs to be readable by non-statisticians (2) write change detection functions to highlight how herbivore responses vary from previous years, (3) convert the code from steps 1 and 2 into an open-source user interface in RShiny to allow park managers to run code without prior programming skills, (4) solicit regular feedback from park management, and (5) provide iterative, ongoing support and tool improvement through GitHub. These five steps provide a link between the environment-species interactions model and a more user-friendly interface for management decision- making. Additionally, as described in my personal statement, I will use my African savanna conservation research to develop open-source RShiny modules, based on Kruger herbivore and environmental data, to engage high schoolers in Durham Public Schools to boost their skills and confidence in math and coding. [1] CC Grant et al. 2002. Koedoe 45. [2] IPJ Smit 2011. Ecog. 34. [3] MP Veldhuis et al 2019. Ecol. Lett. 22. [4] IPJ Smit 2013. Pachyderm 15. [5] Working Group II, Fifth Assessment Report of the Inter- governmental Panel on Climate Change, 2014. In: Impacts, Adaptation, and Vulnerability, Part B: Regional Aspects. [6] JO Ogutu & N Owen-Smith 2003. Ecol. Lett. 6. [7] JS Clark et al. 2017. Ecol. Monogr. 87. [8] JS Clark, et al. 2020. PNAS, 117. [9] JM Kruger et al 2008. Wildl. Res. 35.	0
Novice programmers who are writing code encounter frequent challenges, which they often attempt to address by searching online to learn new concepts or debug their code [3]. However, novice programmers rarely receive explicit instruction on how to effectively resolve programming challenges with online search. Seeking help through online search is a critical self-regulatory skill for students, both in class and after graduation, but professional programmers agree that it can be difficult to learn [8]. Some prior work has investigated the challenges that professional developers face when searching and how to support them. However, there is little research about how programmers learn to use online search, how to teach search effectively, or how adaptive tools can support this process. Further, prior work has been limited to collecting user search queries and developer surveys, without using the programmer's code and errors to understand their context and the reason for their search. My research goal is to 1) understand the strategies that undergraduate novice programmers use to search for help online when writing and debugging code, and 2) explore how to design adaptive learning environments that support students in learning effective online search strategies. Building on Prior Work: Collecting data from novices’ programming environments has become popular in Computer Science (CS) Education due to their ability to provide granular views into the programming process via incremental code snapshots. These code snapshots have been leveraged to extract features such as compiler error encounter rate and error resolution time, which are used to understand novice debugging behaviors. In addition, code snapshots can be used in the design of interventions that dynamically provide feedback during the programming process based on current and past code contexts. It has been shown that intelligent tutoring systems (such as augmented programming environments) that provide timely automatic feedback can positively affect learning [5]. Work has been done on augmenting learning environments with tools such as web browsers that filter search results to be more understandable by novices [2,7]. However, many of these tools focus on improving search results: no tools have been developed with a focus on improving code search behavior and imparting generalizable long-term search skills. RQ1: What barriers do students face and what strategies do they employ when using web-based search systems to write and debug code? In Year 1, I will conduct exploratory lab and classroom studies to identify common barriers that novices encounter and strategies that they use for online search during programming problems. My goal is to discover what unique problems novices encounter when using online search, inform pedagogy on how online search should be taught, and learn what features to consider when designing a tool to support better search behavior. In small scale lab studies during the first semester, novice programming students will be asked to solve programming problems appropriate to their skill level while having access to online search. I will collect think-aloud protocols as students work, pre- and post-surveys, logs of students' search behavior, and fine-grained code snapshot logs. Classroom studies of NCSU’s introductory computing (CS1) course during the second semester utilize augmented programming environments and provide a large volume of search and code snapshot data, which will allow us to test if the findings from the lab studies are generalizable to the classroom. Using this data, we can evaluate how search behavior from beginner programmers may differ from or align with prior work on professional developer behavior, such as in query style, query reformulation rate, and search session length. Building on prior work on using code snapshots to track compiler error resolution [1], we will explore how to track students’ success in using online search to resolve errors. This will allow us to identify which students are struggling, where they are struggling, and what search strategies are effective. RQ2: How can we give students accurate and timely feedback on how to improve the effectiveness of their search queries? In Year 2, I will perform design-based research of a learning environment that will provide automated support to students to improve their search skills. While the ultimate design of the system will be shaped by student and teacher needs identified in Year 1, the system will support developing students’ skills in the key phases of the search process. The key phases of search are derived from theories of help-seeking (e.g. [4]): recognizing the need for help, gathering relevant information, formulating an effective query, identifying an effective source of help, and integrating the help into code. For example, consider a student who is stuck on an error and whose previous search attempts were not successful. The system could then suggest an effective help-seeking strategy that was identified in RQ1, such as query reformulation. The system will offer adaptive help, responding to a student’s current code and error message. I will achieve this by building on foundational work by the HINTS lab at NCSU on using program code to create adaptive feedback systems [6]. My current membership in the HINTS lab puts me in a unique position to design this system. RQ3: What effect does timely automated feedback on search queries have on novice programming behaviors and their ability to use web-based search to resolve future problems? After multiple rounds of development in Year 2, in Year 3, I will deploy the system and perform classroom studies to measure the impact this tool has on novices’ search and programming behavior. These classroom studies will initially take place over the course of a semester in NCSU’s CS1 course. The effectiveness of the intervention will be measured by the change in student programming performance and retention of help- seeking behaviors over time (as evidenced by features of successful help-seeking identified in RQ1, such as query styles and error resolution rate). In addition to quantitative metrics, qualitative feedback by students at the end of the semester will also inform the system’s overall impact. Intellectual Merit: This project will provide novel insight on the strategies that undergraduate novice programmers use to search for help online. By extending prior work on programmer search behaviors through the novel methods of observing program and error state, we can gain granular information about the contexts in which novice programmers seek help. In exploring effective search strategies (RQ1), we can inform the design of better pedagogy for teaching online search. The feedback methods designed in RQ2 and evaluated in RQ3 will inform future designs of intelligent tutors for help-seeking. This project will provide a base for future work on understanding programming help-seeking behavior, and the methods discovered could be extended to learner populations beyond novices. Broader Impact: In many CS programs, searching for code help online is a skill that is not explicitly taught, yet is expected of students in upper-division courses and after graduation. Explicit pedagogy around web search will normalize help-seeking behavior and reduce impostor syndrome. Automated feedback systems allow for the refinement of these skills even if an instructor is unavailable. This project will inform future online search pedagogy and the design of learning environments that reinforce these skills. By addressing these two issues, this project will help make CS more accessible. [1] Jadud. “Methods and Tools for Exploring Novice Compilation Behaviour” ICER‘06 [2] Lu et al. “ ” Information Retrieval Journal ‘17 [3] Muller, et al. Exploring Novice Programmers' Homework Practices: Initial Observations of Information Seeking Behaviors SIGCSE ‘20 [4] Nelson-LeGall. “Help- Seeking: An Understudied Problem-Solving Skill in Children” Developmental Review ‘81 [5] Nesbit, et al. “Intelligent Tutoring Systems and Learning Outcomes: A Meta-Analysis” Journal of Educational Psychology ‘14 [6] Price, et al. “iSnap: Towards Intelligent Tutoring in Novice Programming Environments” SIGCSE ’17 [7] Venigalla et al. “StackDoc - A Stack Overflow Plug-in for Novice Programmers that Integrates Q&A with API Examples” ICALT’19 [8] Xia, et al “What do developers search for on the web?” Empir Software Eng ‘17	0
"Development of a pressure-sensitive kinetic blood-brain-barrier Aβ clearance model Introduction: Alzheimer’s disease (AD) is a chronic neurodegenerative disease and the main cause of dementia worldwide. Recently discovered, the glymphatic system is a waste clearance pathway that exchanges the central nervous system (CNS)’s fluids which has been shown to be one of the main facilitators of the clearance of beta-amyloid (Aβ), one of the hallmark pathology of Alzheimer's disease (AD).1,2 This system carries the soluble Aβ peptide to be discharged through the blood-brain barrier (BBB) by specialized transporters and has been shown to become impaired in ageing, leading to its toxic accumulation in the brain.3 Furthermore, AD is often implicated with cardiovascular alterations affecting blood pressure and though their treatment has been correlated with reduced incidence of AD and slower cognitive declines in AD patients, their mechanistic relationship remains unclear.4 Although, Aβ clearance has been mainly characterized in vivo in animal models, they also involve the interplay of other cellular and enzymatic clearance mechanisms.4 Thus, a validated dynamic in vitro model would provide a platform to quantify and predict the yet unexplored effects of abnormal blood and cranial pressures in the BBB’s clearance of Aβ which are critical to advance our understanding of how one of the most prevalent vascular changes correlated with AD, affects the main clearance gate protecting us from it. The objective of this proposed research is to build a mathematical model from the kinetic characterization of pressure effects in the transport of Aβ through a microfluidic blood-brain-barrier device to predict the effects of pathologic changes in cerebral perfusion pressure in the clearance of Aβ from the brain. Approach: The proposed project will execute the following aims: (1) to adapt and validate current microfluidic BBB technology for this project, (2) to experimentally characterize the in vitro BBB’s Aβ kinetic transport as a function of protein load and pressures, and (3) to develop a mathematic model of the Aβ efflux across the brain in health and in disease. Aim 1: Microfluidic models have recently succeeded in recreating and mimicking the selective boundary properties of the BBB; one example includes the NeuroVascular Unit (NVU) model developed at Vanderbilt University by Dr. Wikswo’s lab (Fig. 1), involving two distinct chambers representing the blood and the brain sides, separated by a three-dimensional and sequential culture of an immortalized line of the human neurovascular cells building the structure of a complete BBB.5 Our device will build upon this model to allow for the controlled monitoring of pressure and flow Fig. 1: Wikswo’s lab blood- rates while applying its ability to recreate the human BBB’s brain-barrier-on-a-chip device.5 heterogeneity and structural complexity. Firstly, the devise will be equipped with loading port valves, a variable pulsatile pump to mimic vascular perfusion pressure and a syringe pump to maintain an accurate pressure and simulate glymphatic flow on the brain chamber. The boundary’s integrity under these dynamic conditions will be validated via transendothelial electrical resistance measurements (TEER) or alternatively via fluorescent microscopy to confirm proper endothelial tight junctions. Thereafter, pH, temperature, viscosity, and salt content on both compartment fluids will be matched to their human physiologic conditions, as protein behavior is sensitive to these factors. Aim 2: Commercially-available Aβ and Aβ peptides, will be infused at different 1-40 1-42 concentrations separately on the brain compartment, at different pressures under physiologic mean blood and glymphatic system flow rates to measure the BBB’s transport rates at which Aβ leaves the brain compartment. Right after running through the devise the solution will be sampled and Sergio Rodriguez Labra | Research Statement Aβ measured by ELISA. After multiple trials, a differential mass balance on the two compartments will be used to calculate two diffusive rate constants, one for the Aβ going from the brain to the blood side, and a reverse rate to account for potential Aβ reuptake into the brain compartment, allowing for the overall rate of Aβ transport to be calculated for different Aβ concentrations and pressures. To validate the physiologic operation of both of the known Aβ transporters, LRP1 and RAGE, in the cells, their commercially-available natural ligands will be similarly run as a control, comparing their transport rates to their physiologic literature values.6 Aim 3: Using the calculated Aβ transport rates, statistics on R will be used to confirm the data’s power and a mathematic model on MATLAB will be built to describe the Aβ efflux as a function of Aβ concentration and the pressures across both chambers, thus, quantitatively correlating the relationship between Aβ transport at different healthy and pathologic peptide burdens in the human brain, and under physiologic and abnormal pressures on both sides of the BBB. The developed model will be validated testing its predictions on the quantitative change in Aβ Fig. 2: Possible effects of pressure accumulation in the microfluidic device after sudden or chronic in A clearance for a fixed A load. pressure changes likely to be produced by stroke, traumatic brain injuries, and cardiovascular diseases. This will produce a predictive computational model of the decrease in BBB-mediated Aβ clearance with ageing and disease. Broader Impact: The development of a mathematical model focused on cranial and blood pressure will allow for the estimation of Aβ accumulation in the brain due to reduced BBB clearance and predict the increased risk to acquire AD after people develop hypertension, hypotension, stroke, or traumatic brain injuries, when it is much easier to take preventative and therapeutic measures than trying to arrest an evolved AD later on. Furthermore, by considering Aβ concentrations, this adaptable predictive model will be able to incorporate future Aβ biomarkers’ data for personalized medicine diagnostics using the patient’s own biometrics. Finally, the improved microfluidic device will serve as a platform for characterizing the effects of abnormal blood pressures in the arterial pulse-driven glymphatic system, thus, describing its synergistic dysfunction in disease, and for studies describing the pharmacodynamics of brain- penetrant drugs in altered vascular perfusion or cranial pressure conditions. Studying the link of the discussed highly prevalent clinical conditions has important repercussions in everyone’s everyday lifestyle decisions. To generate awareness, I will use my continued participation at the Society of Hispanic Professional Engineer’s national conferences and the blog I will manage during grad school to present and break down the significance of my findings to the general public, inspiring them to pursue similar projects in STEM. 1 Nedergaard, Maiken. ""Garbage truck of the brain."" Science 340.6140 (2013): 1529-1530. 2 Tarasoff-Conway, Jenna M., et al. ""Clearance systems in the brain - implications for Alzheimer disease."" Nature Reviews Neurology 11.8 (2015): 457-470. 3 Kress, Benjamin T., et al. ""Impairment of paravascular clearance pathways in the aging brain."" Annals of neurology 76.6 (2014): 845-861. 4 Hamel, Edith, et al. “Neurovascular and cognitive failure in Alzheimer’s disease: benefits of cardiovascular therapy.” Cellular and molecular neurobiology 36.2 (2016): 219-232 5 Brown, Jacquelyn A., et al. ""Recreating blood-brain barrier physiology and structure on chip: A novel neurovascular microfluidic bioreactor."" Biomicrofluidics 9.5 (2015): 054124. 6 Deane, R., et al. “Clearance of amyloid- peptide across the blood-brain barrier: implication for therapies in Alzheimer’s disease”. CNS & Neurological Disorders-Drug Targets 8.1 (2009): 16-30"	1
Imagine an empty room with four walls. Away from the walls and inside the space, a person has the freedom to move 3-dimensionally in any direction and is constrained to move 1-dimensionally with respect to time. However, this person is unable to view the entire room while simultaneously residing in the interior. There are photons, particle interactions, and various physical phenomenon that are unable to transmit their information to the person when these events are out of sight − that is, unless this person is at the wall. At the bound- ary of this room, a person sacrifices a dimension of freedom in exchange for viewing the internal dynamics of their space. This way, a person is able to survey their space and return to the interior with knowledge of the native physical interactions − fully revealing the under- lying dynamics. This duality between an n-dimensional interior and an (n−1)-dimensional boundary is known as the Anti-deSitter Space/Conformal Field Theory Correspondence. This correspondence is powerful. General Relativity asserts that space-time and gravity are fundamentally connected, while a pivotal aspect of Quantum Field Theory is the freedom for symmetries to arise and reduces the number of degrees of freedom. Maldacena[1] was the first to assert that an interior space-time, such as Anti-deSitter Space described by General Relativity, could be connected to a conformal boundary, where a Quantum Field Theory would reside, thereby linking Gravity and QFT. This gauge/gravity duality has a variety of applications ranging from condensed matter experiments to particle physics. With respect to the Standard Model of Particle Physics, Quantum Field Theory asserts that fundamental particles can be described as excitations of quantum fields. These parti- cles, such as quarks and gluons, constitute most of the visible matter in the universe, and are described by Quantum Chromodynamics through the strong force. As a strongly cou- pled gauge theory that lacks a fully theoretical description, the mapping provided by the gauge/gravity duality might reveal a gravitational dual to QCD, which is not only highly desirable but also potentially feasible. Duringtheprevioussummer, Ihadtheopportunitytoconductresearchthroughmysecond NSF REU at the University of Minnesota studying the AdS/CFT conjecture as applied to non-perturbativegaugetheories, specificallyQuantumChromodynamics. Underthetutelage ofDr.JosephKapusta,Istudiedthepropertiesoftheglueball,aparticlecomposedofmultiple gluons predicted by the Standard Model, and a dilaton, a hypothetical particle that arises from the scalar fields accompanying gravity. To incorporate the behavior and structure of these fundamental particles into this duality, one usually embarks on the bottom up approach by assuming the existence of such a dual and, thereby, models QCD as an effective five-dimensional gravitational theory. This approach, known as AdS/QCD, provides the freedom for the computation of physical quantities in QCD that can then be tested at the high energy collisions at particle accelerators. At the REU, I developed the equations of motion from considering an action[2],[3] that connects both gravitational field and the glueball and dilaton fields. This pen and paper work derived an analytic expression for the potential and this result describes the behavior of these particles at the IR and UV energy ranges. Early in the universe’s lifetime and in modern-day particle collisions, a hot QCD quark-gluon plasma exists for short times whose behaviorposesachallengeforQCDphysics. Theresultsofourstudymightshedlightonthis plasma, while furthering a theoretical description for QCD using the AdS/CFT conjecture, thereby connecting theory and experiment. As the lead author, I have attended the DNP 2015 and APS 4C Conferences to present this work and aim to obtain my second publication. 1 Aditya Dhumuntarao Graduate Research Proposal I wish to continue applying the AdS/CFT Correspondence to QCD for my Ph.D dissertation. With my Ph.D research, I aim the further bridge the gap between the plethora of experimental evidence and the developing theoretical considerations for QCD by incorporating additional phenomenological metrics into the AdS/QCD conjecture. A critical question that I wish to pursue is the construction of a model incorporating the dilaton field, glueball field, quark field, and other matter fields with finite temperature field theory. As finite temperature quantum field theory describes the expectation values of physical observables at finite temperatures in (n−1)-dimensions and links them to statistical classical field theories, such as gravity, in n-dimensions, the AdS/QCD conjecture seems to have a natural supplement. Since Dr. Kapusta is a leader in the field of finite temperature theory[4], joining his research group is highly desirable for the formation of this model. Currently, IamworkingwithDr.Kapustatodeterminethepredictedglueballmassspectra from our REU analysis and ultimately compare this value against lattice QCD calculations. In fact, a recent publication[5] has argued that an experimentally detected particle, known as f (1710), is the glueball. Therefore, improving our model to include more realistic glueball 0 dynamics and performing a comparative analysis after determining the mass spectra of the glueball is of principle importance. In addition to tackling fundamental science harmoniously through theoretical and experi- mental considerations, this project contains broader impacts. A significant result from this analysis would lend further credence to the AdS/CFT conjecture. Although the conjecture was founded on the premise of revealing a theory of quantum gravity, a more immediate ap- plication resides in condensed matter experiment where chiral magnetic fields are frequently studied with the conjecture. Also by revealing physics beyond the Standard Model, we may be able to describe or construct new matter from gluons and quarks − one such highlight is the strong evidence for the tetraquark found at CERN. This new matter might impact our searches for dark matter often hypothesized[6] as complex quark matter, additional stable elements that may assist in creating new materials, and efforts in high energy plasma physics which has immediate applications for fusion. The purpose of the AdS/CFT conjecture is to explore and discover a unified description of the universe and the interactions within our universal boundary. Matter is the fundamen- tal link that interacts with and connects gravity with the other forces. In the short term, developing a theoretical framework for QCD will inform and drive further experimental en- deavors, while the long term promises support for a candidate conjecture that currently sees a versatility of applications. With the support from the NSF GRFP, I will have the free- dom to immediately pursue this research and continue my current studies of the AdS/CFT conjecture to develop a unifying framework of quantum matter and gravity. [1] J. M. Maldacena. The Large N limit of superconformal field theories and supergravity. Int. J. Theor. Phys., 38:1113–1133, 1999. [2] S. P. Bartz, J. I. Kapusta. Dynamical three-field ads/qcd model. Phys.Rev.D, 90:074034, 2014. [3] J. I. Kapusta, T. Springer. Potentials for soft-wall ads/qcd. Phys. Rev. D, 81:086009, 2010. [4] Joseph I. Kapusta, Charles Gale. Finite-Temperature Field Theory. Cambridge University Press, second edition, 2006. Cambridge Books Online. [5] F. Bru¨nner, A. Rebhan. Nonchiral enhancement of scalar glueball decay in the witten-sakai- sugimoto model. Phys. Rev. Lett., 115:131601, Sep 2015. [6] E. Witten. Cosmic separation of phases. Phys. Rev. D, 30:272–285, Jul 1984. 2	0
In order to understand evolution, we need to understand the complex relationships between biological systems and fitness. This is a difficult task, because there are an enormous number of possible genetic states, and the mutations underlying these states interact non-additively to produce fitness. We can frame this problem by thinking of evolution as a process occurring on a high-dimensional map between this space of genetic possibilities and the fitness of each possibility, a function often referred to as the “fitness landscape.” As a population adapts to a particular environment, it moves between neighboring genotypes, constrained by the force of selection to follow paths of increasing fitness. By understanding the general properties of the fitness landscape, we can answer questions about the functional nature of a biological system - If a mutation knocks out this gene, what effect will that have on fitness? - and ask broad theoretical questions - Is evolution predictable, or does it depend on chance events? The growing field of experimental evolution provides an avenue for addressing these questions by empirically testing important features of the fitness landscapes of microbes. We now know that in the budding yeast Saccharomyces cerevisiae, the effect of a beneficial mutation depends on the fitness of the genetic background where it arises [1], but whether a similar pattern holds for deleterious mutations is an open question. Deleterious mutations may be common in populations due to environmental changes or population bottlenecks, and they provide a novel way to study adaptation and to test the role of contingency in evolution. In my PhD research, I will study the fitness landscape of S. cerevisiae by investigating the interplay between deleterious mutations and adaptation. I will complete my PhD research in Dr. Michael Desai’s lab at Harvard University, where I am uniquely situated to conduct work that combines genetics, experimental evolution, and deep sequencing. Aim 1. Changes in the fitness effects of loss of function mutations over adaptive trajectories Given that most loss of function (LOF) mutations are deleterious, competing models make different predictions about how their effects should change with increasing population fitness. The Desai lab recently found that in S. cerevisiae, the fitness effect of a beneficial mutation in a particular genetic background is primarily predicted by the fitness of the background, creating a pattern of “diminishing returns” during adaptation [1]. If this “global epistasis” model holds for all mutations, deleterious mutations should also become less deleterious as the population becomes more fit. In contrast, Fisher’s geometric model predicts that the fitness effect of some mutations will change from negative to positive at different levels of adaptation [2]. I will use transposon mutagenesis and sequencing fitness assays (Tn-seq) to measure the fitness effects of a large set of loss of function mutations in populations with different initial fitness backgrounds. Hypothesis: I predict that, in accordance with the global epistasis model [1], LOF mutations will be less deleterious in populations with higher fitness. Methods: First, I will evolve 24 S. cerevisiae populations in standard liquid media for 1000 generations (100 days), following a similar protocol to [3]. I will freeze samples every 250 generations to create a “frozen fossil record” of each population as it adapts and gains fitness. At each of the five timepoints in this record, I will unfreeze my populations and use Tn-seq to systematically probe the fitness effects of a large number of LOF mutations. As shown in the figure at right, Tn-seq consists of two steps. In A, I transform a gene disruption library into the population, causing a diverse set of single insertion mutations. In B, I track the frequency of each mutation over 30 generations using deep sequencing of a barcode region in the insertion [4]. Using this method, I can determine the fitness effect of every mutation in parallel by analyzing the change in its frequency [3,4]. I will create my DNA-barcoded transposon (Tn) gene disruption library using genomic DNA from S. cerevisiae [3], and by sequencing this library, I will associate a unique barcode with each gene disruption. These associations will allow me to connect my data to specific genes, yielding additional biologically relevant information about how S. cerevisiae adapts to laboratory conditions. Aim 2. Adaptation after disruption of the genetic system Evolution often involves transient environmental changes that alter selection pressure or population size, both of which can lead to the fixation of mutations that are not beneficial in the organism’s primary environment. Do these events affect long-term outcomes of evolution? The dynamics of adaptation after a population has been “bumped” off of its adaptive trajectory are not well understood, but they have the potential to distinguish between models of adaptation. For fitness landscape models in which mutations interact only additively, any deleterious mutation simply slows adaptation. However, in “rugged” fitness landscape models where mutations interact non-additively, it is possible that deleterious steps can lead to exploration of a previously inaccessible part of genotype space, potentially allowing a population to ultimately reach higher fitness. I will capitalize on the Tn-seq method to distinguish between these models by evolving “disrupted” populations alongside “undisrupted” populations and comparing their fitness trajectories. Hypothesis: I hypothesize that deleterious mutations will be more likely to improve evolutionary outcomes in poorly adapted populations, as predicted by [5]. Therefore, I predict that disruption due to Tn insertions will lead to higher final population fitness relative to the undisrupted populations only when the original disruption occurs at early time points from the frozen fossil record. An alternate prediction is that disruption will slow adaptation in all cases, which would support additive landscape models. Methods: I will propagate “Tn-disrupted” populations from Aim 1 for 500 generations. I will measure mean population fitness every 100 generations in these populations and at the corresponding timepoints in the “undisrupted” populations using standard fluorescence-based competitions [1]. Intellectual Merit My project aims to connect ideas about the dynamics of adaptation on fitness landscapes to a functional understanding of how a model organism changes as it adapts. Using massively parallel, sequencing-based fitness assays, this project will provide unprecedented resolution of the functional changes a population experiences during adaptation, and through evolution of Tn- disrupted clones, this study will test basic questions about the fitness landscape of evolving S. cerevisiae. Broader Impacts We now know that large asexual populations, in the form of pathogens or cancer cells, are involved in over a quarter of deaths worldwide [4]. While my research is centered on basic science questions, these basic principles of asexual adaptation are an important part of building models of how these diseases progress. I will publish my work in peer-reviewed journals aimed at a scientific audience, but I will also use the power of animations and interactivity to make my evolution research come alive on my web site, where it can be shared with the general public. As detailed in my personal statement, I will also use science communication and video to empower young people to pursue STEM careers by showing them the human side of research. [1] Kryazhimskiy S et al. 2014. Science 344: 1519-1522. [2] Fisher RA. 1930. Clarendon Press, Oxford, U.K. [3] Van Opijnen T et al. 2009. Nature Methods 6: 767-772. [4] Levy SF et al. 2015. Nature 519:181–186. [5] Nahum JR, et al. 2015. Proc Natl Acad Sci USA 112:7530–7535.	0
Hypothesis: The compositional evolution of crustal material (both felsic and mafic) following impact events can be simulated by heating and partially vaporizing such materials at high temperatures in containerless experiments. Varying the temperature (T), time (t), and oxygen fugacity (fO ) in the 2 experiments enables the creation of synthetic impact glass analogues. Comparing the textures and compositions of these analogues to those of natural glasses will inform how these materials evolve during the tektite formation process. Incorporating the results of partial vaporization experiments into mixing models of tektite formation will result in more realistic predictions of tektite parent materials and potentially explain why tektites do not form from mafic protoliths on Earth. Background: There are currently 190 confirmed craters on Earth’s surface1, only four of which are in basaltic targets. Lonar crater, which formed in Deccan Trap basalt, is the only well-known and fully accessible of these. During impact events, shocked and melted material may be ejected from the site of impact. Material that is thrown more than 2.5 crater diameters away from the impact location is known as distal ejecta2. Glassy, chemically homogenous, and aerodynamically shaped distal ejecta are known as tektites. Only four of the 190 terrestrial craters have resulted in tektite distribution over wide geographic regions known as strewn fields, and all known tektites originate from target rock that is felsic, approximating rhyolitic compositions. This research will investigate why tektites have never formed from mafic (basaltic) target rock during a terrestrial impact event, and to seek to better understand the nature and evolution of the starting materials that formed tektites from the four major strewn fields. Existing mixing models3–6 assume compositions of tektites represent idealized end-member mixtures of the melted target material, and most attempts to identify the parent materials involve general comparison among chemical components of tektites and predicted parent materials5. These models are overly simplistic as they do not account for the loss of chemical constituents to volatilization at high temperatures in the impact plume. Research Plan: (Research Goal 1: Tektite experiments) I will synthesize glasses that replicate the geochemistry and textures of tektites and Lonar crater impact glass in an aerodynamic levitation laser furnace7 (ALLF). Melting/vaporization experiments will vary T, t, and fO to identify conditions that form 2 Lonar glass analogues (Table 1). Table 1. Experimental conditions based on previous experiments estimating tektite thermal histories7 Series 1 2 3 4 5 6 7 8 9 T(°C) 1800 1800 1800 2000 2000 2000 2200 2200 2200 t (s) 10-120 10-120 10-120 10-120 10-120 10-120 10-120 10-120 10-120 fO Ar O CO+CO Ar O CO+CO Ar O CO+CO 2 2 2 2 2 2 2 Starting materials are Deccan Trap basalt, USGS basalt (BCR-2), and USGS rhyolite (RGM-2). Experimental methods will follow previous ALLF tektite experiments7. Approximately 10 mg of the starting material is heated with a CO laser at low power to fuse the sample and form spheres suitable for 2 levitation. The fused spheres are then levitated on a flow of gas (Ar, O , or CO+CO ) while being heated 2 2 with the laser for seconds to minutes. After heating to the desired T-t, laser power is cut, and the melt spheres cool quickly to form glass. The chemical, mineralogical, and textural characterization of synthesized and natural glasses will be completed via scanning electron microscopy with energy dispersive x-ray spectrometry (SEM/EDS) at Indiana University–Purdue University Indianapolis (IUPUI) and electron probe microanalysis (EPMA) at Washington University in St. Louis (WUSTL). These analyses will be compared to electron microprobe analyses of Lonar glass8, and previous work on felsic material7. Exploratory experiments using the starting materials and methods described above resulted in dark colored, glassy spheres with little to no vesicles or mineral growth. SEM/EDS analyses of these preliminary experiments show preferential loss of Na and K from all melt compositions relative to non-alkali components. Further, heating basalts in an oxygen-rich environment results in more rapid and complete loss of K O (undetectable after 10 s at 2000 °C). 2 (Research Goal 2: Mixing model calculations) Tektites are not produced from a single, homogeneous source rock – thus, any explanation of tektite generation must involve a multi-component mixing model involving likely upper crustal target rocks (sediments) as the major components9. Previous studies have estimated the contributions of possible target lithologies to final tektite geochemistry using mixing models. However, the assumptions and limitations of existing models may hinder their ability to realistically represent tektite formation. Previous mixing models assume that end-member compositions of likely protoliths are unchanged during tektite formation, i.e., they do not take into account the effects of evaporative fractionation3 on the target materials as they are heated in the impact plume. I will address this by executing computationally intensive mixing models that properly account for fractionation due to vaporization and assess the extent of agreement between assumed target lithologies and tektite geochemistry. I will incorporate my experimentally derived volatilization rates into dynamic multi- component mixing calculations by modifying the GeoChemical Data toolkit (GCDkit) software to create a more realistic representation of the processes attending tektite formation, and to determine the relative contributions of each protolith to different tektite compositions. I will also compare my experimentally derived volatilization rates, and the results of my mixing models with predictions of compositional evolution of tektite melts from MAGMA code. My ongoing research uses the R programming language to perform multivariate analyses and implement machine learning algorithms to investigate tektite compositional trends. I am creating an open- source web-based application to classify unknown tektites into their strewn fields and subgroups within strewn fields. This NSF Graduate Research Fellowship will afford me the opportunity to expand my programming and modeling portfolio to more computationally intensive applications. Intellectual Merit: The modification of planetary surfaces through impact cratering is the most important surface modifying process on most rocky bodies in the Solar System10, yet it remains underrepresented as a field of study. My research will improve both the understanding of the evolution of Earth’s crust and the modeling of impact events. My mixing model calculations will be the first to properly account for the chemical modifications that occur in the impact plume. These calculations will produce a useful tool for the scientific community to analyze (or reanalyze) terrestrial impact products and target lithologies. My experimental data will also provide valuable insight into the volatilization behavior of felsic and mafic melts at conditions relevant to impact plumes. The results may shed light on the absence of mafic tektites. Broader Impacts: The results of my research will have applications in a diverse set of fields such as high- temperature geochemistry, computational modeling of geochemical processes, and impact processes. I will disseminate the results in journals (e.g., Computers & Geosciences), at conferences (e.g., AGU, GSA), and to GK-12 students and the general public at outreach events (e.g., Pacers STEM Fest, Celebrate Science Indiana). As an NSF Graduate Fellow, I will continue working towards a more equitable and inclusive culture. Already as a new graduate student I became a founding member of IUPUI’s Geology COmmunity for Racial Equity (GeoCORE). I also came up with the idea for the Mineral Eponym Crowdsourcing Initiative (MECI), an effort to facilitate an examination, analysis, and synthesis of the origins of mineral names, and the messages this historic naming system has created. This initiative is currently being led by members of the Mineralogical Society of America (MSA) Diversity Task Force (my advisor is a member) to seek funding for its execution. Most recently, I have been invited to participate in a project to create new curricular materials for the IUPUI Earth Sciences Department focusing on issues at the intersection of Earth sciences, ethics, and equity. My role in this project will be to analyze, quantitatively and qualitatively, the assessment results of teaching material effectiveness. The analytical and computational skills I will learn as an NSF Fellow will allow me to perform analyses that will directly impact the inclusivity, diversity, engagement, and retention of underrepresented groups within the geosciences. References: [1] Earth Impact Database, PASSC [2] Glass, B. P. et al. Elements 8, 43–48 (2012) [3] Ackerman, L. et al. Geochim et Cosmochim Ac 276, 135–150 (2020) [4] Ferrière, L. et al. Chem Geol 275, 254–261 (2010) [5] Love, K. M. et al. 52, 2085–2090 (1988) [6] Meisel, T. et al. Meteorit Planet Sci 32, 493–502 (1997) [7] Macris, C. A. et al. Geochim et Cosmochim Ac 241, 69–94 (2018) [8] Ray, D. et al. Earth Moon Planets 114, 59–86 (2014) [9] Koeberl, C. Tectonophysics 171, 405–422 (1990) [10] Koeberl, C. in Treatise on Geochemistry 73–118 (Elsevier, 2014)	0
Background: With the growing implementation of inquiry-based labs in physics, students are no longer following rote procedures and are expected to utilize complex experimental skills like measurement uncertainty, experimental modeling, and computation, which require students to engage in sensemaking [1]. We view sensemaking as “a dynamic process of building or revising an explanation in order to ‘figure something out’—to ascertain the mechanism underlying a phenomenon in order to resolve a gap or inconsistency in one’s understanding” [2]. Given that existing research on sensemaking has focused on textbook problem solving, research is needed to understand how sensemaking appears in inquiry-based labs, given their increasing prevalence [3]. Physics is often viewed by students as a confusing, unapproachable subject; understanding student sensemaking is an important step in developing labs that are more accessible to a range of students, beyond physics majors. Preliminary Results: In response to the call for inquiry-based physics labs, as well as labs that serve as better preparation for pre-medical and other life science students, the PER group at the University of Utah has implemented Introductory Physics for Life Sciences (IPLS) labs, in which students investigate physical mechanisms in the context of biological systems. In my preliminary work with the group, I explored student sensemaking in IPLS labs, and I found that the instances of sensemaking led to students having a deeper understanding about the relationship between their data and the relevant physical systems. Furthermore, this initial analysis was instrumental in determining an appropriate theoretical framework for the proposed research. First, I noticed that students didn’t often fully articulate their thinking and thus my data was limited. To ameliorate this problem, I aim to use a think-aloud protocol in which students are encouraged to share all their thoughts during the lab investigations, which will provide a more complete data set. However, a limitation of interviews is that they are not as authentic, so I find it is important to triangulate interview data with the lab observation data. Second, I observed that students were frequently comparing their model that was generated as a result of collecting and analyzing data to their existing mental model of the relevant system. Theoretical Framework and Research Plan: For my research plan, I draw on two complementary theoretical frameworks. First, I will use the modeling framework for experimental physics, which was first developed for upper-division physics labs and functions on a recursive interaction between a student’s physical system model and their measurement model [4]. Given that measurement models are less common in introductory physics, I focus on a data- based model, which captures the focus of these labs where students are predominately analyzing data; this is a similar adjustment to that which has been implemented elsewhere [5]. Second, I will utilize epistemic games, which are defined as the rules and strategies that guide inquiry; in PER, this framework has been used to study structured problem solving and knowledge development tasks [6-8]. Defining an epistemic game includes specifying the target epistemic form, constraints, entry conditions, moves within the game, and transfers to other games [9]. Based on my initial analysis, student sensemaking in labs has characteristics that parallel the form of an epistemic game, e.g., making moves toward an end goal of resolving inconsistencies. These two frameworks are complementary as the recursive elements of modeling translate to moves in a game. Each framework on its own has certain limits, but together they are more comprehensive and powerful; they will allow for a rigorous analysis of student sensemaking in inquiry-based labs. The steps of my research plan are as follows: Step 1: I will identify all instances of sensemaking in the existing lab observation data, focusing on the classroom environment factors that contribute to the sensemaking. Step 2: Based on identified classroom environment factors from Step 1, I will write a task-based interview protocol intended to prompt sensemaking, and I will conduct these interviews with a different population of undergraduate life-science students. Step 3: I will first identify instances of sensemaking in the interview data. Then, using instances of sensemaking from both the observational data and interview data, I will code my data using a coding scheme developed for the different parts of the modeling process, based on the modeling framework (e.g., revision of the data-based model, comparison between models). Step 4: With the modeling framework analysis done in Step 3, I will define sensemaking epistemic games that occur in the inquiry-based lab environment by coding the data with key features of epistemic games. Depending on the prior analysis, I will either define one epistemic game that describes general sensemaking or a number of games that each describe a different form of sensemaking. The merger of these two datasets, as well as the combination of the modeling framework and the epistemic games framework will allow for an in-depth understanding of student sensemaking in inquiry-based physics labs. Intellectual Merit: The inquiry-based physics labs are designed to better replicate genuine research environments, as well as encourage students’ agency and scientific inquiry; as such, they are increasingly prevalent in undergraduate curricula. But yet, there is limited research on sensemaking in these labs. To address this hole in the literature, I will take a novel approach of combining two disparate theoretical frameworks, epistemic games and modeling, and two complementary data sets, to gain a holistic understanding of sensemaking in inquiry-based labs. Results will be new knowledge about sensemaking in these increasingly prevalent labs that can serve as a foundation for future lab research, along with an example of how to effectively combine disparate theoretical frameworks that can serve as a model for other scholars in the field. Broader Impact: Understanding student sensemaking in inquiry-based labs through the epistemic game framework will have direct instructional implications. For instance, we can improve training so that TAs and instructors can recognize sensemaking epistemic games in labs and utilize them in instruction, asking questions that prompt sensemaking, rather than recollection of facts. Such instructional changes will improve undergraduate physics education, and specifically better prepare pre-medical students for medical school and other life science students for postgraduate studies and careers. Life science students often view physics as a foreign subject; supporting pre-medical students in productive sensemaking in these labs may encourage more students to continue in these labs and be successful in long term careers in the medical field [10]. While typically unintentional, physics can be a “weed-out” course for these students, so these changes intended to increase retention have the potential to improve equity and contribute toward diversifying the medical field. Beyond the effect on pre-medical students, encouraging sensemaking is a step toward teaching students that they have relevant experience that can be used in a physics setting, making physics a more accessible subject for all. References: [1] Holmes, N. G. et al. Proc. Natl. Acad. Sci. (2015). [2] Odden, T. O. B. and Russ, R. S. Sci. Ed. (2018). [3] Odden, T. O. B. and Russ, R. S. Phys. Rev. Phys. Educ. Res. (2019). [4] Zwickl, B. M. et al. Phys. Rev. Phys. Educ. Res. (2015). [5] Vonk, M. et al. Phys. Rev. Phys. Educ. Res. (2017). [6] Tuminaro, J. and Redish, E. F. Phys. Rev. Phys. Educ. Res. (2007). [7] Hu, D. et al. Phys. Rev. Phys. Educ. Res. (2019). [8] Chen, Y. et al. Phys. Rev. Phys. Educ. Res. (2013). [9] Collins, A. and Ferguson, W. Edu. Psych. (1993). [10] Moore, K. et al. Am. J. Phys. (2014).	0
The relationship between craft production and the political and economic development of complex societies continues to generate debate in the field of Anthropology. The study remains relevant because craft production is a firmly embedded element of culture (Costin 1991:2). This field of research has broad impacts as it can be expanded to the understanding of socio-economic organization as well as the identification of gender roles (Brumfiel 2006: 862). Ultimately, knowledge of production methods and their roles in a society can lead to a stronger understanding of political organization (Brumfiel and Earle 1987:3-5). Earle‟s (1987:69) work on Hawaiian and pre-Inkan societies indicates that the centralization of production reflected strengthening political control. Further evidence for an increasing complexity in these societies came from emphasis on specialized craft production. Earle‟s work is an excellent example of how archaeological research can tie craft production to the development of complex society. This proposal discusses conducting further research on this topic in the Early Bronze Age (EBA) settlement at Zincirli, Turkey. This site, known primarily for its Iron Age occupation, has an eight hectare walled town dating to 2500 BCE. Excavation conducted in the late 19th century uncovered ceramic and architectural remains which have been used to date the EBA areas (Lehman 1994:106-107). No further excavation on these areas has been attempted since that time, although magnetometry readings have produced evidence for additional architecture. Excavation of the EBA sections at Zincirli will begin in 2010, led by Dr. Christoph Bacchuber as sub-director to Dr. David Schloen, of the University of Chicago. This excavation will provide an opportunity to examine the relationship between craft production and political centralization. A model well suited for the interpretation of this work is the political development model of specialization, exchange, and complex society, as defined by Brumfiel and Earle (1987:1-4). This model suggests that elites control the organization of local craft production and consequently are the primary beneficiaries. The political development model is reflected by evidence of craft specialization, the organization of local production, and the mobilization of goods from producers to elites. To properly address this model and determine its efficacy in understanding the EBA settlement at Zincirli, the following research questions will be addressed: 1. To what degree were craft production activities specialized at this site? 2. How was craft production organized at this site? (What are the units of production?) 3. What manner of mobilization of craft products to elites is present? Costin (1991:4) defines specialization as a “differentiated, regularized, permanent, and perhaps institutionalized production system” in which an individual does not produce all of the goods he/she consumes, but rather is dependent on others to produce certain items. A supportive example of this would be the discovery of a collection of a particular artifact type bearing a high degree of standardization. Conversely, a highly diverse artifact typology suggests a greater number of individuals and thus less specialization (Costin and Hagstrum 1995: 632). Organization can be determined through the distribution of particular artifacts (tools, raw material, waste). Appearing in a centralized location suggests a „work-shop‟ organization, whereas a more diffuse distribution in individual structures suggests household production (Costin 1991:6, 8, 21-29). Control of craft product by elites can be identified through the proximity of production centers to elite residencies (Earle 1987:68-69) and the appearance of specialized craft goods in elite settings (Brumfiel and Earle 1987:3-5). A wide dispersal of specialized craft goods could indicate a lack of elite mobilization. In order to address my research questions, data will be derived through excavation and the entry of piece plotting measurements into a GIS database. After three field seasons with the Chicago team, I anticipate having a database large enough to derive spatial patterns indicating either the presence or absence of centralized craft production. This analysis, similar to that used at Titriş Hoyuk (Hartenberger 2000), will provide the basis for determining the relationship between craft production and political centralization. Titriş Hoyuk, a comparative case study for craft production, is approximately 115km east of Zincirli, and has a contemporaneous EBA habitation. Excavation has yielded a lithic workshop indicating organized, centralized, and specialized craft production (Hartenberger 2000:51). Intellectual Merit The results of this analysis will increase our knowledge of the conditions of this time period and region of Anatolia. The Anatolian Early Bronze Age marks a significant change in economic, political and technological developments (Yenner & Vandiver 1993: 208). This proposed research will provide information on each of these characteristics. One advantage of increasing this knowledge is the ability to link it with the wealth of research that has been conducted in Mesopotamia. This will aid in producing a larger and more regional picture of complex society in the Eastern Mediterranean. My class work, which has trained me in GIS, and my previous excavation experience, especially at Zincirli, Turkey, have given me the tools to conduct this research. My current research for a Bachelor‟s of Philosophy thesis has enabled me to use these tools on a similar archaeological problem. Through my focus on textile production at the EBA site Karataş, in SW Anatolia, I am familiar with much of the relevant literature for this proposed research. Also, I now have experience in using GIS to derive information from artifact distribution and densities. Broader Impact This work builds on previous research conducted on craft production, and will contribute to our understanding of this phenomenon both regionally and theoretically. To disseminate the outcomes of this research, I will actively engage with the academic community by presenting my findings at academic conferences and by producing publications. This work will also contribute to my abilities to teach anthropology and archaeology, by giving me the knowledge and experience to express these concepts to others. References Cited Brumfiel, Elizabeth (2006). Cloth, Gender, Continuity, and Change: Fabricating Unity in Anthropology. American Anthropologist, Vol. 108, No. 4: 862-877. Brumfiel, Elizabeth and Timothy K. Earle (1987). Specialization, Exchange, and Complex Societies. Cambridge University Press: Cambridge. Costin, Cathy Lynne (1991). Craft Specialization: Issues in Defining, Documenting, and Explaining the Organization of Production. Archaeological Method and Theory, Vol. 3: 1-56. Costin, Cathy Lynne and Melissa B. Hagstrum (1995). Standardization, Labor Investment, Skill, and the Organization of Ceramic Production in Late Prehispanic Highland Peru. American Antiquity, Vol. 60, No. 4: 619-639. Earle, Timothy K (1987). Specialization and the Production of Wealth, in Elizabeth Blumfiel and Timothy K. Earle, eds., Specialization, Exchange, and Complex Societies. pp. 64-75. Cambridge University Press: Cambridge. Hartenberger, Britt et al.(2000). The Early Bronze Age Blade Workshop at Titris Hoyuk. Near Eastern Archaeology, Vol.63, No.1:51-58. Lehmann, Gunnar (1994). “Zu den Zerstörungen in Zincirli Während des Frühen 7. Jahrhunderts v. Chr.” Mitteilungen der Deutschen Orient-Gesellschaft zu Berlin, 126:105–122. Yenner, K. Aslihan, and Pamela B. Vandiver (1993) Tin Processing at Göltepe, an Early Bronze Age Site in Anatolia. American Journal of Archaeology, Vol. 97, No. 2:207-238.	0
Modeling the FeMoco Cluster of Nitrogenase: Synthesis of a μ -Carbide Metal Cluster 3 Ammonia is the second-largest synthetic chemical product worldwide, with 1.68 × 108 tons produced annually.1 Typically, ammonia is synthesized via the Haber-Bosch process, where a heterogeneous iron oxide catalyzes the Figure 1. (left) Structure of the FeMo cluster of nitro- reaction between hydrogen and nitrogen.1,2 genase. The terminal iron is bound to cysteine; the ter- About 85% of ammonia produced is used to- minal molybdenum is bound to histidine and chelated wards crop fertilization; large-scale ammonia by homocitrate. (right) Proposed synthetic target, corre- production has been credited for the quadru- sponding to the left cubane of FeMoco. pling of the world’s population from 1.8 to 7.4 studies have attempted to calculate stable nitro- billion in the last 100 years.2 The process is typ- gen binding sites of FeMoco and possible inter- ically performed in excess of 400°C at 200 atm, mediates of the reduction.5,6 However, in order and is highly energy-intensive, consuming to determine the true mechanism, experiments about 3–5% of the world’s yearly natural gas on a structural model are necessary. production and 1–2% of the global energy pro- Due to the high complexity of FeMoco, a duced. These are consequences of nitrogen’s synthetic model adequate for detailed struc- high stability: the N–N triple bond of N is one ture-function studies has proven elusive thus 2 of the strongest covalent bonds known, with a far; as of yet, no models incorporate the inter- dissociation energy of 226 kcal/mol.3 stitial carbon atom. I propose to develop a syn- Despite the stability of nitrogen gas, many thesis of a novel cubane cluster structurally rel- classes of bacteria and archaea have evolved evant to FeMoco (Figure 1). This tetrametallic the ability to catalytically fix nitrogen via the cluster contains an unusual bridging carbide enzyme nitrogenase, which couples the reduc- ligand coordinated to three iron centers. tion of nitrogen to the hydrolysis of ATP over Trimetallic bridging carbide clusters are 8 single-electron transfer events (Scheme 1). known for various metals, including Ti,7 Co,8 Remarkably, nitrogenase is capable of fixing Ru,9 and Os.10 However, these isolated com- nitrogen at ambient temperatures and pressures, plexes all contain strong-field ligands such as a feat that humans have yet to mimic. CO and cyclopentadienide; μ 3-carbides are ex- tremely rare for complexes containing low- field ligands such as sulfides, making the syn- thesis of a [4Fe-3S-C] cluster such as this a for- Scheme 1. Balanced half-reaction for nitrogenase- mediated nitrogen fixation. P = inorganic phosphate. midable challenge. i Nitrogenase contains three types of metal Lee and coworkers have previously synthe- cluster cofactors: the most complex of these is sized a [4Fe-3S-N] cluster containing a μ -im- 3 the FeMo cluster (FeMoco), where nitrogen is ido ligand, analogous to the proposed μ -car- 3 bound and reduced (Figure 1). FeMoco is com- bide, from a [4Fe-4S] cluster and a nitride- prised of 8 metal centers encapsulating a small carrying bimetallic species.11 I will use this interstitial atom. The interstitial atom’s identity route as a model synthetic strategy (Scheme 2). has been long debated as either C, N, or O; re- The μ -imido nitrogen in Lee’s system is de- 3 cent research has provided strong evidence for rived from an amine; an isoelectronic reagent the identification of this atom as carbon.4 such as an organolithium reagent could react Much is still unknown about the mechanism similarly. A carbene equivalent such as a diazo through which FeMoco reduces nitrogen, in- species could also serve as a carbon precursor. cluding the precise N binding site, the elec- An alternative synthesis draws from the 2 tronics of the cluster during reduction, and the work of Holm and coworkers, who synthesized role of the interstitial carbon. Computational pentametallic cuboidal clusters containing a NSF Graduate Research Fellowship Program 1 Graduate Research Plan Statement Jeremy C. Tran Scheme 2. Two proposed syntheses of the target cubane clusters, inspired by Lee (top) and Holm (bottom). [4Fe-3S] moiety capped by a fifth metal gating the clusters together, a true analog of through sulfide linkages.12 By using a small FeMoco could be synthesized. This full model phosphine such as PMe as L and a bulky phos- could be used in structure-function studies and 3 phine such as P(tBu) as L', the sulfides capping would prove invaluable in determining the pre- 3 the cuboidal face could potentially be more ac- cise mechanism of nitrogenase. cessible. This would allow for selective re- INTELLECTUAL MERIT, BROADER IMPACTS moval of the capping sulfides with a thiophilic The proposed research would provide a syn- reagent such as mercury, opening up a vertex thetic route to low-field μ -carbide metal clus- 3 for carbide insertion. ters, which are very uncommon and not well Although 13C NMR spectroscopy could po- understood. This research would also fill a tentially be used to check for incorporation of wide gap in our knowledge about the mecha- the carbide, the paramagnetism of iron clusters nism of nitrogen reduction via FeMoco. Know- diminishes the usefulness of NMR spectros- ing the mechanism through which biological copy as a characterization tool. Instead, product systems fix nitrogen could lead to the develop- characterization will focus heavily on mass ment of new manmade nitrogen fixation meth- spectrometry and X-ray crystallography, to de- ods, boosting the production of ammonia while termine whether the synthesis was successful. I simultaneously reducing energy inputs. will also use Mössbauer spectroscopy to deter- The primary use of ammonia is as a fertilizer; mine the oxidation states of the iron centers. a more efficient means of ammonia production Upon successful synthesis of the target, fur- would correspond to a potential increase in ag- ther experiments will be performed to fully ricultural productivity, helping to sustain the characterize the cluster. Cyclic voltammetry ever-growing global population. Although this will be utilized to test the redox properties of is not likely to outcompete the Haber-Bosch the clusters. The ability of these clusters to bind process, a small-scale source of readily obtain- N will also be examined, both under redox- able ammonia would be incredibly useful for 2 neutral and reducing conditions. In the event remote regions where fertilizer is scarce. that N successfully binds, characterization to References: (1) Ullmann’s Encyclopedia of 2 determine the binding site(s) will be carried out Industrial Chemistry; 7th ed. 647–698. (2) Nature 1999, 400, 415. (3) Comprehensive Handbook of Chemical via X-ray crystallography. The reactivity of a Bond Energies; 1st ed. (4) Science 2011, 334, 940–940. nitrogen-binding species towards various nu- (5) J. Am. Chem. Soc. 2003, 125, 15772–15778. (6) Annu. cleophiles and reductants will also be explored. Rev. Biochem. 2009, 78, 701. (7) Organometallics 1994, Future directions include using this carbide- 13, 2159–2163. (8) J. Am. Chem. Soc. 1958, 80, 6529– 6533. (9) J. Organomet. Chem. 2001, 633, 51–65. (10) containing cluster as a stepping stone towards Inorg. Chem. 1996, 35, 1405–1407. (11) Inorg. Chem. the full synthesis of a FeMoco-like cluster. By 2012, 51, 12891–12904. (12) J. Am. Chem. Soc. 1993, constructing the other half of the cluster and li- 115, 5549–5558. NSF Graduate Research Fellowship Program 2	0
A Quantum Field Theoretic Approach to Disordered and Amorphous Solids The objective is to formulate a fundamental theory of defects in disordered solids using quantum field theory (QFT) methods, with the ultimate goal of understanding amorphous materials. Motivation: P. W. Anderson said in 1995, “The deepest and most interesting unsolved problem ​​ in solid state theory is probably the theory of the nature of glass,1” glass being the prototypical ​ ​ highly-disordered amorphous material. Defects describe disordered materials in a more concrete, physical way, and are central objects of study in materials science. They govern the deformation of solids and overall mechanical properties, but also significantly affect electronic, optical, and other functional properties.2 This alternative approach to disordered systems holds great promise. ​ ​ Due to their long-range nature and effects, extended defects in particular, including dislocations, grain boundaries, etc., are difficult to model and not well understood theoretically. Common computational methods like density functional theory (DFT) scale poorly with system size, making such defects too costly for simulation.2 Classical molecular dynamics is based on ​ Newtonian mechanics, so doesn’t take into account any quantum effects. While there exist empirical models for the effect of defects on these functional properties, they need many parameters, and none are fully ab initio. A radically different approach is needed. ​ ​ Introduction and Background: The classical theory of dislocations more or less fully explains mechanical properties of materials. However, there remain many open questions on how dislocations and defects in general affect functional properties, that remain so because the classical treatment is insufficient. I propose to describe extended defects in disordered solids as ​ quantum fields that give rise to quasiparticles, enabling a deeper understanding of defect interactions. Motivated by the example of phonons as quasiparticles that quantize the lattice displacement field, we define a new quantum field for dislocations and its associated quanta – the “dislon”3. Much as the phonon theory led to great advances in understanding the effect of ​ ​ lattice vibrations on photons and electrons, this new theory will lead to a similar paradigm shift. Initial Successes–the Dislon: A dislocation is an extended crystal defect. Critical to formalizing ​​ them is the Burgers vector b, representing the dislocation-induced lattice distortion and ​ ​​ associated displacement field u. This u is the classical field we turn into a quantum field for the ​ ​​ ​ first quantization. Conceptually, the constraint imposed by b is very important, as the only thing ​ distinguishing a phonon from a dislon, both being just quantized u at heart. ​ ​​ Already this theory has been successfully applied to supplant previously incomplete explanations of materials phenomena, such as the effect of dislocations on superconducting critical temperature T . The expression above is analytically derived and ​ ​c describes the competition between quantum​ vs. classical effects.3 It predicts the direction of T ​ ​ ​c change after increasing dislocation density, which affects Poisson’s ratio ν and Lamé parameters ​ λ, μ. It is somewhat surprising because the expression combines variables/parameters describing ​ ​ ​ both mechanical and electronic properties, not usually seen together in one formula. Research Plan: I plan to join the Energy Nano Group, led by Prof. Mingda Li, creator of the dislon theory. His group combines theory and experiment, and I will contribute to the theory side of research. With the dislon as the origin, we can organize our future work along three axes. Axis 1–Deepening: The dislon theory still has great potential to be applied to many other types ​ of interactions. One example is dislon-induced topological phase transitions, where we turn a topologically trivial material into a topological insulator by tuning its bandgap. I will include electron-dislon interactions into band structure calculations, which avoids the computationally 1 Haihao Liu Graduate Research Plan Statement October 2018 expensive supercells required for DFT. Another application is dislon-enhanced Anderson localization, the suppression of electron diffusion in disordered systems.4 Dislons give us a way ​ to explore this effect in crystalline materials, which thus far has been challenging to observe. There is still much work to be done extending the theory itself. Most promising is the incorporation of gauge symmetry, a classical theory of which already exists for dislons.5 The ​ goal is to quantize this, analogous to quantum electrodynamics (QED) for electrons. In the same ​ way photons arise naturally from QED as a force carrier, we would expect something similar from a quantum gauge theory of dislons, that can shed light on the nature of plasticity. Axis 2–Broadening: The quantization procedure outlined for dislons can be readily generalized to account for anisotropy and discrete lattices (current derivation is for continuum limit). The initial success with dislons means this is a promising approach to defects in general. In ​ conjunction to the work Axis 1 on 1D dislocations, I will generalize the quantization process to ​ 2D grain boundaries and 3D inclusions. An analytical theory allows computation of interfacial transmission coefficients for the Landauer formula from microscopic and no ad hoc parameters. ​ ​ Axis 3–Experimental: There will be a strong experimental component to inform and augment ​​ the theory development. Working with our collaborators at Oak Ridge, we will probe phonon- dislon interactions using advanced techniques like inelastic X-ray and neutron scattering, to see if measured thermal transport and relaxation coefficients match those calculated from dislon theory. Additionally, we will use our lab’s crystal growth facilities to synthesize materials with controlled dislocation density, to explore dislon-induced topological phase transitions. Intellectual Merit: With the dislon model, all dislocation effects are incorporated: strain, coulombic, and vibrational. We now have a systematic framework to investigate the effect of defects on functional properties, by including a defect Hamiltonian. This allows us to calculate constants such as deformation potential coefficient ab initio, which will lead to a deeper ​ ​ understanding of material properties far beyond any empirical model. We can also now use tools from QFT like Feynman diagrams to study complex electron-phonon-dislon interactions. One may ask, why should extended defects be quantized to begin with? The intuition is that extended (as opposed to point) defects have spatial extent, captured by the “field” part of QFT, and its resulting internal dynamic structure is described by the “quantum” part. We have shown this idea applies to dislocations, suggesting it applies generally to all extended defects. Broader Impacts: These theories will have a profound impact on the direction of condensed ​​ matter and materials research. Much work now is on trying to find new materials with desired properties. By understanding how defects can induce such properties, we vastly expand the family of known materials exhibiting them, without needing to find completely new materials. As a radical theory that sits at the intersection of quantum field theory and materials science, two disciplines with relittle crosstalk, there is much educational potential. Through talking with my colleagues in the materials science department, I aim to start bridging the gap between these two communities. I plan to help develop a course introducing QFT to engineers, that I could be a TA for, and eventually down the line, perhaps even write a textbook on this theory. So far, over 20 years after Anderson’s comments, we still only have the empirical model of electrons in glass developed by Mott.1 By understanding defects, we hope to ultimately have a ​ fundamental theory of amorphous materials, as a limiting case of infinitely many defects. Works cited: [1] Anderson P W, Science 267, 1615-16 (1995) [2] Mott N F and Davis E A, Electronic processes in ​ ​ ​​ ​ non-crystalline materials (2012) [3] Li M, arXiv preprint, arXiv:1808.07777 (2018) [4] Anderson P W, Phys Rev ​ ​ ​ 109, 1492-505 (1958) [5] Kadic A and Edelen D G B, A gauge theory of dislocations and disclinations (1983) ​​ ​ ​ 2	0
Introduction: Over the past 130 million years, flowering plants have evolved a variety of visual and chemical cues that mediate species’ interactions. The diversity of color phenotypes in flowers has been the subject of many ecological studies and the biosynthesis and regulation of the main compounds responsible for pigmentation is well understood1,2. These compounds are well-known for their role in pollinator attraction and additionally have many important biological functions that have been described (e.g., allelopathy, lignification, protection from UV radiation)3. However, despite the significance of pigmentation in plant growth, development, and reproduction, the role of pollen color remains unclear. About 75% of flowering plant species have yellow or white colored pollen, though pollen may also be pigmented red, blue, purple, or black4,5. The composition of specialized metabolites that occur in pollen across several taxa (e.g., phenolic compounds, alkaloids, terpenoids) have also been described6. Several of these compounds are known to be important for pollen development, pollen germination, pollen tube growth, and protection from abiotic stress (i.e., temperature, UV)7. A Recent work from Dr. Shu-Mei Chang’s lab at the University of Georgia, Athens (UGA), has discovered pollen color polymorphism in wild geranium, Geranium maculatum. Field observations along the Appalachian Mountain region show that B C D purple and yellow pollen color morphs persist in different ratios along an elevational cline8. Though pollen color polymorphism Confocal LC-MS/MS has been observed in other plant species, the ecological function Figure 1. Experimental Design. (A) Purple & and adaptive value of this trait is still unknown9-12. Therefore, I yellow pollen color morphs. (B) iNaturalist propose to examine the geographic distribution patterns of pollen distribution data by community scientists. (C) Confocal microscopy & LC-MS/MS color morphs, characterize their phenotypes, and evaluate the characterization of pollen phenotypes. (D) functional role of this trait in an ecological context (Fig. 1). Reproductive trait evaluation under abiotic stress. Aim 1: Determine the distribution of pollen color by integrating field surveys and community science In native G. maculatum populations, dark pollen color morphs have been observed at higher elevations8; a trend that has not been observed in other plant species9-12. iNaturalist is an online platform and smartphone application that allows anyone to record observations in nature. To date, there are over 13,000 records of G. maculatum and over 1,600 individuals that have made observations across the US. To determine if there is a correlation between elevation and pollen color, I will use this data to analyze the occurrence of purple and yellow pollen color morphs in a logistic generalized linear model with latitude and elevation predictors, as described by Austen et al12. I expect to see a positive correlation between elevation and pollen color intensity along an elevation gradient based on previous field observations. Furthermore, I will geo-reference 20 populations of G. maculatum along an elevation gradient to serve as collection sites for my study. To supplement my own collection material, I will advertise this study on the Ecological Society of America listserv and on social media pages of native plant societies to recruit community scientists. I will then create collection kits with an overview of the project and a guide to propagule collection to mail to each individual, who will harvest from their respective site and mail their completed kits to UGA. Clear instructions will be provided to each collector on how to image the population and gather one representative specimen to allow confirmation. Aim 2: Characterize reproductive traits and pollen phenotypes of G. maculatum populations Pollen features that vary among pollen color morphs can have a significant impact on male fitness of a plant13. Thus, I will examine the morphological and biochemical characteristics associated with pollen color. I will propagate G. maculatum from rhizomes from Aim 1 in the greenhouse at UGA and generate an F2 population that segregates in pollen color. Upon flowering, reproductive traits (e.g., flower number, flower size, flower color, pollen color, seed set) for each plant will be described. I will collect and image pollen by confocal microscopy at the UGA Biomedical Microscopy Core (BMC) for characterization of pollen features (e.g., size, surface ornamentation). Then, I will utilize the Proteomics and Mass Spectrometry facility at UGA to quantify specialized metabolite accumulation in anther tissue and pollen by liquid chromatography tandem mass spectrometry (LC-MS/MS). Specialized metabolites have previously been characterized for G. maculatum pollen and it was found that the metabolite profile differed significantly among four collection sites6. I will further characterize the correlation between pollen color intensity and metabolite profile to identify compounds that are important for pollen performance in Aim 3. Aim 3: Evaluate trait-correlated tolerance to abiotic stress To evaluate the pollen performance of the F2 individuals, I will measure pollen viability, germination, and siring success under different conditions. I will expose different colored pollen to a gradient of temperature and light intensity treatments (mimicking field conditions), and assay for pollen viability and germination in vitro, where pollen germination rates and pollen tube length will be recorded. Additionally, I will observe the siring success of each pollen color morph in vivo. I will then conduct common garden experiments to evaluate fitness in field conditions. Replicate F2 populations that contain identical genotypes (by splitting rhizomes) will be planted in common garden plots at the Highlands Biological Station in North Carolina (high elevation) and the UGA State Botanical Garden (low elevation). Floral/reproductive traits (as described in Aim 2) and transplant survival will be recorded for each population over the span of 2 years. I hypothesize that dark pollen individuals will have greater reproductive success in high elevation due to specialized metabolites that confer abiotic stress tolerance. I expect that light pollen individuals will have decreased transplant survival but compensate to some extent by producing more flowers with higher seed set; a maternal reproductive strategy described by Koski et al. in Campanula americana13. I will also collect the same subset of individuals from each garden for metabolite analysis (as described in Aim 2) to determine whether there is a genetic-by-environment (GxE) effect on their profiles. Intellectual Merit: My previous research experience in floral development, pollen biology, biochemistry, and molecular biology makes me uniquely positioned to lead this interdisciplinary and community-science supported endeavor. Under the guidance of Dr. Chang, an expert in plant ecology and plant mating systems, I will expand our limited understanding of the role of pollen color polymorphism as a strategy for reproductive success. Additionally, previous graduate students in the Chang group have led community- science research endeavors using iNaturalist, making this an ideal environment to build upon this infrastructure. In taking advantage of the biological research stations available to graduate students at UGA, this study will be the first of its kind to evaluate pollen color morph-dependent fitness in an ecological context. Moreover, coupling biochemistry and ecology approaches, I will generate a comprehensive understanding of the role that specialized metabolites play in pollen germination and reproductive success – information that has implications in both agricultural production and native plant conservation. Broader impacts: I foresee my graduate research as a vehicle for mobilizing community scientists, providing educational opportunities to underserved communities, and improving diversity in academia. I will work closely with K-12 instructors to develop plant biology lessons with field work and family engagement components to create community awareness of these relevant topics. Students and their families will be given demos on how to use the iNaturalist platform to encourage outdoor activity and participation in community science. In collaboration with Max Barnhart, a graduate student in the Integrated Plant Sciences program who is the lead PI on an American Society of Plant Biology (ASPB) science communication grant, I will create and distribute a zine to provide an overview of my work to the general public and the community scientists involved. I will also share my experience with the scientific community by developing a workshop entitled “Incorporating Community Science Into Your Research Program” for the annual ASPB conference. During this workshop, I will discuss the various platforms available for building community science projects, demonstrate how to navigate and utilize these platforms, and lead an exercise on brainstorming ways to engage community scientists in your research. References: 1Rausher MD. Int. J. Plant Sci. 2008. 2Grotewold E. Annu. Rev. Plant Biol. 2006. 3Jiang et al. Plants. 2016. 4Lunau K. Plant Syst. Evol. 1995. 5 Miller et al. Optics & Laser Tech. 2011. 6Palmer-Young et al. Ecol. Monogr. 2018. 7Muhlemann et al. PNAS. 2018. 8Udell-Perez R. Field Obs. 9Jorgensen & Andersson. New Phytol. 2005. 10 Koski & Galloway. New Phytol. 2018. 11Wang et al. Evolution. 2018. 12Austen et al. Ecology. 2019. 13Koski et al. J Evol. Biol. 2020.	0
Key words: Polymer-clay nanocomposite, brominated flame retardants, sustainability Introduction: As a consumer, you hope that the products you use are safe, sustainable, and non- toxic - but that is not guaranteed. Many chemicals in everyday products have unknown or concerning impacts on human health and the environment. At the same time, a growing number of people use electronic products on a daily basis and are exposed to the chemicals contained in them, like brominated flame retardants (BFRs). BFRs are used in electronics because they are extremely effective at reducing the inherent flammability of polymeric materials. However, there is a desire in the flame retardant community to move away from brominated chemicals because of increasing concerns about the impact they may have on human health and the environment, especially during e-waste disposal[1]. BFRs are heavily used in high performance applications like epoxy based printed circuit boards. Over 90% of boards contain tetrabromobisphenol A (TBBPA) reacted with the epoxy matrix [2]. A promising new sustainable flame retardant is montmorillonite or “nanoclay” (flame retardant mechanism is shown in Fig. 1)[3] . Montmorillonite is attractive because just a few weight percent of it in a polymer composite improves the flame retardant characteristics, it is abundant in many different countries, Fig. 1. A layer of clay and char builds and it is a low cost additive. But to date, nanoclay has not during burning, protecting the polymer been demonstrated to be an effective flame retardant on underneath from further degradation. its own, instead being included in a mix of flame retardants used to achieve the desired properties. These mixes include BFRs in lower concentrations than when used without nanoclay and so still pose sustainability problems. Hypothesis: Even dispersion of clay layers and an epoxy-tailored clay surface will improve thermal, mechanical, and fire retardant properties of the epoxy-clay nanocomposite. This material will therefore be suitable for use as a flame retardant, bromine-free plastic. Research Plan: There are four main challenges in literature regarding the use of nanoclay as a sustainable flame retardant in epoxies, stated below with proposed solutions[4]. 1) Weak bonding between the clay and epoxy leads to poor composite properties In order to achieve strong bonding between the two nanocomposite constituents, organically modified montmorillonite (organoclays) will be made. Organoclays have been used to increase the dispersion in polymeric matrices, but the proposed surfactants will also be used to strengthen the interaction between the two phases. Novel surfactants will be grafted onto montmorillonite through well-established methods[4]. Aminosilane and epoxysilane surfactants were chosen because they will bond strongly to the epoxy network and to silica in montmorillonite (Fig. 2). Grafting and bonding strength in the nanocomposite will be determined by Fourier transform infrared spectroscopy (FTIR), thermal gravimetric analysis (TGA) differential scanning calorimetry (DSC), and dynamic mechanical analysis (DMA). 2) Even dispersion of individual clay layers in the epoxy matrix is hard to achieve Montmorillonite consists of ~1nm thick by 50-100μm diameter sheets stacked together with weak charges and Van der Waals forces bonding them together. Processing conditions such as grafting reaction temperature/time, intercalation procedure, and the kinetics of the epoxy curing inside and outside the clay layers will be factors that affect the dispersion of clay layers and will be optimized for this system. Conditions will be assessed by their potential for implementation in large scale manufacturing. Dispersion will be determined by x- ray diffraction (XRD) and transmission electron microscopy (TEM). 3) Nanoclay has not been shown to meet the UL94 standard when used as a flame retardant by itself Flame retardancy of the final nanocomposites will be Fig. 2. Selected structures of proposed novel tested at the Forest Products Lab in Madison, WI, due surfactants. Silane groups will bonds to to the lack of equipment at Purdue University. All montmorillonite and the amine/epoxy groups will bond into the epoxy network. nanocomposites are not expected to be promising, but once a final epoxy-organoclay composition is settled on it will be sent to the Underwriters Laboratory (UL) to receive a rating for the UL94 standard “the Standard for Safety of Flammability of Plastic Materials for Parts in Devices and Appliances testing.” Flammability will be determined by cone calorimetry and limiting oxygen index (LOI). 4) There is no standard assessment of sustainability for flame retardants In order to determine whether our nanocomposite is more sustainable than what is currently used, a comparative Life Cycle Analysis (LCA) will be conducted focusing on energy & water usage and pollutants produced from cradle to grave. Sustainability will be determined through OpenLCA software used with the EPA’s TRACI impact assessments (Tool for the Reduction and Assessment of Chemical and other environmental Impacts). Expected Results: One or more of the surfactants tested will create an epoxy-organoclay nanocomposite with desired thermal, mechanical, and flame retardant properties when made under specific processing conditions. Higher mechanical and thermal properties are also expected from a fully dispersed nanocomposite. A fundamental understanding of silane modifier structure/properties relationships on epoxy-organoclay nanocomposites will be contributed to scientific knowledge. Four papers will be published on: organoclay and nanocomposite manufacturing and properties, nanocomposite fire retardancy, sustainability assessment of nanocomposites, and reaction scheme of silanes bonding to montmorillonite. Conferences will be attended to present these findings including the American Chemical Society National Meeting and the International Symposium of Sustainable Systems and Technologies. Broader Impacts: This nanocomposite can be used in current epoxy applications, replacing the need for BFRs. Due to improved properties of the nanocomposite less material will be needed to achieve the same strength, reducing the environmental impact of a potential epoxy product. These findings will also be shared through outreach programs such as Project Interchange and Innovation to Reality to inform and inspire the next generation of scientists and engineers. [1.] Sjödin, A., Carlsson, H., Thuresson, K., Sjölin, S., Bergman, A., and Ostman, C. “Flame Retardants in Indoor Air at an Electronics Recycling Plant and at Other Work Environments.” Environmental Science & Technology (2001) [2.] EPA. “Flame Retardants in Printed Circuit Boards” Design for the Environment (2008) [3.] Morgan, A. B. and Gilman, J. W. “An Overview of Flame Retardancy of Polymeric Materials : Application, Technology, and Future Directions” Fire Materials (2012) [4.] He, H., Tao, Q., Zhu, J., Yuan, P., Shen, W., and Yang, S. “Silylation of Clay Mineral Surfaces” Applied Clay Science (2013) Statement of originality: I certify that this proposal is my independent and original work.	0
Americans spend approximately 90% of their time indoors1, and by 2050 over two-thirds of the global population will live in urban environments2. Studies have shown that human exposure to pollutants indoors is orders of magnitude higher than the exposure experienced outdoors3. Focusing air quality research on the places occupied by the most people and where pollutant exposure is highest is important in preventing negative health outcomes. It is paramount to understand the chemical, physical, and societal processes of the interface between urban and indoor air quality. Volatile organic compounds (VOCs) are emitted into the atmosphere from both anthropogenic and biogenic sources, including industrial and transportation activity, biomass burning, and vegetation. Indoor VOCs accumulate both from outdoor sources permeating into indoor spaces and from indoor activities such as cooking, cleaning, and off-gassing of furniture. Besides these gaseous VOCs, other semi-volatile organic compounds (SVOCs) exist both in the gaseous and condensed phase due to their lower vapor pressure and higher boiling points. When VOCs and SVOCs react with sunlight and oxidants such as ozone and nitrogen oxides (NO ), secondary organic aerosols (SOA) are produced, particles with x known hazards to human health. While there have been mobile, real-time measurements of air pollutants such as PM 4 (particles 2.5 less than 2.5 microns in diameter), there have been limitations to similar VOC measurement campaigns: the measurement’s location was stationary5, the hour-long sampling time made it difficult to locate pollution “hot spots”6, or the instruments used could not measure compounds made possible by current technology7,8. Prior to recent advances in VOC mass spectrometry, it had been difficult to measure VOCs and SVOCs at (a) sensitivities that allow identification of compounds and measurement of accurate concentrations and (b) mobile, real-time temporal scales to link concentrations to specific sources. Recent technological advances have made it possible to overcome such limitations. The newest generation proton transfer reaction time-of-flight mass spectrometer, the Vocus 2R PTR-ToF-MS from Aerodyne, Inc., has world-leading real-time mass resolution and sensitivity. The new mass spectrometer measures concentrations of more than 1600 chemical compounds every second, including many high- molecular-weight molecules considered semi-volatile. It can detect concentrations at part per trillion levels with 1-second measurements and even part per quadrillion scales at 1-minute averaging for certain compounds, allowing us to explore the frontier of trace – and very toxic – air pollutants. Using the most advanced mass spectrometer for this project will lead to the reporting of both SVOC molecules and trace VOCs that past mass spectrometers did not have the sensitivity to detect. The new Vocus Inlet for Aerosol (VIA, Aerodyne, Inc.) gives us the novel ability to measure the time-resolved chemical composition of SOA and other particles found indoors and outdoors, which is important for quantifying human exposures. Research Objective As the first study to investigate the interface in VOC and SOA concentration between urban outdoor and indoor environments using world-leading measurement capabilities, this project aims to: (1) Quantify concentrations and human exposure to air pollutants that have been previously unidentifiable due to technological limitations. (2) Identify and apportion the major sources of VOCs and SOA found indoors and in urban areas. Task 1: In controlled experiments, we will characterize indoor concentrations of VOCs and SVOCs such as benzenoids, siloxanes, and hydrocarbons. In UT-Austin’s environmental chambers and UTest House – a full-scale house on UT-Austin’s research campus outfitted with an array of sensors – experiments will simulate common household events such as cooking and cleaning; VOC, SVOC, and particle concentrations will be measured. To supplement our work, we will also use past datasets of the HOMEChem measurement campaign, which used the UTest House to characterize typical household activities9. Due to the increased sensitivity and precision of our novel mass spectrometer, we hypothesize that we will make novel characterizations of SVOCs that previous instruments were insensitive to and that past research campaigns did not report. Task 2: Using citizen science pathways already established from past research campaigns10, we will recruit 20 volunteer homes of diverse economic and geographical locations around Austin, Texas to take 2 air samples of ambient air twice per day for 2 weeks, one inside their home and one directly outside of it. The air samples will be taken off-site for evaluation by our Vocus mass spectrometer to find VOC and SVOC concentrations across both temporal and spatial scales in urban and suburban environments. We expect that VOC concentrations will vary widely from home to home due to individual differences in ventilation, personal care product usage, and cooking routines; however, we hypothesize that homes within a neighborhood will have similarities due to proximity to vehicle and industrial emissions. Task 3: Concurrently with the citizen science measurement, we will drive the Vocus mass spectrometer around Austin to measure VOC concentrations found on Austin’s roads at 1 Hz time scales. Pairing the VOC data with GPS data will be vital in finding point sources and pollution “hotspots”. Every other day we will measure with and without the VIA aerosol inlet, collecting data on both the gaseous concentrations and chemical composition of particles. We hypothesize that many pollution “hotspots” will be from sources usually overlooked as key polluters. Due to the complex chemical and physical processes that impact air quality, it is important to measure other air pollutants, not just VOCs and SVOCs. We will also measure mobile concentrations of ozone, NO , and PM – all interdependent on x 2.5 the presence of VOCs – as well as meteorological data such as wind speed, temperature, and humidity. To add to our data set, we will use the Texas Commission on Environmental Quality’s (TCEQ) 6 Austin-area monitoring stations as supporting data for wind speed, temperature, PM , ozone, NO , and sulfur oxides. 2.5 x Task 4: Combining the mobile air pollutant data with citizen science air samples and environmental parameters will produce a rich dataset across temporal and spatial scales. Using source apportionment and multivariate analysis methods – for example, positive matrix factorization (PMF) and principal component analysis (PCA) – we will identify major sources of urban and indoor VOCs. We hypothesize that our analysis will reveal unknown, as well as verify known emissions from indoor sources – such as cleaning or cooking – but will also quantify indoor exposure to pollutants penetrated from outdoors. Broader Impact Due to the breadth of samples we will collect during Tasks 2 and 3, I will recruit at least 2 undergraduate students from both UT-Austin and Huston-Tillotson University (an H.B.C.U. located in East Austin) to help with the collection of the citizen air samples. To introduce them to air quality research, I will train them in off-line VOC analysis and data processing. By conducting this campaign in actual homes and roads in addition to a laboratory setting, our findings can be used quickly for recommendations in homes and residential developments in Austin and other cities. By utilizing citizen science, this research will increase public awareness of the pollutants that residents are inhaling. The papers published from this campaign will be used as recommendations for urban planners, environmental regulators, and, perhaps most importantly, the general public. By measuring around Austin, we will also report on the accuracy of the TCEQ stations and how each station’s neighborhood-scale measurements differ from precise ground-level measurements. Intellectual Merit While several studies have measured air quality across urban areas, this will be the first study to do so by measuring a wide range of VOCs (with atomic mass units from 30 to 500) in real-time using new mass spectrometry technology. It will also be the first campaign to use such a wide-ranging dataset – the controlled studies in the UTest House, the citizen science household air samples, TCEQ station measurements, and the mobile dataset – to better understand the urban and indoor air quality system. By focusing on air pollutants that are not well-understood due to past technological limitations, we will also report on exposures that have been overlooked. Works Cited [1] Klepeis et al. (2001), J Expo Anal Environ Epidem., 11(3), 231-252; [2] UN World Urban. Proj.: The 2018 Revision (2018), pg. 10; [3] Nazaroff (2008), Build. and Env., 43(3), 269-277; [4] Apte et al. (2017) Environ. Sci. Tech., 51(12), 6999–7008; [5] Deng et al. (2018), Aero. Air Qual. Research, 18, 3025-3034; [6] Zheng et al. (2020), Sci. Total Environ, 703, #135505; [7] Maji, Beig and Yadav (2020), Environ. Pollution, 258, #113651; [8] Crippa et al. (2013), Atmos. Chem. Phys., 13, 8411–8426; [9] Farmer et al. (2019) Environ. Sci: Proc. Imp., 21, 1280-1300; [10] Bi et al. (2018), Environ. Inter., 121(1), 916-930	0
Bio-production of synthetic rubber using engineered Escherichia coli Introduction: In a society with both a growing dependence on energy and a depleting reservoir of fossil fuels, it has become increasingly important to design chemical syntheses that are sustainable, renewable and cost-effective. One synthesis of particular concern is that of isoprene, a precursor to synthetic rubber, since the current production relies on finite petrochemical sources.1 Recent advances in synthetic biology and metabolic engineering have made it possible to biosynthesize isoprene using glucose extracted from plant biomass, a renewable feedstock. Despite these advances, previously studied synthesis pathways report low product yield due to poor catalytic activity, making them economically unfeasible for large scale production.2,3 In order to overcome this roadblock, I propose to use a keto acid-mediated pathway to biosynthesize isoprene. Keto acids can be used as a selection in directed evolution (DE), a vital tool in the enhancement of enzyme activity.4 In order to employ DE however, enzyme specificity must be high enough for the desired conversion. Computational techniques, such as docking and funnel metadynamics, can be utilized to elucidate key amino acids involved in binding and catalytic active sites. These amino acids can then be mutated to enhance enzyme specificity. In this work, I focus on the conversion of citramalate to butanoic acid, a key step in the synthesis of isoprene, by expressing carnitine-CoA ligase (CaiC) and carnitine-coA transferase (mvaE) in E. coli. I hypothesize that engineering specificity and activity in heterologous CaiC and mvaE enzymes using a combined theory-experiment approach will increase butanoic acid yield, making the biosynthesis of isoprene more feasible for scale-up. Research Aims: The primary objectives of this project are to (1) manipulate specificity and (2) increase activity of CaiC and mvaE to optimize conversion of citramalate to butanoic acid, which can then be converted to isoprene through a series of organic reactions (Fig. 1). Figure 1: Workflow to sustainably produce isoprene from plant biomass by designing enzymes, supplementing experiment with modeling to engineer protein design, and using results from computation to inform experiment. Preliminary results: During the summer of 2018, I obtained preliminary data by working on the upstream synthesis of isoprene using engineered E. coli in Dr. Kechun Zhang’s lab at the University of Minnesota through the NSF-funded Center for Sustainable Polymers. Specifically, I was able to produce citramalate from glucose via citramalate synthase (CimA) using a keto acid-mediated pathway (Fig. 1). The next phase of the project is to convert citramalate to butanoic acid, the next intermediate in the biosynthesis of isoprene. Methods: (Aim 1) CaiC and mvaE enzymes are known to perform the desired reduction on carnitine, a molecule of similar structure and functional group composition to that of citramalate.5,6 I will use computational modeling to design more specific enzyme active sites for citramalate. The crystal structure for mvaE will be obtained from the Protein Data Bank. I will Kristen C. Vogt NSF GRFP Graduate Research Plan then build a homology model for CaiC based on the crystal structure of L-carnitine CoA- transferase (CaiB) and generate force fields describing carnitine and citramalate from quantum chemical calculations. I will run molecular docking of CaiC and mvaE enzymes with citramalate followed by molecular dynamics (MD) simulations to equilibrate systems. I will then use funnel metadynamics to calculate protein-substrate binding free energy to provide an estimate of the binding affinity between both enzymes and substrates.7 Insights gained from these simulations will inform key amino acid mutations to improve enzyme specificity. (Aim 2) Next, I will use directed evolution to improve enzyme activity using a keto acid- pathway and growth-based selection.4,8 I will use error-prone polymerase chain reactions (PCR) to create mutant libraries of CaiC and mvaE enzymes. PCR products will be cloned into a DNA backbone, electroporated into E. coli, and plated on Luria broth (LB) plates containing 100 g/mL Ampicillin. I will measure the total product formed from enzyme conversion using a 4- aminoantipyrene assay.8 Enzymes with readings 50% greater than parental standards will be selected for rescreening. Once both enzymes are optimized for activity and specificity, their DNA sequences will be ordered, replicated using PCR, ligated into a DNA backbone, and transformed into E. coli. I will then run fermentation for 48 hours in a 37 C thermoshaker running HPLC and OD every 12 hours to monitor butanoic acid production.9 600 Resources and support: In order to address these aims, I will work in collaboration with the Zhang and Truhlar groups at the University of Minnesota (UMN) to develop the experimental and theoretical components of isoprene synthesis, respectively. Access to supercomputing time through NSF’s XSEDE will allow for the proposed computational simulations. Intellectual Merit: The combined theory-experiment workflow outlined in this proposal is used to overcome low yield by improving enzyme activity and specificity in heterologous CaiC and mvaE enzymes. This methodology can be applied in any biosynthetic reaction to synthesize novel non-natural metabolites at higher yields over varying conditions.7 Even if highly accurate free energies are challenging to obtain from simulation, mechanistic information obtained from MD can be used to inform the next stage of experiment. Future directions of this project include converting butanoic acid to isoprene and investigating gene knockouts to further increase yield. Broader Impacts: The synthesis of isoprene, a commodity used in countless goods, such as adhesives, tires, and shoe soles, is currently unsustainable due to reliance on limited petroleum resources. Biosynthesizing isoprene using fermentation offers a renewable and cost-effective alternative. Increasing yield will make it feasible to utilize this technology in large scale production to move away from harmful, depleting syntheses and towards a more sustainable future. I plan to regularly present results from this work at conferences and make publications available to the public via open-access publication methods. Lastly, because parts of the above project, such as using recombinant DNA technology and running fermentation, lend themselves to undergraduate research, I will mentor and engage undergraduates in order to provide them with access to authentic research experiences early in their careers. 1. Singh, R. Org. Process Res. Dev. 2011, 15 (1), 175–179. 2. Zhao, Y.; Yang, J.; Qin, B.; Li, Y.; Sun, Y.; Su, S.; Xian, M. Appl. Microbiol. Biotechnol. 2011, 90 (6), 1915–1922. 3. Yang, J.; Nie, Q.; Liu, H.; Xian, M.; Liu, H. BMC Biotechnol 2016, 16. 4. Atsumi, S.; Liao, J. C. Appl. Environ. Microbiol. 2008, 74 (24), 7802–7808. 5. Eichler, K.; Bourgis, F.; Buchet, A.; Kleber, H. P.; Mandrand-Berthelot, M. A. Mol. Microbiol. 1994, 13 (5), 775–786. 6. Hedl, M.; Sutherlin, A.; Wilding, E. I.; Mazzulla, M.; McDevitt, D.; Lane, P.; Burgner, J. W.; Lehnbeuter, K. R.; Stauffacher, C. V.; Gwynn, M. N.; et al. J. Bacteriol. 2002, 184 (8), 2116–2122. 7. Limongelli, V.; Bonomi, M.; Parrinello, M. PNAS 2013, 110 (16), 6358–6363. 8. Bloom, J. D.; Labthavikul, S. T.; Otey, C. R.; Arnold, F. H. PNAS 2006, 103 (15), 5869–5874. 9. Kuhn, J.; Müller, H.; Salzig, D.; Czermak, P. Electronic Journal of Biotechnology 2015, 18 (3), 252–255.	0
Dispositional Risk Factors to False Confessions: Personality Traits and Psychopathologies In 2012, Pedro Hernandez was brought in for questioning for the 1979 abduction of six- year-old Etan Patz. Mr. Hernandez, a man with no criminal history, confessed to the abduction and murder of Patz.1 However, his diagnosis of schizotypal personality disorder, extremely low IQ, and prolonged and unrecorded interrogation—all known risk factors to making false confes- sions (FCs)—convinced a lone holdout juror of his innocence, prompting a mistrial in the murder case against Mr. Hernandez. He is scheduled to be retried in February 2016. The potentially ex- culpatory circumstances of this ongoing case share many similarities with other post-conviction DNA exonerations. In fact, research suggests that FCs are present in a significant minority (~27%) of all DNA exoneration cases. The purpose of this study is to empirically examine dis- positional risk factors associated with FCs among a subgroup over-represented within the criminal justice system and among confirmed FC cases: persons with psychopathologies. Background & Rationale. Psycho-legal scholars have proposed compelling theories to explain why innocent suspects admit to criminal acts they did not commit.2 Extensive research indicates that there are two types of risk factors: the use of certain interrogation tactics and dispositional characteristics. Historically, the study of police interrogation tactics has relied on experimental methods, producing a large body of science, while the study of dispositional risk factors has pri- marily employed archival and correlational studies. Thus, less is known about dispositional traits, such as personality. Two such personality traits–suggestibility and compliance–are thought to confer vulnerability to FCs within the context of coercive police interrogations.3,4 Further- more, fewer studies have examined the link between psychopathology and individual differences in these personality traits. Thus this project will investigate whether psychopathology increas- es the risk for suggestibility and/or compliance leading to increased prevalence of FCs. In addition to conferring risk to FCs, suggestibility and compliance may differentially mediate the types of FCs made by innocent suspects. In coerced-internalized confessions, inno- cent, but suggestible, suspects come to believe they are guilty, sometimes even confabulating false memories.2 To date, only one correlational study has assessed the link between individual differences in suggestibility and internalized FCs.5 The Gudjonsson Suggestibility Scale (GSS), a false memory paradigm where participants are presented with misleading suggestive infor- mation, is widely used among forensic psychologists in criminal cases involving disputed con- fessions.6 Yet no experimental research has attempted to use this scale to establish a causal link between suggestibility and internalized FCs—and certainly not among people with diagnosed psychopathologies. Hence, I will evaluate whether psychopathology increases the risk for suggestibility leading to coerced-internalized FCs. In coerced-compliant confessions, the innocent but compliant suspect is induced through interrogation to confess to a crime they did not commit for some immediate instrumental gain. In a naturalistic setting, the Gudjonsson Compliance Scale (GCS), a 20-item, self-report measure of compliance using a true/false format, was shown to discriminate between alleged false confes- sors and defendants who resisted confessing whilst being interrogated.6 Other correlational stud- ies indicate that compliance may be associated with anxiety, low self-esteem, and ADHD symp- toms3,6—factors present in many major psychopathologies. Because compliance is distinct from suggestibility and relevant to the making of FCs 6, I will evaluate whether psychopathology in- creases the risk for compliance leading to coerced-compliant FCs. Proposed Study. I plan to use a well-established experimental paradigm known to elicit FCs among innocent participants to explore these aims (see Kassin & Kiechel, 1996).7 First, partici- pants will be recruited for a ‘typing speed’ task and asked to fill out a demographics Stephanie A. Cardenas Graduate Research Proposal questionnaire. Psychopathologies will be assessed via the Mini-International Neuropsychiatric Interview-PLUS: a 15- minute structured diagnostic interview. Suggestibility and compliance will be measured using the GSS and the GCS, respectively. Next, participants will be assigned to one of four groups: 2 (slow vs. fast task pace) x 2 (presence vs. absence of false incriminating evidence) between-subjects factorial design. During the task, participants’ computers will suddenly ‘crash’ and a distressed experimenter will lay the blame on them. Two forensically relevant components will be manipu- lated: (1) participants’ subjective certainty of their own innocence by varying the pace of the task (i.e., slow-43 or fast-67 letters/min), and (2) the use of false incriminating evidence (i.e., false eyewitness account of alleged participant behavior by a confederate), a common U.S. interroga- tion tactic. To determine whether the paradigm elicited a compliant FC, participants will be asked to sign a handwritten confession admitting their role in the computer crash. To determine whether participants internalized the FC, a ‘curious’ confederate, will ask participants what hap- pened. Independent coders will determine whether participants unambiguously internalized fault based on their description of the event to the confederate (e.g., “I caused the computer to crash.”) Anticipated Results [1] Psychopathologies will predispose individuals to higher levels of sug- gestibility and compliance (Fig. 1). [2] Higher levels of suggestibility will confer increased risk to making coerced-internalized FCs. [3] Higher levels of compliance will confer increased risk to making coerced-compliant FCs. Although different psychopathologies may confer vulnerabil- ity through different pathways (e.g., social anxiety may be associated with increased compliance, and therefore compliant FCs), highly symptomatic individuals are known to have increased rates of alleged FCs regardless of primary diagnoses.8 Therefore, the role played by illness severity (e.g., bipolar disorder vs. hypomania) and time frame (e.g., current/past) will also be evaluated. Broader Impacts. The novel approach of this study integrates findings from criminology, clini- cal-forensic, and social psychology to test a hypothesis with notable implications for identifying and protecting individuals who are vulnerable to persuasion in the interrogation room because of psychopathologies (e.g., providing mandated access to legal advice from individuals sensitive to this population). By disseminating my findings at scientific conferences and in peer-reviewed ar- ticles, I will contribute to the understanding of how dispositional factors interact to influence the wrongful convictions of innocent suspects. This in turn will facilitate further collaboration be- tween attorneys, juries, judges, and social scientists to develop concrete ways to prevent these miscarriages of justice. Moreover, because juvenile status also confers increased risk to making FCs, the proximity at my proposed graduate institution to programs like College Bound and Summer Enrichment Camps and to intercity youths from disadvantaged backgrounds will allow me to provide a positive outlet within the community for academic advancement. Finally, future studies will examine the role of dispositional risk factors in the context of false guilty pleas (FGPs). Even though guilty pleas constitute nearly 95% of convictions in the U.S, FGPs remain grossly understudied. In fact, the same traits that place persons at risk for FCs may also place persons at risk for FGPs.9 Therefore, findings from my proposed project promise to put forward information that will inform this novel and unstudied related area of research Refs: 1 Goldstein & Hager (2015 May). 2 Kassin et al (2010) Law Hum Behav. 3 Gudjonsson et al (2008) Psychol Med. 4 Gudjonsson (1991) Med Sci Law. 5 Sigurdsson & Gudjonsson (1996) Per Indiv Differ. 6 Gudjonsson (2003) Wiley. 7 Kassin & Kiechel (1996) Psychol Sci. 8 Redlich (2010) Law Human Behav. 9 Redlich (in press) APA.	0
Introduction/Intellectual Merit: Advanced metabolic engineering allows scientists to use genetic engineering techniques in lower organisms, such as Escherichia coli and Saccharomyces cerevisiae, to produce molecules of interest through recombinant pathways. Metabolic pathways are groups of genes that encode enzymes that work together to produce the molecule of interest. Scientists have identified the most efficient genetic modification methodologies to create optimal production strains, including promoter libraries. A promoter library consists of a variation in the DNA sequence which varies the transcription initiation rate of the associated gene. This variation can come from promoters in the native organism or in non-native organisms, which are identified using RNAseq data. These libraries have been extensively developed for model organisms such as E. coli and S. cerevisiae. However, these tools are currently limited for non-conventional organisms. Leaders in the field of biochemical engineering have identified the development of genetic tools for use with non-conventional organisms as a foremost goal because model organisms lack the complexity that non-conventional ones can provide1. Leveraging unique properties of non-conventional organisms allows scientists to build upon promising results that push the boundaries of the pharmaceutical, environmental, and cosmetic industries. Using the knowledge that I’ve gained from working in two metabolic engineering labs over the last three years, I intend to further explore the non-conventional oleaginous yeast, Yarrowia lipolytica. This yeast is particularly interesting because of its ability to naturally prevent bacterial contamination3, its utilization of various hydrophobic and hydrophilic carbon sources4, and its ability to efficiently produce large amounts of lipid-based products5. Y. lipolytica was originally identified in environments containing hydrophobic substrates and studied for its ability to biosynthetically produce enzymes and citric acids2. Recently, metabolic engineers have become more interested in this yeast as they explore non-conventional organisms for production of high-value compounds. Research Plan: Although genetic engineers have made significant strides in the toolkits available for Y. lipolytica, they currently lack the diversity necessary to fully exploit its potential. One of the newest toolkits was developed by a group in France and is called the Golden Gate toolkit for Y. lipolytica6. The Golden Gate toolkit allows researchers to efficiently transform and integrate heterologous pathways in the organism. This toolkit includes a validated promoter library and has been successfully used to express a functional xylose utilization pathway. A major goal of my research plan is utilizing this toolkit and others to integrate several metabolic pathways into the organism. Additionally, scientists have discovered more promoters and have begun to look into computational models for the organism. These promoters include TATA box promoters7 and one repressible and one bidirectional promoter2. Repressible promoters provide negative feedback to down regulate transcription pathways, which is useful for products that are inhibitory to growth. A bidirectional promoter can be initiated in both directions for transcription; this becomes important for efficient gene co-expression. However, in order to further enable the metabolic engineering goals for this non-conventional yeast, more native and non-native promoters need to be identified so that researchers can better optimize the genetic conditions for efficient pathway expression. One way I plan to find new native promoters is through the use of RNAseq, described in Figure 1. RNAseq is a technique that can identify transcriptionally active regions of the genome and provide data on the expression levels of genes under various growth conditions or metabolic states. This leads to the identification of relevant promoter regions and data that can then be compared between specified growth conditions and normal growth conditions in which the promoter region should not be active. This technique will also be applied to other organisms in order to create non-native promoters, allowing for the identification of exciting new Figure 1. Simplified steps of RNAseq promoters that behave differently than native ones in the host process for identification of promoters. organism. The identified promoter sequences will then be obtained for both sets of promoters and will be expressed recombinantly and characterized in the host organism. The identification of both native and non-native promoters will allow for a more robust toolkit than what is currently available. These new promoters will then be applied to pathways of high interest in the scientific community. One such pathway is the pathway for production of plant oils. Jojoba oil was identified as a leading ingredient for anti-aging formulas for skincare in the 2000s and is still widely used today with increasing demand8. Harnessing the ability of Y. lipolytica’s high metabolic flux towards fatty acids enables a new route of production for the long chain fatty acids that comprise Jojoba oil. The enzymes in this pathway have been identified9 and can be recombinantly expressed to produce Jojoba oil. This pathway will be a testing ground and motivator for identifying and characterizing novel promoters. After successful integration of the recombinant pathway, scale-up production studies can begin. The experiments for genetic optimization will take place in small volumes within 48-well plates (2mL). After creating multiple genetic libraries through use of promoters discovered and discussed previously, fermentation growth conditions can be studied in shake flasks (250mL). Studying the organism in shake flasks informs decisions about growth conditions in large scale studies, such as those done in a bioreactor. After determining high quality growth characteristics, studies will be moved into a bioreactor (typically 1.5L+) to study the industrial feasibility of the process. Few studies have been done using Y. lipolytica in bioreactors, so this process will require permutations of multiple parameters to determine the best operating conditions for growth and oil production. This objective will be assessed by the ability of the bioreactor process to be scaled and replicated and for Y. lipolytica to produce high oil titers at scale under the optimal conditions. Throughout the completion of genetic cloning and scale-up studies, enhancement of current computational models will be occurring in parallel. Genome-scale-metabolic models (GEMs) have been created for Y. lipolytica. GEMs provide a kinetic model of cellular metabolism, meaning that a GEM can predict the metabolic activity of an organism based on user-defined parameters. The Y. lipolytica models need to be improved so that a wider range of researchers can use them. The best models achieve around 80% accuracy to experimental findings, however for people outside the field of metabolic engineering, the models are hard to understand and use7. In order to enhance the model, experimental data will be collected on new promoters, new growth conditions, and scale-up studies. Confirmation of already existing gene editing tools will also be collected. Utilizing GEMs is radically different than metabolic engineering’s randomized approach for identification of optimal conditions. The GEMs allow researchers actively working with Y. lipolytica to first test their hypotheses in silico so that they do not spend excess time and resources attempting to screen every genetic and/or fermentation parameter. Broader Impacts: Currently, Jojoba oil is produced via extraction of the oil from the seeds of the Jojoba plant. This time-intensive and costly process could be made easier through the creation of a heterologous production host. An efficient recombinant host organism allows for scientists to produce the same quality oil with a significant decrease in the environmental stresses associated with harvesting from the natural Jojoba plant. Metabolic engineering enables Y. lipolytica to sustainably convert widely available carbon sources into high-value natural products. However, until further promoters are developed, and scale-up studies are done on the organism, its full potential cannot be harnessed. Furthermore, the in silico model of Y. lipolytica will allow a large community of scientists to work together to better understand the growth and production capabilities of the organism. Taken together, the work proposed above leads to a continued advancement of metabolic engineering technologies to benefit society through development of advanced methodologies for sustainable chemical production. References: 1. Whitehead, T. et al. Biotechnol. Bioeng. (2020). Journal (2014). 2. Hussain, M. TigerPrints (2017). 6. Larroude, M. et al. Microb. Biotechnol. (2019). 3. Michely, S. et al. PLoS One (2013). 7. Ma, J., et al. J. Ind. Microbiol. Biotechnol. (2020) 4. Ledesma, R. et al. Trends Biotechnol. (2016). 8. Ahmad, A. et al. Biomed. Dermatology (2020). 5. Gonçalves, F. et al. The Scientific World 9. Miklaszewska, M. et al. Plant Sci. (2016).	0
Catherine Alves | October 2016 Keywords: Coral reefs, conservation, community-based fisheries, sustainability, survey analysis Proposed Research: Does community-based fisheries management restore ecological function and improve the livelihood of fishers in Belize? Background: Overfishing is a significant threat to the world’s ocean ecosystems (1) that has caused an 80-95% reduction in large predatory fish biomass (2). This not only disrupts ecosystem functioning but threatens invaluable commercial and subsistence fisheries that provide livelihoods and fish protein to nearly 3 billion people annually (3). Marine reserves are one tool designed to mitigate these impacts and to meet both biodiversity conservation and fisheries management goals. Marine reserves function by restricting fishing access with the intention of increasing fish abundances and diversity within no-take zones, ideally with fish spilling over into adjacent non-protected areas (4). However, poaching, lack of enforcement, and limited spillover often limit the broader success of marine reserves (5). An emerging approach is to more directly involve and incentivize local communities in the restoration and management of overfished stocks. Such “TURFs” (“Territorial User Rights for Fishing”) assign local fishers the rights to fish in designated areas in exchange for reporting their catch. These initiatives encourage environmental stewardship in coastal communities by providing effective ownership of fish stocks, further incentivizing sustainable fishing practices (6, 7). TURFs have been implemented worldwide by the Environmental Defense Fund (EDF), but little is known about their effectiveness, particularly in the tropics where implementation is only beginning (6). Research to examine the impact of TURFs from ecological and social perspectives is limited (6, 7), despite catch improvements reported by fishers participating in the program. TURFs have been designed to prevent the “race to fish” oftentimes accompanying small-scale fisheries because they assign catch shares to fishers. Furthermore, by assigning fishers locations to fish, poaching decreases in restricted areas, enabling fish populations to recover (6, 7). In 2011, the first TURFs in the Caribbean were established by the Belize Fisheries Department and they incorporated The Port Honduras Marine Reserve (est. 2000) and the Glover’s Reef Marine Reserve (est. 1993) (7). The Belize Fisheries Department is currently in the process of implementing a nation-wide TURF system, adding seven additional TURFs to pre- existing marine reserves (7, www.fisheries.gov/bz/#). The purpose of my study is to quantify the efficacy of Belize’s newly implemented TURFs in restoring overfished stocks, general biodiversity, and ecosystem functioning as well as in improving the livelihood of fishers. Specifically, I will test the following hypotheses: H : Fish species richness, density and biomass will be greatest in locations with TURF 1 implementation and lowest in unmanaged control sites. H : TURF implementation will improve the perception, livelihood, and catch per unit 2 effort (CPUE) of fishers who participate in the TURF program versus those who do not. Study Design: Visual fish surveys will be performed in the nine TURF locations plus nine unmanaged control sites (7, www.fisheries.gov/bz/#). Fish species richness, density and biomass will be quantified via underwater transect surveys using SCUBA. At all sites, I will quantify 1 ecological factors that could influence coral reef community structure and potentially compromise co-management efforts – such as reef structural complexity, temperature, chlorophyll, macroalgal cover and human population density (Cox et al., in review). The quantitative social science surveys will consist of structured interviews of 100 individuals randomly selected from four stakeholder groups: fishers participating in the TURF program, fishers not participating in TURFs, natural resource managers, and scientists (8). Closed-ended questions will be asked of all survey respondents to collect socio-economic, demographic and perceptions data including income, number of years in profession, gender, and perceived goals of the TURF program. Specifically, fishers will be asked to identify fishing locations on a map, provide CPUE, and share the percent of their income that comes from fishing. I will use the multilevel, nested framework of studying social-ecological systems (9) to build Bayesian hierarchical models to quantify the relationship between covariates. It is crucial to include fishers in management decisions because they become resources of change in their communities (5, 6, 7). A key component to the success of this project will be my partnership with the Belize Fisheries Department, the University of Belize, and local non-profits like Belize Healthy Reefs – all of whom are currently collecting CPUE data at locations where fishers sell their catch to vendors. During a research trip to Belize this past summer, I began to make connections with individuals at all of these institutions, with intentions to collaborate in the future. My partnership with local contacts is essential for establishing trust among the community because it will increase the likelihood that the fishers, managers, and scientists will consent to the study (5, 9). This collaboration will also enable the survey questionnaire to be implemented in the local languages of English, Spanish, and Belizean Kriol, therefore reaching different communities in Belize. In addition, I will draw upon data collected by my PhD advisor, Dr. John Bruno, who has conducted long-term monitoring research in Belize across 16 sites with varying levels of marine protection. Broader Impacts: This study will advance the field of community-based fisheries management by providing natural resource managers and fishing associations with insights into the efficacy of the TURF program in Belize from social and ecological perspectives. Information gleaned from this study has the potential to maintain livelihoods of the commercial and subsistence fishers in Belize while preserving coral reef fish biodiversity. I will also incorporate public outreach and education to increase scientific literacy and engagement of the public, both among the public in Belize and in my local community in North Carolina. I will collaborate with local institutions to co-organize public forums, workdays and outreach events for citizens of Belize to educate them about their local marine ecosystems. For outreach within my community in North Carolina, I have already developed a lesson plan for grades 8-12 on marine food webs for the Scientific Research and Education Network (SciREN). I hope to incorporate the findings of this study into a different lesson plan that focuses on marine resource management decision making. Both of these outreach programs will show the public the importance of interdisciplinary conservation science, and encourage environmental stewardship among the next generation. Community- based environmental management techniques are emerging across the globe as some of the most promising ways to combat anthropogenic threats to ecosystems, and I look forward to becoming a part of that endeavor. References: 1. Jackson, J.B.C., et al. Science 293:629-638 (2001). 2. Valdivia, A., et al. PeerJ PrePrints 3:e805v1 (2015). 3. FAO. The State of World Fisheries and Aquaculture: Opportunities and Challenges p. 243 (2014). 4. 2 Gaines, S.D., et al. Proceedings on the National Academy of Sciences 107(43):18286-18293 (2010). 5 Valdés- Pizzini, M., et al. Caribbean Studies 40(2):95-128 (2012). 6. Barner, A.K., et al. Oceanography 28(2):252–263 (2015). 7. Foley, J.R. Proceedings of the 12th International Coral Reef Symposium: Evaluating Management Success (2012). 8. Bernard, H.R. Research Methods in Cultural Anthropology ch. 9 (1988). 9. Ostrom, E. Science 325:419-422 (2009). 3	0
Key Terms: wildfire, water stress, land surface temperature Motivation: The primary wildfire monitoring system in the United States is the National Fire Danger Rating System (NFDRS)1,2. NFDRS maps are routinely used by land managers and regional governments to allocate fire mitigation resources and track fire risk. Although NFDRS is a standard risk assessment tool, it faces several disadvantages. Calculation of a NFDRS rating requires advance knowledge of site conditions and manual input of user parameters into closed source software. For non-experts, NFDRS is difficult to use. Despite the complexity of NFDRS, its primary fire danger metric is a coarse 5-level categorical scale (from “low” to “extreme”) that cannot be forecasted beyond 24 hours. This project will improve wildfire modeling with empirical techniques that enable proactive wildfire mitigation and long-term forecasting. Objectives: Climate change is expected to produce more intense wildfires more frequently in the American West3. Wildfires are intensified by high plant biomass (i.e. fuel load) and low fuel moisture. Both of these factors can be remotely sensed over large areas4,5. Given that vascular effects of water stress linger in plants weeks to months after a drought6, drought conditions early in spring may predispose water-stressed forests to wildfire the following summer. This project will produce remote sensing data streams of plant water stress and vegetation growth as inputs for an open source wildfire predictive model covering forested regions of the western United States at 1 km2 spatial resolution. I hypothesize that (1) water stress in early spring increases wildfire intensity the following summer, (2) fuel load can be estimated from a time series of a vegetation growth index and (3) by measuring these variables in early spring (Fig. 1a), wildfire-prone regions can be identified weeks before ignition (Fig. 1b). In short, water-stressed locations with high plant biomass will be identified as wildfire hotspots before the fire season begins. I will perform computationally-intensive spatial analysis using open source cloud infrastructure to ensure usability by non-experts. Aim 1: Calculate and validate water stress index. Water stress can be estimated quantitatively from canopy temperature (i.e. leaf temperature) and vapor pressure deficit using the crop water stress index (CWSI)5. CWSI is related to evapotranspiration and is applicable to all leaved plants. To calculate CWSI, I will use vapor pressure deficit at daily temporal resolution and 1 km2 spatial resolution from Daymet, a continuous, Figure 1. (a) Raster stacks of CWSI and GSI gridded meteorological product covering the become a time series for each grid cell. (b) Early contiguous United States7. To determine plant stress may indicate wildfire hotspots. canopy temperature, I will use land surface temperature (LST) calibrated with the normalized difference vegetation index (NDVI)8. My source of LST and NDVI data will be the Terra MODIS satellite, for which cloud-free, gap-filled LST data have recently been developed9. This method of determining canopy temperature requires only occasional NDVI values, so clouds will not prevent canopy temperature measurement8. Although MODIS is approaching retirement, its 20-year data archive is desirable. For recent fire years I will also work with the current-generation LST sensor ECOSTRESS. A field campaign will be performed in fire-prone western forests to calibrate CWSI, to validate canopy temperature measurements, and to determine the relationship between CWSI and plant water potential. Aim 2: Calculate and validate fuel load index. I will employ the growing season index (GSI) to estimate fuel load over time. The GSI quantifies how plant growth is limited or unconstrained by humidity, air temperature, or photoperiod4. GSI therefore remains measurable under all weather conditions using Daymet data, unlike spectral biomass indices such as the leaf area index. GSI will be validated against in situ measurements of fuel load as part of the field campaign in Aim 1. Aim 3: Model wildfire likelihood. I will use an existing dataset of wildfire occurrence from 2000- 2019 to produce annual wildfire presence maps aligned on the same grid as the CWSI and GSI calculations. For each year of data, I will calculate CWSI and GSI for each grid cell at daily temporal resolution from February 1 to May 31 to produce a stack of CWSI and GSI grids through time (Fig. 1a). Possibly, the end of the data collection period will be adjusted later or earlier in the year to optimize model performance and parsimony. Each cell of the raster stack will be inserted into a high-dimensional dataset where each row is a time series of CWSI and GSI values and a single column indicating whether the cell experienced fire that year. I will use the CWSI and GSI time series as predictors in a partial least squares regression (PLSR) model with a logarithm link function. PLSR reprojects the predictor variables in a typical linear regression to a lower- dimensional space that is maximally correlated with the response matrix. PLSR therefore resolves issues with correlated predictor variables and, via weights, identifies which CWSI and GSI measurements contribute most to the model prediction. Wildfire occurrence is a binary response variable, so a logarithm link function will enable calculation of wildfire likelihood for each cell in the modeling area. A variant of PLSR for binary classification tasks, partial least squares discriminant analysis, is also a candidate modeling approach. I will evaluate the proposed model with standard measures for a binary classifier with an imbalanced response variable. I will also compare the proposed model against NFDRS maps produced on May 31 the same year. Project success is defined as accurate prediction of 1 km2 pixels as fire-present or fire-absent, emphasizing a low false negative rate, and a model which land managers prefer over existing NFDRS maps for allocating management resources. Intellectual Merit: This study will improve wildfire prediction by employing empirical methods and longer forecasting times. Improved wildfire modeling will enable proactive wildfire mitigation and clarify factors that drive wildfire occurrence. In particular, this project will produce the most spatially extensive measurement of forest water stress to date and determine how wildfire is influenced by water stress early in the growing season. This study will also demonstrate how remote sensing datasets can be combined to model ecosystem processes. Broader Impacts: Wildfire intensity and frequency is expected to increase in the American West as climate change continues3. Wildland firefighters must effectively allocate tens of billions of dollars to mitigate this annual emergency. Advance warning of fire risk will enable proactive management that utilizes resources effectively. Such warnings also enable the public to avoid injury and property damage. In society at large, less wildfire smoke reduces respiratory illness and avoids air travel disruption. Knowledge of forest response to drought conditions will also improve timber production in the logging industry. All computer code and data products generated by this project will be open source. Model results will be distributed via a non-technical online dashboard. I will demonstrate the analysis workflow at workshops and reach out to potential users who would be interested in using the wildfire model produced by this work. References: [1] U.S. Forest Service p. INT-GTR-169. [2] W.M. Jolly, National NFDRS 2016 Rollout Workshop (2018). [3] A.P. Williams et al., Earths Future 7, 892 (2019). [4] W.M. Jolly et al., Glob. Change Biol. 11, 619 (2005). [5] S.B. Idso et al., Agric. Meteorol. 24, 45 (1981). [6] C.R. Brodersen et al., Annu. Rev. Plant Biol. 70, 407 (2019). [7] J.T. Abatzoglou, Int. J. Climatol. 33, 121 (2013). [8] M. Blum et al., Agric. For. Meteorol. 176, 90 (2013). [9] S. Shiff et al., Sci. Data 8, 74 (2021).	0
The Magnetic Origins of Solar Coronal Plumes Sun: corona — Sun: magnetic fields —Sun: UV radiation I. Introduction The corona is a diffuse cloud of plasma that surrounds the Sun that is ~104 times hotter ​ ​ than the photosphere at the surface of the Sun. The mechanism for coronal heating is one of the most sought after solutions in solar physics, as it could explain the origin of the solar wind, a stream of charged particles that bombards Earth and other planets. In order to determine a solution to the coronal heating problem, solar physicists study structures that arise in the corona as a consequence of the ever-changing solar magnetic field, like coronal plumes. Plumes are sporadic, fountain-like structures that are rooted in a strong patch of dominant-polarity photospheric magnetic flux, surrounded by a predominantly-unipolar magnetic field. Plumes are located in the least active regions in the solar corona: in either coronal holes or in quiet regions. Studying the formation of plumes could shed light on how the corona is heated, as there may be a fundamental mechanism that heats plumes that may be the same in other coronal structures. Several observations have been presented regarding how plumes form and disappear, but none have succeeded in determining the mechanism that generates plumes. In the Summer of 2017, to further investigate plume evolution, I tracked the lifetimes of six coronal plumes, three in quiet regions and three in coronal holes using SDO/Atmospheric Imaging Assembly (AIA) 171 Å images and SDO/Helioseismic and Magnetic Imager (HMI) magnetograms with Dr. Sanjiv Tiwari at an NSF REU program at the University of Alabama in Huntsville and NASA Marshall Spaceflight Center. We based our study on two previous studies, one by Raouafi et al.1 and Wang et al.3. Raouafi et al.1 infer from observation that plume heating ​ ​ ​ ​ ​ ​ is a consequence of magnetic reconnection at the base, whereas Wang et al.3 observe that plume ​ ​ heating is a result of convergence of the base magnetic flux. Both papers suggest that the base flux in their plumes is of mixed polarity, most of which is unobservable due to the spatial resolution of current instruments. Raouafi et al. and Wang et al. both suggest that this is the primary mechanism responsible for sustaining plume heating, and do not consider other contributing factors. I specifically investigated whether or not a critical magnetic field strength is ​ necessary for plume production, and determined from that a critical field strength of 250-500 Gauss, along with base flux convergence and divergence, is necessary for plume formation. While these preliminary results were extremely promising, indicating that a critical field strength may be necessary for plume production, further work needs to be done in order to confirm them. II. Specific Aims As a graduate student, I will extend this study to include coordinated high-resolution spectroscopic data from the Interface Region Imaging Spectrograph (IRIS) and the Extreme Ultraviolet Imaging Spectrometer (EIS) on the Hinode spacecraft in conjunction with SDO/AIA and SDO/HMI data. We will also extend our SDO/AIA observations to include those in 193 Å, 211 Å, and 304 Å emission. This increased emission coverage will allow us to observe plumes from the upper layers of the corona down to the chromosphere, the region between the surface of the sun and the corona where smaller-scale jet eruptions occur. Including spectroscopic data will allow us to further investigate smaller-scale jet eruptions in the chromosphere and transition region by observing cooler emission lines, and will allow me to create dopplergrams, which show the redshift and blueshift of spectral lines, in order to observe how flow patterns evolve throughout each plume’s lifetime. In order to determine a solution to the problem of underlying mixed polarity, we plan to propose observations to the Daniel K. Inouye Solar Telescope (DKIST) when it comes online in 2019. As the largest solar telescope in history, the increased spatial resolution of DKIST will allow us to resolve regions as small as ~20 km2, thus giving us ​ ​ the opportunity to observe minute emergence of opposite-polarity regions. To ensure that any conclusions of this study are statistically significant, we will expand our dataset to include a larger sample of 100 coronal plumes, 50 in quiet regions and 50 in coronal holes. III. Preparation and Relation to Career Goals This project allows me to continue my already fulfilling work on coronal plumes with Dr. Tiwari, who has since moved to working at the Lockheed Martin Solar and Astrophysics Lab (LMSAL) in Palo Alto. Stanford is an ideal setting at which to conduct this research, as its close affiliation with LMSAL would allow me to continue working with Dr. Tiwari, as well as collaborate with other solar physicists at LMSAL. Extending this project to include spectroscopic data and ground-based telescope observations would give me the skills to conduct more comprehensive observations of the solar corona, and increasing my previous study’s sample size would give me the statistical and computational skills I need to conduct further, more rigorous studies on larger datasets. This fellowship would give me the freedom to focus on obtaining a conclusive result, thus bringing me another step closer to a career in solar physics research, education, and outreach. IV. Broader Impacts With DKIST beginning operations in 2019, and the Parker Solar Probe, a mission to collect in-situ data on the corona, planned to launch in 2018, a wealth of data will soon be available for scientists to learn more about our nearest star. For most of my undergraduate career, I was unaware that there were so many unanswered questions about our nearest star. During my time as a graduate student, I look forward to involving younger students in this research in order to help the field of solar physics gain more exposure among young scientists. I have been inspired by the Pre-Major in Astronomy Program in place at the University of Washington, which is a mentorship program aimed at involving underrepresented undergraduate students in research early in their careers. I hope to do similar work by mentoring high school students in the greater Bay Area through Stanford’s Science in Service program. V. References 1Raouafi, N.-E., & Stenborg, G. 2014, ApJ, 787:118 — 2Tritschler, A., Rimmele, T. R., ​ ​ ​ Berukoff, S., et al. 2016, Astronomische Nachrichten, 337, 1064 — 3Wang, Y.-M., Warren, H. ​ ​ P., & Muglach, K. 2016, ApJ, 818:203	1
"the environment, our previous knowledge, and our underlying motivation in order to make goal-directed choices. Several brain regions are recruited in this decision-making process. The mediodorsal thalamus (MD) has been shown to be necessary for cognitive tasks like working memory and goal-directed decision-making1. The mediodorsal thalamus (MD) takes higher order ​ ​ feedback and sensory information and relays it to the orbitofrontal cortex (OFC) and basal ganglia (BG). The orbitofrontal cortex (OFC) is necessary for value-based decision-making and inferring2. The OFC sends projections to the striatum, the input nucleus of the basal ganglia, ​ ​ which is needed for action performance3. ​ ​ While there is evidence that both OFC and MD project to dorsal striatum 4, how and what ​ ​ information is being sent or modulated through these paths is less clear. I plan to use transgenes and recombinase technologies to limit the expression of a fluorophore or a calcium indicator in a cell-type and projection-specific manner to measure the interactions between these regions during a decision-making task. I hypothesize that distinct sensory and valuation information ​ processing is occuring in OFC and MD. These aims address how information in cortico-basal ​ ganglia-thalamic (CBGT) loops is being passed on and selectively used to control decision-making. Aim 1: Characterize the cortico-thalamic-basal ganglia circuit using anatomical tracing. ​ Previous research has focused on individual streams of information from the thalamus to prefrontal cortex5. However, ​ ​ there have been no studies examining overlapping OFC and MD collaterals in striatum. I seek to address how OFC and MD projections are overlapping. In order to understand how information is flowing through this particular CBGT loop, I will first conduct an anatomical study. I will perform stereotactic surgery in mice, which I have previously done in the Gremel lab6. I will inject two adeno-associated viruses ​ ​ ​ into the mice: a retrograde Cre-GFP in DS and a Cre-dependent mCherry in MD. After waiting for adequate ​ viral expression, I will visualize the neurons using fluorescence microscopy. The presence of mCherry labeled MD terminals in OFC would indicate that those MD neurons project to both OFC and DS. The same strategy will be used to look for OFC neurons that synapse onto both MD and DS. This anatomical information (Figure 1) will inform how to ​ ​ proceed with functional investigations; if the same neurons are synapsing in OFC and DS (or MD and DS), there may be an interesting mechanism controlling behavior. An undetermined portion of thalamostriatal neurons synapse onto inhibitory interneurons, so there is a possibility that they function as a clamp to perform gain control on the information coming from OFC. If I do not see overlap of collaterals in the microcircuitry of striatum, I intend to investigate how MD is contributing to holding information in a decision-making task. Aim 2: Examine the activity of MD and OFC neurons synapsing in striatum during a ​ ​ self-initiated decision-making task. To examine how MD and OFC are synapsing in striatum, I will use genetically encoded calcium indicators (GCaMP and RCaMP) as a proxy for synaptic activity in behaving mice. I will perform fiber photometry to measure calcium activity in two groups of mice, one group with MD and DS, and another with OFC and DS. I will use an axon-targeting GCaMP in the MD and OFC and RCaMP in the DS so I can simultaneously record from both populations (Figure 2). To look at specific ​ ​ cell types, I will restrict RCaMP expression in DS to either indirect or direct pathway medium spiny neurons (iMSNs or dMSNs). These methods will allow me to look at how the activity in the MD or OFC is related to the two striatal output pathways. Mice will be trained to hold down a lever for a specific duration in order to earn a food reward. I hypothesize that information may be accruing actively over the period of holding ​ down the lever, and this information may be maintained through the MD. Calcium transients will ​ be examined around lever press initiation/stop and reward delivery. I will compare the transients over time (days of learning) and between MD, OFC, and DS to examine decision-making computations that may be supporting behavior. We will use regression analyses to quantify the dependency between the activity of MD and DS and OFC and DS when the animal is deciding to let go of the lever. I expect that there is more correlation in MD-DS when the animal holds down the lever long enough to earn a reward. Intellectual Merit This work will provide insight on circuitry underlying decision-making. The findings will also inform how neurons integrate information and use that integrated information to generate actions. Results may provide insight into circuit motifs like gain control that allow for rapid problem-solving, potentially applicable to other neural circuits and artificial intelligence. Broader Impacts Findings may inform the development of treatment for disorders where appropriate action selection is disrupted- OCD, mood disorders, schizophrenia, and addiction, hopefully improving well-being. Treatment for these diseases may mitigate the cost of disability in the US and help our economy grow by including more people in the workforce. Moreover, funding this project directly ensures the full participation of myself, a woman with a disability, in STEM. Results will be shared in peer-reviewed publications and at conferences, and will also be shared with the general community by writing blog posts for NeuWriteSD7. I will also share my ​ ​ research and general scientific topics through demonstrations at K-12 schools and at community events with all ages. I am excited to have matched with a ""pre-scientist"" 6th grade pen pal, and we are exchanging letters about going to college, overcoming obstacles, and scientific careers. All of this outreach generates curiosity and better scientific literacy in the general public. References 1 ​ Hallassa & Kastner, Nat Neuro (2017). 2 ​Gremel et al., Nat Comm (2013). 3 ​Yin, Neuroscientist (2017). 4 ​Hunnicutt ​ ​ ​ ​ ​ ​ ​ ​ ​ et al., eLife (2016). 5 ​Parnaudeau et al., Biol. Psychiatry (2018). 6 ​Baltz et al., eLife (2018). 7 ​neuwritesd.org ​ ​ ​ ​ ​ ​ ​ ​"	0
Deriving Language Signatures for Bilingual Code-Switching Keywords: Code-Switching, Probabilistic Language Models, Sociolinguistics ​ Research Question: How do bilingual speakers of the same language pairings code-switch between them differently? More specifically, what components can be extracted from bilingual data to differentiate speakers of the same languages? Background: Linguistic scholars have observed that there is wide variation in code-switching (CS) due to social differences (Gardner-Chloros, 2009). For example, Post (2015) observed variations in frequency and type of CS as a function of gender among Arabic-French speakers in Morocco. Unfortunately, findings like these have been restricted to specific languages and small datasets and until recently, there have been no tools to classify CS at the level of large corpora (Gambäck & Das, 2016). Furthermore, there have been no attempts to distinguish the unique CS patterns presented by speakers, i.e., individual language signatures. The key problem is that CS can include small word-level insertions of single lexical items or long stretches of dialogue across several speakers, which make its study difficult as speakers can vary their patterns of speech considerably from one utterance to another. I propose to address these gaps in the study of CS by applying statistical models to extract and extrapolate patterns from bilingual corpora. The crux of my approach is to look at CS as a sequence of language spans, in which a speaker remains in one language before switching to another. Solorio and Liu (2008) have previously exploited this idea to predict switch points and to tag for Part-Of-Speech, yet their approach made no attempt to distinguish or identify different patterns. Outside of current analyses at the level of corpora I do not know of any statistical approach to studying bilingual CS across speakers that exploits this idea of language spans. By adapting this concept to the alternation of language at the individual level and not corpora, I believe that it is possible to distinguish the CS of one bilingual speaker from others to produce a distinctive language signature, regardless of small changes in speech. Hypothesis/expected findings: Based on my previous work with the Killer Crónicas, Yo-Yo ​ ​ ​ Boing!, and Bon Cop, Bad Cop datasets, I hypothesize that CS can uniquely characterize ​ ​ individual speakers and that the relative frequencies of language spans across speakers are distributed differently, with some speakers preferring spans of one length to another. I expect that speakers of the same language pairings vary enough in the length of languages spans that there are statistically significant differences in the speech of two bilingual individuals. I anticipate that there are also several variables contributing to these differences such as region, attention paid to speech, and social factors. My methods to extract different features of CS and provide unique signatures of CS across bilingual speakers will be language-neutral and applicable across different language pairings. Approach and Methods: The first component of this project will be the gathering of a large number of bilingual corpora involving CS in order to introduce as much variation as possible. Given my previous work in language annotation, I am free to work with larger, untagged datasets so long as enough training data exists. By far the largest collection of such datasets is the Linguistic Data Consortium (LDC), which charges a fee for unlimited use and access. The remaining resources are access to powerful computing resources (or XSEDE access) and the expertise of faculty members working on CS and statistical language models. After preliminary analysis of the datasets, I expect distinct patterns of CS to emerge across speakers such as switching differently around breaks in speech, which will lead to differing patterns in the corresponding language signatures. At this point, I will work to examine Gualberto A. Guzman Graduate Research Plan Statement NSFGRF 2016 bilingual CS with statistical models by looking at the distribution of language spans per speaker and by examining the probability of switching in a discourse as a function of time, both of which necessitate large datasets. Assuming the simplest case, the distribution of language spans of a speaker can be modeled as a stochastic exponential decay after normalization for length of text. I expect that different speakers will present different rates of decay in their language, resulting from varied use of language spans. In addition, by looking at bilingual text as a series of switches between monolingual spans, I will model CS as a Poisson process and find the most apt parameters for individual speakers by working backwards from their speech. I will tune different stochastic models to speakers in order to find statistically significant differences in speech patterns and I plan to subject the data to a rigorous analysis of different stochastic models to test my hypotheses. I will also perform a regression analysis to find correlations between the social and environmental factors mentioned above and any differences from the models. Intellectual Merit/Broader impacts: A Graduate Research Fellowship will allow me to ​ promote further research between the fields of Linguistics and Mathematics. My work will inspire the development of a general framework with which to examine cases of switching phenomena within Linguistics. It will have broad impacts in computational linguistics, sociolinguistics, and bilingualism both as a tool and as a theoretical construct. My model will contribute a new, language-neutral approach to examining bilingual CS, freeing researchers from being constrained by the availability of data in dominant languages like English. In addition, my proposal has potential contributions to the fields of linguistic methodology and linguistic anthropology. Its development may lead to the possibility of deconstructing seemingly homogenous language or CS use into discrete subgroups by geography, ancestry, or culture. Finally, it must be noted that the development of my model need not be restricted to the study of switching between two languages. My ambition is to generalize my model to work with as many languages as needed. In addition, a refined version of my proposed model would be able to uniquely identify changes in style, dialect, or register given enough training data. As an example, learners of a second language could apply the principles of my model to pinpoint exactly where their usage differs from that of a native speaker, which provides a new possibility for accelerated language learning and for the study of second-language acquisition. Björn Gambäck and Amitava Das. 2016. Comparing the level of code-switching in corpora. Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), pages 1850–1855. Penelope Gardner-Chloros. 2009. Sociolinguistic factors in code-switching. In Barbara E. Bullock and Almeida Jacqueline Toribio, editors, The Cambridge handbook of linguistic code-switching , pages 97–113. Cambridge University Press, Cambridge, UK. Thamar Solorio and Yang Liu. 2008. Learning to predict code-switching points. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 973–981. Association for Computational Linguistics. Thamar Solorio and Yang Liu. 2008. Part-of-speech tagging for English-Spanish code-switched text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1051-1060. Association for Computational Linguistics. Rebekah Elizabeth Post. 2015. The impact of social factors on the use of Arabic-French code-switching in speech and IM in Morocco. Ph.D. thesis, University of Texas at Austin.	0
Stress KEYWORDS: personality; cognitive appraisal; emotion; stress reactivity; ecological momentary assessment INTRODUCTION: Everyone experiences stressful life events, but the magnitude of stress experienced in response to the same stressor can vary considerably between two people. For example, certain people tend to express more heightened negative emotion in response to life events and therefore perceive their lives as more stressful.1 Personality factors such as increased neuroticism, behavioral inhibition, and negative attributional style are also implicated in this greater stress reactivity1,2. I propose that these individual differences in response to stressors are strongly influenced by personality, and lead to variation in cognitive-emotional appraisal and processing of the stressful life STRESSOR event. In turn, this cognitive- emotional appraisal contributes to Cognitive Appraisal varying physiological reactivity, Personality -Worry/Rumination as measured by immune, -Neurotic/Anxious -Perceived Control Physiological cardiovascular, and endocrine -Behaviorally- -Perceived Severity Reactivity Inhibited -Immune response (See Figure 1). -Cardiovascular -Negative Cognitive appraisal involves -Endocrine Attribution Style Emotional the perceived severity and Reactivity perceived controllability of the -Negative Affect stressor, as well as the magnitude Figure 1. Proposed model of personality, cognitive appraisal, and frequency of persistent, emotional reactivity, and physiological reactivity in response to stress maladaptive, negative thoughts leading up to and following the stressor (worry/rumination). Emotional reactivity involves the presence of positive or negative affect in response to the stressor (See Figure 1). A better understanding of how subjective cognitive-emotional reactions to stress relate to individual variation in physiological reactivity is needed to better understand key individual differences between people. It is well established that stress relates to immunological dysregulation (e.g. slowed wound healing and increased viral susceptibility).3 Yet the interplay between individual differences in stress reactivity and immune system reactivity has seldom been investigated, particularly with regard to how such changes play out in real time, real-life human contexts.2 One reason for this lack of understanding is that studies of immune reactivity have largely been limited to short-term laboratory studies that use blood draws for serum/plasma-based biomarkers. With less invasive salivary biomarker techniques that are currently in development, more ecologically-valid studies of individual differences in stress reactivity could be conducted. RESEACH AIM 1: To investigate how individual differences in personality relate to differences in physiological reactivity, specifically markers of immune system function. RESEARCH AIM 2: To examine how cognitive appraisal of a stressor and emotional reactivity to a stressor mediate the relationship between personality and physiological reactivity. RESEARCH AIM 3: To determine how relationships between personality, cognitive appraisal, emotional reactivity, and physiological reactivity play out over time in an ecological context. METHODS: I have already independently conducted an extensive literature review to elucidate the connection between salivary and blood measures of inflammation and plan to publish a review article on this work. Along with Penn State investigators who are spear-heading further investigation of salivary immune diagnostics, I hope to incorporate salivary measures of immune reactivity into novel methodological approaches to studying individual variation in the stress response. My research program during my graduate training will culminate in my implementation of three original studies described below. STUDY 1 (S1): I will begin by using an existing data set to examine connections between individual differences and immune responses to stress. My advisor is PI on a longitudinal investigation of the degree to which inflammation mediates connections between stress and cognitive aging among diverse adults. I will be able to use those data to explore new dimensions of how personality (e.g. behavioral inhibition, negative attribution style) is related to cognitive- emotional responses to stress and, consequently, to physiological reactivity over a 4-year period. STUDY 2 (S2): To experimentally model the relationship between personality and immune function, I will expose participants to an acute social lab stressor in order to measure individual differences in stress reactivity through endocrine, immune, and cardiovascular measures and through self-reported assessments of cognitive-emotional state. Specifically, immune function will be measured through circulating inflammatory markers obtained via saliva and blood. STUDY 3 (S3): The data from S1 and S2 will be used to inform a larger naturalistic study which will utilize ecological momentary assessment (EMA), a method whereby participants report what they are feeling and/or how they are behaving in real-time in natural settings. In collaboration with Penn State’s Dynamic Real-Time Ecological Ambulatory Methodologies (DREAM) initiative – a unique program designed to popularize and educate researchers on EMA methods – participants will be given smart phones which will prompt them to fill out assessments of cognitive-emotional states and to give saliva samples at specific time points. State-of-the-art hierarchical linear modeling and structural equation modeling will be utilized for mediation analyses in S1 and S2, as well as to examine between-subject and within-subject variation, including how multiple daily assessments change over time in S3. INTELLECTUAL MERIT and BROADER IMPACTS: With the levels of stress that many in our society are facing, it is imperative to better understand the mechanisms by which some people become more susceptible to the physiological consequences of stress. My program of research has potential implications for improving quality of life, coping strategies, interpersonal relationships, and productivity in the workforce, as well as fostering self- actualization through stress reduction. These three studies will advance the field of psychoneuroimmunology by examining the underutilized combination of less-invasive salivary inflammatory biomarkers with respect to individual differences. The additional assessment of this concept through EMA will allow for greater external validity of results and help popularize more ecologically-valid studies. Pre-existing partnerships with a number faculty who already investigate individual differences in the stress response makes me well-poised to implement the tri-part research initiative I am proposing. In conjunction with the DREAM initiative and Penn State’s Centers for Healthy Aging and “De-Stress Zone” (a biofeedback facility), I also plan to design and hold workshops for education on ecological measurement of stress and stress self-management to implement with diverse populations. With all of the opportunities for research and outreach available at Penn State, my current position places me in an ideal situation to begin explaining the individual variation that is often overlooked in physiological psychology studies. REFERENCES: [1] Suls, J., Green, P., & Hillis, S. (1998). Emotional reactivity to everyday problems, affective inertia, and neuroticism. Pers & Soc Psych Bltn. 24: 127-136. [2] Segerstrom, S.C. (2000). Personality and the immune system: Models, methods, and mechanisms. Annals of Behav Med. 3:180-190. [3] Contrada, R., & Baum, A. (Eds.). (2010). The handbook of stress science: Bio., psych., & health. New York, NY: Springer Publishing Co.	0
Impacts of Climatic Change on the Arid Savanna Fire Regimes of West Africa Keywords: Fire Regime, Climate Change, West Africa, Land Degradation, Arid Savanna Introduction: Changing global conditions may be driving increasingly severe fires that are further deteriorating already highly altered arid savanna ecosystems in West Africa. With the continued rise in anthropogenic fires, vulnerable ecosystems have undergone changes in species dominance, ecosystem structure, and even ecosystem collapse. I hy- pothesize that altered climatic variables are at least partially responsible for increases in fire severity that degrade the West African arid savanna. Background: Arid savannas, dry grasslands with scattered shrubs and drought-resistant trees, are regarded as one of the ecosystems most likely to be affected by climate change (Louppe et al 1995). Though arid savannas require periodic surface fires to maintain their characteristic vegetation, increasingly severe fires have resulted in the annual degradation of more than 200,000 hectares of sub-Saharan arid savanna (Nsiah-Gyabaah 1994). In the past, these losses were attributed either to unsustainable agricultural prac- tices, or the belief that the Sahara was in a period of growth as part of the Earth’s axial obliquity cycle. Today we accept the sophistication of West African land management techniques, and there is evidence that the Sahara is actually in a greening period (Nsiah- Gyabaah 1994). Many suspect that the increase in land degradation is the result of cli- mate change driving fire intensity and frequency increases. The consequences of altered fire regimes are clear, but West African fire regimes are themselves poorly understood. I became interested in studying fire-influenced land degradation when, during a Watson Fellowship studying fire, I returned to my childhood home in Ba’Nso, Camer- oon. Throughout the region, I encountered Fulani herdsmen who had relocated to find better pastureland. The Francophone farmers I met described barren fields, complaining that no amount of burning restored the land’s fertility. Objectives: The study I propose is designed to investigate the impact of altered climatic conditions on fire-induced land degradation, producing some of the first quantitative re- search on fire regimes in West Africa. Fire regimes have been defined using characteris- tics such as frequency, intensity, burning season, and fire size (Goldammer 1988). I will study these physical parameters in West African arid savannas in order to determine if there is a relationship between altered climatic variables and fire severity, and what the impact is on land degradation. After conducting field research in the Ghanaian Accra plains, I will analyze remote sensing data to determine the change at longer timescales. Methodology: Working with Dr. Kwesi Orgle and Forest Resources Management, I will conduct field research on the arid savanna in the Accra Plains on fire plots maintained by the University of Ghana (Swaine 1992). We will analyze three study plots: a control in which fire has been excluded since 1957, another which has been burned annually since 1957, and a plot of extant vegetation which we will burn annually beginning in 2007 and for a subsequent two years. Our research team, including traditional landowners, will col- lect annual data on fires in study plots, measuring consumption of plant biomass, fire ex- tent, and intensity with thermocouples. We will document plant species turnover, the Proposed Plan of Research Smith, Rachel relative proportions of vegetation repopulation, and evaluate changes in rates of land deg- radation through a comparison with data collected since 1957. To identify change over a longer timescale, I will conduct an analysis of NOAA- AVHRR satellite image data captured for West Africa between 1985 and 2005. These previously processed data will be compared to monthly temperature and precipitation datasets by the University of East Anglia’s Climate Research Unit to determine what, if any, relationship exists between changes in numbers of fires, timing, and area burned, and altered precipitation and temperature. Dr. Johann Goldammer, head of the UN-FAO Team of Specialists on Forest Fire has agreed to collaborate with me on this analysis. We’ll use these data to inform a broader scale analysis, using severity indexes and NDVI trends to discern any relationship between changing fire severity and land degradation. Anticipated Results: Climate change may be the dominant influence on the surge in fire- influenced land degradation in West Africa, but the current paucity of quantitative data on fire in this region makes it difficult to understand the relationship between fire and climate. My research will provide the first quantitative measurements of fire regime char- acteristics, identifying fire frequency, size, and intensity thresholds above which ignitions may result in land degradation. Furthermore, it will explore the possibility of interactions between individual fire regime characteristics. I anticipate my research being able to con- clude whether fire-influenced land degradation is caused by climate change. Broader Impacts: This original project will provide critical information on fire regimes and offer insight into the extent to which climatic change is altering fire regimes in West Africa, constituting some of the first quantitative research on basic fire regime parameters in the West African arid savanna. Future researchers will utilize my data in their studies. Traditional landowners, such as the subsistence farmers in Ba’Nso and the Fulani nomadic herders whose livelihoods are tied up in the complex relationships linking fire and land arability, have a vital interest in this research. We will engage key stakeholders as research collaborators and make all literature available in English, French, and Fulbe. Regionally, this project will provide resource managers with ground-truthed data linking fire and climatic change. These data will assist them in formulating management plans and help them anticipate future changes in fire behavior from climate change. Implications of this research stretch beyond Africa to other arid rangelands where fires might be increasing in incidence or severity. My study may help scientists and land managers identify vulnerable areas in other parts of the world. My partnership with The Nature Conservancy and other organizations will make my results widely available. References: Goldammer, J.G. 1988. Rural land-use and fires in the tropics. Agroforestry Systems 6: 235-252. Louppe, D., Ouatara, N. & Coulibaly, A. 1995. Effet des feux de brousse sur la vegeta- tion. Bois et Forêts des Tropiques 245: 59-74. Nsiah-Gyabaah, K. Environmental degradation and desertification in Ghana. Brookfield: Aldershot, 1994. Swaine, M. D., Lieberman, D. and Hall, J. B. 1992. The effect of fire exclusion on savan- nah vegetation at Kpong, Ghana. Biotropica 24,2a: 166-172.	0
(cid:80)(cid:83)(cid:1)(cid:86)(cid:84)(cid:70)(cid:1)(cid:88)(cid:74)(cid:85)(cid:73)(cid:80)(cid:86)(cid:85)(cid:1)(cid:81)(cid:70)(cid:83)(cid:78)(cid:74)(cid:84)(cid:84)(cid:74)(cid:80)(cid:79)(cid:15) (cid:36)(cid:80)(cid:81)(cid:90)(cid:83)(cid:74)(cid:72)(cid:73)(cid:85)(cid:1)(cid:19)(cid:17)(cid:18)(cid:17)(cid:1)(cid:88)(cid:88)(cid:88)(cid:15)(cid:83)(cid:66)(cid:68)(cid:73)(cid:70)(cid:77)(cid:68)(cid:84)(cid:78)(cid:74)(cid:85)(cid:73)(cid:15)(cid:68)(cid:80)(cid:78)(cid:1)(cid:34)(cid:77)(cid:77)(cid:1)(cid:51)(cid:74)(cid:72)(cid:73)(cid:85)(cid:84)(cid:1)(cid:51)(cid:70)(cid:84)(cid:70)(cid:83)(cid:87)(cid:70)(cid:69)(cid:1)(cid:85)(cid:80)(cid:1)(cid:48)(cid:83)(cid:74)(cid:72)(cid:74)(cid:79)(cid:66)(cid:77)(cid:1)(cid:34)(cid:86)(cid:85)(cid:73)(cid:80)(cid:83)(cid:15)(cid:1)(cid:37)(cid:80)(cid:1)(cid:47)(cid:80)(cid:85)(cid:1)(cid:37)(cid:86)(cid:81)(cid:77)(cid:74)(cid:68)(cid:66)(cid:85)(cid:70) (cid:80)(cid:83)(cid:1)(cid:86)(cid:84)(cid:70)(cid:1)(cid:88)(cid:74)(cid:85)(cid:73)(cid:80)(cid:86)(cid:85)(cid:1)(cid:81)(cid:70)(cid:83)(cid:78)(cid:74)(cid:84)(cid:84)(cid:74)(cid:80)(cid:79)(cid:15)	0
BACKGROUND RNA modifications, also known as epitranscriptomics, are emerging as a novel layer of dynamic gene regulation [1]. RNA modifications alter existing RNAs’ structure and function to influence various cellular pathways via RNA processes such as transcription and translation [2, 3]. Pseudouridine (Ψ) was the first RNA modification discovered [1]. Despite being the most abundant and widespread RNA modification in living organisms, little is known about its function. In this proposal, I will establish an approach to systematically investigate the biological roles of pseudouridine by focusing on the unique activities that this modification imparts to RNA. The isomerization of uridine to pseudouridine by pseudouridine synthases (PUS enzymes) structurally stabilizes RNAs via the formation of an extra hydrogen bond donor [3]. Ψ was previously thought to primarily stabilize tRNA and rRNA; however, new developments in modern-sequencing techniques reveal the more interesting downstream effects of pseudouridylation. For example, there is evidence that H/ACA RNPs, RNA-dependent PUS enzymes convert stop codons into sense codons in yeast [2]. Interestingly, PUS7-mediated pseudouridylation has been found to regulate stem cell growth and fate determination partly through tRNA modification, which activates tRNA-derived fragments to inhibit protein synthesis [3]. This provides evidence that Ψ modifications may be critical in cell lineage commitment. Specifically, mutations associated with several pseudouridylating enzymes, namely PUS1, PUS3, and PUS7, are associated with neuronal disorders and intellectual disability [4]. PUS1 mutations are associated with cognitive impairment [5]. Additionally, PUS1 acts on the steroid RNA activator, a co- activator of the nuclear estrogen receptor α regulating neuronal survival [5]. Truncated PUS3 and reduced levels of ψ U39 in tRNA were detected in patients with intellectual disability [5]. Lastly, mutations in PUS7 can cause intellectual disability and microcephaly in humans [4]. These studies suggest that Ψ can substantially impact neuronal differentiation and function; however, the mechanisms regarding pseudouridylation in neurogenesis are poorly understood. My results will contribute to our understanding of the importance of pseudouridylation in neurogenesis and neuron function. PROPOSED RESEARCH My overarching goal is to elucidate the molecular and functional roles of Ψ in neuronal cells. Based on the reported links to human brain function, I hypothesize that Ψ is a critical modification for neuronal differentiation and function. I further propose that the Ψ landscape between stem cells and neurons is unique and specific. I will test the above hypotheses with the following aims: Aim 1—To identify pseudouridylating enzymes for investigation. There are 13 known PUS enzymes in human cells; however, only a few predicted human PUS enzymes have been studied to date. In addition to PUS1, PUS3, and PUS7, I will predict other PUS enzymes associated with neuron function by first performing weighted gene co-expression network analysis (WGCNA). This analysis will reveal sets, or modules, of highly correlated genes. To perform WGCNA, I will use multiple human tissue RNA sequencing datasets from the GTEx project. I expect PUS1, PUS3, and PUS7 to appear in one module because these genes share connections to neuronal function. Other PUS enzymes that are expected to correlate with neuronal genes will fall in this module. I will then conduct gene ontology enrichment analysis to verify that the PUS candidates in that module are associated with neuron processes. These experiments will predict key pseudouridylating enzymes in addition to PUS1, PUS3, and PUS7 that will be investigated in neurons. Aim 2—To investigate the effects of PUS enzymes on neurogenesis. To define the impact of PUS enzymes on early neurogenesis, I will use CRISPR/Cas9 to knock out each candidate gene identified in aim 1 in iNGN cells. iNGN cells are a human induced pluripotent stem cell line which has been engineered to be readily induced into neurons within four days by doxycycline [6]. I will design a gRNA that targets the N-terminal coding exon of each gene to induce nonsense-mediated mRNA decay and validate that the enzyme is no longer expressed via Western Blot. I will explore two different methods of inducing edited-iNGN cells into neurons to gather more comprehensive results. For the first method, I will supplement cell media with doxycycline to induce the formation of neurons. Since this procedure only takes four days, it will allow for efficient generation of easily reproducible data. However, the rapid induction of robust neuronal morphology by doxycycline may limit the resolution of detectable phenotypic changes in these cells. Thus, the second, slower method enables me to mark differences at each stage during differentiation. I will differentiate successfully edited colonies of iNGN cells using a slow differentiation method following a previously published protocol [7]. I will use an inducible Cas9 system to knock out the PUS enzyme at different time points to determine the most critical points of pseudouridylation during neurogenesis [8]. To detect potential morphological alterations, I will use a neurite outgrowth assay and monitor the expression of neuronal markers via immunofluorescence. Rescue of the phenotypes by ectopic expression of the wild- type protein will control for the specificity of the effects. I will select the cell lines with the strongest phenotypes for additional study in aim 3. These studies will reveal how PUS enzymes impact neuronal maturation. Aim 3—To determine neuron-specific RNA targeting by PUS enzymes. To determine the positions of RNA binding by PUS enzymes at single-nucleotide resolution, I will perform UV cross-linking and immunoprecipitation, followed by high-throughput sequencing (iCLIP-seq) in iNGN-derived neurons expressing Flag-tagged PUS enzymes. I will corroborate these results by using specific antibodies to pull down endogenous PUS complexes. Mock-infected cells will be used as a control to exclude non-specific RNA binding. I will complement these results with Ψ-seq to confirm that the PUS-bound sites are catalyzed. A comparison of the sites bound in stem cells and neurons will reveal neuron-specific Ψ sites. Because PUS enzymes may have multiple substrates, it may be unclear how to assign mutant phenotypes to loss of modification in specific RNA species. I will control for this variable via rescue experiments by transfecting specific synthetic pseudouridylated RNA substrates identified by Ψ-seq experiments. These experiments will reveal the Ψ landscape in neurons and identify RNA targets for future study. Summary: Successful completion of these aims will shed new light on the poorly understood but critical roles that pseudouridylation plays in neuronal development. A potential follow-up study to investigate the function of Ψ in the identified RNA targets is an RNA pull-down assay. To determine the role of Ψ on the complex composition of RNA species, Ψ and non-Ψ RNA probes tagged with biotin can be pulled down, and the interactome can be analyzed by mass spectrometry. Future research into the biological roles of RNA modifications will reveal novel causes of neurological disorders. Intellectual Merit: In Dr. Murn’s lab, I’ve gained the necessary training in molecular biology techniques and RNA biochemistry to carry out this project. I am currently optimizing pseudouridine-seq and will use my experience in this technique for this proposal. I will receive the bioinformatic training necessary to carry out my proposal through future mentoring from Dr. Chaolin Zhang. Dr. Zhang’s lab at Columbia University is an ideal fit because of his focus on RNA-protein interactions, RNA regulatory networks in neural development, and expertise in high-throughput transcriptomic data analysis. Broader Impacts: As a graduate student and later as a professor, I will mentor undergraduate women of color and encourage them to pursue research careers. I will continue to empower young high school women by establishing additional chapters of Queens of STEAM. The support of the NSF GRFP will enable me to carry out my research while continuing STEM outreach. [1] I. A. Roundtree, M. E. Evans, T. Pan, and C. He, “Dynamic RNA Modifications in Gene Expression Regulation,” Cell. 2017. [2] J. Karijolich and Y. T. Yu, “Converting nonsense codons into sense codons by targeted pseudouridylation,” Nature, 2011. [3] N. Guzzi et al., “Pseudouridylation of tRNA-Derived Fragments Steers Translational Control in Stem Cells,” Cell, 2018. [4] H. Darvish et al., “A novel PUS7 mutation causes intellectual disability with autistic and aggressive behaviors,” Neurology: Genetics. 2019. [5] M. T. Angelova et al., “The emerging field of epitranscriptomics in neurodevelopmental and neuronal disorders,” Frontiers in Bioengineering and Biotechnology. 201. [6] V. Busskamp et al., “Rapid neurogenesis through transcriptional activation in human stem cells.,” Mol. Syst. Biol., Nov. 2014. [7] Y. Shi, P. Kirwan, and F. J. Livesey, “Directed differentiation of human pluripotent stem cells to cerebral cortex neurons and neural networks,” Nat. Protoc., Oct. 2012. [8] K. I. Liu et al., “A chemical-inducible CRISPR-Cas9 system for rapid control of genome editing,” Nat. Chem. Biol., 2016.	0
Introduction: Habitat loss and fragmentation due to land development disrupt ecosystem services, degrade carbon stocks, and threaten biodiversity1. Restoring connectivity between habitat patches is an effective approach to conserve biodiversity in a fragmented landscape, but limited funding requires prioritization of reconnecting habitat patches with high diversity. The theory of island biogeography2 suggests that area of a habitat patch and amount of space between patches (hereafter the “matrix”) are both important for restoration3. Consideration of only patch and matrix area, however, is inherently a flat perspective: an abundance of life lives above- ground, around 40% of biodiversity in forests4. Habitat volume may better predict biodiversity than area, and if so should be an important metric for establishing restoration priorities. The concept of an ecosystem as a multidimensional space of discrete niches is well- established5, and although a species-volume relationship has been recently proposed, it has not been rigourously tested6. My previous research shows that distinct microhabitats exist in a multidimensional space within forests7, and evidence suggests that canopy height and structure, which in part determine the number of available niches, influence diversity8. Taller, more structurally complex canopies may provide more “vertical niche space” (VNS) for species to use. VNS crossed with patch area can be a metric for patch volume. Some evidence suggests that VNS is more important for determining alpha diversity (hereafter alpha) than patch area9, and high-volume patches (fig. 1B) are expected to maintain higher alpha than low-volume patches (fig. 1C). Patch isolation (mean distance to closest patch on all sides) determines how easily organisms can cross between patches, and therefore influences alpha3. Depending on the land use of the matrix (fig. 1E&F vs. fig. 1G&H), matrix VNS will vary, and high matrix VNS may also increase alpha of adjacent patches. In regions with immense biodiversity but rapid rates of deforestation, such as Madagascar10, careful consideration must be given to restoring land between high-diversity patches. Conservation International (a conservation NGO) has scheduled large-scale restoration work in the Ambositra- Vondrozo Corridor (AVC) of Madagascar, but baseline biodiversity and forest cover monitoring is necessary. Herpetofauna (reptiles and amphibians) are particularly threatened both in Madagascar11 and globally12. Herpetofauna are abundant at all forest heights and some species are persistent in even small forest fragments, making them an excellent model taxon to test a species-volume hypothesis. By integrating high-resolution remote sensing of forests from satellites and an unmanned aerial vehicle (drone) with on-the- ground herpetofaunal surveillance, I will test the species- volume hypothesis to inform both ecological theory and restoration efforts. Hypothesis: I hypothesize that volume of habitat patches and inter-patch matrix more accurately predicts alpha diversity than patch area and isolation distance alone. Aim 1: I will determine how well patch area and isolation of habitat patches predict patch alpha. Aim 2: I will then build upon the species-area theory by evaluating how well patch volume (area crossed with VNS) and matrix volume (patch isolation crossed with VNS) predict patch alpha. Aim 3: I will use forest structure and patch alpha to hierarchically prioritize restoration sites. Methods: To select sites, I will first use Landsat satellite-derived estimates of forest cover13 and ICESat LiDAR-derived coarse estimates of canopy height14, and will calculate patch area and isolation using the SDMTools R package15. I will choose sites that are accessible and encompass forest patches that vary in size and structure. To acquire high-resolution models of volume, I will monitor a total of ~84 km2 (40 days of flights, 0.7 km2 coverage each) via a senseFly eBee® drone. An equipped CANON Powershot® will record photographs that I will stitch together using Pix4D photogrammetry software to produce 3D orthomosaic point clouds. I will divide the study region into 30m2 cells and substract orthmosaic points from a digital elevation model to derive mean and variance in canopy height (synthesized into a single VNS index) for each cell16. For both patches and matrix, I will multiply VNS by the standardized cell area to calculate volume of each cell. I will record herpetofauna richness and abundance in forest patches across the region by conducting 30 vertical surveys (from forest floor to top of canopy), sufficient for species accumulation (Scheffers, personal communication). I will perform generalized linear models17 (GLM; pending data structure and distribution) to model alpha in relation to 1) the interaction between patch area and isolation and 2) the interaction between patch volume (sum of cell volumes) and matrix volume (sum of non-forest cell volumes within a 5-km buffer zone of a patch). I will then conduct three restoration prioritization analyses using the software Zonation, which will prioritize cells by accouting for desired habitat inputs while iteratively removing the least valuable cells. In analysis (A1) inputs will be patch area and isolation; in (A2), patch/matrix volume; and in (A3) alpha of patches. I will contrast site selection for restoration by analyzing the correspondence between output cells from A1 and A2 to cells with high diversity (A3). Resources: I will be advised by Dr. Brett Scheffers (University of Florida; UF), an expert on Malagasy herpetofauna, I am familiar with single-rope canopy access, and the members of the Scheffers lab are certified with 1,000s of hours of canopy access. I will build point cloud models using UF’s HiPerGator 2.0, the world’s third fastest university computer. Intellectual Merit: My proposed research extends a classic ecological theory, the species-area relationship, by combining cutting-edge tools with conventional field methods. If my hypothesis that habitat patch volume can predict diversity more accurately than area is correct, this study will contribute to Understanding the Rules of Life, one of NSF’s 10 Big Ideas, and my workflow will establish an efficient pipeline for estimating biodiversity. Once such drone methods are achievable via satellite, my study can be replicated without any site visitation. My diversity monitoring may also contribute novel data on critically endangered and data-deficient species11. Broader Impacts: My work will inform the ambitious forest restoration projects planned for the AVC (2019-2024) and I have communicated with Conservation International to monitor sites of mutual interest. Forest restoration will enhance ecosystem services, absorb carbon emissions, and mitigate flooding of local communities, and my forest structure data can be used to quantify carbon stocks for offsetting projects in the region. As part of USAID PEER funding (2017-2021) to Dr. Brett Scheffers, we will collaborate with a Malagasy graduate student to train communities to monitor on-the-ground carbon stocks, thereby assisting them with active protection of their forests. I will also facilitate a letter exchange between Malagasy students and the Riverside Elementary School in the US, with which I have established contact on the matter. Citations: [1] Foley, J. A. et al. (2005) Science. [2] MacArthur, R. H. & Wilson, E. O. Princeton University Press, 2001. [3] Laurance, W. F. et al. (2002) Conserv. Biol. [4] Ozanne, C. M. P. et al. (2003) Science. [5] Hutchinson, G. E. (1957) Cold Spring Harb. Symp. Quant. Biol. [6] Gatti, R. C. et al. (2017) Plant Ecol. [7] Klinges, D. H. et al. Amer Nat. in review. [8] Bergen, K. M. et al. (2009) J. Geophys. Res. Biogeosciences. [9] Basset et al. (2015) PLOS One. [10] Myers, N., et al. (2000) Nature. [11] Andreone, F. et al. (2008) PLOS Biol. [12] Böhm, M. et al. (2013) Biol. Conserv. [13] Hanse, M. et al. (2013) Science. [14] Simard, M. et al. (2011) J. Geophys. Res. [15] VanDerWal, J. et al. (2014). R package. [16] Lisein, J. et al. (2013) Forests [17] Webster, C. et al. (2018) Remote Sens. Environ.	0
"wetlands (UPOWs), are a practical, cost-effective, and highly scalable approach to managing environmental water quality.1 UPOWs utilize microbial growth in the benthic region within a photosynthetic biomat, hosting a stratified population in aerobic, anaerobic, and anoxic zones. Biomat microbe ecosystems have demonstrated treatment of influent water for nutrients, trace organic compounds, and other contaminants at rates that match—and in many cases exceed—those of traditional vegetated or subsurface wetlands.1 These microbial populations develop independently over multiple months, using algae and other detritus as carbon and electron sources. The low implementation and maintenance costs of these systems have drawn attention to their use for increasing water availability and decreasing risk in otherwise water-poor or unprotected communities, such as the Arequipa region in Peru. These communities utilize surface water from local rivers such as the Tambo, which contains concentrations of arsenic (As) exceeding Autoridad Nacional de Agua regulations for both domestic and agricultural use.2 Elevated concentrations are likely a result of high background levels of As in local geology and introduction from mining operations in the area. The surrounding community can no longer safely consume or export important commercial products such as rice or river shrimp because of this threat. The long-term effects of these economic limitations and human health impacts cannot be understated and will continue to persist so long as the region suffers from degraded surface water quality. Dr. Josh Sharp, professor of Civil and Environmental Engineering at the Colorado School of Mines, is currently working with the Universidad de San Augustín de Arequipa (UNSA) in Peru to study the potential for UPOWs to treat surface waters for metal and metalloid contaminants. UPOWs with the capability to remove these hazards would provide communities with a first step toward improving environmental water quality. Challenges of degraded water quality do not adhere to national or state boundaries. Communities all over the world, including here in the United States, contend with a lack of access to safe water sources. My goal is to apply research on UPOWs to ensure no one has to suffer from contaminated water. I will build upon Dr. Sharp’s research on UPOWs by determining their capacity for arsenic removal. Hypothesis: UPOWs are able to remove trace organics and nutrients from surface waters and are potentially capable of immobilizing metal and metalloid contaminants.1 Arsenic (V) is the most prevalent form of As in surface water, and poses a hazard to human and environmental health.3 I hypothesize that UPOWs incorporating sulfate-reducing bacteria have the capability to remove As (V) from surface water via precipitation with sulfide in a variety of geographic settings. Research Plan: Objective 1: Analyze the potential for As (V) precipitation as As S in the presence of 2 3 sulfate-reducing bacteria in UPOWs. Arsenic (V) removal by precipitation relies on the transformation of As (V) to As (III), as biological systems have previously been observed to utilize sulfate-reducing bacteria to precipitate As (III) with sulfide.3 H AsO , the naturally occurring form of As (III), reacts with sulfide to produce As S (Equation 3 3 2 3 1) which is insoluble in typical surface water conditions.4 2𝐻 𝐴𝑠𝑂 +3𝐻 𝑆 ⇌ 𝐴𝑆 𝑆 +6𝐻 𝑂 Equation 1 ! ! "" "" ! "" Some sulfate-reducing bacteria hold the potential to reduce As (V) to As (III).5 The presence of these bacteria could allow for the precipitation of As (V), proceeding as in Equation 1. Sulfate-reducing bacteria colonize and function well in biomat ecosystems,4 but they have yet to be tested for specific removal of As. Objective 2: Evaluate As removal efficiency of UPOWs across largely differing geographical areas. The formation and function of biomats in UPOWs rely on both biotic and abiotic constituents of surface water. Successful arsenic removal depends on the colonization and function of sulfate reducing bacteria, algae, and other diatoms, all of which could differ by region. I will observe whether biomat ecosystems will support microbe composition necessary to perform key contaminant removal functions regardless of UPOW location. Differing water chemistry and abiotic constituents could alter the As removal pathway or block As precipitation altogether. In some cases, As may complex with other constituents to produce undesired byproducts, such as Thioarsenic.6 A particularly interesting factor in the success of As removal may also be sulfate concentration, though determining direct influence is outside the scope of this project. Approach/Methods: Testing will occur at both the bench and field scales. After the effectiveness of the mesocosm-sized UPOW biomat has been determined, the biomat composition and design will be tested at the field scale at the Prado Wetlands in California and then at the UNSA in Peru. Objective 1: Bench scale biomats will be harvested from existing UPOWs in the Prado Wetlands and deployed in the lab at the Colorado School of Mines to be tested with spiked influent water. Test water will consist of local Colorado water samples spiked with known levels of arsenic. Sulfate will be held constant in a similar concentration to that of the Tambo. Removal efficiency will be calculated using a mass balance on As, with a known influent concentration, precipitation concentration determined using a modified Tessier extraction of the biomat,7 and effluent concentration determined using ICP-MS chromatography. Selective inhibitors for sulfate reducing bacteria, such as molybdenum8, will be used to evaluate arsenic precipitation in response to sulfate reducing capacity. Testing will move to the field scale as a validation step in large-scale performance after bench- scale testing has been completed. Field testing will first take place at the Prado Wetlands, where a colonized UPOW will be allowed to run for an extended period of time. As concentrations will be measured using the same methods as at the bench scale. The continual rise of precipitated arsenic in the biomat could lead to concern after long periods of time in field operating conditions. To mitigate potential release of highly concentrated arsenic from a biomat, I recommend an operating system in which sections of biomat could be harvested prior to the accumulation of dangerous levels of As or other harmful constituents. These sections could go on to be used as fertilizer to utilize their elevated levels of nitrogen, resulting from abundant nitrifying bacteria. Objective 2: Successful function of a UPOW system in Peru will first be estimated at the bench-scale using replicated Tambo River water at the Colorado School of Mines. Biomat colonization and As removal will be monitored over the course of several months using these waters, with an emphasis on the perception of sulfate reducing bacteria presence. Field-scale testing will occur at the UNSA and will mirror procedures in the United States, now using unmodified water from the Tambo River. Biomats will colonize and undergo testing in mesocosm and field-scale UPOWs and arsenic concentrations will be determined using a mass balance approach. Intellectual Merit: Metal and metalloid removal has not yet been tested in UPOW systems, but the successful performance of sulfate-reducing bacteria in previous UPOWs could lead to an innovation in As removal methods. This study would quantify the effectiveness of this mechanism for arsenic removal from surface water via UPOWs. UPOWs have been tested in the United States but have not been observed abroad. This study will demonstrate the effects of differing surface water characteristics of different regions on arsenic precipitation. Additional application of UPOWs, including biomat harvesting, could have basis for further investigation to increase utility of these already compelling systems. Broader Impacts: The development of this system, and other natural treatment systems, would make progress on the priorities set forth by the UN Sustainable Development Goals and the Grand Challenges of the National Academy of Engineering. My work would directly contribute to these efforts aimed at increasing global sustainability and standards of living. More immediately, an UPOW system has the potential to mitigate the threat of arsenic for the Arequipa community and others like it. Depending on sequestered metal concentrations, the periodic harvesting of UPOW biomats, for fertilizer or otherwise, could also prove UPOW potential to serve a purpose beyond water treatment. Further research on natural treatment systems will continue to drive down cost, increase understanding, and foster education in communities using natural water treatment. References: 1Jasper, et al. (2013). Environ. Eng. Sci. 30(8), 421-436. 2Autoridad Nacional de Agua. (2020) Ministerio de Agricultura y Riego. 3Lizama, et al. (2011). Chemosphere. 84(8), 1032-1043. 4Jones, et al. (2017). Appl. Environ. Microb. 83:e00782-17. 5Macy, et al. (2000). Arch. Microbiol. 179, 49-57. 6Stucker, et al. (2014) Environ. Sci. Technol. 48(22), 13367-13375. 7Tessier, et al. (1979). Anal. Chem. 51(7), 844- 851. 8Blum, et al. (1998). Arch. Microbiol. 171, 19-30."	0
of bacteria (~99%), followed by archaea, then eukaryota, collectively forming the human microbiome.1,2 Over the past decade, research has revealed impacts of colon microbiota on human physiology, including roles in digestion, metabolism, immune system regulation, hormone signaling, and development.3,4 Colon microbiota reside in a specialized niche—a layer of mucus secreted by the colon epithelium, heavily comprised of the gel-forming protein MUC2.5 Adenomatous Polyposis Coli (APC) is a scaffolding protein that has long been studied as a tumor suppressor antagonist of the Wnt/-catenin signaling pathway, with additional roles less defined. In Dr. Kristi Neufeld’s lab at the University of Kansas, we have accumulated evidence suggesting multiple novel functions of APC, including in the expression of MUC2. Given this evidence, I hypothesize that APC directly or indirectly promotes MUC2 expression, and therefore has a role in colonic mucus generation, microbiome homeostasis, and colon function. Uncovering roles of APC in MUC2 expression would fill a critical knowledge gap not only in our understanding of the human microbiome and its homeostasis, but also in the cellular function of highly conserved APC-like orthologs found nearly universally across invertebrates and vertebrates.6 In a recent study, using Human Colon Epithelial Cells (HCECs), we found that reduction of APC led to a 75% decrease in IL-1R, an integral membrane receptor protein that, when bound to a ligand, promotes MUC2 expression.7 We found a similar trend in the IL-1R ligand, IL-1. To further investigate the relationship between APC, IL-1 signaling, and MUC2 expression, we modified a human colon cancer cell line, DLD-1, using CRISPR/Cas9. We inserted wildtype (WT) APC under control of a doxycycline (Dox) responsive promoter, which allowed us to treat cells with Dox and induce expression of WT APC. Using unmodified (parental) DLD- 1s as a control, we found that IL-1 and WT APC individually increased MUC2 expression 2-fold, but when expressed simultaneously, MUC2 expression increased 15-fold (Fig. 1).7 Figure 1. RT-qPCR for MUC2 in This evidence suggests that IL-1 signaling and APC act Parental and APC-Inducible DLD-1 synergistically to promote MUC2. This evidence also suggests that cells. (Two-way ANOVA with IL-1R activation increases MUC2 expression, which may occur Tukey’s range test, **P <0.005, through signaling of PKC-, a protein previously correlated with IL- ***P<0.0005, ****P<0.0001) 1R activation and MUC2 expression.8 To investigate DNA- binding of APC that might be involved in MUC2 promotion, we used a dataset from a recent ChIP-seq study9 and found that APC binds to the promoter of the MUC2 gene. Combined with our APC gain and loss of function studies, I created a simple model to reflect hypothesized interactions between IL-1R, APC, PKC-, and MUC2 (Fig. 2). Aim 1: Elucidate the Molecular Roles of APC in MUC2 Expression. As evidenced by our research, APC expression Figure 2. Model of relationships that would leads to an increase in MUC2 mRNA levels and APC loss allow IL-1R and APC to increase MUC2 leads to reduction in IL-1R. However, whether APC induces expression individually and synergistically.* translation of MUC2 or IL-1R is yet to be determined. With the help of Dr. Yoshiaki Azuma, a CRISPR/Cas9 expert at the University of Kansas, I have designed and begun creation of two novel cell lines to uncover the molecular roles of APC. By modifying HCEC and DLD-1 cells, we will be able to perform quick, cheap, efficient, and consistent 1) APC KO using an Auxin- Inducible-Degron (AID), 2) basal-level APC expression with no treatment, and 3) APC overexpression using Tetracycline-Inducible-Expression (TIE). The AID system uses OsTIR1, an auxin-responsive ubiquitin ligase, to polyubiquitinate AID-tagged proteins for degradation in the proteosome.10 I am tagging endogenous WT APC with an AID for APC KO. The TIE system initiates transcription of a gene through a tetracycline (TET)-responsive promoter11 and will be paired with the endogenous AID-tagged APC. I will use MUC2 promoter/Luciferase Reporter Assays (LRAs) and our in-lab luminometer to confirm response to various levels of cellular APC. An increase in luciferase indicates that APC increases MUC2 promoter activity, a neutral result indicates no effect of APC, and a negative result indicates APC reduces MUC2 promoter activity. This design can be repeated with reporters for all my candidate gene promotors, including IL-1R, MUC2, synergism between APC and IL-1 for MUC, etc. Regardless of my findings, these results will direct my further studies on APC and the other proteins of interest in this system, including investigation on protein localization, protein modifications, and signaling pathways. I will use co- immunoprecipitation followed by mass spectrometry performed at KU’s Core Mass Spec Lab to identify signaling proteins, florescence microscopy using our in-lab microscope to understand protein localizations and treatment phenotypes, SDS-PAGE and western blots for exposing signaling pathway patterns, mobility shift assays for uncovering post-translational modifications and protein activation, and much more to understand my proteins of interest and their signaling pathways. Aim 2. Monitor Impacts of APC on Colon Microbiome Homeostasis. In another recent study, we created transgenic mice with an APC allele containing mutations in the protein’s Nuclear Localization Sequences (APC-mNLS), preventing APC from entering the nucleus, and thus, inhibiting potential direct APC-driven nuclear promotion of gene expression. In this study, we uncovered that APC-mNLS mice had a >90% decrease in MUC2 expression following colon epithelial injury compared to control mice.12 I hypothesize that APC-mNLS mice have lower levels of MUC2, reducing the amount of mucus habitable by microbiota in the colon and thereby decreasing microbiome diversity and microbiota count. Using this mouse strain, with our extensive mouse care facilities and IACUC approval (137-01), I will collect fecal and colonic mucus swab samples of both mutant mice and control littermates for Zymo Research’s Microbiome Analysis Service,13 which provides publication-ready data of a list of microbiomic data, including absolute microbiota counts, multi-kingdom accounts of microbiome diversity within and between samples, and more. In addition to these data, I will use immunohistochemistry to visualize MUC2 in the colon epithelium using anti-MUC2 antibodies, comparing mutant and control littermates. This will provide comprehensive evidence of any changes to the microbiome and the colonic mucus layer in response to loss of nuclear APC, providing insight into APC’s physiological function and connection to the microbiome in vivo. Broader Impacts: The proposed research will expand our understanding of non-Wnt functions of APC, the function of APC in the generation of the colon mucus layer, the impact of APC on the microbiome, and provide insight on the function of APC orthologs across the animal kingdom. These data have the potential to uncover numerous novel functions of APC and better our understanding of microbiome homeostasis in relation to physiological function, a field of study that is still in its infancy. The cell lines I will create and data I will collect will be powerful research tools for myself, the Neufeld lab, and other researchers studying APC and colon microbiomics. I will communicate the conclusions of my research through publications, as well as through posters and oral presentations at regional and national conferences. I will also continue to develop the Rural Scientist Initiative (RSI) outlined in my personal statement; the NSF GRF would allow me to dedicate more time to the RSI while attending graduate school. Development of the RSI includes presenting this research to rural students along with information on scientific careers to ~15-30 of the 70 high school students at Norwich High School per year, based on student interest. I will continue communicating my experience as a scientist from several underrepresented communities to these students, encouraging rural students to pursue careers in scientific research. References. 1. Sender et al (2016) PLOS Biol 14(8):e1002533; 2. Qin et al (2010) Nature 464:59-65; 3. Heintz-Buschart et al (2018) Trends Microbiol 26(7):563-74; 4. Schroeder et al (2016) Nat Med 22:1079- 89; 5. Johansson et al (2016) Nat Rev Immunol 16:639-49; 6. Bienz et al (2002) Nat Rev Mol Cell Biol 3:328-38; 7. Gomez et al (2020) Exp Physiol 105(12):2154-67; 8. Tiwari et al (2011) J Immunol 187(5):2632-45; 9. Hankey et al (2018) Oncotarget 9(58):31214-30; 10. Natsume et al (2016) Cell Reports 15, 210-18; 11. Das et al (2016) Curr Gene Ther 16(3):156-67; 12. Zeineldin et al (2014) Carcinogenesis 35(8):1881-90; 13. Zymobiomics (2020) 5:159-63; *Created with BioRender.com	0
Improving the production of biofuels by understanding metabolic pathways in microalgae The objective of the proposed research is to quantitatively assess the metabolic capabilities of microalgae to identify inefficiencies that limit cell growth and lipid synthesis. The long term goal is to create a strain of microalgae that is maximized for biofuel production. Currently, plant biodiesel is the main source of biofuels. However, the demand for oil has outpaced the amount of biodiesel that can be produced in this manner. Microalgae are a viable alternative to plants due to their ability to produce up to 370 barrels of oil per hectare, which is more than 100 times greater than the oil produced through soybeans- the main crop for biofuel.1 Even so, efforts to enhance their biofuel- producing capability is still needed in order to realize their industrial viability. One of the biggest challenges with algae biofuels is the understanding of mechanisms that influence the production of triacylglycerol (TAG), a precursor to biodiesel. The objective of the proposed research is to develop a platform to identify reactions that limit TAG production in the central carbon Figure 1: Metabolic network for Pt metabolism of algae (Fig 1). The The colored boxes indicate different cellular compartments. microalgae, Phaeodactylum Dark yellow circles indicate the metabolites that can be tricornutum (Pt), is ideal for biofuel measured using GC-MS and LC-MS. While circles indicate production: 1) they efficiently fix metabolites that cannot be measured. Red dotted lines indicate atmospheric CO 2- responsible for that reactions exist between TP, ACA and TAG. absorbing at least 25% of the total amount of carbon dioxide processed by the seas, 45-50 billion tons of organic carbon 2 2) they accumulate up to 45% of their dry cell weight in TAG 3. In recent years, advances in genomic tools for Pt such as genome editing4 and stable delivery vectors5. allow us to further enhance the ability of Pt to produce TAG. However, the genes that correlate to these metabolic inefficiencies are currently unknown so gene alterations are conducted through educated guessing. Stable isotope tracers, such as 13C, are added to biological systems to track patterns of isotope incorporation into numerous products synthesized by the cell, including TAG. Coupling tracers with Isotopically Nonstationary Metabolic Flux Analysis (INST-MFA), the metabolic fluxes in central carbon metabolism as well as the transport rates across cellular compartments (Fig 1) can be determined. With this knowledge, reactions that divert carbon away from TAG can be identified and subsequently targeted for gene editing. Through this method, we will be able to create a strain of Pt that is optimized for TAG production. 1 Amy Zheng NSF Research Proposal The proposed work will be broken down into two major tasks: 1) Describing the central carbon metabolism of Pt using INSTA-MFA 2) Highlight metabolic targets for deletion and assess its effect on TAG production. Task 1: Developing flux map of central carbon metabolism in Pt The objective is to find which reactions contribute to TAG production using INSTA-MFA. INSTA-MFA combines of computational and experimental methods in order to describe the metabolism of an organism. This is due to the infeasibility of measuring all metabolites within an organism. Therefore, the metabolites that cannot be explicitly measured (white circles in Fig 1) must be interpolated using a model. In the computational portion, a model is compiled from literature and biochemical databases focusing on the central carbon metabolism. Central carbon metabolism the main focus of our model because these pathways contain the major carbon reactions. Carbon enters the organism as CO and is turned into biomass or TAG.6 TAG is synthesized from ACA and TP 2 (red dotted line Fig 1). We have created this model for Pt which includes all of the measurable metabolites and the major carbon cycles such as the Calvin, Tricarboxylic Acid Cycle and Pentose Phosphate Pathway (Fig 1). On the experimental side, metabolites are tracked by feeding the 13C tracer as sodium carbonate (yellow circles Fig 1), allowing us to track the pathway of the carbon through the organism. The metabolites containing the tracer in the organism will increase over time as it becomes incorporated into the major cycles. This pattern of incorporation over time is called the Metabolite Labeling Data (MLD). In INSTA-MFA, MLD is combined with the computational model to understand the labeling pattern of unmeasurable metabolites. The results gives us the fluxes through all of the reactions present in the model, also known as a flux map. Using the flux map, we can identify bottlenecks, which are genes corresponding to reactions that direct CO away from TAG.6 2 Task 2: Highlight metabolic targets for deletion and assess its effect on TAG production The objective of the second area is to produce the diatom with the maximum amount of TAG production by targeting the genes correlated to the bottlenecks. From our simulation created in Area 1, we can identify reactions that improve TAG production through gene alteration. However, there is a gap in knowledge on how these gene mutations affect the metabolism of the diatom. Using INSTA-MFA, we can predict which genes limit TAG production by looking for bottlenecks. Then, we can delete those genes with help from our collaborators at Colorado State University, who have developed methods for genome engineering in diatoms, to produce desired mutants. After the mutants have been synthesized, we can assess the changes in metabolism and identify additional bottlenecks by using INSTA-MFA again. Through this iterative process, we can create a strain of diatoms that produce the maximum amount of TAG. Broader Impacts: Within the scientific community, this project will help us understand how CO is incorporated into photosynthetic organisms. On an industrial scale, increasing the 2 efficiency of biofuel production in Pt will eliminate our reliance on fossil fuels. INST-MFA allows us to understand the metabolism of Pt by combining 13C isotope experiments and simulations to create a unique flux map. From this map, we can identify genes that can be mutated to increase TAG production. By understanding how metabolism within Pt functions, we can create a diatom that produces the maximum amount of TAG physically possible. Sources: [1] Chisti Biotechn Adv 25, 294-306 (2007). [2] Young et al.Metab Eng 13, 656-665 (2011). [3] Falkowski, et al. Aquatic Photosynthesis. 2nd edn, (Princeton University Press, 2007). [4] Hu et al. Plant J 54, 621- 639 (2008). [5] Weyman et.al Plant Biotechnol J. 13, 460-470 (2015) [6] Cheah et al. Systems Biology 25-70 Wiley (2017). 2	0
Measuring Rayleigh Wave Phase Velocity in the Antarctic Upper Mantle from Ambient Seismic Noise Background and Motivation Two compelling questions make the Antarctic region worth studying: 1) Why is there a significant age difference between West and East Antarctica? And 2) How exactly will global sea-levels rise in the future? Divided by the Transantarctic Mountains, West Antarctica is significantly younger than the East Antarctica craton (Hansen et al., 2014). Resolving the age difference between Antarctica’s two halves will help us understand the tectonic history and evolution of the Antarctic region. On the other hand, sea-level rise has serious implications on infrastructure displacement as we lose land surface area. Simulations of global sea-level rise have high uncertainty but could benefit from incorporating bedrock uplift, mantle viscosity, and geothermal heat flux (Gomez et al., 2015). Approaching both questions requires better constraints on mantle properties underneath Antarctica. Investigating Antarctica poses unique challenges for many traditional measurement techniques. About 98% of Antarctica’s land surface area is underneath a thick ice-sheet (Fretwell et al., 2013), and uncertainty in mantle viscosity convolutes anticipating changes in the Earth’s surface in response to ice-sheet melting and growth. These two factors make measuring geothermal heat flux impractical. Furthermore, traditional seismic tomography methods rely on earthquakes for seismic signals which are scarce in and around the Antarctic region. An emerging approach in seismic tomography is to use ambient seismic noise – signals primarily generated from interactions between Earthquakes 1966-2017 Figure 1: Topography map of ocean water waves and solid Earth – in addition to earthquake data to the Antarctic plate. Colors characterize mantle properties. This method has been published by represent bedrock elevation in Bensen et al. (2007) and has been widely adopted by seismologists meters and pink dots are earthquakes that occurred in with over a thousand citations. One study shows incorporating ambient 1966-2017. Tectonic plate noise can increase phase-velocity map resolution in the Indian Ocean boundaries are plotted as by 20% relative to maps generated by relying solely on earthquake white lines. data (Ma & Dalton, 2016). While there exists many studies employing ambient noise, the number of similar studies on the Antarctic region are relatively scarce. Proposed Methodology I will obtain long-period vertical component data from the Incorporated Research Institutes of Seismology (IRIS) for all active, unrestricted stations south of -55 degrees latitude which encompasses all of Antarctica. The data will be processed in 4 h stackable (i.e. able to be combined through addition via linearity) windows. To ensure a high signal-to-noise ratio in our phase arrival time measurements, I will discard 4 h windows that are not entirely full. Correlation measurements between all station pairs will be done in the frequency domain by computing the cross-spectrum ρ (ω) as done in Ekström (2014): ijk Kevin Trinh NSF Research Statement 𝑢 (𝜔) 𝑢∗ (𝜔) 𝜌 (𝜔) = 𝑖𝑘 𝑗𝑘 𝑖𝑗𝑘 √𝑢 𝑖𝑘(𝜔) 𝑢 𝑖∗ 𝑘(𝜔)√𝑢 𝑗𝑘(𝜔) 𝑢 𝑗∗ 𝑘(𝜔) The letter u represents a 4 h seismogram passed through a fast Fourier transform. ω is frequency, i and j are station indices, and k is a 4 h window index. The asterisk * denotes a complex conjugate. Performing an inverse fast Fourier transform on the cross-spectrum yields a cross- correlation in the time domain. Measurements of phase arrival times can be made from cross-correlations. These arrival times will be used to perform inversion and thus yield phase-velocities for many small, discrete regions in Antarctica. Earthquake data can be incorporated with ambient noise data to account for regions with low data count (i.e. not many paths traversing the discrete region). Additionally, the smoothing of our inversion will account for regions with little data and can be adjusted to yield the best results. I will conduct numerous tests to identify optimal smoothing parameters and relative weighing between ambient noise and earthquake-based data. These steps result in a 2D phase-velocity map and can be repeated to map varying depths of the Antarctic upper mantle. Anticipated Results The speed at which wave phases propagate through solid Earth is related to material properties such as temperature, composition, and partial melt. I expect to see West and East Antarctica to be dominated by slow- and fast-velocity anomalies, respectively, which should agree with past studies using p-waves to image the Antarctic mantle. Improved resolution in tomographic maps of the Antarctic upper mantle may help me observe undiscovered geological features such as cratons and oddly pronounced and heterogenous velocity anomalies. Proposed Timeline: Year 1: Download and process seismogram data from IRIS from 1900 to 2017. Year 2: Generate 2D seismic tomography maps. Year 3: Identify optimal smoothing parameters and relative weights. Repeat mapping process for varying depths of the Antarctic upper mantle. References Bensen, G. D., Ritzwoller, M. H., Barmin, M. P., Levshin, A. L., Lin, F., Moschetti, M. P., et al. (2007). Processing seismic ambient noise data to obtain reliable broad-band surface wave dispersion measurements. Geophysical Journal International, 169(3), 1239–1260. https://doi.org/10.1111/j.1365-246X.2007.03374.x Ekström, G. (2014). Love and Rayleigh phase-velocity maps, 5–40 s, of the western and central USA from USArray data. Earth and Planetary Science Letters, 402, 42–49. https://doi.org/10.1016/j.epsl.2013.11.022 Fretwell, P., Pritchard, H. D., Vaughan, D. G., Bamber, J. L., Barrand, N. E., Bell, R., et al. (2013). Bedmap2: improved ice bed, surface and thickness datasets for Antarctica. The Cryosphere, 7(1), 375–393. https://doi.org/10.5194/tc-7-375-2013 Gomez, N., Pollard, D., & Holland, D. (2015). Sea-level feedback lowers projections of future Antarctic Ice-Sheet mass loss. Nature Communications, 6, 8798. https://doi.org/10.1038/ncomms9798 Hansen, S. E., Graw, J. H., Kenyon, L. M., Nyblade, A. A., Wiens, D. A., Aster, R. C., et al. (2014). Imaging the Antarctic mantle using adaptively parameterized P-wave tomography: Evidence for heterogeneous structure beneath West Antarctica. Earth and Planetary Science Letters, 408, 66–78. https://doi.org/10.1016/j.epsl.2014.09.043 Ma, Z., & Dalton, C. A. (2016). Evolution of the lithosphere in the Indian Ocean from combined earthquake and ambient noise tomography. Journal of Geophysical Research: Solid Earth, 122(1), 354–371. https://doi.org/10.1002/2016JB013516	0
Animals harbor a suite of innate fears, knowing them from birth in the absence of firsthand experience. These fears are rooted in evolution and are often species-specific. For example, mice respond defensively to odors related to foxes and cats, two of their most common predators. Alternatively, humans respond defensively to snakes and spiders, both of which can be highly-lethal and are endemic to East Africa, the original range of genus Homo. These factors strongly imply a genetic origin for these fears, though none has yet been investigated. Aversive responses have both behavioral and hormonal components. The two brain regions necessary and sufficient for innate aversive odor responses in mice are the cortical amygdala (CoA) and the amygdalo-piriform transition area (APir). The CoA mediates innate olfactory behavior, and is organized spatially based on the emotion a given odor evokes — neurons responsive to aversive odors are located in the anterior CoA, and neurons responding to all other odors are posterior.1 APir controls the hormonal stress response to innately aversive odors, stimulating secretion of corticotropin-releasing hormone (CRH) by the hypothalamus, raising peripheral corticosterone levels.2 Neither region has any other known functions. A distinct, spatial organization to neuronal populations that mediate specific behavioral functions, independent of individual experience, strongly implies genetic control over the developmental programs creating these pathways. For instance, a past study examining similar populations in other regions showed each one expresses a set of marker genes specific to their own population.3 Thus, neuronal populations in CoA and APir may similarly express their own sets of marker genes. I propose to identify the first set of marker genes for neurons controlling innate aversive responses to a set of specific stereotyped odors. Aim 1: Identify neurons specifically mediating innate olfactory aversion. Hypothesis: If these regions respond to innately aversive odors, then the specific responsive neurons within these regions should be identifiable based on the population’s activity in odor exposure. Method: I can mark these neurons using a transgenic mouse strain with neurons that Cre- dependently express eYFP if active within a transient, hours-long period after peripheral tamoxifen injection.4 I will identify innately aversive odor-responsive neurons by exposing mice to either water or trimethylthiazoline (TMT), a well-validated innately aversive fox odor, shortly after peripheral tamoxifen injection. The water- responsive group represents neurons active at rest, while the TMT- responsive group represents neurons activated by innately aversive odors. Differences in response between the two conditions should reflect regional activity differences. Anticipated Results: The eYFP- Figure 1. Preliminary Results and Research Plan. After odor expressing population should be exposure, cells (blue) in the amygdala express Arc (green). A. enriched in CoA and APir in the Amygdala neuron activity after water exposure. B. Activity after TMT-exposed mice compared to the TMT exposure. C. Proposed research plan. BLA: basolateral amygdala nuclei, CeM: centromedial amygdala nuclei. water-exposed mice. The regions NSF GRFP 2018 1 James R. Howe Graduate Research Plan they innervate, the BLA, CeM, and the hypothalamus, which control general aversive responses, should be enriched as well. Preliminary data corroborates these predictions (Figure 1A, 1B). Aim 2: Identify genes exclusive to innate olfactory aversion neurons. Hypothesis: Neurons active during TMT exposure should express a suite of marker genes not expressed in neurons active during water exposure throughout the brain. The eYFP-expressing activated neurons can be dissociated on ice via an optimized combination of RNA polymerase inhibitors, physiological solutions, and extracellular matrix-specific cold-active proteases to keep cells alive and eliminate gene expression artifacts.5,6 I can combine this technique with dissection and fluorescence- activated cell sorting to isolate single live eYFP-expressing cells from CoA with high fidelity.7 Method: I will use an efficient, well-validated, high-resolution form of single cell RNA- sequencing to precisely assay the expression of all genes in all isolated cells (Figure 1C).8 This approach closely resembles the method used in a recent series of experiments in the neuroscience literature.9 A custom computational pipeline purpose-built for this experiment will analyze the data. Machine learning algorithms will classify cells into groups based on similarities in underlying gene expression. Differential expression analysis will identify the most highly upregulated genes in each group of cells compared to all others. Gene ontology (GO) analysis will then identify these genes’ functions. RNAscope, a multiplexed single-molecule RNA fluorescent in situ hybridization platform, will externally validate these results.10 I collaborate with three groups across multiple disciplines and institutions to perform these techniques: in biology at the University of Cincinnati, biochemistry at UCLA, and bioengineering at UCSD. Anticipated Results: Using this framework, I expect to identify at least one group of neurons present in the TMT-responding population but not the water-responding population, with at least one corresponding suite of highly-expressed genes. These will both be confirmed via RNAscope. We expect these genes to display enriched neural development-related GO terms. Intellectual Merit: This would be the first study to identify and validate the heritability of innate behaviors, the specific neurons mediating these behaviors, and their underlying genes. Finding such genes would allow the targeted stimulation and genetic access of these neurons for the first time, making modification or simulation of specific odor responses (even in the absence of prior experience) possible, a valuable future research tool. Using such tools, researchers could create far more precise experimental designs, allowing researchers to answer more specific hypotheses than ever before. The availability of such novel technologies at the intersection of psychology, molecular biology, and sensory neuroscience will have many implications for studies in all three fields and will further stimulate interdisciplinary research incorporating aspects of all three. Broader Impacts: During this project, I will train undergraduate students from underrepresented communities at UCSD in molecular biology and behavioral techniques, as well as mentor them in research methods, both at the bench and away from it. The computational and molecular methods will be made open-source on GitHub and protocols.io. I will communicate the results in open-access peer-reviewed academic journals, and I will also write articles in popular media outlets throughout the project based on what I study and discover along the way. The marker genes identified in this project could lead to advancements in commercial research technologies and pharmacology, as each one could serve as a possible target for future drug development should any of these populations become relevant to certain research questions or diseases. References: 1Root et al. Nature 2014 515:269-273. 2Kondoh et al. Nature 2016 532:103-105. 3Kodama et al. J Neurosci 2012 23:7819-7831. 4Guethner et al. Neuron 2013 78:773-784. 5Wu et al. Neuron 2017 96:313-329. 6Adam et al. Development 2017 144:3625-3632. 7Hempel et al. Nat Methods 2007 2:2924-2929. 8Torre et al. Cell Syst 2018 6:171-179. 9Tasic et al. Nat Neurosci 2016 19:335-346. 10Wang et al. J Mol Diagn 2012 14:22-29. NSF GRFP 2018 2	0
"suffered severe facial trauma. I underwent several procedures to repair the damage, including maxillofacial reconstruction (Figure 1) and extensive dental surgery. During my recovery, I became fascinated with the sophistication of the materials used in my repairs and became deeply interested in orthopedic and dental biomaterials. This interest, combined with my strengths in physics Figure 1. and mathematics, led me to pursue my undergraduate education in bioengineering with an emphasis in materials science at The Pennsylvania State University. Through my educational, teaching, leadership and outreach activities, I have since formulated my career aspiration to become a professor at a research institution. My desires to address biomedical challenges creatively, to advance my education, and to share my knowledge are perfectly suited to this profession. Educational Experiences. In addition to maintaining a high GPA, I have participated in academic activities to prepare myself for graduate study. For example, I joined the Schreyer Honors College my sophomore year to gain experience in writing an honors thesis. Being in the honors college also has afforded me the opportunity to take higher-level courses that incorporate my research interests. For instance, I enrolled in a graduate-level musculoskeletal mechanics course for the spring. I am also a scholar in the Ronald E. McNair Post-Baccalaureate Achievement Program, a federal TRIO program designed to prepare low-income, first generation, and underrepresented students for doctoral study. The program initially appealed to me because it organizes professional development activities for its scholars and financially supports a nine-week summer research internship with a faculty advisor. This program was a unique opportunity to develop my career objectives and gain valuable research experience. In the spring of 2007, I was accepted into the program and joined the laboratory of two faculty advisors whose research aligned with my interest in orthopedic biomaterials: Dr. Erwin Vogler from Materials Science and Engineering and Bioengineering and Dr. Andrea Mastro from Biochemistry and Molecular Biology. These two professors collaborate on studying breast cancer metastasis to bone. Teaching Experiences. As a low-income, first-generation college student, I work to support my education and have sought opportunities where I both can be compensated and can reinforce concepts from my coursework. For instance, I have served as a grader for the Math Department in six differential equations courses over Teaching Experiences two semesters and have volunteered to run • Physiology Teaching Assistant chemistry, physics and calculus study groups for the Women in Engineering Program. During this time, I • Engineering Design Lab Assistant also accepted an internship with the Chemistry • WEP Facilitator – Chemistry, Department, which combined research, volunteer and Calculus, Physics groups teaching components. Under the advisement of Dr. • Chemistry Dept. internship Joseph Keiser, I researched the life and scientific • Grader, Differential equations achievements of George Washington Carver, • 400-level Bioengineering course focusing on his experiments with peanuts. Extracting information from farming bulletins, Carver’s patents and my own laboratory experiments, I developed a laboratory curriculum for students in general chemistry, and implemented the curriculum in the form of a make-up laboratory. The curriculum was later adapted into a calorimetry laboratory for an honors introductory chemistry course. Because I also wanted to be 1 involved in teaching activities more closely related to my academic pursuits in bioengineering, I accepted a position as a laboratory assistant for an engineering design course, helping students in the development and presentation of design projects. As my teaching skills strengthened, I wanted to challenge myself to lead a classroom. For that reason, I became a teaching assistant for an introductory physiology laboratory, a role in which I could teach independent from a faculty instructor. I have taught six sections of this course over the past two years. Each semester, I manage approximately 60 students. As the only TA studying bioengineering, I also mentor bioengineering students from other sections. My responsibilities for the course include preparing and instructing pre-laboratory lectures, developing weekly quiz questions, demonstrating laboratory techniques, leading post-laboratory discussions, and formulating instructions for written reports. I have received Institutional Animal Care and Use Committee (IACUC) training for the use of animals in the laboratory. With the help of another TA, I developed an academic writing tutorial as a supplement for the course. I will further challenge myself in the spring as an assistant for a 400-level bioengineering course by teaching more difficult material. Leadership Activities. In addition to assuming leadership roles in the previously mentioned activities, I currently serve as treasurer for the Penn State chapter of the Biomedical Engineering Society. I help coordinate volunteer activities and professional development workshops for our members, including an American Red Cross blood drive, an H1N1 vaccination clinic, and a graduate school portfolio workshop. Our chapter has also established “Lunch and Learn” activities, which give bioengineering students an opportunity to learn about the research and career paths of department and adjunct faculty members. Additionally, I am presently working with other bioengineering students and faculty to establish a chapter of AEMB, the National Biomedical Engineering Honor Society. Outreach. I have also participated in numerous educational outreach activities. Currently, I work with a program called Engineering Ambassadors to encourage high school students, particularly women, to pursue an education in science or engineering. In this program, I travel to biology, chemistry, and physics courses at high schools across Pennsylvania and give presentations about research emerging at the intersection of science and engineering. I was one of two students to pilot this program over the past summer and present the results of the pilot to administrators in the College of Engineering. Positive feedback from our presentation has allowed the program to expand to include 15 female presenters, with plans for visits to schools across the state throughout the year. While I was taking a physiological systems bioengineering course, I observed that a considerable amount of time was spent reviewing basic physiology because it was being presented from the engineering perspective. This academic year, I am re-designing pre- laboratory PowerPoint presentations used in the physiology course that I teach in order to approach the basic physiology from this engineering perspective. These modified materials may be incorporated into a bioengineering-section of the course in the future. I have also contributed to an engineering communications course by recording a class presentation as an educational web resource for students in a wide variety of engineering disciplines. Additionally, the slides I developed for the class are now used by course instructors as strong examples. As I near the end of my undergraduate career, I look forward to working toward a Ph.D. and a career as an academic. The NSF fellowship would provide me with the resources to achieve this goal. 2 In the spring of 2008, I began undergraduate research in the laboratories of Dr. Erwin Vogler and Dr. Andrea Mastro, who collaboratively study breast cancer metastasis to bone using an engineered three-dimensional bone tissue developed in a compartmentalized bioreactor. This research offered me the opportunity to learn fundamentals of osteobiology and osteopathology that would translate to studies of orthopedic biomaterials later in my career. I was fascinated by the simplicity of the bioreactor design, the sophistication of the tissue it supported and the potential for this in vitro model to enhance the investigation of metastatic bone disease. I have since dedicated myself to this research by working full-time throughout the summers of 2008 and 2009 as well as throughout the past two academic years to complete my honors thesis. Background. The American Cancer Society estimates that one in eight women will be diagnosed with breast cancer in the course of their lifetime. Breast cancer is the second most commonly diagnosed cancer in women in the United States, accounting for nearly 27% of all female cancers in 2009.1 Breast cancer frequently metastasizes to bone (Figure 1), with bone metastases occurring in approximately 70% of patients with advanced breast cancer.2 Breast cancer disrupts normal bone remodeling by suppressing the function of osteoblasts (bone- forming cells) and increasing the activity of osteoclasts (bone-resorbing cells), resulting in increased bone degradation coupled with the release of factors from the bone matrix that support tumor growth. Current Figure 1. therapies target this “vicious cycle” between tumor cells and the skeleton.3 Bisphosphonates are a family of drugs that bind avidly to mineralized bone where they are internalized by osteoclasts and signal osteoclast destruction, resulting in reduced bone degradation. They are often administered alongside chemotherapy drugs (taxanes).4 The interaction of bisphosphonates with osteoclasts is well understood, but little is known about effects of bisphosphonates on osteoblasts. Undergraduate Thesis Research. My research involves the study of these drugs with considerations for their effects on osteoblasts. The purpose of this study is to characterize the effect of a bisphosphonate (zoledronic acid) and a taxane (docetaxel), alone and in combination, on osteoblasts in conventional tissue culture and osteoblasts challenged with metastatic breast cancer cells in the compartmentalized bioreactor. I hypothesize a combination therapy will show synergistic antitumor effects but have little effect on the integrity of osteoblast tissue. In the summer of 2008, I gained valuable skills in cell culture and maintenance by assessing the effects of these drugs on osteoblast proliferation and differentiation in conventional tissue culture. During this time, I was supported by the Ronald E. McNair Summer Undergraduate Research Program and fulfilled all program requirements, which included documenting a minimum 40 hours of research each week, writing a research paper for the Penn State McNair journal, and presenting the research at the annual McNair summer conference. Throughout the 2008-2009 Figure 2. academic year, I became skilled in developing osteoblastic tissue in the bioreactor, challenging that tissue with breast cancer cells, and monitoring cancer progression with confocal microscopy (Figure 2A). I participated in the McNair program again the following summer and was additionally supported by the Undergraduate Summer Discovery Grant awarded through the university. I enhanced my previous data by optimizing cell viability assays and expanded my 1 experiments to the bioreactor, evaluating the effects of zoledronic acid on this model of breast cancer in bone. I discovered a single dose of zoledronic acid at 0.50 µM administered three days after co-culture reduced the formation of breast cancer colonies and disrupted cancer cell alignment with osteoblast tissue (Figure 2B). This finding suggests that, in addition to effects on osteoclasts, zoledronic acid may have a direct antitumor affect on breast cancer cells. These findings will be published in the next McNair journal and were presented at the McNair summer conference in 2009. My abstract was also accepted for poster presentations at two professional conferences, one in the field of engineering and the other in bone metastasis research. The opportunity to engage in conversations with engineers, biologists, and oncologists and receive their feedback was a rewarding experience that taught me the value of being an active member of the scientific community. This academic year, I will complete my thesis research by evaluating the combination bisphosphonate and taxane therapy in the bioreactor model. I am currently supported by the Pennsylvania Space Grant Consortium Sylvia Stein Memorial Scholarship and Federal Work Study. In addition to detailing my research project, a portion of my thesis will be devoted to evaluating the potential for the bioreactor to serve as a system for testing therapeutics. I have also begun a small research project in collaboration with Dr. George Engelmayr, a bioengineering professor, to investigate the effects of surface topography on osteoblast development in the bioreactor. We predict that microfabricated surface topographies will accelerate or enhance maturation of osteoblastic tissue in the bioreactor. While I work alongside my advisors, a graduate student, and a laboratory technician to develop my research skills, my application of these skills is original and my research is independent. Perhaps the most important thing I learned throughout this experience is that approaching a research problem from only one perspective is inefficient. I truly value my capacity to work within a highly interdisciplinary team and my ability to leverage skill sets from different fields to achieve a more holistic understanding of my research problem. Additionally, the experience of working with the bioreactor has taught me that successful strategies in research do not necessarily have to be complex – success can sometimes be achieved through simplicity. Publications and Presentations 1. Miller, Genevieve (forthcoming). “Bisphosphonate effects on breast cancer colonization of three-dimensional osteoblastic tissue.” The Penn State University McNair Scholars Journal. 2. Miller, Genevieve. 2008. “Bisphosphonate and taxane effects on osteoblast proliferation and differentiation.” The Penn State University McNair Scholars Journal. 3. “Bisphosphonate Effects on Breast Cancer Colonization of Three-Dimensional Osteoblastic Tissue.” Poster presented at The IX International Meeting on Cancer-Induced Bone Disease, October 29-30, 2009 and the Biomedical Engineering Society Annual Fall Scientific Meeting, October 8, 2009. 4. “The effect of a bisphosphonate, zoledronic acid, on osteoblasts in vitro.” Presentation offered at Penn State McNair Summer Research Conference, July 18, 2009. 5. “Bisphosphonate and taxane effects on osteoblast proliferation and differentiation.” Presentation offered at Penn State McNair Summer Research Conference, July 19, 2008. --------------------------------------------------------------------------------------------------------------------- 1American Cancer Society, American Cancer 3Steeg P; Theodorescu D, Nature Clinical Society, Inc. 2009, 1-72. Practice Oncology 2008, 5(4): 206-19. 2Coleman RE, Cancer 1997, 80(S8): 1588-94. 4Green J, The Oncologist 2004,9(suppl 4):3-13. 2 Engineered Bone Tissue for the Study of Mechanotransduction in Osteocytes Numerous studies have shown that micro-gravity conditions induce bone loss during long-term inhabitation of space; decreased mechanical stimulation of bone results in uncoupled bone remodeling favoring bone resorption.1 Efforts to prevent or treat micro-gravity induced bone loss typically involve resistive exercise to impart mechanical loads on the skeleton. While some studies indicate that exercise may reduce the uncoupling of bone remodeling, exercise has yet to effectively reduce bone loss.1 Thus, an understanding of the mechanisms by which external loads are sensed by bone cells and translated to cellular signals may elucidate targets for pharmaceutical interventions. Proposed Research. The objective this study is to investigate osteocyte mechanobiology using principles of tissue engineering and to ascertain mechanisms by which osteocytes respond to mechanical loading. I propose to study the effects of small-magnitude, high-frequency fluid shear stresses on the maturation of osteocytes cultured in a compartmentalized bioreactor. Background. Osteocytes are stellate cells abundant in cortical bone that develop from osteoblasts that become entrapped in secreted extracellular matrix (Figure 1). Osteocytes occupy lacunar spaces within bone matrix and are thought to influence their surroundings via cytoplasmic intercellular processes that extend through microscopic canals in the bone called canaliculi.2 It is widely accepted that osteocytes are responsible for sensing mechanical signals (mechanosensation), but the mechanisms by which those mechanical signals Figure 1. are translated to the cells (mechanotransduction) are still unclear.2 Osteocytes are difficult to study in situ because they are embedded in mineralized matrix and relatively inaccessible. Similarly, demineralization of bone matrix for in vitro studies fails to reproduce tissue with an appropriate three-dimensional architecture and a complex lacunocanalicular network.2 Principles of tissue engineering may lend improvements to developing biologically relevant bone models for mechanobiology studies.3 My undergraduate thesis research involved engineering osteoblastic tissue in a compartmentalized bioreactor (Figure 2) and challenging that tissue with breast cancer cells.4 Through this experience, I gained an appreciation for the ability of engineered tissues to serve as in vitro models of osteopathologies and developed an interest in applying Figure 2. these models for studying osteobiology. Krishnan et al. recently reported that pre-osteoblasts proliferate in the bioreactor to form a three-dimensional mineralizing osteoblastic tissue that progressively develops an osteocytic phenotype.5 Cobblestone-shaped osteoblasts mature into stellate cells embedded in a dense matrix with numerous intercellular processes.5 Therefore, the bioreactor provides access to the continuum of osteocyte development in vitro, including the aforementioned lacunocanalicular network. Previous studies have indicated that external forces are transduced to osteocytes by means of “fluid shifts” within the lacunocanalicular network.2 This theory (the poroelastic model) accounts for the effects of low-amplitude, high-frequency forces associated with the majority of daily human activities, such as sitting, standing and changing posture, as well as effects of high-strain activities such as exercise.2,6 While the effects of shear stresses have been evaluated in vivo and on osteocyte-like cell lines such as MLO-Y4, effects of fluid shear on the progressive maturation of osteocytes have yet to be investigated.2 Materials and Methods. Compartmentalized bioreactors based on the principle of simultaneous growth and dialysis7 will be assembled to develop engineered bone tissue that can 1 be mechanically manipulated. Murine calvarial osteoblasts (MC3T3-E1) can be cultured in the bioreactor for up to 10 months to progressively develop an osteocyte-like phenotype.5 Cells can be microscopically monitored throughout the culture interval using confocal microscopy. The bioreactor can be inserted into a mechanical device (Figure 3) that produces small- magnitude deformations in the membrane encasing the device. This external force will generate a pressure gradient within the cell-culture medium, and incorporation of a flow-loop for each compartment will allow the fluid to shift within the lacunocanalicular network. This system, once fully developed, can be used to investigate proposed mechanotransduction pathways and biochemical markers, such as ATP, prostaglandin E and nitric oxide.8 I 2 am particularly interested in quantifying the expression of the protein sclerostin, an osteocyte-specific product of the SOST gene that inhibits Figure 3. bone formation.8 Murine models have shown decreased sclerostin expression following loading, suggesting another potential mechanism for mechanotransduction in osteocytes.9 Agenda. In the first year of my fellowship tenure, I will design and construct the bioreactor and characterize the development of osteocytes in the culture system. Throughout the second year, I will determine a protocol for mechanically loading the bone tissue and investigate proposed mechanotransduction pathways. I will focus on developing quantitative methods of measuring signaling pathways in the bioreactor system. The final year will be devoted to analyzing effects of mechanical stimulation on osteocyte biomarkers and inducing mechanisms of osteocyte mechanotransduction in bone. Broader Impact. The aim of this project is to gain an understanding of the complex mechanisms underlying mechanotransduction in bone. Discovering these mechanisms could contribute to the development of therapies to treat micro-gravity induced bone loss experienced by astronauts during space travel. This research also has implications for the treatment of other bone diseases, such as osteoporosis and Paget’s disease. The collaborative nature of the Harvard- MIT Division of Health Sciences and Technology will allow for numerous perspectives to contribute to this work and my doctoral studies. Furthermore, participation in the Bioastronautics Training Program would guide my research and enhance my understanding of space medicine. I have actively participated in educational outreach activities during my undergraduate studies and will continue to do so throughout my graduate career by mentoring underrepresented students as a McNair alumna and encouraging high school students to pursue careers in science and engineering. As I continue to pursue my aspiration of becoming a professor, I look forward to probing new research questions and sharing my knowledge and experiences with students from a variety of disciplines and backgrounds. --------------------------------------------------------------------------------------------------------------------- 1LeBlanc A.D. et al., J Musculoskelet 6Hwang S.J. et al., Clin Orthop Relat Res Neuronal Interact 2007, 7(1): 33-37. 2009, 467: 1083-1091. 2Allori A.C. et al., Tissue Engineering: Part B 7Rose G.G., Int. Rev. Exp. Pathol. 1966, 5: 2008, 14(3): 285-293. 111-178. 3Freed L.E. et al., Tissue Engineering 2006, 8Riddle R.C.; Donahue H.J., Journal of 12(12): 3285-3305. Orthopaedic Research February 2009, 143- 4Dhurjati R. et al., Tissue Engineering 2006, 149. 12(11): 3045-3054. 9Robling A.G. et al., The Journal of 5Krishnan V. et al., In Vitro Cell.Dev.Biol – Biological Chemistry 2008, 283(9): 5866- Animal 2009, Accepted 3 Sept. 2009. 5875. 2 Rating Sheet 1: Overall Assessment of Intellectual Merit: Very Good Explanation to the applicant: Student's proposed plan of research was well organized and well presented. She documented several awards and scholarships as well as a few publications and presentations Noteworthy that she researched accomplishments of Geo. Washington carver and then developed lab curriculum for students in general chemistry. Also noted that she mentored students in bioeng. and became TA for physiology lab excellent letters of recommendation Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: active in Biomed Engin. Society as treasurer and helped coordinate volunteer outreach activities Sought to motivate more young women to go into engineering and other STEM fields by creating 20-minute presentation served as Engineering Ambassador and presented data from pilot program to administrators which brought about funding for the group. Applicant demonstrated leadership through this initiative and spearheaded visits to her former high school Rating Sheet 2: Overall Assessment of Intellectual Merit: Excellent Explanation to the applicant: excellent academic performance excellent leadership and outreach activities Honors college student Chemsitry intern-includes research, volunteer and teaching components a number of publications and presentations - both oral and poster excellent research experiences innovative course/curriculum slides, example approach basic physiology from engineering perspective and other similar ones good communication skills, very good write-up with suitable figures for previous and proposed research studies Excellent choice of institution excellent references Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: excellent prior accomplishments excellent community outreach-volunteer for Red cross blood drive, H1N1 vaccination clinic, engineering ambassador-to encourge high school students especially girls, to pursue carrers in science/engineering very good future plan excellent individual experience very good inegration research and education very good leadership experiences and skills and got the potential to be a future leader, considering first generation, low-income college student involved in Women In Eng society activiites Designed an innovative lab course which was used for honors degree Rating Sheet 3: Overall Assessment of Intellectual Merit: Excellent Explanation to the applicant: The student is strong academically, with an extremely strong supportive package. She has maintained an excellent standard of work while simultaneous working at many jobs. She does not merely fulfill the obligations of her jobs, but goes above all expectations to develop new processes, teaching materials, etc. The applicant has substantial research experience leading to a poster presentation. The work is with tissue engineering, but not in order to produce replacement tissues. Instead, the engineered tissue is used to study the role of osteoblasts, osteocytes, and pharmaceuticals in diminishing the potential of breast cancer cells to metastasize. Her previous experience will translate directly to her proposed research project. Again, she will use tissue culture as a means of studying bone formation and loss. In this case, she will look at the mechanisms of bone loss and the interaction of exercise with bone loss. The proposal mentions that the mechanisms of cell signaling will be investigated. I would have liked to see those outlined more clearly, and I highly recommend that the student have a clear protocol before beginning the task of building the bioreactor. Overall Assessment of Broader Impacts: Excellent Explanation to the applicant: As previously mentioned, the student has held several jobs where she has not only completed her duties, but has worked to improves processes, teaching materials, etc. I have the opinion that she has done this not to further her own ambitions, but out of extreme conscienciousness. In addition to many hours of work, a rigorous academic schedule, and participation in research, the student has also contributed as a leader in the BMES club and is helping to charter an AEMB Honor Society. She has shown her capability to disseminate scientific results through preparation of a manuscript, presentation of a poster, and being a ""poster child"" of oral presentation skills. I believe that she has the ability to make a meaningful impact in the field of biomedical engineering."	0
"Davidow, Juliet Y. I am fond of the saying that research is “me-search”, meaning that some researchers tend to focus their work on issues that are personal or that they are curious about understanding in some way about themselves. I am one of these researchers. My broad interest in how adolescents learn and apply learned information stems from my own learning experiences as well as my professional and research experiences working with different developmental populations. Here I will highlight aspects of my educational experience that have contributed to my interest specifically in learning mechanisms, and in the period of adolescence. Interest in Adolescence: As an undergraduate student, I wanted to gain direct experience to explore my interest in clinical psychology. To this end, I volunteered at St. Luke’s-Roosevelt Hospital in New York City, working with an ethnically and socioeconomically diverse group of patients in the Alternative Adolescent Day Program (AADP) and the Comprehensive Addictions Program for Adolescents (CAPA). This particular set of outpatient programs appealed to me because in addition to therapy and treatment, the patients took classes towards their General Equivalency Diploma or High School Diploma in a curriculum taught by New York Board of Education teachers. I tutored the patients in their class work, helped them complete their homework, as well as facilitated social milieu therapy projects, such as a school newspaper. The patients reminded me of the Montessori philosophy; Adolescence is a period characterized by changes and challenges, and these patients additionally struggled with psychopathology and addiction, and despite all these obstacles were committed to their schooling. Interest in Learning: My own passion for learning developed in the Montessori school I attended from 3 to 13 years of age. Two major principles of the Montessori philosophy are that when children are motivated to learn they will engage in self-directed study and that every child has different optimal strategies for learning. Peer-to-peer-learning is also pivotal, and the Montessori classroom is built around encouraging it, with 3 grade-level groups in a single classroom (e.g. 4th, 5th and 6th graders). This structure allows students to tap multiple resources, from books and other materials to more senior students in the classroom, before turning to the teachers (of which there are usually 2-3 per classroom). This unique early experience of seeking academic help from my classmates, and in turn sharing my knowledge with my peers, instilled in me the desire to learn so that I could in turn teach another. Because of my strong research skills and academic record, I have had the rare opportunity to work under an extraordinary group of mentors. Specifically, Drs. Dima Amso, BJ Casey, and Daphna Shohamy have deeply impressed me as rigorous, female scientists, whose research directly impacts the lives of children and adults, in healthy and other populations. I aspire to be a strong female role model in science, a researcher whose work can improve educational practices across the country, and an educator directly bringing my love of learning to students, and towards these ends I have begun my PhD work at Columbia University. I believe my proposed research holds the potential to expand into a career dedicated to understanding the brain-and-behavior interactions necessary for successful navigation through adolescence. Broader impact in the community: I am beginning to share the knowledge I have gained from my mentors with a rising generation of hopeful scientists. In early December, I will be presenting and discussing my research at a ‘Careers in Science’ fair for middle and high school students, organized by the Women in Science at Columbia (WISC) and the Center for Environmental Research and Conservation (CERC). This event will be attended by a socioeconomically diverse group of students from all over New York City, with a focus on encouraging young women in particular to consider careers in the sciences. This emphasis is of particular importance to me, because I am woman working in the sciences, and my greatest role models and influences to follow this course have been women. Page 1 of 2(cid:1) (cid:1) 2010 NSF Graduate Research Fellowship: Personal Statement Davidow, Juliet Y. A key goal of mine, expressed in my personal statement from my previous submission, was to begin sharing my work directly with adolescents in the community. To this end, I will be making presentations at local high schools, starting with The Calhoun School in Manhattan, speaking in classrooms about what cognitive neuroscience has taught us about changes in behavior and the developing brain during adolescence. I have been mentoring a high school student, Michael, and supervising him through a research project in the lab that will culminate in his applications to the Intel National Science Talent Search, the New York Science and Engineering Fair, and other award programs. To this end I have been teaching Michael the foundations of research methods, including how to survey the existing literature, statistical tools, and the ethical principles researchers must abide by. Michael is interested in how socioeconomic differences in adolescent populations might relate to the learning and generalization mechanisms outlined in my research proposal and the possible role generalization plays in racial stereotyping. Mentoring Michael has been rewarding, and I was excited at the opportunity to interact one-on-one with a student at his level on such a long-term project, given how uncommon it is to have students before the undergraduate level work hands-on on research projects. Graduate Training: My first year of graduate study has been punctuated with acknowledgements of my accomplishments. I was the recipient of the Leo Rubinstein Endowed Fellowship. I applied and was awarded a travel grant from the Kavli Institute for Brain Sciences to present results at the recently past Annual Meeting of the Society for Neuroscience [1]. This study, conducted in my first year of graduate work, tested 74 healthy adults behaviorally, and tested a subset of 53 of these adults in a functional Magnetic Resonance Imaging scan. This sample of adults will be used as a comparison group in the cross-sectional, developmental study that I propose in this application. And perhaps most notably, I was recognized with an Honorable Mention on my first submission of my NSF GRFP application. I was pleased and excited to see the enthusiastic responses from the raters of my essays, and found their feedback to be extremely helpful. The points raised by my raters indentified clear areas where my proposal could be improved, and by directly addressing these issues, I believe my current, revised proposal is stronger. This past semester, I have been taking a course in Methods of Teaching towards improving my skills as a Teaching Assistant (TA) for Undergraduate courses at Columbia University. I will have the opportunity to directly apply my new skills this spring, when I TA for the introductory Developmental Psychology course, a class I am eager to work on, given my strong background in developmental research and methods. With the support of Dr. Shohamy and her expertise in neural networks for learning, I have an ideal mentor for exploring the development of learning processes over the course of adolescence. With cutting edge behavioral and brain imaging facilities at Columbia, I have access to the testing resources my research requires. Situated in Upper Manhattan, I am surrounded by people with diverse ethnic and socioeconomic backgrounds, providing me access to a representative population of potential research participants that will allow me to generalize my results for the benefit of many children. These elements combined with my research background give me confidence that I will be an outstanding student and researcher whose contributions will impact a number of fields within psychology and beyond. Reference: 1. Davidow, J., Kahn, I., & Shohamy, D. (November, 2010). The ability to learn and generalize knowledge is related to intrinsic interactions between multiple memory systems during rest. SfN Annual Meeting, California, USA. Page 2 of 2(cid:1) (cid:1) 2010 NSF Graduate Research Fellowship: Research Proposal Juliet Y. Davidow Learning in adolescence: Neural mechanisms and implications for education. Keywords: Learning, education, development, adolescence, striatum, hippocampus, fMRI. Introduction. There have been significant advances in understanding cognitive and neural mechanisms for learning in adults, with important implications for everyday learning situations [e.g. 1, 2, 3]. However, far less is known about the cognitive and brain mechanisms underlying learning during the critical developmental stage of adolescence. This research program aims to bridge this gap. The proposed research will use behavioral studies combined with functional Magnetic Resonance Imaging (fMRI) in healthy adolescents to delineate the cognitive and neural development of specialized learning systems over the course of adolescence (10-18 years of age). The results will provide a novel understanding of learning mechanisms during adolescence and will inform educational approaches towards learning in the classroom. Background & Rationale. Converging evidence from research in animals, patients and healthy adults demonstrates that there are different kinds of learning that depend on distinct neural systems [2, 3]. Gradual, feedback-based learning of stimulus-response associations – often referred to as “habit” or “incremental” learning - depends on the striatum [4, 5]. This system is sensitive to feedback and results in knowledge that is relatively inflexible and specific to the context in which the learning took place [6]. A distinct and independent “declarative” or “episodic” system supports rapid learning of events and depends on the hippocampus [1, 6]. In contrast to the striatum, the hippocampus is thought to form knowledge that is flexible and easily generalized to novel situations and contexts [1, 6]. It has been shown that individuals who show more hippocampal activation during learning are more likely to integrate and generalize what they learned [5]. Together, these findings indicate that in the healthy adult brain, there is a balance between activity in the striatum and the hippocampus during learning, with consequences for how learned knowledge is used. A key open question is how developmental changes in learning mechanisms relate to changes in the striatum and the hippocampus during adolescence. Some insight into this question comes from longitudinal research of structural brain changes, which revealed differential developmental trajectories in the striatum and the hippocampus [7]. The volume of the striatum peaks around pubertal onset and then diminishes, with reductions continuing into early adulthood [7, 8]. Far less is known about the developmental trajectory of the hippocampus [7, 9]. However, existing results support relatively early maturation of the hippocampus, consistent with adult-level episodic learning performance in children [10]. Together, these findings suggest that the striatum and the hippocampus may develop at different rates during adolescence, with important implications for learning. Given the structural differences in the brain’s learning systems over the course of adolescence, what is the trajectory of different forms of learning (‘incremental’ vs. ‘episodic’)? To address questions about interactions between behavior and brain mechanisms, I will characterize the relationship between learning processes and brain development in human adolescents using a paradigm demonstrated in adults to be a sensitive index of both striatal-dependent incremental learning and hippocampal- dependent episodic learning. These studies will investigate the neural mechanisms in adolescent learning so that educational programs can be informed directly by cognitive neuroscience data from adolescents, instead of making inferences from studies of adults. Experimental methods. I propose three studies that make use of an incremental learning and generalization paradigm (‘acquired equivalence’)[e.g. 5], shown to selectively probe different forms of learning and their neural substrates in adults. Although ideally these studies would be conducted longitudinally, the constraints of my time as a graduate student make cross-sectional and between-subject studies most feasible. The acquired equivalence (AE) task consists of two phases. First, subjects engage in feedback-based learning where they learn to associate a series of (cid:1) Page 1 of 2 2010 NSF Graduate Research Fellowship: Research Proposal Juliet Y. Davidow faces with different objects. This feedback-based learning phase has been shown to depend on the striatum. In the second phase, subjects are tested on their memory for the previously learned associations. Critically, they are also asked to generalize what they learned to novel stimulus combinations. This ability to generalize depends on the hippocampus [e.g. 5]. Figure. ‘Acquired Equivalence’ task structure. a. In Phase 1, subjects learn a set of independent but overlapping associations. Learning is incremental and based on feedback. b. In Phase 2, subjects are tested for their (1) memory on learned pairings (Learned) and for (2) inferential transfer to novel combinations they have not previously seen (Generalized). This paradigm is particularly well suited for studying special populations, including younger subjects. Specifically, it allows as many learning trials as is necessary before the second test phase. This unique feature will permit us to independently explore adolescent learning trajectories for both incremental learning and for episodic generalization.(cid:1)Using fMRI, I will record brain activity at learning and test phases of the task while measuring subjects’ behavioral responses. By using converging methods, I can simultaneously characterize incremental learning from feedback and flexible generalization of learned information in adolescents while exploring individual differences in reliance on neural networks implicated in adult performance. Study 1 will use the AE task with feedback during the learning phase to ask how adolescents learn to associate items and generalize to novel pairings, and delineate the underlying neural networks. Study 2 will use an AE task that compares active learning-with-feedback to passive learning-by-observation, with no feedback. This manipulation has been shown to modulate both hippocampal and striatal contributions [4] and will allow me to compare memory and generalization from active versus passive learning, qualify what kinds of learning are most effective for different kinds of outcomes, and probe the role of active engagement for successful learning. Study 3 will use the AE task and will manipulate whether the feedback involves monetary rewards or not, to explore the impact of heightened reward sensitivity during adolescence [8] on learning behaviors. Together, these studies will build a foundation for understanding (I) learning and generalization behaviors, (II) differential maturation of brain function, and (III) behavior-and-brain interactions over the course of adolescent development. This foundation will be critical in bridging the research fields of neuroscience, development, and education, and also holds potential to impact society with implications for special education, early learning intervention, and psychopathology. Future studies will probe the role of individual differences on learning behaviors, including influences from social environment and genetic phenotypes. Due to the dearth of learning research in healthy adolescents, my proposal promises to put forward novel information about behavior and behavior-and-brain interactions to bear on several fields. Understanding typical adolescent development of learning mechanisms and how knowledge is applied in novel circumstances is imperative to adolescents’ success in the classroom and when faced with challenging decisions. References 1. Eichenbaum, H.E. & Cohen, N.J. From “Conditioning to Conscious Recollection”, 2001. 2. Gabrieli, J.D. Annual Review of Psychology, 1998. 49: 87-115. 3. Knowlton, B.J., Mangels, J.A., & Squire, L.R. Science,1996. 273: p. 1399-1402. 4. Shohamy, D., et al. Brain, 2004. 127: p. 851-859. 5. Shohamy, D., & Wagner, A.D. Neuron, 2008. 60: p. 378-389. 6. Shohamy, D., et al. Journal of Cognitive Neuroscience, 2008. 21(9): p. 1820-1832. 7. Giedd, J.N. Journal of Adolescent Health, 2008. 42: p. 335-343. 8. Somerville, L.H., Jones, R.M., & Casey, B.J. Brain and Cognition, 2010. 72(1): p. 124-133. 9. Gogtay, N., et al. Hippocampus, 2006. 16: p. 664-672. 10. Ofen, N., et al. Nature Neuroscience, 2007. 10(9): p. 1198-1205. (cid:1) Page 2 of 2 Ratings Sheet 1 of 3 Score for Davidow, Juliet Intellectual Merit Criterion Overall Assessment of Intellectual Merit Excellent Explanation to Applicant Thecandidatehasanexcellentrecordofresearchexperience. Thisisevidencedbyhertwoco-authoredpapers and multiple presentations. Also, her references speak to her independent contributions to the research programs. She has presented a very well crafted research proposal. The justification for her hypotheses is compelling and the methods are sound. It will be an interesting and potentially important study. Broader Impacts Criterion Overall Assessment of Broader Impacts Excellent Explanation to Applicant I appreciate the candidate’s recognition of the important role that scientists have in improving science education in their communities. She shows ample evidence of giving back to her community through her mentorship of young people and participation in community projects. She will be an engaged scientist who will continue to help others in her broad community. 2011 NSF GRFP Applicant: Juliet Davidow Applicant ID: 1000100934 Ratings Sheet 2 of 3 Score for Davidow, Juliet Intellectual Merit Criterion Overall Assessment of Intellectual Merit Excellent Explanation to Applicant The applicant demonstrates are strong record of research training through their undergraduate and post- baccalaureate employment including an excellent record of publication/presentation. She has demonstrated both independence and insight. Her broad training is noted. Broader Impacts Criterion Overall Assessment of Broader Impacts Excellent Explanation to Applicant The applicant demonstrates a sustained commitment to the ideals of this criterion through her volunteer, mentoring,andserviceactivities. TheseincludeparticipationinWISCevents,speakingatlocalhighschools about cognitive neuroscience, and mentoring high school students. Her application would be even stronger with concrete plans that address this criterion in the near and far terms. 2011 NSF GRFP Applicant: Juliet Davidow Applicant ID: 1000100934 Ratings Sheet 3 of 3 Score for Davidow, Juliet Intellectual Merit Criterion Overall Assessment of Intellectual Merit Very Good Explanation to Applicant The applicant has considerable experience working in various laboratories and has mastered a large set of research skills that have prepared her to function as a research scientist. She has two journal articles, one article submitted for publication, and several national conference presentations. Thus she understands the process of scientific research from beginning to end. Her proposed line of research is based on a broad appreciation of the literature and is thoughtfully and carefullyplanned. Therewereafewdetailslackingintheresearchplanthatwereimportantinunderstanding the practicality of the proposed effort. Broader Impacts Criterion Overall Assessment of Broader Impacts Excellent Explanation to Applicant Evidencefortheapplicant’senthusiasmforeducationanddiversityissuescanbefoundinherpastvolunteer work, her efforts to mentor students in research, and in her choice of research topics. Theproposedresearch,ifsuccessful,hassomebroadapplicationsthatwereclearlyindicatedbytheapplicant. 2011 NSF GRFP Applicant: Juliet Davidow Applicant ID: 1000100934 file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt Logo-asee-print NSF GRFP RESULTS *Juliet Davidow* | logout </ratingsheets/logout> Rating Sheets * *2010 Rating Sheet 1* * 2010 Rating Sheet 2 </ratingsheets/ratingsheets/84303> * 2010 Rating Sheet 3 </ratingsheets/ratingsheets/84304> Overall Assessment of Intellectual Merit: Very Good Explanation to the applicant: Excellent academic preparation with impressive GRE scores. Sought out undergraduate and postgraduate research experiences and has continued to produce at the graduate level, with one published paper and nine conference papers (one first-authored). her letters of recommendation were very strong and attested to her fine research ability. Her proposed research plan was well-articulated and based on past published research. Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: The broader impacts of her research and her role as a scientist were not articulated directly. She has an excellent record of community service and has demonstrated an understanding of the scientist's roe in communicating findings to the larger community. Articulated an interest in montoring and teaching, OTHER YEARS * *2010* OUTSIDE LINKS * Fastlane <https://www.fastlane.nsf.gov/grfp/> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt (1 of 2) [9/21/11 12:19:11 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt * NSF GRFP Program Announcement <http://www.nsf.gov/grfp> National Science Foundation Graduate Research Fellowship Program Operations Center Administered by: American Society for Engineering Education (ASEE) 1818 N Street NW, Suite 600 Washington, DC 20036 | 866-NSF-GRFP, 866-673-4737 (toll-free from the US and Canada) or 202-331-3542 (international) | info@nsfgrfp.org <mailto:info@nsfgrfp.org> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt (2 of 2) [9/21/11 12:19:11 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt Logo-asee-print NSF GRFP RESULTS *Juliet Davidow* | logout </ratingsheets/logout> Rating Sheets * 2010 Rating Sheet 1 </ratingsheets/ratingsheets/84302> * *2010 Rating Sheet 2* * 2010 Rating Sheet 3 </ratingsheets/ratingsheets/84304> Overall Assessment of Intellectual Merit: Excellent Explanation to the applicant: The applicant has extensive research experience and a strong academic background. Her proposed research plan is well laid out. She has good communication skills, works well with others and independently. Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: The applicant expresses a desire to be a role model and participate in outreach programs to deliver science to those who might not otherwise be exposed to it. Her research will enhance scientific and technical understanding. Improved understanding of how adolescents learn will do doubt benefit society. The applicant could more explicitly address the broader impact criteria, especially regarding encouraging diversity. OTHER YEARS * *2010* OUTSIDE LINKS * Fastlane <https://www.fastlane.nsf.gov/grfp/> * NSF GRFP Program Announcement <http://www.nsf.gov/grfp> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt (1 of 2) [9/21/11 12:19:55 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt National Science Foundation Graduate Research Fellowship Program Operations Center Administered by: American Society for Engineering Education (ASEE) 1818 N Street NW, Suite 600 Washington, DC 20036 | 866-NSF-GRFP, 866-673-4737 (toll-free from the US and Canada) or 202-331-3542 (international) | info@nsfgrfp.org <mailto:info@nsfgrfp.org> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt (2 of 2) [9/21/11 12:19:55 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt Logo-asee-print NSF GRFP RESULTS *Juliet Davidow* | logout </ratingsheets/logout> Rating Sheets * 2010 Rating Sheet 1 </ratingsheets/ratingsheets/84302> * 2010 Rating Sheet 2 </ratingsheets/ratingsheets/84303> * *2010 Rating Sheet 3* Overall Assessment of Intellectual Merit: Very Good Explanation to the applicant: Strengths: The applicant has participated in undergraduate research and as a lab manager, followed by work as an RA, and is now enrolled in graduate study. From these experiences, the applicant learned basic psychological research tools, including behavioral study design and analysis, measuring eye movements, ERPs and fMRI. All of these skills will be highly valued in the applicant's ongoing training. The applicant's research efforts have resulted in an honors thesis, one local invited talk, one first authored international conference presentation, authorship on numerous additional conference abstracts, and authorship on two manuscripts in various stages of publication. Weaknesses: The research plan lacks some important information. For example, is there any behavioral evidence that the incremental and episodic learning systems have different developmental trajectories? This is important to establish prior to moving the experiment into the scanner, which isn't going to tell us much about mechanisms, just more about ""where"". I'm assuming that this is a between-subjects design, and not a longitudinal study, because a graduate student career is likely too short to measure longitudinally. However, this is not specified. The applicant does not have any first-authored peer-reviewed manuscipts at this time. Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: The applicant has served both as a peer counselor and as a volunteer file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt (1 of 2) [9/21/11 12:20:15 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt tutor working with teenagers with drug addiction difficulties. These experiences motivated the applicant to pursue studies on the typical adolescent development. The applicant intends to promote neuroscience in K-12 classrooms with the help of SFN's online materials. OTHER YEARS * *2010* OUTSIDE LINKS * Fastlane <https://www.fastlane.nsf.gov/grfp/> * NSF GRFP Program Announcement <http://www.nsf.gov/grfp> National Science Foundation Graduate Research Fellowship Program Operations Center Administered by: American Society for Engineering Education (ASEE) 1818 N Street NW, Suite 600 Washington, DC 20036 | 866-NSF-GRFP, 866-673-4737 (toll-free from the US and Canada) or 202-331-3542 (international) | info@nsfgrfp.org <mailto:info@nsfgrfp.org> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt (2 of 2) [9/21/11 12:20:15 AM]"	0
Gravitational Waves in Astrometry and Pulsar Timing Arrays Abstract: Gravitational waves are fluctuations in the fabric of spacetime and, in general ​​ relativity, they can be characterized by two polarizations states (commonly called the tensor E and B modes). A background of such gravitational waves can be detected through astrometric measurements and pulsar timing arrays, and the angular power spectra of these measurements for each polarization state is known. However, the capability of these experiments for localizing gravitational wave sources has not been studied in depth—this is of great importance since it will facilitate comparison with electromagnetic data. In addition, it is often assumed that a background of gravitational waves will conserve parity; however, this is not guaranteed since sources such as supermassive black hole mergers are known to produce circularly polarized gravitational waves that may give rise to chiral backgrounds. Thus, my intention is to determine how the locations of gravitational wave sources may be determined from astrometry and pulsar timing array data, as well as calculate the parity-breaking power spectra. Proposal: The advent of gravitational wave astronomy has provided a new window onto the ​​ universe for astrophysicists. Traditional barriers that are opaque to electromagnetic radiation can be penetrated by gravitational waves, which thus provide a novel way to research black holes, early universe processes, and other phenomena that are otherwise difficult to study using light. While it is possible to detect gravitational waves by direct observation, the existing interferometers of LIGO and Virgo are not suitable for studying the polarization content of gravitational waves, since at least five detectors are required to isolate the polarization states. Pulsar timing arrays and astrometric measurements may provide better constraints on the polarization content of gravitational waves. Light interacting with a gravitational wave will have its trajectory altered and this causes the apparent position of light sources to be deflected and the arrival of light pulses to be delayed. Thus, it is possible to study a stochastic background of gravitational waves using astrometric missions such as Gaia by observing how the positions of a field of stars changes over time, as well as by timing millisecond pulsars against each other to measure correlated signatures in pulse arrival times. The correlation functions and angular power spectra of such experiments have been calculated for each polarization state of gravitational waves by a variety of methods [1-3]. The power spectra will provide a starting point for analyzing a gravitational-wave background detected by astrometry or pulsar timing arrays; however, clearly there is more to a study of gravitational wave backgrounds than just the polarization content. I propose to investigate two key aspects of these gravitational wave experiments: their ability to localize sources and their ability to probe chiral gravitational waves. Neither issue has been studied in great depth—however, both projects will enhance experiments by facilitating comparison with non-gravitational-wave data and challenging the common, but possibly false, assumption that a background of gravitational waves will preserve parity. First of all, after detecting a background of gravitational wave, is it possible to localize sources of the signals? In other words, will the signal exhibit any preferred directions? I propose to study whether a gravitational wave background will exhibit such an asymmetry using bipolar spherical harmonics [4, 5]. The coefficients of the bipolar spherical harmonics can be constructed from the same quantities used to calculate the power spectra and should be able to show whether the signal will have a dipole asymmetry. In addition, it is sometimes assumed that this stochastic background of gravitational waves will conserve parity. Then the energy densities 1 Wenzer Qin NSF GRFP: Research Plan in the left and right-handed circularly polarized states should be the same, which implies that cross-correlation of tensor-E and B modes and the redshift-B mode cross-correlation will be zero. However, sources such as supermassive black hole binaries are expected to to emit circularly polarized gravitational waves, and in this case the detected background would be chiral and the cross-correlations would not vanish. Therefore, I propose to calculate the parity-breaking power spectra, i.e. the E-B and redshift-B cross-correlations for a chiral gravitational wave background. Since gravitational wave astronomy is a relatively new field, the results of these research projects will be useful for the development of pulsar timing arrays and astrometric surveys as gravitational wave detectors. The ability to locate gravitational wave sources from these experiments will be extremely useful, as it will allow us to compare measurements with electromagnetic data from the same region and therefore study gravitational-wave-emitting phenomena in greater depth. In addition, as stated earlier, for a chiral gravitational wave background, the EB and redshift-B cross-correlations will not vanish. Normally, if these functions were expect to be zero, then they could be used as null tests for systematic errors in these experiments; however, since we do expect some astronomical sources to emit chiral gravitational waves, subtracting these cross-correlations out of the data may result in the loss of real and important physics. Therefore, an expectation of what the cross-correlations should look like beforehand will be important for analyzing the data from astrometry and pulsar timing arrays, and that is what this project seeks to establish. I would like to carry out this research at the California Institute of Technology, since this topic of research overlaps significantly with the work of Sterl Phinney and Yanbei Chen. I would also be happy to work with Daniel Holz at the University of Chicago or Franz Pretorius at Princeton University, since their research also focuses heavily on gravitational waves. The NSF ​ fellowship will support me in my goals by allowing me to begin research in graduate school as soon as possible and spend as much time as I can developing the skills and knowledge necessary for my future career in physics research. Timeline: Years 1-2: Calculate the bipolar spherical harmonic coefficients from the redshifts and ​ position deflection of stars. Use these to determine whether the gravitational wave signal will exhibit a preferred direction/dipole. Publish results on how to use astrometry and pulsar timing arrays to locate gravitational wave sources. Year 3: Use the total-angular-momentum formalism to calculate the parity-breaking ​ cross-correlation power spectra. Publish the functions and their derivations. [1] L. G. Book and E. E. Flanagan, “Astrometric Effects of a Stochastic Gravitational Wave Background,” Phys. Rev. D 83, 024024 (2011) [arXiv:1009.4192 [astro-ph.CO]]. [2] L. O'Beirne and N. J. Cornish, “Constraining the Polarization Content of Gravitational Waves with Astrometry,” Phys. Rev. D 98, no. 2, 024020 (2018) [arXiv:1804.03146 [gr-qc]]. [3] D. P. Mihaylov, C. J. Moore, J. R. Gair, A. Lasenby and G. Gilmore, “Astrometric Effects of Gravitational Wave Backgrounds with non-Einsteinian Polarizations,” Phys. Rev. D 97, no. 12, 124058 (2018) [arXiv:1804.00660 [gr-qc]]. [4] A. Hajian and T. Souradeep, “Measuring statistical isotropy of the CMB anisotropy,” Astrophys. J. 597, L5 (2003) [astro-ph/0308001]. [5] L. G. Book, M. Kamionkowski and T. Souradeep, “Odd-Parity Bipolar Spherical Harmonics,” Phys. Rev. D 85, 023010 (2012) [arXiv:1109.2910 [astro-ph.CO]]. 2	0
Introduction:In recent decades, reports of re-emergingand novel phytopathogens have increased dramatically in forests.1These pathogens threatenforest health and pose serious risks to plant biodiversity. Studies indicate climate change (e.g. warmer temperatures, wetter growing seasons) has accelerated forest decline within the United States by expanding plant pathogen ranges.2The effectsof climate change have heightened and extended the infection period for pathogens, making trees more vulnerable to outbreaks of less aggressive phytopathogens.3Plant pathogens in the family Nectriaceae, including undescribed species, have been indirectly linked to climate change.4In addition, these changes in temperature are known to increase sporulation and virulence of fungal pathogens, as cold periods would ordinarily reduce the populations of pathogens by arresting their growth. Trees at high elevations including red spruce, Fraser magnolia, yellow birch, striped maple and mountain ash are buffered from many pathogenic fungi due to persistent cold temperatures in their habitat; however, warmer winters have increased the risk for biological invasion of these species.5 Although significant progress has been made regarding the taxonomy of these nectriaceous fungi, additional data are needed to clarify species boundaries and their evolutionary relationships. Likewise, these fungi pose risks that must be fully assessed by more robust studies on host range and pathogenicity. Long studied forests are now experiencing epidemics of these emergent plant pathogens (EPPs).6While beech bark disease andFusarium-associated diseasesare highly-studied pathosystems, native, often less virulent, nectriaceous fungi are becoming more abundant. My objective is to protect Appalachian forests by 1) drawing connections between abiotic stressors and the prevalence of nectriaceous fungal pathogens, 2) identifying these fungal pathogens, and 3) assessing the effects of temperature on the aggressiveness of these pathogens.I propose to expound upon currentclimate change models and forest pest predictions, particularly for nectriaceous fungi on these high-elevation tree hosts. Aim 1:Assessing abiotic stressors contributing tothe emergence of fungal pathogens. Using the ‘Climate by Forest’ tool provided by the U.S. Forest Service, I will review changes in forest health and climate projections for forests throughout the Appalachian region. The ‘Climate by Forest’ tool is a novel interface in which users can select regions of national forests and look at various climate trends and variables.7From these projections, forests thatare predicted to have significantly warmer winters will be selected for sampling. High-elevation tree species will be selected based on their known range across high elevation zones throughout Appalachia. Symptomatic tissues and conspicuous fungal fruiting bodies from these species in our sample sites will be surveyed, collected and processed for culture- and DNA-based studies. I will also compile and analyze temperature data across the Appalachian region to quantify and evaluate the abiotic stress these forests have endured.Hypothesis: Fungal pathogens and abiotic stresses are synergizing declines in native tree species. Aim 2: Characterizing known and unknown nectriaceousfungal diversity in Appalachian forests. Hypothesis: Despite known diversity of Nectriaceous fungal pathogens across Appalachian forests, many remain undetected.Molecular tools must be usedin combination with existing morphological methods to capture the full diversity of phytopathogenic fungi. Sanger sequencing will be used for pure cultures of my suspected fungal pathogens recovered from trees sampled inAim 1. Targeted loci (LSU and EF1-α) are widely used for phylogenetic inference in Nectriaceae.8Illumina amplicon sequencing—a multiplexed PCR approach—will be used to identify asymptomatic fungi that may also be contributing to forest decline. Aim 3: Determining the interaction between individualnectriaceous fungi and targeted tree species in central Appalachia along a temperature gradient. Hypothesis:Nectriaceousfungi have contributed differentially to tree disease epidemics, which are driven in part by changes in temperature throughout our Appalachian forests. To simulate globalwarming and to assess the effects of temperature on fungal growth and pathogenicity, temperature-dependent pathogenicity assays will be conducted. In climate-controlled growth chambers, saplings of the aforementioned five species will be grown at varying temperature ranges (0°C, 10°C, 20°C, and 30°C). Trees will be inoculated with select nectriaceous fungi discovered inAim 2and tree healthwill be monitored at 6-MPI and 12-MPI. Any notable canker formation will be measured at the end of the inoculation period. To fulfill Koch’s postulates, trees will also be sampled to see if the original inoculum can be recovered. This experiment will quantify the aggressiveness of suspected nectriaceous pathogens at varying temperatures, allowing me to infer the impact novel phytopathogens will have on our forests as global warming worsens. Intellectual Merit:As a member of Dr. Kasson’s forestpathology lab since mid-2020, I have direct experience identifying and characterizing diverse fungal phytopathogens in West Virginia. During my efforts to delimit the species boundaries ofNeonectriamagnoliae,I have already identified numerous nectriaceous fungi on a wide range of hosts, including some novel species we are describing. Putative pathogens in the Nectriaceae are abundant and appear to be emerging as the result of the unique overlap of biotic and abiotic factors. I have and will continue to collaborate with forest pathologists throughout the Appalachian region to compare DNA sequences, host range, and pathogenicity of these fungi under supervision of Dr. Matt Kasson (WVU), my current advisor and forest pathology expert. With his support and the support of my forest pathology colleagues, I will not only unravel the contributions of these fungal phytopathogens to the decline of our forests, but provide novel information to our Appalachian communities, our foresters, and our scientists.WestVirginia is the “black box” of biodiversity: severely understudied with much to discover. There are an estimated 150 different tree species in WV: more than anywhere else in North America. My contributions to forest pathology will revolutionize how we look at and care for our trees in Appalachia. Broader Impacts:West Virginia (WV), my homestate, is suffering from educational neglect. It has the lowest number of Bachelor’s degrees (20.6%) per capita of any state, and we have the second lowest per capita graduate degrees.9In Appalachia—and WV specifically—we deal with low science literacy and a fear of science. Our region once powered the country with coal, but the profits of this mono-cultural economy were not reinvested in our communities. As coal has faded away and global temperatures rise, there is much skepticism and fear around climate action in West Virginia. West Virginians need scientists from our own communities trained to identify the challenges we face, develop solutions to these problems and share them with our own.Thesescientists, like myself, will have a broad and immensely positive impact on my community. As science outreach has been an integral part of my undergraduate career, I will curate my own environmental science outreach program to invest my project in our Appalachian youth. Through my Appalachian Children’s Environmental Research program(ACER), I will recruit fellow young scientists to bring presentations to K-12 students and also offer field trips to teach a variety of environmental concepts. In this novel program, I will provide resources (i.e., guides, lessons, and accessible information) on climate change, forest health, mycology, and other topics pertaining to preserving the integrity of our ecosystems.My lifetimeof learning has prepared me to fulfill this next stage of my career, one that will ensure West Virginians are not left behind. References:[1] Karunarathna et al. 2021.Front CellInfect Microbiol. [2] Kasson et al. 2009.Mycologia. [3] Dukes et al. 2008.Can J For Res. [4] Pavlov etal. 2020.Sci Rep. [5] Pauchard et al. 2015.Biol Invasions. [6] Corredor-Moreno et al. 2019.New Phytologist.[7] U.S. Forest Service. 2018. Climate by Forest. [8] Stauder et al. 2020.Fungal Ecol. [9]West Virginia QuickFacts. U.S. Census Bureau.	0
Background: The vestibular system, located in the inner ear, provides sensory information regarding spatial positioning and balance, enabling coordination of movement and orientation1. Additionally, the vestibular system plays a key role in enacting compensatory eye movements in response to body movement through the vestibulo-ocular reflex (VOR)1. This system can become unilaterally impaired in the presence of vestibular schwannoma, a benign tumor that develops on the vestibulocochlear nerve connecting the sensory organs to the brain2. Due to the vestibular system’s active role in locomotion, impairment via vestibular schwannoma may lead to balance deficiency, vertigo, and oculomotor process changes2. Previous work has demonstrated that in patients with vestibular schwannoma, head movements during locomotion and gaze stability exercises are less rapid and the VOR is impaired2,3. Encouragingly, resection, or removal, of vestibular schwannoma has been demonstrated to result in improvement of kinematic parameters and other head movements, with major changes occurring within the first six weeks post-operation4. Generally, normal vestibular activity is reflected in the parieto-insular and temporo-parietal junctions; however, little is known about cortical changes that occur due to vestibular impairment, with most studies focusing on kinematic parameters and quantification of specific visual reflexes5. Therefore, there is a need to examine the impact of vestibular schwannoma on neural activity at large. Obtaining this information about the ways the human brain copes with vestibular system deficiency will elucidate both patterns of neural plasticity in response to specific sensory input alteration, and how best to approach treatment of those who are impacted by vestibular schwannoma. I propose to determine the patterns of neural activity and gaze focus in response to locomotor tasks in people with vestibular schwannoma, both pre- and post-resection. This work will be conducted across three Aims: first, a comparison of electroencephalography (EEG) and gaze-tracking patterns in healthy subjects and vestibular schwannoma patients; second, a longitudinal study of EEG and gaze patterns in vestibular schwannoma patients pre- and post-resection; finally, generation of a support vector machine (SVM), a machine-learning technique which will discriminate between gaze patterns of patients with vestibular schwannoma and healthy controls. Intellectual Merit: This project will contribute to the body of knowledge surrounding normal vestibular function, as well as provide insight into the specific pathological state inherent to vestibular schwannoma. The insight into the changes in gaze functionality will be especially important because the impact of vestibular schwannoma on overall gaze patterns is not well-understood beyond interruption to the VOR. Changes to further parameters such as saccade frequency, fixation time, and primary areas of focus during locomotion are unknown. This project will elucidate changes in those patterns. Research Plan: Aim 1: Collection of baseline patterns in vestibular schwannoma patients vs. healthy subjects Hypothesis: Vestibular schwannoma patients will display greater primary motor cortex activation than healthy control subjects during gait, representing a more effortful process due to balance impairment. In this phase, we will focus on establishing a baseline of functionality in healthy subjects and patients with vestibular schwannoma, recruited from Johns Hopkins Acoustic Neuroma Center, a specialty clinic focused on the treatment of vestibular schwannoma. Testing will consist of a modified functional gait assessment (FGA) battery. The ‘gait with eyes closed’ portion of the FGA will be excluded, due to the inability to record gaze location while eyes are closed. During the FGA, subjects will have gross brain activity recorded via a 58-channel EEG cap connected to a wearable Arduino Uno microcontroller to allow for continuous mobile data collection. Additionally, subjects will don wearable eye-tracking glasses to assess continuous gaze location. This phase will conclude with successful collection of gaze-tracking and EEG data for a matched number of healthy subjects and vestibular schwannoma patients executing the FGA tasks. Aim 2: Longitudinal study of vestibular schwannoma resection Hypothesis: Between six weeks and six months after vestibular schwannoma resection, patterns of neural activity in vestibular schwannoma patients will become more like that of healthy subjects during gait. This phase will enact a longitudinal study of patients with vestibular schwannoma pre-resection, approximately six weeks post-resection, and at least six months post-resection. This experimentation will consist of the same paradigm as Aim 1, with subjects performing the modified FGA. Additionally, during this phase, the EEG signal collected in Aim 1 and Aim 2 will be processed and analyzed. Preprocessing will consist of an independent component analysis, wherein the multivariate EEG signal for each subject will be decomposed into additive ‘components’, which combine at different weights to compose the overall EEG signal. These components will be assessed via visual inspection to remove extraneous signal, such as eye blinks and motion artifacts. Then, processed data will be examined for event-related potentials at several key points in the gait cycle. This phase will conclude with the accomplishment of two tasks: successful collection of gaze-tracking and EEG data for vestibular schwannoma patients pre- and post-resection, and analysis of differences in patterns of neural activation between healthy controls, pre-resection vestibular schwannoma patients, and post-resection vestibular schwannoma patients. Aim 3: Generation of Support Vector Machine to categorize gaze patterns Hypothesis: Individuals with vestibular schwannoma will display distinct gaze patterns, including greater gaze latency to area of interest, as compared to healthy control subjects. This phase will center around the generation and validation of a support vector machine (SVM) that will enable automatic machine classification of vestibular schwannoma patients versus healthy controls. An SVM is a supervised machine learning approach that uses a hyperplane to split groups of variables, or support vectors, into discrete classes (shown in two dimensions in Figure 1); once trained on the stereotypical values for each class, it compares new data to those clusters to classify the state of the new input6. An SVM has been used to accurately detect pathology based on gaze patterns; Figure 1: Design of an SVM 7 specifically, individuals with dyslexia versus healthy controls while reading text6. Measured parameters will include number of saccades, number of gaze fixations in key areas of interest (ground underfoot, ground ahead of stride, wall), length of gaze fixations, and latency of gaze arriving at areas of interest. Within this model, some erroneous classification is inevitable. Because the primary purpose of the model is quantifying impacts of vestibular schwannoma to the ocular system, it is preferable to bias the model towards detecting vestibular schwannoma in order to find any and all gaze-pattern disruptions. Accordingly, the SVM will be tuned to have higher sensitivity to prevent false negatives. This phase will conclude with the demonstration that the resultant SVM is able to discriminate between the gaze patterns of healthy individuals and those who suffer from vestibular schwannoma, with at least an 85% detection rate for positive cases. Alternative Approaches: An SVM can have limited efficacy if the training dataset is of insufficient size. If there is not be a significant dataset that will allow for training of the model and model accuracy is below the 85% threshold, an alternative approach may be the use of a convolutional neural network (CNN), which consists of a cluster of signal-transmitting kernels that loosely resemble neurons. A CNN has previously been used to classify gaze-tracking data based on what website a user was viewing; however, this methodology has not been extended to pathology detection8. If needed, this possibility could be explored. Facilities: This work will be conducted with Dr. Kathleen Cullen at the Cullen Laboratory at Johns Hopkins University. This laboratory has previously conducted studies of vestibular schwannoma patients using kinematic parameters and is equipped to continue this work with other methodologies. Broader Impact: While the primary purpose of this work is to learn about the unimpaired vestibular system through a study of vestibular schwannoma as a disease state there is potential for secondary application as a diagnostic measure. The diagnostic process for vestibular schwannoma is two-stage – first-round testing consists of a battery of hearing tests, and if those tests suggest the presence of a tumor, second-round testing consists of an MRI with contrast. With the rising costs of healthcare in the United States, finances can be a prohibiting factor to patients pursuing this diagnostic testing. The creation of a lower-cost intermediate diagnostic would prevent unnecessary clinic visits for final diagnostic testing for patients whose hearing loss may have a different cause. If the SVM can recognize patients with vestibular schwannoma, then an intermediate gaze-tracking diagnostic tool could be developed and used to screen patients with hearing loss to determine whether vestibular schwannoma is a likely culprit for their symptoms. References: 1. K. Cullen, Nat Rev Neurosci, 2019; 2. A. Batuecas-Caletrio et al. Laryngoscope, 2015; 3. L. Wang et al. Sci Rep, 2021; 4. O. Zobeiri et al. Sci Rep, 2020; 5. E. Nakul et al. Front Neurol, 2021; 6. L. Rello et al. W4A ’15, 2015; 7. “Support Vector Machine”, javaTpoint; 8. Y. Yin et al. ICMLA ’18, 2018.	0
projects, and written reports, as well as quizzes and exams. In large enrollment courses, instructors often use multiple-choice questions as an assessment method because of the ease and perceived objectivity in grading.1 Although multiple-choice exams are useful for providing fast feedback about student performance, the multiple-choice item format has been criticized for primarily assessing low levels of cognition.2 Biology assessments that fail to target higher-order thinking can be detrimental to the student learning process because these low-level assessments limit the development of critical reasoning and problem-solving skills, do not promote the long- term retention of course material,3 negatively affect study habits,4 and hinder scientific inquiry.5 Bloom’s taxonomy is widely used in biology education research as a tool for evaluating student performance and guiding the development of instructional strategies.6,7 Bloom’s taxonomy consists of a hierarchy of cognitive skills: remember, understand, apply, analyze, evaluate, and create.8,9 This framework can be used to categorize the cognitive levels assessed by multiple- choice and constructed-response items on biology exams. The division of biology courses into introductory and advanced courses implies that the higher-level courses provide opportunities for students to gain a greater depth of conceptual knowledge and to practice the higher-order cognitive skills that are necessary for STEM careers. Although previous research has identified that introductory biology courses primarily assess the two lowest levels of Bloom’s taxonomy,10 there are few studies that analyze if assessments in 300- and 400-level courses target the higher- order thinking that is presumed in advanced biology courses. The advantages and disadvantages of multiple-choice and constructed-response items are well-studied,1 but there is little research on the extent to which the different item formats are used when assessing content knowledge and cognitive skills in introductory and upper-level biology courses. Multiple-choice items are traditionally associated with assessing the lowest levels of Bloom’s taxonomy and constructed-response items are often thought to target higher- level thinking, but there has not been extensive research in biology courses to determine if there is evidence to support these stereotypes about item format. There is a gap in the literature regarding the frequency with which multiple-choice and constructed response items are used in introductory and upper-level biology courses to assess higher-order cognitive skills. My research will fill this gap, highlight strengths of the current methods of biology assessment, and identify the areas where assessment can be improved to better reflect the knowledge and skills that are required for success in STEM careers. Research Questions 1) Is there a difference in the cognitive levels assessed in introductory and upper-level biology courses? 2) What is the relationship between item format and cognitive level assessed on undergraduate biology exams? 3) What decisions, processes, and methods are instructors using to design undergraduate biology exams? Methods To answer these research questions, I will survey exams from biology instructors at a range of undergraduate institutions. Biology instructors will be recruited for participation in the research through professional networks such as the Ecological Research as Education Network and the Society for Advancement of Biology Education Research. The collected exam documents will be reviewed using a directed qualitative content analysis, a process in which each question on the exam will be categorized by a Bloom’s cognitive level as well as by item type. I will mentor undergraduate research assistants and teach them how to use Bloom’s taxonomy to review exam items. I will use a Cohen’s kappa analysis to determine interrater reliability for consensus of the classifications of Bloom’s level and item type between the reviewers. To determine which factors predict the Bloom’s level of exam items, I will run ordinal regressions with item type and course level as predictor variables and instructor as a random effect. In the analysis of Bloom’s levels on individual exams, I will calculate a weighted average because items designed to assess higher-order thinking may tend to have a higher point value than items assessing lower-level cognitive skills. I will conduct semi-structured interviews with instructors to clarify the decisions, processes and methods used to design biology exams. The interview protocol will consist of three sections: 1) questions about possible constraints, such as large class size, that might limit the type of assessments administered in their courses, 2) participant familiarity with Bloom’s taxonomy or other frameworks for assessing cognitive skills, and 3) goals for exam design. This research focuses on exams because this form of assessment tends to reflect the types of knowledge and skills that students are expected to master in a course, but I acknowledge that there are assessment methods other than exams. There are some limitations to Bloom’s taxonomy as a framework because of its design for broadscale application in education research. These limitations will be addressed by using the Blooming Biology Tool, which is a modification of the Bloom’s framework tailored for the analysis of questions on biology topics.7 Intellectual Merit My experiences as a high school science teacher and as an Assessment Specialist at the Educational Testing Service provided the skillset that I will use to conduct the proposed research. Previous studies have identified that introductory biology courses primarily consist of items assessing low-level cognitive skills,10 but there are few studies that have examined either the assessment methods in upper-level biology courses or the relationship between item type and cognitive level assessed on biology exams. Broader Impacts One goal for this research is to strengthen the quality of the undergraduate biology education experience through identifying areas of assessment that can be improved. Students who are administered high-level items throughout their science courses are more likely to acquire deep conceptual understanding of the course material,2 so determining where assessments can be modified to target higher levels of Bloom’s taxonomy is a step in the process of promoting intellectual development in biology students. This research also addresses the disparity between the cognitive skills assessed on introductory biology exams and the cognitive skills required for solving real-world scientific problems. Although this research will be conducted primarily on assessments from American undergraduate institutions, biology exams are not unique to the United States, and the implications of this research will have international reach. A second goal of this research is to promote the advancement of biology education research, which will be accomplished through training undergraduate students in education research methodology, involving students in the process of qualitative and quantitative data analysis, and collaborating with students to present the research at science conferences. (1) Stanger-Hall, K. F. CBE Life Sci. Educ. 2012, 11(3), 294-306. (2) Martinez, M .E. Educational Psychologist. 1999, 34(4), 207-218. (3) Jensen, J. L. et al. Educ. Psychol. Rev. 2014, 26(2), 307-329. (4) Entwistle, A.; Entwistle, N. 1992, 2(1), 1-22. (5) Momsen, J. et al. CBE Life Sci. Educ.2013, 12(2), 239-249. (6) Bissell, A. N.; Lemons, P. P. BioScience. 2006, 56(1), 66-72. (7) Crowe, A. et al. CBE Life Sci. Educ.2008, 7(4), 368-381. (8) Bloom, B. S. et al. McKay: New York, NY, 1964. (9) Anderson, L. W.; Krathwohl, D. R. Allyn & Bacon: Boston, 2001. (10) Momsen, J. et al. CBE Life Sci. Educ. 2010, 9(4), 435-440.	0
Intellectual Merit: Alternative reproductive tactics (ARTs) are phenotypically distinct reproductive strategies that achieve approximately equal fitness (different fitness peaks). As a model system for studying the evolution of variation1, ARTs of males have been extensively studied, characterized by color and/or size, morphology, behavior (i.e. territorial vs. sneaker males), etc.1. By contrast, female ARTs are poorly studied. Female ARTs occur in oviposition site selection, mating behavior, and ontogenetic shifts in female size and fecundity, but many open questions remain1, 2: What selective factors cause divergent female behavior and/or morphology? Are they driven by predator avoidance, developmental limitations, physiology, or did they evolve in other functional contexts, for example, trophic niches1? Have morphological and reproductive behavioral differences evolved as correlated responses to sexual selection, which then impact other life history aspects, such as feeding? Or does natural selection cause feeding dimorphisms that in turn shape morphological and reproductive behavioral differences1? My research will explore phenotypic variation and ecological niches as underlying mechanisms of female alternative reproductive tactics in a novel, model system. Model System: Olive ridley sea turtles (Lepidochelys olivacea) exhibit strikingly divergent female reproductive tactics (Table 1). In the same population, some nest synchronously (SYN) en masse (>10,000 individuals) on a few, distinct, beaches whereas others nest solitarily (SOL) on multiple beaches over thousands of kilometers of coastline3. L. olivacea are the only sea turtle species to exhibit these ARTs, which were not formally recognized until 2002. Virtually nothing is known about why or how the ARTs occur3. I hypothesize that these alternative reproductive tactics are a result of an ecological dimorphism. SYN nesters migrate throughout the E. Pacific and aggregate to mate offshore of SYN nesting beaches to ensure copulation3. I predict SOL nesting females are neritic foragers, allowing them to nest more frequently and find mates more often making SYN aggregations unnecessary. I will sample females at 2 SYN and 3 SOL study site (6 if logistics permit). Table 1: Known characteristics of L. olivacea divergent reproductive tactics Characteristic Synchronous nesters (SYN) Solitary nesters (SOL) Inter-nesting period4 28 days 14 days Nesting phenology3 Rainy season All year Site fidelity 4 High Low Female body & clutch size3 Larger Smaller Eco-morphology AIM 1 AIM 1 Spatially explicit foraging ecology3 Nomadic, pelagic; AIM 2 AIM 2 AIM 1: DEFINE THE MORPHOMETRICS OF SOL AND SYN NESTING L. OLIVACEA. Morphological differences are common attributes of ARTs1. There is some evidence that SYN are larger than SOL nesters3 but basic morphology of these divergent ARTs is unknown. Using morphometric tools I will test my hypothesis that there are significant differences in size, shell depth, shell shape and flipper morphology between the two tactics. Morphological differences relating to foraging behavior are known in other sea turtle species5-7. Ecological dimorphisms have been shown in three populations of Caretta caretta6, 7 where small females forage in pelagic habitats and larger in neritic habitats. In Chelonia mydas, a pelagic population has larger flippers than a neritic one5. Methods: I have defined 10 flipper landmarks related to underlying skeletal and muscle structure. These landmarks and standard sea turtle body measurements8 (i.e. shell width & length, body depth & mass) will be quantified. I will use principal components analysis to test for morphological differences, and if found, to evaluate which attributes drive the variation. I estimated from a power analysis9 (F-test, p =.05, 10% effect size) that a sample of 100 females per study site (N=500) will provide a power of 89% to detect a difference. AIM 2: DEFINE THE FORAGING ECOLOGY OF SYN AND SOL NESTING L. OLIVACEA. Stable nitrogen (δ15N) and carbon (δ13C) isotope ratios, coupled with satellite telemetry, have proven to be effective tools for defining sea turtle ecological dimorphisms in 3 of the 6 other species7, 10, 11. I will utilize these tools to test my hypothesis that SYN nesters are nomadic, pelagic (open ocean) foragers with no localized foraging ground, whereas SOL nesters are neritic foragers with distinct neritic foraging grounds. Methods: Skin and dorsal shell samples will be taken to provide recent (skin) and multi-year (shell) foraging histories12. Samples will be taken during early, mid and late nesting season to account for migrations from various foraging grounds and will be collected, prepared and analyzed using established methods12. The power analysis demonstrated that a sample size of 35 turtles per sample period, per site, for skin and shell tissue (105 per site, total N=524) is sufficient. To examine spatially explicit foraging ecology I will attach satellite tags to randomly assigned females sampled for stable isotopes (10 at each study site, total N=50). Implementing robust state space modeling, I will analyze the data using established protocols13. Sampling from multiple sites and using spatial statistical analyses will account for the possibility of pseudoreplication (spatial autocorrelation in this system). This is the first detailed morphological analysis of L. olivacea ARTs and the first examination of ecological niches as an underlying mechanism driving them. Both aims are feasible; the methods have been successful in other sea turtle studies, I have tested them in the field and I have support of international collaborators. My results will contribute to a meta- analysis creating a stable isotope landscape for the E. Pacific Ocean, headed by a NSF GRF. I am organizing the first L. olivacea working group to address the unknown life history traits, which will have important management applications for this vulnerable species. My field season includes fall semester and at least two are needed. This fellowship is crucial in allowing me to be decoupled from campus and will greatly increase my capacity to do fieldwork. Broader Impacts: Communicating my research is an important part of my career path and professional development. Using social media I share my research and discuss science issues with scientists and lay people. Working with Texas Sea Grant I am developing STEM educational materials, using charismatic sea turtles as flagship species to promote watershed education in K-12 classrooms. I will develop a network of graduate students across Texas to speak with classes about their adventures in pursuit of higher STEM education. I will assemble and train undergraduates (including those in the Texas A&M, NSF- funded, Louis Stokes Alliance for Minority Participation program), Costa Rican community members and personnel from NGOs and national parks to assist in my research. Participants will receive a hands-on opportunity to learn about experimental design, fieldwork, data analyses and ethics of working with animals all while engaging in cultural exchange. I will continue to disseminate my work to the scientific community via presentations and peer-reviewed papers. This fellowship is key in allowing my work to impact the evolutionary understanding of ARTs, life history of an understudied species and a wide nonscientific audience through education and collaboration. References: 1Oliveria et al. 2008 Alt Repro Tactics.2Henson & Warner 1997. Annu Rev Ecol Syst 28:571-92.3Plotkin 2007. Biol and Conserv of Ridley Sea Turtles.4Kalb 1999. Ph.D. Diss. 5Balazs et al. 1997. Proc Ann Sea Turtle Symp.6Hawkes et al. 2006. Curr Biol 16, 990-5.7Hatase et al. 2002. Mar Ecol-Prog Ser 233:273- 281.8Wyneken. 2001 The Anatomy of Sea Turtles.9Cohn1988. Stat Power Analysis for the Behav Sci.10Hatase et al. 2006. Oecologia 149:52-64. 11Caut et al. 2008. PLoS One e1845. 12Reich & Seminoff 2010. Proc Ann Sea Turtle Symp. 13Block et al. 2011. Nature doi:10.103/nature10092.	0
Keywords: Chlamydomonas reinhardtii, flagella assembly, kinase, length regulation Introduction: Recent discoveries concerning cilia assembly suggest complex signaling pathways play a prominent role in cilia length regulation and function.1 Far less is known about the about the kinases that regulate these pathways. The proposed research will attempt to uncover the signaling pathways responsible for growth and regulation by establishing which kinases and mechanisms are responsible for the regulation of cilia length. Background and Rationale: Cilia and flagella are found on almost every cell in the human body and consist of microtubules that extend from the cell surface. Cilia are typically divided into two types, primary and motile, which both sense extracellular signals. Primary cilia, found on the majority of cells in the human body are immobile. Motile cilia, found on the majority of epithelial cell surfaces, create wave-like patterns to propagate fluid. As flagella and motile cilia have identical structures, the words cilia and flagella are used interchangeably. The process of assembling cilia, ciliogenesis, is highly regulated as the centrioles that nucleate cilia are also required for cell division. The mechanisms regulating ciliogenesis, including initiation, assembly and resorption, are poorly understood. Learning more about these mechanisms will facilitate the study and treatment of diseases involving ciliary dysfunction. Flagella of the unicellular green alga Chlamydomonas reihardtii are essentially identical to cilia of vertebrate cells and provide an excellent model to study ciliogenesis. Chemical studies in Chlamydomonas have demonstrated length-regulating roles for G-protein coupled receptors2. Similarly, flagella assemble in a length-dependent manner, with rapid early assembly and very slow assembly as they approach their steady-state length. The rate of disassembly is length independent and the length at which assembly and disassembly are in equilibrium is Figure 1. Preliminary kinase inhibitors with known as the balance point3. To identify kinase increased rate of assembly during regeneration. pathways that affect flagellar assembly, we performed a small-molecule screen using a kinase inhibitor library. Preliminary data show inhibiting Protein Kinase A and G causes an increased rate of flagellar assembly during regeneration over of 2 hours as compared to wild type. Also, inhibiting Protein Kinase C causes a slower rate of assembly during regeneration at 1 hour as compared to wild type (Figs. 1,2). We hypothesize these inhibitors target regulators that control the switch from rapid to slow assembly rates. Reversing this switch has the potential to rescue defects caused by slow or impaired assembly. Aim 1: Validate Targets and Phenotypes with Novel Inhibitors and Activators: To confirm phenotypes and identify the kinases responsible for the observed phenotypes, we will use different inhibitors of the same targets. In contrast to the kinase inhibitors, the activation of these kinases should show us the opposite effects, confirming the targets and phenotypes previously identified (Figs. 1,2). Following pH shock to induce flagellar loss and regrowth, we will treat Chlamydomonas cells (CC125) with inhibitors and activators of protein kinase A, G, C and Figure 2. Preliminary kinase inhibitors tyrosine kinases (Table 1). Flagella will be imaged using with decreased rate of assembly during regeneration. automated phase contrast microscopy and flagellar length Table 1. Inhibitors and Activators of measurements will be performed using ImageJ software. With proposed targets to be used in Aim 1 these experiments, we expect to confirm the data seen in Figures 1 and 2 while helping us to further confirm these kinases as regulators of the switch from rapid to slow assembly. Aim 2: Identify Regulatory Pathways for Flagellar Assembly: As many of the kinases identified in the preliminary screen have both cytoplasmic and nuclear downstream targets, we will identify which subset of targets are responsible for the flagellar assembly phenotype. To discriminate between the targets, we will use inhibitors from the preliminary screen simultaneously with cyclohexamide, a protein synthesis inhibitor that will prevent gene expression in downstream transcription factors (Table 2). Next, we will perform an epistasis experiment by inhibiting or activating a preliminary target as well as a potential downstream targets to see if they are in the same pathway. The process of pH shock and flagellar length measurement will be followed according to the steps described in Aim 1. These experiments will narrow down the pathway components regulating the switch from rapid to slower assembly rates. Aim 3: Determine the Role of Identified Kinases in Trafficking of Flagellar Cargo: We will use total internal reflection fluorescence (TIRF) microscopy to Table 2. Cytoplasmic and determine the role of kinases in trafficking of flagellar cargo by Nuclear compounds to be used assessing the preliminary targets’ effect on transportation of tagged for experiments in Aim 2. proteins in regenerating flagella. We will treat the cells with the kinase inhibitors during flagellar regeneration and use TIRF imaging to compare the amount of cargo traveling from the base to the tip of the regenerating flagella by quantifying fluorescence intensity of tagged cargo.4,5 Results from this visualization and quantification of tagged cargo will identify the mechanism with which identified pathways regulate flagellar assembly. Intellectual Merit/Broader Impacts: My familiarity with the culture conditions and flagellar phenotyping of the model organism Chlamydomonas reinhardtii will facilitate the proposed experiments. I will gain the necessary skills to perform TIRF microscopy through future mentoring from Dr. Avasthi. The initial microscopy work in the outlined project, allows Rockhust University undergraduate students participating in research at the University of Kansas Medical Center to be trained on microscopy. The findings from these experiments impact the science community through the identification of fundamental principles of ciliary regulation. Society is influenced by these findings as they will provide the foundation for the treatment of diseases of ciliary dysfunction. Results will be shared in relevant conferences, preprints and publications. Also, the proposed experiments and results, will be shared with undergraduate students at Rockhurst University with the intention of using relevant basic science research to engage future students. This will capture their attention and spark their interest for research opportunities at the University of Kansas Medical Center. Support from the NSF through the Graduate Fellowship Research Program will promote my success as a future scientist by facilitating research during my graduate career, but will also benefit society by providing a more approachable path to science careers for women. [1] Nachury,Maxence V.(2014) Philosophical Transactions of the Royal Society B: Biological Sciences 369.1650 [2] Avasthi, Prachee et al. (2012). ACS Chemical Biology 7.5 [3] Marshall, Wallace F. et al. 2005.Molecular Biology of the Cell 16.1 [4] Engel, Benjamin D. (2009) Method Cell Biol.93 [5] Avasthi, Prachee et al. (2014).Curr Biol. 24.	0
been political debated, although empirical work suggests that low-skill immigration has a small negative effect on average British wages. However, individual-level migration data suggests that Eastern European migrants tend to be highly educated and highly skilled [1], motivating my proposed investigation of the effects of the emigration of high-human capital individuals from Eastern Europe following the collapse of communism in the nineties. In particular, what are the macroeconomic impacts of a shock to the distribution of human capital as a result of immigration? Empirical work suggests that there are large differences in output per worker across countries [2], and that differences in total factor productivity (TFP) account for the bulk of these differences in output [3]. The mass emigration of high human capital workers decreases aggregate productivity directly through the decrease in labor productivity. Research on the emigration of academic Jewish scientists in Nazi Germany [4, 5] shows that there are persistent negative impacts on emigrants’ departments’ performances because the replacements are of lower intellectual caliber and positive effects on receiving departments’ patenting. This micro work validates macro-theoretical research on shocks to human capital. One aspect of Schumpeterian growth theory is that innovations are the result of firms’ investment decisions based on expectations of future profits. Previous work has made the connection between national knowledge stock and innovation at the scientific frontier. My work bridges the connection between negative shocks to population and aggregate human capital with innovation in a technology- follower setting. I incorporate the firm’s trade-off between investment in R&D and production as a function of the distributional shift in human capital, specifically firms’ decisions to invest in R&D as a result of Eastern European emigration in the 1990s. Methods: I will construct a general equilibrium overlapping generations (OLG) model of a small open economy populated by heterogeneous agents that vary in their stock of human capital and age, and then apply this framework to estimate the impact of the migration-induced productivity shock on Bulgaria’s economic growth. Using the structural model, I am able to analyze the welfare effects on heterogeneous individuals through equilibrium values of the model. Human capital accumulation is an endogenous process, meaning that agents choose their optimal level of human capital. The agents allocate their time between labor, leisure, and human capital accumulation in each period. Firms split labor and capital inputs into production and research, where the research production function is governed by a Poisson process. Intuitively, investment in research yields innovations or tangible improvements in technology at random and fairly rare points in time, and this understanding of knowledge production closely fits within a Poisson process. The parameter that governs this Poisson process is endogenously determined by the stock of human capital in the labor force available to the firm and the existing stock of knowledge. In other words, within a discrete period of the model, the discovery of one innovation does not have any bearing on the discovery of a second innovation (memoryless property of the Poisson distribution), but between periods the total innovations impact the rate of knowledge production. The firm faces a tradeoff between production for profit in a given period and investment in research for a potential payoff in a future period. This decision involves risk. Firms also consider distortionary taxes and expectations about economic conditions in their decision. One metric of innovation is patent applications. Applications are a reasonable proxy for immigration because patent applications are not dependent on a government agency’s determination of the worthiness of an innovation for being patented. In a revealed preference framework, applying for a patent indicates that the firm believes it has produced innovation worthy of patenting. This belief informs their decision to invest in research and development. The preparation of a patent application is not without effort; therefore, applying for a patent represents the firm beliefs I am interested in capturing. I will calibrate my structural OLG model to Bulgarian data to conduct a policy experiment on emigration. Using individual-level data, the Mincerian earnings function for the returns to schooling and experience can be calculated [6], which gives direct estimates of the parameters governing the human capital accumulation decision by the agent. The weight of consumption in utility is adjusted to capture aggregate hours worked in the data (available from OECD), and this parameter characterizes the labor- leisure decision of the agents. The parameters governing the relationship between the rate of knowledge production and human capital are estimated in the literature. These parameters combined with the first- order conditions of the structural model determine the firm’s behavior. Because I can fluently read and speak Bulgarian and my personal experiences, I am uniquely able to obtain the necessary innovation data to calibrate the rest of model. Otherwise, standard data and computational resources are sufficient. Because the migration decision depends on a number of unobservable characteristics, it is implausible to include this decision in a structural model. Information about the types of people who migrate, include their ages, educational levels, and work experience are observable in individual-level surveys conducted in Bulgaria. Accordingly, emigration is captured by changes in the relative sizes of human capital and age cohorts as well as level changes in population. Additionally, as a result of bottlenecks related to work visas, documentation, and language barriers, this mass migration does not happen immediately after the collapse of communism. This delay in the timing of the migration shock allows for a few years post-collapse to establish the baseline macroeconomic trends. Thus, there are two balanced growth paths of interest. One growth path is the case where no migration occurs, and the model is calibrated to match the known pre-shock periods. The other growth path includes the migration shock, and data on immigrants is used to adjust the measures of ages and human capital types. The comparison between the second balanced growth path and the data measures the performance and predictive capacity of the model, while the comparison between the second growth path and the first (the simulated counterfactual) represents the effect of the migration shock. A successful model will replicate targeted moments of the data. Broader Impacts: Understanding the effects of high-skilled emigration is crucial to reconciling why the economies of Eastern and Western Europe have not converged since the 1990s and designing policies that encourage talent to remain in Eastern Europe. The model I propose captures another dynamic of migration through shifts in the age distribution. Empirical work indicates that migrants tend to be younger, and a large migration event such as in Eastern Europe following 1991 may shift the age distribution upward. Previous work has analyzed the macroeconomic implications of an aging population as well as changes to human capital separately, but the interaction between the two remains an open question. My model and associated calibration would partially fill this gap in the literature. Furthermore, the mass migration of young people negatively shocks the population growth rate. Because the growth rate of the population is determined by the proportion of young people, the one- period shock to the population growth rate propagates through R generations, where R is the cutoff between young and old. Combining this with the level decline in population as a result of the shock, my model also captures the absolute population declines observed in some Eastern European countries. Because population declines independent of wars, pandemics, and the like are rarely observed, there is little empirical work on the macroeconomic implications of declining populations. My model and computational approach incorporates these effects and could isolate them via simulating an age-biased, human capital-neutral migration event, where migrants are young but equally skilled as the population. Several countries in Asia, Western Europe, and Latin America are expected to experience population decline in the near future, and the mass-migration events of the nineties started this process earlier in Eastern Europe than the rest of the world. Accordingly, my findings on the effects of population decline are of significant interest and would contribute to an open and deeply relevant question. Moreover, the relationship between high-skilled emigration and innovation is not limited to Eastern Europe. There are several US states, including my home state, Kansas, with net high school and college graduates leaving, resulting in a negative shock to human capital. During the Covid-19 pandemic, several Midwestern cities, including Topeka, Kansas, adopted policies that gave workers a lump-sum transfer of money in exchange for moving and living in the city for at least a year. A natural extension of my work is to evaluate the prevalence and demographics of uptakers of such policies, and then analyze my model with shocks to the human capital distribution as observed in the data. References: [1] IMF report “Emigration and its Economic Impact on Eastern Europe” [2] McGrattan and Schmitz, (1998) Federal Reserve Bank of Minneapolis [3] Hall and Jones (1999) QJE [4] Moesa et al (2014) American Economic Review [5] Waldinger (2016) Review of Economics and Statistics [6] Patrinos (2016) IZA World of Labor	0
Core-collapse supernovae (CCSNe) are the spectacular explosions that accompany the deaths of massive stars. CCSNe have been the subject of ongoing research for decades but the explosion mechanism is still not fully understood. Proper treatment of the CCSN problem requires all areas of physics; in particular, general relativistic gravity, complex neutrino transport, turbulent magnetohydrodynamics, and nuclear physics. CCSNe are the primary engines of galactic chemical evolution; many of the elements heavier than H and He are synthesized here, those necessary to life in particular. Furthermore, a complete view of CCSNe is necessary for understanding the compact objects that arise from core-collapse, such as the binary neutron stars and stellar mass black holes that have been detected by Advanced LIGO and Advanced Virgo[1]. Intellectual Merit As current high-fidelity CCSN simulations lack the ability to properly predict electro- magnetic (EM) emission, I propose to investigate the explosion mechanism driving CCSNe by further developing the current CCSN simulation capabilities to pro- vide accurate EM predictions. While yielding far more insights into the processes at work in the CCSN, it will also allow for comparison with observational astronomy. This collaboration of full multimessenger signals is a yet untapped resource that could give new insights into the explosion mechanism. The proposed project is broken down into three stages: (i) upgrading our nuclear physics, (ii) getting 1D LC and EM information, and (iii) going to multiple dimensions. My background in stellar astrophysics and computational methods makes this project a natural next step. Upgrading our nuclear physics. In the hot interior of massive stars, material is said to be in nuclear statistical equilibrium (NSE), meaning that forward and backward reactions are balanced such that elemental abundances are given by relatively simple statistical relations. Current high-fidelity CCSN simulations assume that NSE is satisfied throughout the entire star. While this is a good approximation in the interior regions of interest most pertinent to the explosion, it breaks down quickly at large radii so that only the central regions can be accurately modelled. I will work to transition the equation of state (EOS) to the non-NSE regime in the FLASH code. This will allow for whole-star simulations and more accurate nucleosynthesis calculations. Inclusion of the outer regions of the star will also allow for modelling of the neutrino-driven wind – a supersonic outflow of stellar material powered by neutrinos emitted from the core of the star. This wind is proposed as a possible site of heavy element nucleosynthesis. We can then begin to meaningfully study the full nucleosynthetic signatures of CCSNe. Getting 1D LC and EM information. The ultimate goal of the study of the CCSN explosion mechanism is the ability to make predictions and understand observations. Armed with a more realistic EOS and accurate nucleosynthesis, I will study the EM signals emitted during a CCSN. To achieve this, I will use a new model for driving 1D explosions that includes the crucial effects of turbulence and convection and map simulation data into SuperNu[2], a multi-D Monte Carlo radiation transport code, to produce the EM signals. This is imperative as, to date, we have only one observation of a CCSN that includes any signals other than electromagnetic. The FLASH code is already capable of handling the gravitational wave (GW) and neutrino emission, so this extension will provide complete predictions of the multimessenger signals. With this, we can begin to make direct connections between physical conditions of the explosion and what is observed. An understanding of how, for example, uncertain nuclear physics affects the electromagnetic signals is crucial to the success of the CCSN problem. This will allow us to compare our findings to observations and, for the first time, connect observations of CCNSe with details of the progenitor stars. Going to multiple dimensions. The final goal of this project is the ability to run fully 3D CCSN simulations that for the first time include a proper treatment of the non-NSE EOS and EM information. This project will push the frontiers of current high fidelity 3D simulations and greatly enhance their explanatory and predictive powers. Due to the extreme computational resources required of these simulations, the simulations would begin during years 3-4 using computing allocations available to Dr. Sean Couch, the PI of the Michigan State University (MSU) research group. At this stage of the project, I will have the ability to study the full range of multimessenger signals from CCSNe including EM, GW, and neutrino signals in addition to the nucleosynthetic signatures of the explosion. With all of this in hand, we can make accurate and meaningful predictions of how the various physics that go into the CCSN impact the explosion, and how that in turn affects the observations. Broader Impact The work presented here will result in the advancement of our understanding of the CCSN explosion mechanism, galactic evolution, and ultimately, the origin of the elements including those that comprise life. The results of this work will promote constructive collabo- ration between theoretical stellar astrophysicists and observational astronomers. EM data produced through this project can be compared against observational data as a benchmarking tool and may also be used by observers to inform future studies. This project aligns with the goals of the DOE’s Scientific Discovery through Advanced Computing (SciDAC) initiative, as well as the National Strategic Computing Initiative, in the push to exascale computing. MSU houses the Joint Institute for Nuclear Astrophysics, National Superconducting Cyclotron Laboratory, Facility for Rare Isotope Beams, and a new De- partment of Computational Mathematics, Science and Engineering and as such it the ideal campus for this interdisciplinary work. In an effort to reach first generation students, I will create a chapter of Ask A Scientist at MSU utilizing my connections with the national organization. To best leverage the available resources, I plan to create collaborations with existing programs at MSU such as the 4-H Michigan Extension, MSU Science Fair, and first generation student mentor program. I will travel to rural Michigan schools to show first generation and low income students that a college education and career in science are options for them. Through this chapter of Ask A Scientist, these communities can continue to benefit after the conclusion of my graduate studies. Astronomy has amazing potential to transform both lives and communities1 and the NSF GRFP would give me the resources necessary to begin my career while using my research as a tool for change. [1] Abbott, B. P., Abbott, R., Abbott, T. D., et al. 2016, Phys. Rev. Lett., 116, 061102 [2] Wollaeger, R. T., van Rossum, D. R., Graziani, C., et al. 2013, The Astrophysical Journal Supplement Series, 209, 36 1 https://www.nature.com/collections/xtxtmqfrgf	0
Background. Frontal association cortex is a brain region critical for flexible action selection in mammals. In humans and other primates, this area includes the supplementary motor complex (SMC), which has been shown necessary for suppression of inappropriate motor plans--an extreme case being 'alien hand syndrome,' in which SMC damage leads to complex and seemingly purposeful hand movements in the absence of voluntary control.1 Premotor cortex (M2) is thought to be the rodent homolog of SMC. Lesions to M2 selectively disrupt goal-directed behavior2, and in particular the ability to adapt choice of action to changes in reward values.3 Despite the evidence for its causal role in behavioral flexibility, the mechanisms by which neural networks in frontal association cortex realize this vital function remain a mystery. Preliminary Findings. To investigate the neural substrates for flexible action selection in M2, I will use a combination of rodent behavior, in vivo imaging of neural ensemble activity, and local silencing methods. During my first year of PhD research, I developed a two-choice decision task for mice that requires flexible switching between different action selection strategies in order to obtain optimal reward. The first phase of the task requires a cue-guided strategy in which the animal must discriminate between two distinct auditory stimuli that each indicate the availability of water reward at a corresponding lick port on either side of the animal's mouth. In the second phase, an action-guided strategy is necessary: reward is contingent upon licking a specific port regardless of the cue presented. The two phases are alternated many times within a single session without any sensory cue to indicate the phase-switch. Thus, the task requires flexible adaptation of action selection strategy to changing contingencies between cue, action, and reward. In a first step toward understanding the neural basis of behavioral flexibility, I have begun imaging ensembles of M2 neurons at cellular resolution as mice perform this task (Fig. A&B). To measure changes in neural activity, the genetically encoded calcium indicator GCaMP6 was first transduced into layers 2/3 of M2 using an adeno-associated virus (AAV). A cranial window was implanted above M2, and fluorescence traces were recorded using 2-photon microscopy. By aligning the traces recorded from individual neurons to specific events in a trial (e.g., cue or response onset), I have correlated animal behavior with activity changes in single neurons, as well as with the aggregate activity of all neurons in the recorded ensemble. Preliminary analyses have produced two key findings that motivate detailed investigation: (1) A large proportion (>25%) of individual neurons recorded in M2 were choice-selective, i.e., these neurons showed significant differences in activity depending on which port was chosen (Fig. C). Interestingly, the fraction of choice-selective neurons increased following reward delivery, and peaked ~2-sec post-reward (Fig. D). (2) A greater proportion of neurons were choice-selective when the task required an action-guided versus a cue- guided response. Furthermore, a large fraction of neurons showed pre-response choice selectivity when an action-guided strategy was utilized. Aim 1: Test causal role of M2 ensemble dynamics in behavioral flexibility. Our preliminary findings indicate delayed choice-selective activity in M2 that may serve as a feedback signal important for reinforcement of the current action-selection strategy. In order to test this hypothesis, A C E x a m p le C e ll D B G r o u p D a t a n = 5 6(5 m 2 ce llsice ) I will silence M2 in a temporally specific manner using an optogenetic approach. First, the light- sensitive neuronal silencer ArchT will be transduced bilaterally into M2 using an AAV. Light pulses will then be delivered through an optical cannula to inactivate M2 specifically during the 3-sec post reward in order to block delayed choice-selective activity. If such activity is important for reinforcement of action-selection strategy, then this manipulation should increase the number of trials taken to reach a criterion rate of correct response after phase switches, as well the number of perseverative errors. Preliminary observations also reveal early choice-selective activity during action-guided correct trials that may bias action selection toward the appropriate response. If this is the case, silencing M2 during the 3-sec prior to response should increase trials-to-criterion during the action-guided phase while having no effect on performance during the cue-driven phase of the task. Aim 2: Investigate contribution of GABAergic inhibition to M2 ensemble dynamics and determine causal role in flexible action selection. GABAergic inhibition is known to serve essential computational roles in the neocortex. For example, fast-spiking PV+ interneurons (PV- INs) are known to generate the gamma rhythm and sharpen feature selectivity in sensory areas.4,5 However, the function of PV-INs in cognitive areas of cortex remains unexplored. The idea that PV-INs modulate choice selectivity within M2 ensembles is an intriguing hypothesis. To test this possibility, I will modify the in vivo imaging experiment described above to include PV-specific silencing, using a transgenic mouse line (PV-cre) that expresses cre-recombinase only in PV-INs. Because an optogenetic approach would preclude simultaneous imaging, a cre-dependent inhibitory receptor activated by the drug CNO (Gi-DREADD) will be transduced into M2 using an AAV, to allow PV-specific silencing as mice perform the decision task. I hypothesize that silencing PV-INs with CNO will reduce choice selectivity within the imaged ensemble by removing task-related inhibitory control of choice-selective neurons. Additionally, if PV-INs in M2 are critical for strategy reinforcement, then PV-specific silencing should disrupt adaptation of action selection strategy, and thus increase trials-to-criterion and perseverative errors after shifts in cue-action-reward contingencies. Broader Impacts. Research on flexible decision-making will benefit a wide variety of fields that concern human and animal behavior. Economic decisions are essentially a form of goal-directed behavior in which appropriate error signals derived from expectation, reward, and punishment must play a key role. By improving understanding of how decision strategy is adapted, we might develop a more informed view of how markets operate, and possibly reduce the human toll of market dysfunction. Similarly, the justice system requires a nuanced understanding of concepts such as incentive structure, deterrence, and risk, all of which must be rooted in goal-directed behavior. As a complement to the study of normal decision-making, it will also be important to study how flexibility of the system is hampered by stress, distraction, mood, etc. A more mechanistic understanding may lay the foundation for discovery of measures we can all take to optimize our level of cognitive flexibility, in order to make better decisions as individuals and as a society. Finally, flexibility is notoriously difficult to implement in current hardware and software. Since behavioral adaptation is evidently a great talent of animals, biomimetic engineering based upon our own neural wetware may one day deliver some very smart machines. References: 1) Nachev et al. (2008) Functional role of the supplementary and pre-supplementary motor areas. Nat Rev Neurosci, 9(11), 856-869. 2) Gremel et al. (2013) Premotor cortex is critical for goal-directed actions. Frontiers Comp Neurosci, 7. 3) Sul et al. (2011) Role of rodent secondary motor cortex in value-based action selection. Nat Neurosci, 14(9), 1202-1208. 4) Cardin et al. (2009) Driving fast-spiking cells induces gamma rhythm and controls sensory responses. Nature, 459(7247), 663-667. 5) Lee et al. (2012) Activation of specific interneurons improves V1 feature selectivity and visual perception. Nature, 488(7411), 379-383.	0
Interferometric Reflectance-Based Nanoparticle Imaging with Patterned Illumination Introduction: A significant issue in current medical standard-of-care is the accurate detection of infectious diseases. Viruses, bacteria, parasites, and other microorganisms causing these diseases are difficult to detect directly due to their micro- and nanometer length scales. Existing diagnostic techniques typically rely on indirect detection through monitoring bulk tissue changes in a patient, analyzing biological samples in vitro, or determining an infection based on the patient’s symptoms and immune response. While these techniques are effective in certain cases, indirect detection methods increase the difficulty of achieving a proper diagnosis which can lead to harmful consequences for patients [1]. One such primary diagnostic tool experiencing this limitation is the optical microscope. New optical technology has improved microscopy’s capabilities in imaging small-scale objects, but many modern systems have become diffraction limited. Diffraction limits occur when the particles of interest are smaller than the imaging wavelength of light. This sizing issue results in light scattering that prevents nanoparticles from being resolved with conventional microscopy techniques. This limit has been bypassed previously using methods such as fluorescence microscopy, where the particle of interest is indirectly detected by imaging a fluorescent dye that has been bound to the particle. Such techniques are successful, but they have significant drawbacks including the need for extensive sample preparation, augmentations to the sample prior to analysis, and expensive imaging hardware [2]. These factors create significant barriers of entry for these modalities from becoming common disease diagnosis platforms in developing and developed countries. Thus, a substantial need exists for an affordable diagnostic platform capable of nonspecifically detecting nanoscale biological particles. Proposal: I propose a new microscope design combining the imaging modalities of Single- Particle Interferometric Reflectance Imaging Sensors (SP-IRIS) and Fourier Ptychography (FP) Microscopy for high resolution, high throughput imaging of biological nanoparticles. SP-IRIS, developed in Dr. Selim Unlu’s lab at Boston University, utilizes wide-field interferometric imaging techniques to acquire weak scattered light signals from nanoparticles over a large sample region. These signals provide information regarding nanoparticle geometry and have been used for label-free detection of viruses at attomolar concentrations (Figure 1). These factors make SP-IRIS a desirable option for both large sample virus diagnostics and biological nanoparticle characterization applications. However, drawbacks including the requirement of mechanical sample scanning and device limitations in detecting differences between floating and Figure 1: Label-Free Figure 2: Standard adhered nanoparticles limit the system’s current Virus Particle Microscope (Top) and abilities as a diagnostic tool [1]. Visualization with SP- FP-Reconstructed IRIS Microscope [1] (Bottom) Image [3] Fourier Ptychography techniques could remove these existing issues in SP-IRIS technology. FP is a computational microscopy approach wherein different angled illumination patterns are projected on the sample via an LED array to obtain low-resolution image sets. These images can be recombined to create images with higher resolution and wider field-of-view than standard microscope techniques (Figure 2). These angled NSF Research Statement Alex Matlock illumination measurements also enable tomographic and 3-D reconstruction of the imaged sample. With the capabilities of FP in achieving near real-time imaging while providing high- resolution images, the synthesis of FP with SP-IRIS could create a highly sensitive and specific nanoparticle detection platform with volumetric information regarding each particle [3]. These additions would remove the need for depth sectioning in the SP-IRIS system and would allow the user to differentiate between floating and static particles as well as provide additional information for nanoparticle characterization. Year 1: Proof-Of-Concept Prototype The first year will focus on proof-of-concept research illustrating the successful combination of SP-IRIS and FP. I have already constructed an SP-IRIS bench-top microscope and will be validating the instrument’s operation prior to adding FP. This modification will require the addition of a programmable LED array for angled illumination, adapting FP algorithms for reflection microscope geometries, changing SP-IRIS forward modeling to use FP images, and determining whether volumetric FP results are viable with SP- IRIS imaging methods. This year’s goals will be achieved when floating and static customized carbon nanotubes can be identified with the system and an improvement in particle visualization is achieved with the combined system over SP-IRIS alone. Year 2: System Design and Speed Improvement: The primary work in this phase will focus on achieving real-time imaging using the combined software platforms from both modalities. Additional hardware and software modifications will likely be necessary to determine whether different illumination patterns, LED arrays, lens setups, or other aspects of the system can improve the imaging quality or speed. This year’s success criteria will be satisfied once real-time imaging of floating carbon nanotubes in a microfluidic channel is achieved. This phase can be extended into Year 3 if additional time is required for real-time imaging with the system. Year 3: System Validation in Biological Particles: The third year will investigate the device’s applications in biological imaging. The system will be tested for its sensitivity to biological particle detection and characterization of different nanoparticles. The throughput and speed of this system will also be tested by analyzing samples with increasing particle counts under different fluid flow conditions. Should this device exhibit reliable results in identifying and characterizing biological samples, the use of this device in clinical trials at Boston University’s medical hospital will be explored. Intellectual Merit: Achieving high-resolution, high throughput imaging of nanoparticles would open opportunities for nanoparticle imaging in many other scientific fields including the semiconductor industry. This technology also uses relatively low-cost optical components allowing other research facilities to build their own systems. This research will be published in research journals and presented at conferences. Broader Impacts: This technology would be viable as a low-cost, high sensitivity and specificity diagnostic platform for infectious diseases. The high throughput capabilities of this proposed device would be significant for detecting diseases with low concentrations of biological markers in the body. The results from this project will also be published in multiple journal articles and presented at optics-focused and biological research-related conferences. [1] Avci, O., Ünlü, N. L., Özkumur, A. Y., & Ünlü, M. S. (2015). Interferometric Reflectance Imaging Sensor (IRIS)--A Platform Technology for Multiplexed Diagnostics and Digital Detection. Sensors (Basel, Switzerland), 15(7), 17649–65. http://doi.org/10.3390/s150717649 [2] Jesus, D. M.; Moussatche, N.; McFadden, B. B. D.; Nielsen, C. P.; D’Costa, S. M.; Condit, R. C. Vaccinia Virus Protein A3 Is Required for the Production of Normal Immature Virions and for the Encapsidation of the Nucleocapsid Protein L4. Virology 2015, 481, 1−12. [3] Tian, L., Liu, Z., Yeh, L.-H., Chen, M., Zhong, J., & Waller, L. (2015). Computational illumination for high- speed in vitro Fourier ptychographic microscopy. Optica, 2(10), 904. http://doi.org/10.1364/OPTICA.2.000904	0
2 Keywords: carbon capture, amine degradation, aminosilica adsorbents, adsorbent regeneration Hypotheses: Aminosilica adsorbents used for post-combustion carbon capture can be partially regenerated through treatment with acid, transesterification, and a Hoffman rearrangement. Introduction: The most readily available technology to reduce carbon emissions is carbon capture through amine scrubbing followed by geological sequestration. However, amine scrubbing has environmental risks: volatile amines escape into the atmosphere and carcinogenic nitrosamines form from NO .1 x Other technologies exist to capture carbon, but most do not have the fundamental knowledge necessary for pilot testing. Amines covalently tethered to silica supports for adsorption (ASA) use similar chemistry as amine scrubbing to selectively bind CO but avoid the environmental 2 side effects because of a lack of volatility and by binding nitrosamines. The method for attaching amines to silica is common among industry,2 so scaling up ASA production would be more feasible than other novel adsorbents. Similar to amine solvents, flue gas components, NO and x O also reduce effectiveness of ASA. NO preferentially bind to the amine group3 while pure O 2, x 2 oxidizes ASA to form imines, amides, and carboxylic acids, thus degrading the adsorbent.4 Since production of ASA has both environmental and economic costs, reusability is vital for any industrial application. A study by Hallenback and Kitchin explored using NaOH to regenerate adsorbent poisoned with SO ,5 but there have been no studies to evaluate the 2 regeneration capacity of amine adsorbents for carbon capture after exposure from NO or O . I x 2 propose investigating the degradation of ASA from NO and O and developing regeneration x 2 methods to reduce the frequency of adsorbent replacement. More specifically I will 1) Synthesize and characterize the ASA from primary, secondary, and tertiary amines 2) Degrade the ASA using NO and O in a packed bed reactor x 2 3) Regenerate NO treated ASA using aqueous acetic acid and bromide x 4) Regenerate O treated ASA using the Hoffman rearrangement or transesterification 2 1) Synthesis and Characterization: ASA will be created through condensation of MCM-41 mesoporous silica with a chloroalkoxysilane followed by reaction with ammonia, ethylamine, and diethylamine to form primary, secondary, and tertiary ASA. Since the experimental procedures involve toxic chemicals, a risk assessment will be conducted to determine necessary safety precautions. Between sections 1, 2, 3, and 4, ASA will be tested for surface area (using Brunauer-Emmett-Teller method3), pore size (Barrett- Joyner-Halenda method3), elemental analysis (sent for external Primary ASA degradation testing), bonding structure (using C NMR and Fourier Transform 13 from NO and O to form Infrared Spectroscopy4,5), and CO 2 capacity and kinetics (in a nitrosamix ne (uns2 table) and packed bed reactor with live analysis of exiting gas composition5). amide 2) Degradation: The characterized ASA will be placed in packed bed reactors with a mixture of gas containing N , CO , H O, and either NO or O . The NO experiments will be conducted at 2 2 2 x 2 x 50°C, the standard adsorbing temperature, while the O experiments will run at 150°C, the 2 desorption temperature. The ASA will be considered degraded when the exiting concentration of pollutant approaches the inlet concentration. The degraded ASAs will then be characterized. I expect that ASA will initially react with NO and O in a similar manner with amine x 2 solvents: NO will form nitrosamines and nitramines, and O will oxidize the α-carbon. x 2 Furthermore, primary ASA should show a decrease in nitrogen content after exposure to NO x since primary nitrosamines degrade into N . A mixture of alcohols, amides, imides, and acids 2 should result from the O exposure at elevate temperatures.4 These all decrease the capacity of 2 the amine solvent.3–5 3) Regeneration-NO : To remove nitrosamine and nitramine functional groups, the x denitrosation procedure for aliphatic nitrosamines described by Dix et al.6 will be followed. If more stringent conditions are necessary, testing an ASA embedded with 2-amino acetic acid would indicate whether oxidative degradation improves amine denitrosation.6 Regeneration from NO may not work well for primary amines since they degrade into N . Secondary and tertiary x 2 amines treated with NO should increase in capacity as the original amines reform. x 4) Regeneration-O : For primary amines, a Hoffman rearrangement can produce an amine from 2 the degraded amide using mild reagents.7 Secondary and tertiary amines would require too strong reducing agents and cause significant damage to the silica. For these, transesterification with ethanol amine will be employed to recover lost activity. Due to the variety of products formed, regeneration from O exposure may be more difficult than denitrosation. 2 Further Analysis: The effluent from the regeneration steps will be tested for silica to determine the extent of support degradation. Assays requiring acidic treatments will use adsorbent with t- butyl groups near the organosilane bond to protect from hydrolysis. For the methods that show marginal improvement, conditions for regeneration will be altered to improve effectiveness. For these optimized methods, repeated degradation and regeneration cycles will assess durability. Expected Results: These results will indicate the most suitable ASA given different capture conditions. For example, when NO are removed before carbon capture, primary amines may be x most beneficial since the Hoffman rearrangement can easily reform amines from O oxidation. 2 Furthermore, the findings may also correlate well with other solid amine adsorbents, so a general trend in regeneration can be seen regardless of the type of amine adsorbent. Broader Impacts: Reducing CO emissions will help stabilize our planet’s temperature to 2 prevent negative effects of climate change like desertification and a rising sea level which would decrease food supply and increase land scarcity. ASA can isolate CO to reduce emissions and 2 slow climate change. The degradation studies in this project will further characterize ASA, and the regeneration methods developed will help make using ASA more economical. Since amine based adsorbents can capture CO with reduced emissions of toxic amines and carcinogens, this 2 work will also help reduce the negative environmental impact of carbon capture. I will disseminate my results through conferences and publications so other researchers can improve upon and apply the findings towards further development and application. I will mentor undergraduate students and encourage them to develop their own projects so that they gain valuable research skills before graduate school. Since solving climate change requires international cooperation, I plan to collaborate with foreign institutions specializing in carbon capture like the Norwegian Technical University to accelerate the application of CO capture. 2 (1) Jackson, P.; Attalla, M. I. Rapid Commun. mass Spectrom. 2010, 24, 3567–3577. (2) Materne, T.; de Buyl, F.; Witucki, G. L. Organosilane Techonology in Coating Applicaions; 2012. (3) Co, P.; Adsorption, C. S.; Rezaei, F.; Jones, C. W. Ind. Eng. Chem. Res. 2013, 52, 12192–12201. (4) Bollini, P.; Choi, S.; Drese, J. H.; Jones, C. W. Energy & Fuels 2011, 25, 2416–2425. (5) Hallenbeck, A. P.; Kitchin, J. R. Ind. Eng. Chem. Res. 2013, 52, 10788–10794. (6) Dix, L. R.; Oh, S. M. N. Y. F.; Williams, D. L. H. J. Chem. Soc. Perkin Trans. 2 1991, 1099–1104. (7) Patel, I.; Opietnik, M.; Bohmdorfer, S.; Becker, M.; Rosenau, T. Holzforschung 2010, 64, 549–554.	0
An Adaptive Chemistry Reduction Method for Detailed Modeling of Advanced Combustion Systems Key words: combustion, mechanism reduction, stiffness removal, CFD Combustion of hydrocarbon fuels provides 85% of energy in the modern United States [1]; the current energy crisis is in reality a fuel crisis. While renewable forms of energy are being pursued to supplement combustion-based sources, hydrocarbon fuels will remain the major component for the next few decades. Currently, there is high demand to improve the efficiency of combustion technology to decrease the amount of fuel consumed and to reduce the emissions in an effort to lessen the environmental impacts; in addition, fuel-flexible designs that can run on both conventional and alternative fuels are desired. Computational modeling drives the design of new combustors and engines for aerospace, transportation, and energy applications, but accurate prediction of fuel consumption and pollutant emissions requires detailed chemical reaction mechanisms. Detailed mechanisms for liquid hydrocarbons of interest contain large numbers of species and reactions; for example, the reaction mechanisms for n-heptane (C H ) and iso-octane (C H ), important molecules for 7 16 8 18 gasoline modeling, contain almost 1000 species and 8000 reactions [1]. Despite rapid advancements in computing power, it is generally formidable to integrate such detailed reaction mechanisms into large-scale computational simulations, in terms of CPU time and memory requirements. In addition, the wide range of time scales (from nanosecond to second) and the nonlinear coupling between species and reactions induces stiffness when governing equations are solved. Due to these computational demands, practical simulations using detailed chemistry are impossible with modern computational tools. Mechanism reduction schemes are used to allow quantitative modeling while keeping realistic chemistry effects. Non-adaptive methods perform reduction based on a predicted range of conditions typically by removing unimportant species and reactions and identifying the species with fast time scales for further reduction, providing a single global mechanism. Most adaptive reduction methods, on the other hand, operate by storing chemical kinetics information and retrieving necessary data during the simulation to avoid direct integration of the governing differential equations; newer techniques use multiple mechanisms reduced prior to the simulation at various points in the flow. I propose the development of a novel adaptive and computationally friendly reduction method that will remove unimportant species and reactions and eliminate stiffness on the fly. I aim to explore and develop new algorithms while using existing reduction methods as a basis. Non-adaptive reduction methods attempt to provide a valid reduced mechanism by predicting the range of conditions (pressure, temperature, mixture composition) of interest in a simulation. However, the size of the reduced mechanism is limited by locations in the computational domain that require more detailed chemistry due to high reaction activity. Many methods have been developed to reduce mechanisms in this manner, but the application of directed relation graph (DRG) theory [2] is particularly useful. In this method, nodes of the DRG represent species and directed edges represent coupling of species. Important target species are defined (e.g. fuel, oxidizer, pollutants) and a graph-searching algorithm finds the dependent sets of species needed to accurately predict the targets’ production rates. Species with contributions below an error threshold are considered unimportant and removed from the mechanism, and the algorithm eliminates reactions containing unimportant species. For further elimination of stiffness in reaction systems, the quasi steady state (QSS) and partial equilibrium (PE) assumptions are applied [3]. QSS species and PE reactions have very short time scales, 1 NSF GRFP 2009 – Proposed Plan of Research – Kyle Evan Niemeyer causing stiffness, and the approximations seek to replace differential equations with algebraic relations to solve for species’ concentrations. Computational singular perturbation (CSP) and intrinsic low dimensional manifold (ILDM) [3] are traditional methods for finding QSS species and PE reactions by separating fast and slow processes. Adaptive reduction methods rely on different approaches to increase computational efficiency during simulations. Approaches such as in situ adaptive tabulation and artificial neural networks [4] perform storage and retrieval of chemical kinetics information to save processing time. Newer adaptive methods such as genetic algorithms [5] and optimization-based approaches [6] use various techniques to provide multiple reduced mechanisms for use during the simulation at different points in the flow. Highly detailed chemistry needs to be considered at locations where reactions are actively occurring, while regions with low reactive activity can use extremely reduced mechanisms. However, all of the methods currently rely on predictive reduction, which will not provide the highest level of accuracy or reduction. I propose the investigation of a new adaptive reduction methodology that will perform on the fly removal of species and reactions and elimination of stiffness. Identification and removal of unimportant species and reactions based on local conditions allows for the highest level of reduction and therefore the least computational demand, while keeping high accuracy. First, I will explore a novel algorithm for species and reaction removal using the DRG concept as a starting point; previous studies [2] based on DRG have shown it to be fast and reliable, suitable characteristics for on the fly application. This work will build directly on the preliminary non- adaptive reduction method I developed [7]. Second, I will investigate efficient methods for identifying fast processes such as QSS species and PE reactions; traditional methods such as CSP and ILDM are time-intensive [3] and therefore not well suited for on the fly stiffness removal. The adaptive reduction method I have proposed can be directly applied to the simulation of combustion processes for aeropropulsion, transportation, and energy applications. The incorporation of detailed chemistry while providing speedy simulation will allow accurate modeling of fuel consumption and emissions and help drive the design of next-generation engines and combustors. A method based on graph theory could also be applied to the modeling of other complex systems; broader applications consist of food web/ecosystem modeling, disease spreading modeling, climate modeling, and biological systems modeling. Also, a new methodology developed to perform mechanism reduction could also be used to collect important information about complex systems. For example, a CSP-based method was used to gather information about explosive processes in a simulation of a hydrogen/air turbulent lifted jet flame [8] - the new method I propose could be used similarly for data mining. References [1] “Basic Research Needs for Clean and Efficient Combustion of 21st Century Fuels,” DOE/BES Workshop Report (2006). [2] T.F. Lu, C.K. Law, Combust. Flame 144 (2006) 24-36. [3] T.F. Lu, C.K. Law, C.S. Yoo, J.H. Chen, Combust. Flame 156 (2009) 1542-1551. [4] J.-Y. Chen, J.A. Blasco, N. Fueyo, C. Dopazo, Proc. Combust. Inst. 28 (2000) 115-21. [5] I. Banerjee, M.G. Ierapetritou, Combust. Flame 144 (2006) 619-33. [6] O.O. Oluwole, P.I. Barton, W.H. Green, Combust. Theory Model. 11 (2007) 127-46. [7] K.E. Niemeyer, C.-J. Sung, M.P. Raju, “Skeletal mechanism generation of surrogate fuels using directed relation graph with error propagation and sensitivity analysis,” Combust. Flame (submitted). [8] T.F. Lu, C.S. Yoo, J.H. Chen, C.K. Law,AIAA 2008-1013 (2008). 2	0
Introduction Oligodendrocyte precursor cells (OPCs) are the progenitor cells responsible for forming mature, myelinating oligodendrocytes (OLs) in the central nervous system during development. While the majority of OPCs differentiate early in life, there is a small pool that generate OLs over the life span and can differentiate into OLs following white matter injury (WMI). Previous work has shown that this differentiation is impaired in aging, reducing the ability to recover from WMI.1 Significant upregulation of senescence markers in old vs young OPCs suggests senescence, a stress induced state in which cells no longer proliferate, contributes to the impaired ability of OPCs to differentiate2. Studies of senescence in other cell types has shown that it leads to reduced functional capacity and mediates the physiological consequences of aging. Thus, better characterizing OPC senescence and the mechanisms involved would greatly improve our understanding of the role of OPCs in brain aging. Although studies have identified some canonical senescence genes in OPCs, OPC senescence genes have not been completely characterized, making accurate identification of senescent OPCs more difficult. Furthermore, due to our lack of understanding of OPC senescence mechanisms, there is a need to identify novel regulators of senescence in OPCs. Understanding these mechanisms would greatly enhance both our understanding of OPC development, and our ability to promote CNS myelination in injury. Given this, I propose to identify an OPC-specific senescence signature, and to identify novel regulators of senescence via a genome-wide CRISPR- Cas9 knockout screen. I intend to carry out this project with two faculty experts in glial cell biology and genome-wide CRISPR screens. Research Plan: Aim 1: Define canonical and OPC-specific senescence markers in vivo by single-cell RNA-seq Rationale: A comprehensive OPC-specific senescence signature has yet to be established. Such a signature would allow for a more accurate identification of senescent phenotypes in different OPC populations and may provide insight into the mechanisms underlying senescence in OPCs. Experimental Design: OPCs will be taken from 20-24 month old mice and senescent cell clusters identified by flow cytometry using fluorescent antibodies to established senescence markers such as NOTCH3, B2MG and DEP1, which have been shown to have high levels of expression in senescent cells3. Following identification of senescent OPCs, RNA from both proliferating (non- senescent) and senescent OPCs will be extracted and single-cell RNA-seq performed, with non- senescent OPCs serving as a control to identify genes implicated in aging, but not senescence. Data will then be analyzed to identify differentially expressed transcripts in senescent OPCs, which will comprise our OPC senescence signature. This experiment will yield genes both up- and downstream of senescence and will allow for more accurate identification of senescent OPCs. In the future, candidates may be validated through in vivo knockout studies. Possible Outcomes: It is possible that with multiple senescence markers, we may miss OPCs which display lower levels of markers or do not display some at all. We can account for this in adjusting the sensitivity of our gating and analysis, or utilizing fewer antibodies for selection. Aim 2: Identify regulators of OPC senescence via a CRISPR-Cas9 knockout screen Rationale: Discovery of OPC-specific regulators of senescence will establish mechanisms underlying OPC senescence and can be used to inform the development of mechanistic in vivo studies. Experimental Design: To identify regulators of OPC senescence, I will conduct a genome- scale CRISPR-Cas9 knockout screen with a pooled lentiCRISPR library. Complex pooled DNA libraries will be combined, and delivered in lentiviral constructs in 4-8 week old OPCs, which should not display high levels of senescence. OPCs can be cultured and transduced at a large scale which allows for genome-wide screens. After transduction and selection, I will select senescent cells using the flow-cytometry approach described Fig 1. Experimental outline for a senescence marker- in Aim 1 with multiple antibodies against based pooled CRISPR screen. senescence markers. Sorted cells will then be analyzed to identify genes whose knockout increases or decreases senescence, yielding insight into mechanisms of OPC senescence. Possible Outcomes: If Aim 1 yields OPC-specific markers for which there are robust reporters or antibodies, these can be used in a parallel screen to support our initial findings. Future studies may also use the signature from Aim 1 to validate hits from Aim 2 in vivo. Intellectual Merit While senescence has been characterized in microglia and astrocytes, significantly less work has been done in understanding OPC senescence and its relevance in aging. Given that the efficiency of myelination has been shown to decline over time, this proposal, which aims to better characterize OPC senescence, will greatly contribute to our understanding of the effects of aging on oligodendrocyte development and myelination. Furthermore, characterization of OPC senescence opens up the possibility of mechanistic and in vivo studies, which will broadly increase our knowledge of the roles of OPCs, and how they change over time. Broader Impact This work has implications for the understanding and treatment of neurological diseases, as aging is linked to an increase in disorders such as dementia and Alzheimer’s. Dementia is linked to loss of white matter and decreased myelination, and myelin breakdown is implicated in Alzheimer’s, pointing to a critical role for OPCs. This work could eventually allow for the reversal of senescent phenotypes in OPCs, potentially leading to curative treatments. Additionally, understanding OPC senescence has important implications for demyelinating diseases such as MS, for which there are no remyelinating therapies. Recovery from such diseases is impaired by lack of understanding of OPC differentiation; thus, understanding blockades to differentiation such as senescence may inform future therapeutic development. Feasibility My previous work with oligodendrocyte development will significantly contribute to the success of the project. Furthermore, I am rotating in the lab of Dr. Ophir Shalem, whose lab has a strong and successful history of genome-wide CRISPR-Cas9 knockout screens, and who will provide the resources and mentorship need. I am also rotating with Dr. Chris Bennett, who has expertise in the isolation, culture, and sequencing of glia, and can offer the resources and mentorship needed for this project. Having access to a wide breadth of experts in my fields, as well as Penn’s world-class facilities and resources, also ensures the feasibility of this project. References: (1) Swenson et al. Translational Medicine of Aging 2019; (2) Neumann et al. Cell Stem Cell 2019; 3) Althubiti and Macip, Methods in Molecular Biology 2019.	0
"Intellectual Merit: Inspired by the adaptability of biological organisms, soft robots have emerged to address some of the technical limitations of conventional rigid robots. Although rigid robots are remarkably capable at high-precision and load-bearing tasks, their stiff material properties, with a Young’s modulus in the range of 109 – 1012 Pa, inherently limit their ability to physically interact with their environment. In contrast, soft robots are composed from gels, fluids, and elastomers with a Young’s modulus of 106 – 109 Pa. These soft materials mimic the mechanical properties of biological tissues and can bend, stretch, and compress. This ability to conform is key for applications in human-robot interaction, biomedical devices, and space-restricted environments. For example, a rigid wearable accessory has limited placement locations that are both comfortable for the user and informative for the device. In comparison, a soft wearable device, that can compress and stretch, greatly increases compatible locations and therefore potential applications, such as biometric monitoring or activity tracking. These advantages have also been shown to enable successful applications in medicine, such as organ-assist sleeves, drug delivery, prostheses, and surgical tools [1]. However, soft robot applications still face prevalent barriers to improved functionality over rigid robot solutions. The same material properties that give soft robots their versatility also create challenges in actuation, control, sensing, and modelling. In contrast to rigid systems with a small number of joints, soft robots possess many more degrees of freedom due to their continuous elastic bodies. In order to fully understand their environment, soft robots require effective sensing tools in a stretchable format. However, soft and stretchable sensing solutions have only been recently developed for strain and pressure sensing. The lack of sensory information has resulted in the absence of sensor-based control and higher-level decision making that would be customary for a rigid robot [2]. The Soft Machines Lab at Carnegie Mellon University led by Professor Carmel Majidi made a breakthrough in soft robotic sensing capabilities by demonstrating a hybrid soft sensor skin with orientation, pressure, temperature, and proximity sensing processed on-board [3]. Finally armed with multimodal sensing to determine the soft robot’s environmental and internal state, a unique opportunity has arisen for the development of sensor-based control for soft robots. Proposal: As part of the team that developed the soft sensor skin in [3], I will build on our previous work and implement sensor-based control of a robotic arm and gripper using the sensor skin. This will serve as the first demonstration of the feasibility of sensor-based feedback control on this soft robot system. The approach can then be extended to a wider range of soft robotic systems, which I will further explore in my graduate research. I. Physical System: The sensor-based control system will be implemented on a robotic arm and elastic sensor skin adhered to a two-finger soft Fig. 1: Hybrid soft sensor gripper. The sensor skin will have two soft strain sensors made of liquid skin described in [3]. metal traces, a time-of-flight (TOF) sensor to measure distance, an IMU, and an on-board microprocessor to process sensor data. The strain sensors will be located on each of the inner fingertips to detect the presence of the object. The TOF sensor will be placed on the palm of the gripper, parallel to the scanning surface. The IMU will be placed next to the on-board processor, which will be located at the top of the gripper, to sense the gripper’s orientation. The sensor skin will be connected to a computer hosting a finite state machine to sequence behaviors. The computer will be connected to the robotic arm, so that the finite state machine can generate its movement. 1 II. Control System: The sensor-based control system will consist of two main components. i. TOF data processing: The gripper will scan the table in uniform rows, collecting TOF data to create a 2D array of distance measurements. This 2D array can then be analyzed as an image using OpenCV to identify the size, orientation, and location of each object on the table. The development of this TOF algorithm is crucial to the development of soft robot feedback control by allowing for rudimentary image processing when camera sensors are inaccessible. ii. Finite state machine: The robot arm and gripper will be controlled by a finite state machine (FSM) that uses sensor input to govern the state. The size, orientation, and location of the object will be determined by the TOF algorithm in the first state. After the object has been identified, the strain, IMU, and TOF sensor data will be used to sense the presence of the object in the grasp of the gripper. The presence or absence of the object would inform the system of a successful grasp, lift, transport, or release of the object. This information will be used to traverse the states of the system. The FSM will demonstrate basic autonomy with feedback control of a soft system. III. Testing: The performance of the sensor-based control of the robotic arm and gripper will be tested against an open-loop robotic arm and gripper in a grasping and placement task. Objects of varying size will be placed on a flat surface in front of the gripper. The gripper and robot arm will be programmed to have four potential actions: grasp, lift, move, and release. The open-loop system will use provided object locations and complete each of the actions strictly in sequence. The sensor-based control system will not have prior information about object locations; instead, it will use sensor data to determine the object locations after scanning the workspace. The sensor-based control system will also have the opportunity to decide whether to move onto the next action or repeat previous actions based on sensor data. The success rate of grasping, lifting, placing, and overall task completion for each system will be compared over multiple trials. One potential challenge with this system is noise from ambient light conditions affecting the accuracy of the TOF data. Because the system relies on the TOF data to locate and interact with the object, the accuracy of this data is crucial. A potential solution would be calibrating the TOF sensor to the specific environment that it will be operating in. Broader Impacts: This proposal addresses a key challenge in the control of soft robot systems by demonstrating a novel implementation of sensor-based control using a multimodal sensing skin. A successful demonstration will further push the boundaries of control and autonomy in the field of soft robotics, as well as provide a platform for more complex control architectures in the future. These advancements are necessary for ubiquitous soft robots in everyday life; for example, soft robot applications in the care and improved quality-of-life for the elderly. One practical application for this specific soft robot system consisting of a sensing skin, soft gripper, and robot arm would be to sort and grasp food objects in a food processing facility. The soft gripper would be better-suited to handling delicate food objects than a rigid gripper, which would reduce the amount of damaged food from handling errors. The sensor-based control system would allow for unsupervised operation, as expected in a modern facility. In addition, this project could be demonstrated in STEM outreach to inspire interest in STEM from K-12 students. Because the differences in performance of this system could be easily seen and understood with little technical background, and the task of grasping and placing is familiar, this demonstration would be particularly well-suited to a young audience. [1] M. Cianchetti, et al.,""Biomedical Applications of Soft Robotics."" Nature Reviews. Materials 3.6 (2018): 143-53. [2] T. Thuruthel, et al.,“Control Strategies for Soft Robotic Manipulators: A Survey,” Soft Robotics, Vol. 5, No. 2 (2018). [3] T. Hellebrekers, K. B. Ozutemiz, J. Yin and C. Majidi, ""Liquid Metal-Microelectronics Integration for a Sensorized Soft Robot Skin,"" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2"	0
Key Terms: mechanotransduction, micropipettes, chemotaxis, phagocytosis, β integrin 2 Introduction: Future medical innovation will require a detailed knowledge of the causal sequences of events in biological processes. Currently, much of the understanding of these processes comes from correlative studies, whereas cause-effect relationships are less often explored. For instance, in immune cells, several signaling pathways are associated with dramatic, global bursts in cytosolic calcium concentration, but it remains unclear which pathways trigger the calcium burst and which depend on it. In human neutrophils, these bursts are correlated with several mechanically demanding processes, including β -integrin-mediated cell arrest1, the onset 2 of active cell spreading on immobilized IL-82, and the acceleration of β -integrin-mediated 2 phagocytosis3. On the other hand, my prior work in the Heinrich Lab has shown that pure (i.e. adhesion-free) complement-mediated chemotaxis neither causes nor requires such global calcium surges4 (see Fig. 1B). We further demonstrated that unphysiologically high levels of chemoattractant can cause calcium bursts, but contractile forces stalled or even reversed pseudopod formation in such cases. These findings imply a close connection between calcium bursts and mechanical behavior. The purpose of this project is to examine the cause-effect relationship between changes in cytosolic calcium concentration and mechanical responses of human neutrophils to chemotactic and phagocytic stimuli on a single-cell basis. Background: Store-operated calcium entry (SOCE) is considered the dominant mechanism for calcium bursts in human neutrophils. In this paradigm, ligation of certain receptors, such as G- protein coupled receptors (GPCRs), triggers a signaling cascade that leads to the depletion of intracellular calcium stores (usually via IP production). This prompts a calcium influx from the 3 extracellular space through channels such as Orai1. However, our own findings and several earlier studies indicate that this view of SOCE is incomplete, as GPCR ligation can cause chemotaxis without triggering a calcium burst4,5. Furthermore, shear force on high-affinity β - 2 integrins is known to mediate calcium influx1, which implies that mechanotransduction is important for SOCE in neutrophils. It also remains largely unclear which cellular activities depend upon the elevated calcium levels after store release and calcium influx. Calcium bursts often precede F-actin-mediated spreading1-3, but this connection is not fully understood. I hypothesize that β -integrin-mediated 2 mechanotransduction is key to inducing a global calcium signal in human neutrophils, which controls a mechanistic switch between two distinct modes of cytoskeletal organization and dynamics. I will primarily use single-cell, single-target micropipette experiments (Fig. 1A) to quantify aspects of the mechanical response (e.g. cell morphology, cortical tension, or surface area) while monitoring intracellular calcium concentration using a calcium-sensitive dye (e.g. Fluo-4 or Fura-2). This will be supplemented by data from Fig 1. A: Experimental setup. B: Neutrophil other biophysical experiments, as well as shows a calcium burst during phagocytosis, but mathematical modeling. not during pure chemotaxis4. Aim 1. Uncover specific mechanical or biochemical cues that are necessary and/or sufficient to induce calcium bursts. Before micropipette experiments, human neutrophils will be treated with an actomyosin inhibitor (e.g. latrunculin A, cytochalasin D, blebbistatin) or a β - 2 integrin (LFA-1 or Mac-1) blocking antibody. Our analysis of the calcium response will reveal the roles of the respective molecules in calcium burst induction. I will also use reflection interference contrast microscopy (RICM) to measure the contact areas of neutrophils spreading on glass with a known ligand density, determining if a threshold of engaged receptors can trigger a calcium burst. Application of a measurable force on β integrins on a neutrophil using atomic 2 force microscopy (AFM) will allow me to explore whether force can directly stimulate a calcium burst or if there is a synergistic effect between force and number of integrins engaged. Aim 2. Characterize the mechanical and morphological changes that require calcium store release and/or calcium influx. I will conduct micropipette experiments after depleting extracellular calcium with EGTA or emptying internal calcium stores with thapsigargin. In similar experiments, I will block IP -dependent store release using a PLC inhibitor (U73122), or 3 use murine neutrophils with a deficient calcium influx (Orai1+/- or Orai1-/-, collaboration with Dr. Scott Simon, BME Dept.). These experiments will indicate the relative importance of calcium store release and calcium influx for mechanical changes such as elevated cortical tension or surface area expansion. I will also collaborate with Dr. Soichiro Yamada (BME Dept.), using confocal microscopy to simultaneously image actin arrangement and calcium concentration in a neutrophil-like cell line (PLB-985) transfected with GFP-actin and loaded with Fluo-4. I will assess actin structure and dynamics following calcium bursts in these cells. Aim 3. Incorporate global calcium signaling into an established computational model of neutrophil phagocytosis. I will collaborate with Dr. Samuel Walcott (Math Dept.) to build on the computational model developed by Herant et al.6, which accurately describes the phagocytic behavior of neutrophils. The model predicts a key role for cytoskeletal membrane anchors, connections that form between integrins and F-actin via adaptor proteins. The assembly of these complexes is associated with elevated calcium levels1,3. I will incorporate the calcium- dependence of cytoskeletal membrane anchor strength into the model and leverage this revised model against the phagocytic behavior of neutrophils in the above experiments. Intellectual Merit: My three years of experience with micropipette experiments and quantitative data analysis have prepared me well for this important work. This project will fill a fundamental knowledge gap regarding one of the most dramatic signaling events in the life of a neutrophil. In addition, clarifying the sequence of molecular events leading from receptor engagement to calcium burst to protrusive force generation may elucidate similar mechanisms in other cells. Broader Impacts: An improved quantitative and mechanistic understanding of immune cells will strengthen the foundational knowledge for novel medical treatments such as immunotherapy. With an understanding of the cause-effect relationship between calcium signaling and immune cell motility, new therapeutic targets for immunodeficiencies and autoimmune diseases could also be identified. Furthermore, because calcium bursts are easily detectible and are strong indicators of immune cell activation, my research may inform the development of future diagnostic tools. I will include undergraduate students in this project, share findings in publications and at international conferences, and inform the public by creating and sharing videos online (www.youtube.com/heinrichlab). References: [1] Dixit, N. et al. (2011) J Immunol. 187(1):472-481 [2] Beste, M.T. et al. (2015) Ann Biomed Eng. 43(9):2207-2219 [3] Dewitt, S. and M.B. Hallett (2002) J Cell Biol. 159(1):181-189 [4] Francis, E. and V. Heinrich (2017) Revision under review. [5] Laffafian, I. and M.B. Hallett (1995) J Cell Sci. 108(10):3199-3205 [6] Herant, M. et al. (2011) PLoS Comput Biol. 7(1):e1001068	0
topologically relevant materials. Successful execution of this approach will lead to new materials discovery and generate new methods for electronic band structure engineering. Introduction: Linking desired physical properties to structural motifs is a fundamental goal in solid-state chemistry. As we will see, topological semimetals are exemplars to this goal: ultrahigh mobility electrons arising from linear band crossings can be derived from specific structural motifs, further predicted from basic electron counting rules1. Previous work regarding topological materials has largely focused on systematically scanning the thermodynamic stability of compositions in particular space groups, followed by selected synthesis of the most promising candidates. More recently, scientists have explored how slight perturbations of topological systems can induce charge density waves through structural modulations rationalized via electronic considerations.2 That is, seemingly simple chemical substitutions used as n- or p-type dopants, aimed to adjust the Fermi energy to lie in a precise electron state. Changing the composition of a structure may induce a structural change, such as a Peierls distortion, which would open a up bandgap at the Fermi energy. Further, exactly how a given structural transition navigates its potential energy surface to form such specific distortions is largely unexplored in the literature. Finally, even if such chemical substitution does not significantly alter a structure, development of predictive strategies for diversifying the possibilities of elemental substitution should be developed. Exploring synthetic control over such distortions in topological materials can provide one such prescription to these issues. Thus, understanding how such distortions lead to thermodynamic favorability, paired with how such distortions alter physical properties, may then open a playbook for “band engineering” where finely tuning composition can gap out unwanted bands from the Fermi surface. Background: Topological materials can be well understood through the Zintl-Klemm concept: an electron counting method where transfer of electrons is assumed from the most electropositive element to the most electronegative element in a crystal structure. The remaining atoms then form covalent networks to reduce the total thermodynamic energy of the system. In Figure 1a, a Walsh diagram shows the thermodynamic stability of a Te chain as a function of bond angle. The linear geometry is found to be stabilized at 22 3 electrons, the third level, due to less overlap of anti-bonding interactions. In extended solids, it would then be predicted that linear chains form when there are 7 electrons per chain atom. Such is the case in UTe , 2 seen in Figure 1b, a promising candidate host for the sought-after Majorana quasi-particle.1 Each uranium is found to be in the 3+ oxidation state and transfers two electrons to the nearest tellurium sites, forming what can be thought of as [UTe]+ slabs and a linear Te- chain. As a result, the geometry enforces a linear crossing at the Fermi energy, shown in Figure 1c. Doubling of the unit cell, when considering 2 atoms per unit cell, folds the band structure back on itself creating a linear band crossing located at the Fermi energy. Figure 1. a) A Walsh diagram of a Te molecular unit. b) The unit cell of UTe , a topological 3 2 superconductor containing linear chains. c) The resulting band structure for an isolated linear chain of Te- ions. A linear crossing at the Fermi energy occurs halfway between the Γ and Ζ points. d) A folded band structure, arrived at by doubling the unit cell, forming a Dirac node at the Fermi energy. Proposal: Derivation from ideal electron counts in such covalent networks can lead to a structural distortion. In the classical example of a linear chain of atomic orbitals, a half-filled band will favor dimerization, leading to differing bond lengths, opening a band gap at the Fermi energy. However, recent investigations have found, counterintuitively, that such distortions can lead to improved topological band structures or high-mobility electrons. Despite a structural modulation in the square-net layer, which can be viewed as a two-dimensional linear chain, NdTe exhibits ultra-high mobility electrons and anomalous 3 quantum oscillation behavior1. Moreover, the distorted square nets in GdSb Te was recently shown to 0.46 1.48 gap out trivial band crossings, “cleaning” the band structure by retaining the screw axis associated with its structural distortion.2 For my analysis of similar systems, I will utilize recent adaptations of Density Functional Theory (DFT) outputs that establish direct links between local features in solid state compounds and their contribution to electronic and steric favorability. Specifically, the reversed applied Molecular Orbital (raMO) analysis aims to fit tight-binding parameters from DFT band structure calculations. As a result, DFT-calibrated molecular orbital diagrams are visualized to explain formation of closed-shell configurations (see Figures 1a) and c)). Parallel with this, DFT-Chemical Pressure (DFT-CP) resolves local packing frustrations that can arise within dense atomic packings, from which the role of atomic size can be assessed. Utilization of both these computational analyses can explain why a structure may obey or derivate from predictive bonding schemes such as the Zintl–Klemm concept.3 Both raMO and DFT-CP software packages are freely available via the GNU Public License and will be used in this work. I will also synthesize and structurally as well as electronically characterize the targeted compounds. Therefore, I will employ an iterative approach of experiment and theory to implement a frame of understanding structural distortions in topological motifs, targeting the following research objectives: 1. Determination of topological systems in which unexpected electron counts or steric packing frustrations may favor charge density wave formation. 2. Synthesis of candidates, as well as detailed structural and physical characterization of materials. 3. Demonstration of band engineering through the tuning of new superstructures guided by theoretical analysis. Intellectual Merit: Princeton University offers many unique multidisciplinary approaches needed to study topological materials. In the Schoop laboratory, I have access to core instruments needed for this investigation, including furnaces for solid-state synthesis, a single-crystal X-ray diffractometer for materials characterization, a magnetic properties measurement system, and a physical property measurement system with a dilution fridge. Through NSF supported opportunities, such as the Princeton Center for Complex Materials (PCCM), frequent collaborations will be drawn through an interdisciplinary research group focusing on topological quantum matter. Specifically, the Department of Physics offers many avenues for collaborative work in physical properties characterization with Dr. Nai Phuan Ong, Dr. Ali Yazdani, and Dr. Sanfeng Wu. There are also many opportunities through the NSF-funded Imaging and Analysis Center which provides access to instruments such as the scanning electron microscope. Frequent collaborations outside of Princeton University will also be utilized. So far, single-crystal diffraction data obtained for a linear chain system indicate missed satellite peaks with a modulation vector q = 0.06a*, indicating a massive 50/3 supercell periodicity. Despite prediction of such a distortion ruining the metallicity of the structure, electrical transport data suggests that the system remains metallic. To this end, a proposal has been written and sent to Dr. Yusheng Chen at Argonne National Laboratory, hopefully to be accepted for next cycle in March of 2022. Additionally, the Schoop laboratory frequently collaborates with scientists at Helmholtz-Zentrum Berlin such as Dr. Andrei Varykhalov at the BESSY II beamline for angle-resolved photoemission spectroscopy. Looking ahead, probing the band structure of a crystal related to this project may significantly improve the impact of my investigations. Broader Impacts: The predictive framework I develop will open paths to tailoring band structure through composition, allowing for more control of physical properties used in technological applications. In the future I will expand to other structural motifs such as the square- and Kagome-nets. Finally, visualization tools, like raMO, have chemical education implications for use in NSF-funded operations, such as PCCM’s Princeton University Materials Academy. References: 1. J.F. Khoury and L.M. Schoop. 10.1016/j.trechm.2021.04.011 Trends in Chemistry, (2021). 2. Lei S.; Theicher, S.M.L.; Topp, A. et al. 10.1002/adma.202101591 Adv. Mater. (2021). 3. Warden H.E.M.; Lee S.B.; Fredrickson D.C. 10.1021/acs.inorgchem.0c01347 Inorg. Chem. (2020).	0
permission. Investigating Informal E-Waste Recycling Methods and Associated Soil Pollution Key words: environmental pollution, heavy metals, e-waste, recycling, informal, Delhi Where does your computer go to die? Electric and electronic waste (e-waste) contains hazardous materials and much of it is processed with few environmental controls. Annually, an estimated 20 to 50 million tons of e-waste is produced worldwide1 and due to the substantial amount of labor involved in the recycling of electronic devices, many e-waste dealers turn to developing economies for processing2. Policies designed to address the movement of e-waste recycling increasingly require robust scientific evidence of toxic leaching and the nascent body of evidence describing the environmental effects of unregulated or informal e-waste recycling is largely anecdotal3, 4. The few empirical studies have operated at an inappropriate scale to make associations between processing categories and associated levels of pollution5; have analyzed policy at a more global scale2, 6; or have focused on pollutant leaching in a laboratory setting as a proxy for environmental leaching7. The recent US Government Accountability Office report (GAO-08-1044, August 2008) criticizing the US EPA’s handling of e-waste highlights the relevance of this research. My study will make a significant contribution to both research and policy by addressing this gap in scale and context and by testing for robust associations between quantified pollutant levels and specific e-waste industrial processes in the field environment. This investigation will classify and map e-waste recycling operations and quantify associated pollutant levels in the soil. Hypothesis: the concentrations of key pollutants in the soil will increase in association with more destructive recycling processes (e.g. repair and resale will be associated with lower toxin concentrations as compared with smashing cathode ray tubes for the copper yokes). Methods: My proposed field site is Delhi, India, due to the city’s established e-waste recycling industry and well-documented specialized processing areas4, 8. I will collaborate with the following non-governmental organizations in the United States and on-site in India: Silicon Valley Toxics Coalition; Toxics Link, Delhi, India; and Chintan Environmental Research and Action Group, Delhi, India. Base maps of Delhi will be collected from map archives, university departments, and government offices. These will include streets and historical land use to assist in navigation and pollutant baseline controls; elevation for surface/hydrological modeling; and geology, soils, and streams for environmental controls. Additional information will be gathered from public records if historical maps are not available. All data will be translated into geographic space, and digitized in a Geographic Information System, with coordinate systems defined, projected and transformed as necessary. Following this, key individuals from local NGOs, government offices, and universities will be interviewed to provide qualitative data for three critical components: types of e-waste recycling operations in Delhi, site locations, and historical land use not captured in the base map construction. Combining the results of these interviews with published literature on recycling processes8, 9, a series of recycling operation categories will be categorized (e.g. one code for gold extraction from printed circuit boards using acid-baths versus another code for cell-phone refurbishment), resulting in approximately 3-5 analytical codes. Information gained in the interviews will be verified and geo-located in the field by surveys assisted by a differentially corrected geographic positioning system (GPS). Using the population of coded recycling operations, a stratified simple random sample of individual recycling sites within each code will be selected. Soil analysis will primarily be performed with a field-portable x-ray fluorescence analyzer (fp-XRF), supplied by Ron Amundson’s lab, based in the Department of Environmental Science, Policy and Management at UC Berkeley. The fp-XRF will be calibrated for lead, Copyright 2008. All rights reserved to original author. Copyright 2008 All rights reserved to original author. Do not duplicate or use in any way without permission. arsenic, cadmium, bromine, mercury, and chromium, toxic elements most commonly associated with e-waste2. All samples collected and fp-XRF readings will be catalogued with geographic coordinates using GPS. The first sampling phase will define pollutant plumes and concentration strata using transect sampling with the fp-XRF. Systematic random sampling will assign sampling locations within each plume strata. In situ soil readings and ex situ samples will be collected during the second phase. To aid in precision and bias control, two methods will be employed: 1) one randomly selected site will provide field calibration values by double sampling: ex situ soil samples and in situ readings 2) 20% of all in situ readings will be accompanied by ex situ samples at remaining sites as a further control for environmental variations10. Potential sources of error include sampling design insensitivity to local variations, sampling obstructions at individual sites, non-soil ground-cover (mitigated by alternate collection protocol for dust using thin-sample), dynamic environmental conditions (mitigated by hydrologic modeling or averaging multiple series), and changing land use at observation sites (mitigated by a study design with more sampling sites than necessary). After ex situ soil samples have been laboratory tested, all soil data will be digitized and geo-referenced. In situ readings will be calibrated against quantitative laboratory results producing a measure of estimated bias. Multivariate Analysis of Variance (MANOVA) will be used to assess the combination of toxin concentrations against the categories of recycling processes. Additional variables will be tested for inclusion in the model such as historical land use and environmental features such as slope and soil type. Weights will be applied in the model for the concentration strata and to control for spatial autocorrelation. Covariance will be addressed in the MANOVA model. Anticipated Results: As the recycling process becomes more destructive, the concentrations of key soil pollutants are expected to increase. The results of this study can aid more precise targeting of particular waste-handling practices for environmental controls, thus facilitating a more nuanced approach to improving e-waste recycling operations. Methods used in this project could also be replicated in other locations to examine environmental contamination associated with formal and informal e-waste recycling. Support: My advisor, Rachel Morello-Frosch (study design and environmental pollution); Ron Amundson (soil sampling methodology); John Radke (spatial analysis and sampling techniques); Alan Hubbard (statistical analysis); and Alastair Iles and Kate O’Neill (hazardous and e-waste trading). Oladele Ogunseitan at UC Irvine, and Jaco Huisman with the UN’s StEP Initiative (e-waste toxicity and recycling processes). 1. Schwarzer, S., et al. in Env Alert Bulletin 4 (UNEP, 2005). 2. Widmer, R., et al. Global perspectives on e-waste. Env Impact Assess Rev 25, 436-458 (2005). 3. Puckett, J., et al. (Basal Action Network, 2005). 4. Puckett, J. B. et al. (Basal Action Network & Silicon Valley Toxics Coalition, 2002). 5. Wong, C. S. C., et al. Evidence of excessive releases of metals from primitive e-waste processing in Guiyu, China. Env Pollution 148, 62-72 (2007). 6. Iles, A. Mapping Environmental Justice in Technology Flows: Computer Waste Impacts in Asia. Global Env Politics 4, 76-107 (2004). 7. Lincoln, J. D., et al. Leaching Assessments of Hazardous Materials in Cellular Telephones. Env Sci & Tech 41, 2572-2578 (2007). 8. Agarwal, R., et al. 1-57 (Toxics Link, 2003). 9. Streicher-Porte, M. et al. Key drivers of the e-waste recycling system: Assessing and modelling e-waste processing in the informal sector in Delhi. Env Impact Assess Rev 25, 472-491 (2005). 10. Kalnicky, D. J. & Singhvi, R. Field portable XRF analysis of environmental samples. J of Haz Materials 83, 93-122 (2001). Copyright 2008. All rights reserved to original author.	0
Introduction: High latitude permafrost soils contain vast reserves of organic carbon (C) that, with warming, may become a significant source of greenhouse gases (GHGs) due to increased microbial activity. Current climate model predictions for C storage and fluxes in these ecosystems depend heavily on the rate at which soil organic matter (SOM) is broken down.1 At present, there are significant uncertainties regarding how the varying chemical and physical differences among these SOM pools and their interactions with the surrounding environment will affect the rates at which they are being degraded by the local biological community. It has been shown that organic C molecules sorbed to sediment mineral surfaces tend to decompose more slowly, and to a lesser extent than dissolved organic matter (OM).2 This may be due to the physical occlusion of the OM by minerals, reducing its susceptibility to microbial attack, thereby changing its long-term accumulation and translocation in the soil profile.3 Recent radiocarbon (Δ14C) values taken from Alaskan permafrost soils indicate that the carbon dioxide (CO ) and methane (CH ) being released to the atmosphere are derived from older C buried deeper 2 4 in the soil profile.4 Mineral phases and complexation mechanisms responsible for the stabilization of permafrost SOM are largely unknown and the role of these C-mineral interactions in the Arctic ecosystem and global C budgets is not well understood. Research Objectives: The following research questions will be investigated to better quantify and predict GHG emissions from thawing permafrost in response to climate change: R1) What are the major mineral species present in Arctic permafrost soils, and how do they affect SOM distribution in the soil profile? R2) What are the key chemical bonding mechanisms between soil organic C and mineral phases in these systems? R3) How does topographical variation impact C-mineral associations and, in turn, SOM stabilization and GHG emissions? Hypotheses: H1) The major mineral elements I expect to find in these systems are Fe, Ca, and Al. Fe oxides will prove to be an important sorbent in the spatial distribution of SOM-mineral complex formation because of their strong selectivity for aromatic compounds and high molecular weight fractions,2 qualities commonly associated with permafrost soils. H2) OM preservation will largely be controlled by electrostatic interactions and sorption of minerals into micropores along the soil profile. H3) Concentrations of Fe oxides and the relative proportion of OM bound to mineral surfaces will increase at lower topographic regions. Greater portions of OM will be physically protected from the microbial community leading to increased recalcitrance and decreased fluxes of CO and CH to the atmosphere. 2 4 Preliminary Results: Addressing these questions will require close monitoring of permafrost soil dynamics throughout seasonal thaw and at different depths along the soil profile. Working with the NGEE-Arctic biogeochemistry team at Oak Ridge National Lab (ORNL), I have helped design and test a large, temperature-controlled soil column apparatus for intact soil cores. Using a two- stage cooling mechanism, this system successfully mimics or accelerates seasonal thaw patterns in the lab.5 Access ports for the collection of gas and liquid samples allow for continuous, nondestructive monitoring of biogeochemical properties and their changes over time and space, on the order of days to weeks and cm to m, respectively. Study Site: Field observations will take place at the Barrow Environmental Observatory (BEO), located at the northernmost point of the Arctic coastal plain, near the remote, native Iñupiaq village of Barrow, Alaska. This high-latitude ecosystem is characterized by a dynamic landscape Mallory Ladd Graduate Research Statement 11/8/13 dominated by distinct morphological subunits: ice-rich polygonal tundra and drained thaw lake basins (DTLB). The site consists of continuous permafrost with the active layer reaching ~20-55 cm deep. Frozen soil cores (~7.7 cm diameter by 1m depth) will be obtained from low- and high- centered polygons from DTLBs of varying age using a SIPRE auger6 and a hydraulic drill. Experimental Approach: Using cores obtained in the field, (R1) I will first physically and chemically fractionate samples from the organic, mineral, and permafrost horizons and then characterize the mineralogical components with energy-dispersive x-ray (EDX) spectroscopy. Intact soil grains will be examined with micro-Raman and Fourier-transform IR spectroscopies to obtain compositional information, including the relative proportions of polysaccharides, amino sugars, phenols, lignin, and lipids to estimate the mineral interactions with different OM functional groups. (R2) The stability of these interactions will be tested using batch equilibrium techniques, while the aggregate surface area and soil micropore volumes will be determined by scanning electron microscopy (SEM). (R3) Using the soil column mesocosms, I will monitor changes in temperature, moisture, pH, redox potential, and concentrations of gases and solutes throughout a controlled thaw, helping to identify where CO , CH , and dissolved organic C are being generated. 2 4 During my summer field campaigns, I will continuously measure land-surface fluxes of CO and 2 CH using chamber measurements and laser-based infrared gas analyses. The isotopic composition 4 and age of mineralized C and SOM C from each core will be determined at the Center for Accelerator Mass Spectrometry at Lawrence Livermore National Lab. Intellectual Merit: Collectively, this research will provide a deeper mechanistic understanding of the physical and chemical controls on SOM stabilization in permafrost soils, while advancing our fundamental knowledge of the role of C-mineral chemistry in the Arctic ecosystem. These data will help provide a firmer empirical foundation for predicting the most important drivers of C degradation and GHG emissions in these high-latitude regions. As a graduate student at UTK working with researchers at ORNL, I will have full access to the extensive resources provided by both, including all instrumentation mentioned, and the support of expert faculty and scientists who specialize in terrestrial biogeochemistry and can help optimize any necessary techniques. Broader Impacts: Given the importance of climate change to all sectors of society, these results will provide critical data for improving global climate models. Various researchers from a variety of disciplines may use this data to better interpret Arctic systems chemistry and make more informed decisions on current and future governmental policies. My continued involvement with the ACS local section and my tutoring efforts will allow me to actively recruit high school and undergraduate students from underrepresented groups to gain valuable research experience on this project. The interdisciplinary nature of this research will significantly broaden their scientific experience and enhance their understanding of the importance of chemistry in the Arctic to global climate change. I will encourage them to apply for REU support to join me in the field where they will participate in weekly public scientific discussions with native Iñupiaq community members. I am applying to host a PolarTREC teacher, who has recently contacted me from the East Tennessee area, during a summer field campaign where we will connect with her students to share our experiences from the field. Given the close proximity, we will be able to connect more directly with her students through class presentations and scientific demonstrations. On a more regular basis, I am able to share the importance of environmental research, Arctic biogeochemistry, and their impacts on public policy and education, to the broader public, via my website and blog “Think Like a Postdoc” (www.malloryladd.com). References: [1] Jenkinson et al. (2008) Euro. J. of Soil Sci., 59, 400 [2] Gu et al. (1994) Env. Sci. & Tech. 28, 38 [3] Mikutta et al. (2006) Biogeochemistry, 77, 25 [4] Vogel et al. (2009) J of Geophys. Res., 114 [5] Ladd et al. (2013) ORNL Ann WIS Poster Session: http://malloryladd.com/ [6] Bockheim et al. (2007) Soil Soc. of Am. J., 71, 1889	0
The continued reliance on agricultural and petrochemical-based methods of production for many high-value compounds threatens future generations with shortages of essential manufacturing materials, organic solvents, biofuels, and pharmaceuticals. The application of synthetic biology to metabolic engineering has worked to address this growing concern by transferring requisite enzymatic pathways from native organisms to standardized chasses and manipulating them to both decrease dependence on non-renewable inputs and increase overall production yield. Despite continued efforts to improve the tunability and consistency of reaction progress within large-scale bioreactors, traditional controller-based methods of optimization remain stymied by excessive variability characteristic of biological systems. Intellectual Merit: Recent developments in the application of optogenetic tools to metabolic pathways have demonstrated potential in addressing the lack of tunability and irreversibility of traditional chemical-inducer based control schemes. Leveraging the implicit reversibility of optogenetic induction, Milias-Argeitis et al. (2016) developed an automated transcriptional control system to maintain a constant concentration of fluorescent protein as a proof of concept. While the proposed systems can be used to rapidly increase protein production, the tunability of the system is limited by the slow rate at which the proteins degrade. In the context of metabolic applications this delay could contribute to the non-optimal accumulation of toxic intermediates and decrease the applicability of feedback structures, reducing overall fermentation yield. The development of a reversible post-transcriptional control mechanism presents a novel, generalizable solution to this meaningful challenge. Hypothesis: A reversible, post-translational system for the control of selective protein degradation can be created using existing optogenetic toolkits to rapidly and precisely decrease protein concentration. Approach: A selective protein degradation tag is conjugated to the coding sequence of a target protein. The tag marks the target protein for degradation via ClpXP, a selective protease comprised of ClpX and ClpP subunits2. The reconstitution of these subunits is facilitated through heterodimerization of the cryptochrome Cry2 to the protein CIB13. Sufficiently orthogonal to the green light (535nm) and red light (672nm) utilized for transcriptional control, instances of Cry2 and CIB1 reconstitute in the presence of blue light (470nm) and spontaneously disassociate in its absence4. A B Fig. 1 Genetic circuit for proposed selective degradation scheme. (A) Representation of “slow response” transcriptional control used in Milias-Argeitis et al. (2016) updated with a protein degradation tag. (B) Representation of “fast response” post-transcriptional control featuring the reconstitution of the ClpXP protease in the presence of blue light to degrade tagged proteins. 1 Research Statement Kevin Fitzgerald Aim 1: Ensure conjugation of CIB1 tag to ClpX subunits has negligible impact on hexamer formation. The ClpX subunit is itself composed of six ClpX subunits. In the context of this project, each a ClpX subunit would be conjugated to a CIB1 dimerization domain and constitutively produced. a Although a small protein, it is critical to ensure that the conjugation of CIB1 to ClpX does not a interfere with the formation of hexameric ClpX. To achieve this aim, a library of mutant ClpX a subunits with CIB1 conjugated at different locations will be generated via rational design and screened for their ability to recombine via western blot. Utilizing a non-conjugated ClpP subunit, the functionality of structurally-promising CIB1-ClpX mutants will then be evaluated by their a enzymatic capacity to degrade tagged fluorescent proteins. Aim 2: Ensure ClpX/ClpP fusion is negligible in the absence of blue light and reversible following exposure to blue light. Wild type ClpX and ClpP subunits independently recombine via the formation of hydrogen bonds between key looping peptides on their exteriors. For the system to selectively degrade tagged proteins of interest, it is necessary to minimize any spontaneous recombination and subsequent functionality of conjugated ClpXP in the absence of blue light. Simultaneously, the utility of this light-based system is contingent on the reversible nature of the optogenetic reactions. Once background ClpXP functionality is minimized, it is also possible that the ClpXP complex formed upon initial Cry2/CIB1-mediated binding interactions will remain cohesive and functional despite the dissociation of conjugated light domains. It is therefore necessary to rationally generate and screen mutations within the ClpX/ClpP binding domains capable of both minimizing subunit binding affinities and maintaining enzyme functionality in the presence of blue light. Broader Impacts and Future Directions: Properly tuned to compatibly function with existing transcriptional control systems, a rapid, light-controlled protein degradation system would serve as a valuable tool in improving the sensitivity of optogenetic feedback systems in industrial fermentation processes. By effectively decreasing system lag, target concentrations of potentially toxic enzymes can be maintained more consistently despite excessive background noise. In the context of metabolic engineering this increase in control has the potential to improve both the speed and yield of fermentations. For those individuals relying on fermentation-based pharmaceuticals for the treatment of disease or fermentation-based biofuels for energy, even minor improvements in fermentation yield could decrease costs and improve accessibility to such essential compounds. In the future, the implementation of optogenetic protein controllers at each step in a metabolic process could serve to drastically improve the tunability and engineering capacity of large-scale fermentation processes. References: 1. Milias-Argeitis, A., Rullan, M., Aoki, S., Buchmann, P., & Khammash, M. (2016). Automatedo ptogenetic feedback control for precise and robust regulation of gene expression and cell growth. Nature Communications, 7(1). doi: 10.1038/ncomms12546 2. Baker, T., & Sauer, R. (2012). ClpXP, an ATP-powered unfolding and protein-degradation machine. Biochimica Et Biophysica Acta (BBA) - Molecular Cell Research, 1823(1), 15-28. doi: 10.1016/j.bbamcr.2011.06.007 3. Park, H., Kim, N., Lee, S., Kim, N., Kim, J., & Heo, W. (2017). Optogenetic protein clustering through fluorescent protein tagging and extension of CRY2. Nature Communications, 8(1). doi: 10.1038/s41467-017-00060 4. Kennedy, M., Hughes, R., Peteya, L., Schwartz, J., Ehlers, M., & Tucker, C. (2010). Rapid blue-light–mediated induction of protein interactions in living cells. Nature Methods, 7(12), 973-975. doi: 10.1038/nmeth.1524 2	0
Tour Orbiter to the Ice Giant Planets Introduction and Background The ice giant planets, Uranus and Neptune, have only been observed directly during flybys of the Voyager 2 probe in 1986 and 1989. As it stands, the ice giants are two of the most under explored objects in our solar system, and raise many of the most important questions about planetary and solar system evolution. In order to complete a thorough survey of the outer planets, the Ice Giants Pre-Decadal Survey (IGPDS) decided on two areas of interest to pursue in the first flagship missions: the atmospheric composition of the ice giants, including the tropospheric 3-D flow, heat balance, and meteorology, and the composition, structure, and evolution of the ice giant satellites1. To accomplish both objectives, a flagship missionto either system would require a complex trajectory that takes into account both an atmospheric entry probe and a satellite tour of the system. Due to the lengthy flight time involved in such a mission, it would be beneficial to consider a pair of twin atmospheric entry probes in the interest of ensuring that the scientific objectives are met. Two atmospheric probes would allow for a greater spatial resolution as well as a second sampling of the atmosphere, which decreases the chances of the atmospheric probe landing in an unrepresentative region, as the Jovian Galileo atmospheric entry probe experienced in 19952. A flagship mission with twin atmospheric entry probes and a satellite tour brings forth a complex trajectory design problem when factoring in the vehicle weights, atmospheric entry locations, and launch windows of each planet. Proposal To further investigate the feasibility of a flagship mission to either Uranus or Neptune including two atmospheric entry probes and a satellite tour, I propose using two NASA trajectory design and optimization softwares, the Copernicus Trajectory Design and Optimization System and the Program to Optimize Simulated Trajectories II (POST2), in order to systematically test combinations of spacecraft weights, atmospheric probe weights, and separate atmospheric probe latitudinal/longitudinal entrances alongside traditional ballistic (chemical) trajectories and solar electric propulsion (SEP) trajectories. Methods In order to design and evaluate the various mission combinations with end-to-end optimization, I will utilize POST2 alongside the Copernicus software. Copernicus serves as the primary trajectory optimization tool for mission design at NASA, and as such, has many degrees of customizability in terms of low and high thrust trajectories.3This will allowfor the testing of various combinations of SEP in the inner solar system flight with a later transition to chemical propulsion. POST2 gives the ability to introduce multiple vehicles at any point in the simulation- these “child” vehicles inherit the state of their “parent” vehicle, and will be vital in further analyzing the trajectories of the two atmospheric entry probes once they begin separation from the parent spacecraft.4I plan to utilize the Copernicus API in order to rapidly test various configurations for the launch stage up to the entry probe separation stage of the mission, including the transition from SEP to chemical propulsion. I will then push this output into the POST2 software, which will complete the trajectory design by simulating the separation of the atmospheric entry probes, the atmospheric entrances, and the satellite tour of the remaining orbiter. I plan 1Ice Giants Pre-Decadal Survey Mission Study Report.,2017. (JPLD-100520) https://www.lpi.usra.edu/icegiants/mission_study/ 2Irwin, P. G. J.,2009. Giant Planets of Our Solar System. Giant Planets of Our Solar System: Atmospheres, Composition, and Structure, Springer Praxis Books. ISBN 978-3-540-85157-8. Springer Berlin Heidelberg, 2009. 3Williams, J. et al. “Overview and Software Architecture of the Copernicus Trajectory Design and Optimization System.” (2010). 4NASA Langley Research Center, “Overview of the Program to Optimize Simulated Trajectories II (POST2)”, https://post2.larc.nasa.gov/overview/ to write the data interchange software in either Python or Julia, a new programming software used for trajectory design, depending on the requirements of the two trajectory design softwares. The outputs of these combinations can then be compared in terms of the fastest route, the most cost-efficient route, and the lightest route. The combinations explored will be constrained by the following scientific considerations outlined in the IGPDS report5. Uranus The flight length and severe axial tilt of the Uranus brings forth many constraints to the mission: A combination of SEP and chemical trajectory could deliver a basic probe and orbiter combination to Uranus in ~11 years of interplanetary time, when considering an average vehicle, such as the Atlas V 551. Due to the axial tilt, the planet’s seasons last ~21 years. In order to observe a different season than that of Voyager 2’s observations, we must launch before the end of the 2030 decadal window so that the mission is completed before the 2049 equinox. Due to the alignment of the planets during this window, a gas giant flyby and/or gravity assist can only be considered with Jupiter. As such, the best gravity-assist sequence for this window is Venus-Earth-Earth-Jupiter (VEEJ). Neptune The flight length of a Neptune flagship mission would require covering a much larger inter-planetary distance than that of a Uranus flagship mission, and would in turn require a stronger launch vehicle. The Delta-IV Heavy or the SLS Block 1-B would lend itself better to a Neptunian system mission than the Atlas V 551, and would allow the spacecraft to complete only an Earth-Jupiter (EJ) gravity assist in order to reach Neptune. Neptune’s largest satellite, Triton, raises many questions about satellite evolution due to its retrograde orbit, and thus it may also be beneficial to explore multiple flybys of this moon during the satellite tour. Intellectual Merit Though the Uranian and Neptunian systems were brieflyanalyzed in the Voyager 2 flybys, there have been no further missions to these systems. Furthermore, the only scientific data regarding these systems are from remote telescopic observations and the limited Voyager 2 observations.6As the IGPDS report stressed the importance of a flagship mission with an optimal launch window in the 2030 decade, we must prepare a mission as soon as possible. Due to the extreme inter-planetary distances, we must optimize the mission to ensure that all scientific goals are met in both a timely and efficient manner. The inclusion of twin atmospheric entry probes would ensure that the in-situ atmospheric readings are precise and representative of the planet, while the satellite tour would allow for further exploration of the system and its evolutionary path. This proposed study will explore the trajectory design and optimization of a mission with twin atmospheric entry probes and a satellite tour, ensuring that the decades of preparation and execution involved in an ice giants mission will be as fruitful as possible. Broader Impacts A flagship mission to the ice giants would constitute a new age in space exploration, particularly if the mission included a set of twin atmospheric entry probes. As a twin probe setup has never been executed, this would revolutionize the future of in-situ atmospheric measurements, while also providing precise results for a relatively unknown part of our solar system. The trajectory considerations of a satellite tour could also result in the discovery of new satellites, as well as previously unknown chemical signatures and geographical landscapes. This study would allow for the exploration of a brand-new spacecraft and probe configuration as well as the potential for a flagship mission to one of the most under-explored areas of our solar system, allowing for technological and scientific research for decades to come. 5Ice Giants Pre-Decadal Survey Mission Study Report.,2017. (JPLD-100520)https://www.lpi.usra.edu/icegiants/mission_study/ 6Irwin, P. G. J.,2009. Giant Planets of Our SolarSystem. Giant Planets of Our Solar System: Atmospheres, Composition, and Structure, Springer Praxis Books. ISBN 978-3-540-85157-8. Springer Berlin Heidelberg, 2009.	0
Kyle T. David Introduction. Arrow worms (chaetognaths) are a phylum of free-living predatory marine plankton. They are the second most abundant zooplankton group and represent a significant proportion of marine biomass1. Despite their abundance and ecological significance, arrow worms are very poorly understood. Charles Darwin2 commented on the “remarkable… obscurity of their affinities” and in the 174 years since, arrow worms have been placed in many different bilaterian groups, including nematodes, annelids, molluscs, crustaceans, arachnids, and chordates1. Most modern molecular analyses place arrow worms within protostomes (Fig. 1), but a consensus has not yet been reached3,4,5. Internal relationships within the phylum are similarly ambiguous1,6. I will broadly sequence arrow worm transcriptomes to determine relationships within and outside the group and use these transcriptomic data to elucidate the evolution of development within animals. 3,4,5 Figure 1. Four conflicting topologies that have all been recently recovered from molecular phylogenies Aim 1: Infer a Robust Chaetognath Species Tree. The inclusion of arrow worm transcriptomes to a larger protostome dataset will add significant power to phylogenetic analyses and resolve evolutionary relationships that have confounded biologists for hundreds of years1. Aim 2: Explore the Origins of Bilaterian Development. Though considered protostomes by most modern researchers, arrow worms possess deuterostome-like development (enterocoely, secondary Photo: Michael Le Roux mouth formation, radial cleavage). This makes arrow worms uniquely positioned to explore novel questions on the origins of development in bilaterians. If arrow worms are indeed the sister-group to protostomes (Fig. 1A), it is likely that deuterostomous development was present in the bilaterian ancestor. Alternatively, if arrow worms are instead nested somewhere within protostomes (Fig. 1B-D), it is likely these features are an example of convergent evolution. Methods. The Halanych Lab has an established history of collecting and sequencing transcriptomes of non-model marine invertebrates. Our lab has sequenced and annotated 59 transcriptomes listed on NCBI’s Sequence Read Archive (SRA). I will collect specimens of at least one representative from each of the 11 arrow worm families recognized by the World Register of Marine Species. I will be able to accomplish this through a previously established relationship with Dr. Janet Voight, an Associate Curator at the Field Museum of Natural History, who has access to these groups as well as with samples previously collected from my lab. I also aim to participate in the Graduate Research Internship Program (GRIP) available to GRFP fellows, which would allow me the opportunity to intern at the Smithsonian under Dr. Jon Norenburg in order to study and sample their collections. It may be necessary to collect from the field as well, which will be possible through research cruises like the Icy Inverts Antarctica Cruises with which my lab has a history of participation. RNA samples will be extracted, prepared, and sequenced through previously validated Halanych lab protocols3. The generalized bioinformatics pipeline is represented in Figure 2. I will use the skills I have learned from my recent participation in the Workshop on Molecular Evolution to infer maximum likelihood and Bayesian gene and species trees while using a variety of model assumptions and parameters in a comparative approach. Several deuterostomes (sea urchin, acorn worm, mouse, human) will serve as an outgroup. Figure 2. Simplified bioinformatics workflow for species and gene tree inference Intellectual Merit. There is currently only a single arrow worm sequence on the SRA. This project will increase the amount of genetic data for this poorly understood group by an order of magnitude. A well resolved tree will also provide a phylogenetic framework for understanding the evolution of several key features in animal evolution and provide evidence for the ancestral bilaterian state. Arrow worms are known to have many unique features including lamellar photoreceptors7 and mosaic hox genes8 in addition to a putative whole genome duplication event9. Increasing the availability of coding sequences in this group will allow myself and others to explore expansions/losses of several significant gene families (e.g., opsin and hox genes) and test for evidence of whole genome duplication within this enigmatic group. Broader Impacts. Results will be disseminated widely to expert (i.e., publications, symposia, talks) and non-expert (i.e., Skype a Scientist, outreach events, for details see Personal Statement) audiences. Through connections already established with faculty, I will also be able present my work as a guest lecturer through Auburn University’s Alabama Prison Arts + Education Project, which provides pre-college classes to prisoners. In 2016, the New York Times reported that inmates who participate in college programs have a 4% re-offence rate, creating a 500% return on investment in prison education initiatives. Alabama law does not allow prisoners to take remote classes meaning courses must be run on-site and in-person, something that would only be possible for me to participate in with GRFP support. All assembled transcriptomes and raw reads from this project will be made publically available on the SRA. I am committed to open-source software and will continue to upload all scripts required to reproduce analyses to my public repository (github.com/KyleTDavid). I will also mentor students through the NSF Research Experience for Undergraduates (REU) program, of which my lab is a participating member in computational biology. Students will receive a primer in basic programming skills and an introduction to phylogenomic workflows, as well as an opportunity to pursue independent projects. [1] Bone & Pierrot-Bults. (1991). Oxford University Press. [2] Darwin. (1844). Journal of Natural History. [3] Kocot et al. (2017). Systematic biology. [4] Marlétaz et al. (2006). Current Biology. [5] Matus et al. (2006). Current Biology. [6] Gasmi et al. (2014). Frontiers in zoology. [7] Goto et al. (1984). Cell and tissue research. [8] Papillon et al. (2003). Development genes and evolution. [9] Marlétaz et al. (2008). Genome biology. 2	0
Background: Consistent individual differences in behavior – or “personalities” – are ubiquitous in animals and have long captivated biologists1,2. Individual differences are a prerequisite for natural selection, and many evolutionary biologists explore how variation in behavior predicts survival and reproduction. Meanwhile, neuroendocrinologists experimentally alter an animal’s internal state to change behavior. Only recently have these disciplines been integrated to begin evaluating the mechanisms that maintain natural individual differences in adaptive behaviors in wild animals3. So far, this work has advanced our understanding of the neural mechanisms of social behavior, but remarkably, has found few patterns linking brain gene expression to individual behavioral differences4,5. This suggests that top-down processes are missing key determinants of individual variation in behavior. Certainly, behavior requires more than motivation; it also requires that both brain and body are properly fueled. Thus, I propose that peripheral metabolic processes may be the fundamental force driving consistent individual differences in behavior. The liver is the primary driver of metabolism and is under high demands to power the brain and muscle to execute energetically expensive behaviors (Figure). In times of endurance, the liver undergoes ketogenesis to secrete ketone bodies into the blood for organs to use as energy6. This metabolic pathway is associated with behavioral variation: Migrating birds use ketogenesis to maintain energy homeostasis7, and racehorses have higher ketone levels during long-distance compared to short-distance races8. Given supplemental ketones, bees behave more aggressively9, and human athletes improve exercise efficiency relative to controls10. These observations suggest that variation in ketogenesis is critical for performing energetically expensive behaviors, but this has never been assessed at the individual level. Consequently, there is uncertainty about how natural selection maintains animal personalities. I hypothesize that natural individual differences in behavior stem from variation in the ability to mobilize energy. I will test my hypothesis with two specific aims, focusing on social aggression in free-living female birds. First, I will assess how natural differences in aggression correlate with: (a) hepatic HMGCS2, the rate-limiting enzyme in ketogenesis, and (b) beta-hydroxybutyrate (BHB), the main ketone body produced6. Second, I will manipulate circulating BHB and test effects on individual aggressiveness in a repeated- measures design. Both aims build off preliminary data I generated in my first year as a PhD student. Study system: Tree swallows (Tachycineta bicolor; TRES) are obligate secondary cavity-nesters; they cannot excavate a nesting site and must fiercely compete for a pre-made cavity to reproduce. Females readily take to artificial cavities (i.e. nestboxes), and they are more aggressive than males. Social aggression requires endurance, as females engage in extended aerial chases and intense physical attacks during competition for nestboxes. High aggression individuals have better body condition11 and are more likely to breed than low aggression females, showing that aggression is adaptive12. Such strong natural selection should erode this trait variation, and yet substantial individual differences in aggression persist. Preliminary data: Last spring, I conducted 5-minute simulated territorial intrusions (STIs) on free-living female TRES. I measured aggression (e.g. time spent hovering, diving, pecking) towards a conspecific decoy placed at the nestbox. Individual aggression was repeatable in consecutive STIs (R=0.90; p<0.001). 2-7 days after the last STI, I collected 10 high and 10 low aggression females and conducted a genome- wide analysis of their brains (i.e. RNAseq in hypothalamus and amygdala). Despite a well-powered design, I found very few differentially expressed genes between high and low aggression birds, indicating that substantial behavioral variation cannot be explained by differences in baseline neural gene activity. These results further support my hypothesis that behavioral differences emerge beyond the brain. AIM 1: To what degree do individual differences reflect variation in ketogenesis? From these same high and low aggression birds, I will extract RNA from the liver, where I confirmed HMGCS2 is highly expressed based on TRES transcriptomic data13. Using established lab protocols, I will design HMGCS2 primers and perform qPCR to measure HMGCS2 gene expression, running samples in triplicate for HMGCS2 plus two endogenous control genes. I will also quantify BHB concentration in 10μl of blood from these individuals, using test strips read by a handheld ketone meter that is already validated in wild birds14. I will employ linear models to examine the degree to which natural variation in aggression is predicted by HMGCS2 expression, BHB concentration, or a combination of the two. AIM 2: How does experimentally manipulating BHB alter individual aggressiveness? I will expose incubating females to a commercially available BHB cream (BPI Keto Cream) applied to a fake egg in the nest for 12 hours. As the female incubates overnight, BHB will be absorbed via the brood patch, a featherless area of vascularized skin on the belly. Control females will receive a fake egg with a vehicle cream. Past work has used this noninvasive approach to manipulate hormones in TRES15. Here, it will allow manipulation of ketone levels independent of handling-induced stress. In a within-subjects design, I will use 30-minute (prolonged) STIs to measure intensity and duration of aggression the morning before and after BHB (or control) treatment, analyzing results with a repeated-measures ANOVA. Blood BHB will be quantified in both groups after the second STI using the ketone meter described in Aim 1. I will also separately validate that BHB treatment elevates BHB blood concentration in a subset of birds. Predictions, Alternatives, & Next Steps: I predict that individual variation in aggression will positively correlate with both HMGCS2 and BHB, with greater levels in high vs. low aggression individuals (Aim 1). Likewise, I predict that individuals will increase aggressiveness in response to supplemental BHB (Aim 2). Support for my hypothesis in Aim 1, but not Aim 2, would suggest that birds must engage in prolonged competition to promote ketogenesis, considering that females in Aim 1 were unprovoked at the time of collection. In this case, a future step would be to assess ketogenesis during sustained competition by manipulating nestbox availability, which is shown to increase aggression and metabolically challenge the brain16. I am also well-positioned to explore additional tissues from these same birds, such as the pectoral muscle where BHB is converted into usable fuel6. Nearly all TRES fighting occurs in flight, suggesting the pectoral muscle is a promising tissue to connect energetic constraints to social behavior (Figure). Intellectual Merit: Individual differences serve as the raw material for evolutionary change, leading to the diversity of behaviors seen in nature. However, we have limited insight into the origin of this variation. My work explores the potentially critical role of peripheral energetics in shaping natural individual differences in the wild. Recent work reveals other routes by which the periphery influences brain and behavior (e.g. gut-brain axis, microbiome), a view that my research extends. In the long-term, my proposal will not only build the foundation of my dissertation, but it will also serve as a springboard for applying energetic perspectives more broadly, to understand how metabolism accounts for diverse behavioral differences within and among species. Ultimately, my work will examine how both evolutionary and proximate mechanisms work together to build an aggressive female, an overlooked perspective in a field that, since Darwin, often assumes that females do not compete or that their aggression is just like that of males. Broader Impacts: As a first-generation, biracial graduate student, I strive to diversify STEM by helping historically underrepresented undergraduates overcome institutionalized barriers, thereby demystifying academia’s hidden curriculum. My efforts include the creation of a “how to” guide for applying to graduate school, which I disseminated to local and national groups. As a co-facilitator of an anti-racism group at Indiana University (IU), I developed action plans to hire and support diverse undergraduate researchers in my lab. These efforts set the foundation for my goals as a graduate student and future faculty member to improve recruitment and retention in STEM. Mentorship is central to this plan. I honed these skills with mentorship training while working with an undergraduate mentee in IU’s Center for the Integrative Study of Animal Behavior NSF REU summer program. Moving forward, I will work with the Jim Holland Summer Enrichment Program, which provides research experience for high-achieving minority high school students and helps them transition into an IU STEM major, extending my efforts to broaden inclusion. References: 1Koolhaas et al. Front Neuroendocrinol (2010). 2Sih et al. TREE (2004). 3Hofmann et al. TREE (2014). 4Bell et al. Behaviour (2016). 5Benowitz et al. Behav Ecol (2019). 6Grabacka et al. IJMS (2016). 7Frias-Soler et al. Biol Lett (2021). 8Volek et al. Metabolism (2016). 9Rittschof et al. J Exp Biol (2018). 10Dearlove et al. Med Sci Sports Exerc (2021). 11Rosvall. J Avian Bio (2011). 12Rosvall. An Behav (2008). 13Bentz et al. Sci Rep (2019). 14Sommers et al. J Field Ornithol (2017). 15Vitousek et al. Proc B (2018). 16Bentz et al. PNAS (2021).	0
for the Multiple Mirror Telescope There are planets where it rains rubies. Specifically, some planets orbiting alien suns show evidence of clouds made of corundum, which is the basis on earth for rubies and sapphires (Wakeford, 2016). This is a romantic discovery from observing exoplanet atmospheres, but more practical research looks for possible signs of life on other planets, be it bacteria or dog in an alien suit, in the form of combinations of molecules that indicate non-equilibrium chemistry (e.g. water, oxygen, methane, ozone). In ground based observatories, research of this nature is only made possible with adaptive optics (a system that corrects observations in real time by using wavefront sensors to observe the shape of incoming light). Infrared (IR) observatories, like the Multiple Mirror Telescope (MMT) have the potential to observe these signs of life, but at present lack the instrument sensitivity or wavelength range to achieve these goals. In collaboration with the NSF funded project MAPS (the MMT Adaptive optics exoPlanet characterization System) I will test, model, integrate, and perform on sky commissioning with two IR and visible pyramid wavefront sensors on the Multiple Mirror Telescope. This proposed upgrade to the Multiple Mirror Telescope provides observations of fainter targets yet to be observed, increases wavelength range, and tests new wavefront sensing techniques that inform the next generation of telescopes. Intellectual Merit : In the field of exoplanet astronomy, ground-based observations are limited by the Earth's turbulent atmosphere, which causes incoming light to a telescope to spread irregularly over a larger area. When the object is a faint point-like source, better adaptive optics systems enable observations of star-planet-systems that we could not have seen before. With adaptive optics (AO), observations are corrected in real time by applying a correction (calculated by wavefront sensors) with mirrors that deform to correct the atmospheric perturbation. Figure 1 demonstrates an example of how light appears after corrections by adaptive optics at Lick Observatory, as well as the predicted improvement to the AO system at MMT with the MAPS upgrade. Figure 1. Left : Lick Observatory image with and without applied correction to the wavefront. (Max, 2019) Right : Using Strehl (the ratio of peak intensity for an image with and without aberrations) as a metric, we can see the predicted performance upgrade due to AO on MAPS through J and H bands (the IR regime) (Morzinski, 2018). My proposal is unique because I plan to incorporate the two wavefront sensors in a single system with a dynamic choice of IR or visible wavefront sensing, drastically improving image quality for the final IR science images. The observer will choose which wavefront sensor will provide better correction for their observation; for a source that is significantly brighter in the IR or visible, collecting more photons means faster and more accurate corrections, which is especially vital when the atmosphere is moving in real time. To that end, the switch to pyramid wavefront sensors will provide an additional improvement to photon collection over the present Shack-Hartman wavefront sensor installed on MMT. Additionally, two new detectors will be used for the wavefront sensors: a CCID-75 visible detector (which is new to astronomy applications), and a SAPHIRA IR avalanche photo diode detector; both have reduced readnoise over the current detector used for wavefront sensing with MMT (Morzinski, 2018). As shown in Figure 1, the proposed improvements to the AO system will provide a 40-50% improvement in the observed intensity in the IR as compared to the existing MMT system. Plan of Work : (1) Run an initial analysis to select our exact infrared regime. Specifically I will predict if the J band (1.1-1.4 microns), H band (1.5 – 1.8 microns), or an overlap, will be better for our proposed targets. This will be informed by my work with the Exoplanet Characterization Tool Kit, with an emphasis on the atmospheric retrieval tools contributed by Mike Line (Fowler, 2018.) I will evaluate to what extent water, carbon monoxide, and carbon dioxide (the molecules we expect to find in our initial round of Jupiter-like targets) are observable in these proposed bands given varying host stars shining through varying planetary atmospheres. (2) Perform initial experiments to setup and test the two wavefront sensors in the optics lab facilities at the University of Arizona. Specifically I will integrate the two wavefront sensors in the Arizona lab facilities and find ways to programmatically take images and control hardware on the testbed. This work will be streamlined by my work on GLARE (the Generalized Lab Architecture for Restructured optical Experiments), a Python suite of automated experiment software and testbed- agnostic controllers for hardware common in optical testbeds (Fowler, 2020). (3) Perform further testing to reduce and correct for alternate noise factors including dark current, readnoise, optical ghosts, etc. Specifically, I will generate multiple images, isolate signs of these alternate noise factors, and test alternative hardware configurations and modes and/or calibration software to optimize final image quality. My work on the Wide Field Camera 3 Quicklook project (a codebase including a dark current and readnoise monitor) will inform the detection and removal of noise from these experiments. (4) Integrate the wavefront sensors into MMT alongside on-sky commissioning of the full adaptive optics system. Unique Resources : Many of the investigators of MAPS are at the University of Arizona, including Dr. Katie Morzinski (the principal investigator and an Assistant Astronomer) who will advise me for this project. The University of Arizona has two lab spaces that will support this project, as well as a vibrant instrumentation group to support and facilitate this work. The University of Arizona has unfettered access to MMT as well as 50% of its telescope time for calibration observations and experiments, and as it is local to the university at Mount Hopkins, we can actively iterate with MMT and the lab to test and improve new components. Broader Impact : The original NSF MAPS proposal includes a Winter School, a brief winter workshop aimed at graduate students, postdocs, and professionals to teach the science of exoplanet instrumentation. The Winter School is based on previous NSF Professional Development Program funded programs like Adaptive Optics Summer School led by the Center for Adaptive Optics. As part of this work, I will design a lab demonstration exploring the distinction between an IR and visible wavefront sensor. My experience as a teaching assistant and Software Carpentry instructor will facilitate creating and leading a lab for my colleagues, under the advisement of Dr. Morzinski who is leading the Winter School. Fowler, J. et al. “G.L.A.R.E..”, AAS Meeting #235, 2020 --- Fowler, J. et al. “ExoCTK”, AAS Meeting #231, 2018 --- M., Claire “Introduction to AO and the CfAO.” AO Summer School. 2019. --- Morzinksi, K. “MAPS: The MMT AO ExoPlanet Characterization System .” Cf AAO Retreat. 2018. --- Wakeford, H. R. et al. “High- Temperature Condensate Clouds in Super-Hot Jupiter Atmospheres.” MNRAS, (2016)	0
Keywords climate change; exoskeleton; crustacean; calcification; gene expression Introduction Predicted ocean acidification levels (OA) have been shown to change calcium carbonate structures of taxa ranging from corals to oysters, many of which experience decreases in calcification.1,2 Surprisingly, crustaceans appear to increase calcification, likely because of their osmoregulatory capacity.3 Increased temperature can also hinder internal pH regulation,4 so multivariate experiments are integral to understanding real-world responses of crustaceans to future ocean conditions. The California spiny lobster, Panulirus interruptus, is the fifth most important fishery in Mexico and is fished recreationally by 30,000 people yearly in California alone.5,6 As urchin predators, they play an important role in kelp forest ecology by reducing urchin disease and macroalgae overgrazing.7 P. interruptus rely on their calcified exoskeletons as armor to prevent predation and as a tool for sound production to warn away predators.8 Predicted changes in ocean conditions could alter the integrity of the lobsters’ exoskeleton, ultimately affecting its predation and, given the species’ key role in the ecosystem, producing far-reaching trophic effects. Proposed Research To determine the potential impact of OA and increased temperature (hereafter referred to as multiple stressors) on P. interruptus exoskeletons, I will integrate tools from biomechanics, genetics, and ecology. I aim to understand how multiple stressors potentially affect (1) exoskeleton morphology, (2) gene expression, and (3) defensive sound production. Experimental Design Juvenile P. interruptus are readily collected along the Southern California coast. Sixty-four animals will be maintained at Scripps Institution of Oceanography (SIO) in an existing flow-through, experimental aquarium system for six months. In four header tanks that feed to individual animals, I will maintain four combinations of pH and temperature: ambient conditions, and pH and temperatures that are adjusted to reflect changes predicted by the year 2100 (pH reduced by 0.3; temperature increased by 3°C).1 All equipment is available in my advisor Dr. Jennifer Taylor’s lab or open for use on campus. Aim I: Exoskeleton morphology I will test the hypothesis that lobsters will respond to multiple stressors with increased calcification. I will examine exoskeleton ultrastructure using scanning electron microscopy (SEM) and then quantify elemental composition using both inductively coupled plasma mass spectroscopy and energy dispersive x-ray analysis. Results will establish potential changes in mineralization and thickness of the lobster exoskeleton. Aim II: Regulation of calcification Spiny lobsters will likely respond to multiple stressors by regulating genes like those that concentrate and bind Ca2+ near the calcification site.9 With assistance from Dr. Ron Burton’s lab (SIO), tissue from all treatments will be combined and RNA-seq will be used to assemble a reference transcriptome. After its annotation, I will compare transcriptomes of three animals per treatment against the reference to look for up- or downregulation of epidermal genes. Results will establish the important link between environmental conditions and morphology in P. interruptus. Aim III: Anti-predation strategies Increases in calcification due to multiple stressors tend to confer hardness but also brittleness.10 Attacks may require greater bite forces, yet successful bites may be more catastrophic. I will measure the hardness, stiffness, and brittleness of the exoskeleton using materials testing machines. Results from this study will reveal how multiple stressors affect the integrity of the calcified exoskeleton. These data will be compared to published analyses of the bite mechanics of common predators like horn sharks.11 In addition to armoring, the exoskeleton is critical for other anti-predation strategies. Spiny lobsters rub their antennae across a file near the eye to produce a sound that startles away potential predators.12 Individuals will be stimulated with a predator model while behavior and sounds are recorded using video and acoustic techniques. The file will then be excised and examined using SEM for any ultrastructural changes that may influence ability to produce sound. Alternative hypothesis It is possible that the natural range of temperature and pH experienced by P. interruptus makes them more adaptable to forecasted changes in environmental conditions. Thus, if my hypothesis in Aim I is not supported and there are no morphological responses to multiple stressors, I will expand this research to include multiple spiny lobster species that inhabit different environments. Conducting this experiment across multiple species within a phylogenetic framework will provide robust information about how this important group of animals will respond to climate change. Intellectual merit Studying potential morphological changes and the underlying mechanisms will help us understand how these animals may adjust to predicted ocean conditions and determine if these responses are robust or if they leave them vulnerable to other threats like predation. Examining morphological changes that may impact anti-predation mechanisms elucidates how OA may not only affect individual species, but potentially have much larger community impacts if predation rates change. Spiny lobsters must contend with fishery pressure too, and it is important that managers anticipate any changes in population structure due to a changing environment. This study focuses on juvenile lobster that are not sexually mature until age 4-6, so results will help us understand how potential changes in predation will impact recruitment of mature and legal-size spiny lobster.13 Broader impacts Expanding opportunities: I will involve undergraduate lab volunteers and recruit students in the Scripps Undergraduate Research Fellowship (SURF) program, which provides summer research experience to undergraduates that do not otherwise have access to ocean science opportunities. Promoting education: Given their recognition in Southern California, spiny lobsters create an avenue to communicate climate change impacts with the general public. As such, I will work with SIO’s Birch Aquarium to expand their OA and spiny lobster education programs and develop the results of this research into a SEA day at Birch Aquarium, a half-day event where children learn science through hands-on activities. I will plan a similar event for Expanding Your Horizons San Diego, an annual conference designed to interest young girls in STEM fields. Fishery impacts: I will complete a report for the CA Dept. of Fish and Wildlife’s Spiny Lobster Advisory Committee that clearly details how my results may show changes in juvenile survival, especially via potentially increased predation. I will plan a forum meeting through my interdisciplinary program, which includes graduate students from UC San Diego and El COLEF in Tijuana, Mexico, and invite fishery managers to discuss this regional issue. References [1] Solomon, S. 2007. Cambridge University Press. [2] Ries, J. et al. 2009. Geology 37: 1131-34. [3] Whiteley, N.M. 2011. Marine Ecology Progress Series 430: 257-71. [4] Dove, A.D.M. 2005. Journal of Shellfish Research, 24: 761-65. [5] Castañeda-Fernández-de-Lara, V. et al. 2005. New Zealand Journal of Marine and Freshwater Research 39: 425-35. [6] Neilson, D. 2011. California Department of Fish and Wildlife. [7] Lafferty, K. 2004. Ecological Applications 14: 1566-73. [8] Patek, S.N. et al. 2007. The Journal of Experimental Biology 210: 3538-46. [9] Luquet, Gilles. 2012. ZooKeys 176: 103-21. [10] Wainwright, S.A. 1982. Princeton University Press. [11] Huber, D. et al. 2005. Journal of Experimental Biology 208, 3553-71. [12] Staaterman, E. R. et al. 2010. Behaviour 147, 235-58. [13] Engle, J. M. 1979. PhD Thesis, University of Southern California, 298pp.	0
Feature Discovery in Link Mining Keywords: link mining, feature discovery, machine learning, graph theory, relational data Background: Traditional data mining approaches attempt to find patterns in a data set characterized by a collection of independent instances of a single relation. This is consistent with the classical statistical inference problem of trying to identify a model given a random sampling of an underlying distribution. A key challenge for machine learning is the problem of mining more richly structured data sets in a way that leverages the linkages between records [1]. In this paradigm, which more accurately resembles real-world data, instances in the data set are relational where different samples are related to each other, either explicitly as typified by friendship relationships in a social network, or on the web by hyperlinks [2]. However, in most large data sets, relationships also exist that are not explicitly annotated. According to Jensen, naively applying traditional machine learning methods to this type of data can lead to inappropriate conclusions [3]. Therefore new approaches are needed to appropriately correlate inherent relationships (i.e. links) in real-world data sets. In recent years, there has been a growing interest in learning from structured, real-world data. This type of data can be described by a graph where the nodes in the graph represent objects, and edges in the graph represent relationships between objects. Perhaps the most famous example of exploiting link structure is the PageRank algorithm [4] employed by the Google search engine. Link mining is situated at the intersection of graph theory, machine learning, and web mining. This research is potentially useful in a wide range of application areas including bio-informatics, bibliographic analysis, financial analysis, national security, social network analysis, and internet search to name a few. While my research is focused more on the theoretical aspects of this topic than in the applicative possibilities, I was happy to see that my work has already been adapted to the bioinformatics domain to study the interactions of proteins [5]. Research: Despite the recent advances in link mining, this topic is still relatively new and there are many fundamental challenges that remain. Unlike more mature fields of research there does not exist any public package or toolkit that provides a standard baseline from which to explore. Therefore, I propose to create a link mining framework that adapts several of the core principles of link and graph mining into a scalable, shared package. This toolkit would be an essential research and teaching tool similar to the University of Waikato’s WEKA toolkit [6] or George Mason’s ECJ system [7]. Initially, this project would only incorporate fundamental and highly- extendable principles of link mining, but most importantly it will serve as a launch-pad for more interesting, collaborative theoretical work. With a core link mining package in place I propose to study the dynamic temporal and graphical nature of relationships within various domains in order to advance the theory of and methodology for determining probabilities of link existence where none are explicitly annotated. This process involves several steps. First a domain must be selected that exhibits the relational attributes applicable to the link mining paradigm. Data from social networks, protein inter- actions, citations, microarrays, etc. all contain necessary attributes; therefore this step is arguably the most straightforward because many real-world data sets are inherently relational [1]. After the domains are defined, features that describe the relationships need to be extracted. For example, friendship in a social network is annotated by the inclusion of the friend’s name on a Tim Weninger 2008 NSF Graduate Fellowship Research Proposal user’s homepage. Pair-dependent features, such as the size of the intersection of interests, etc., offer supplementary evidence for the existence of a friendship. These pair-dependent features will be used to determine the probability for link existence where it is not annotated. Finding the non-obvious pair-dependent features is arguably the most difficult part. Therefore, I propose the use of recent developments in association rule mining and frequent pattern mining by Dr. Jiawei Han et al. [8] to find correlations between data points that best suggest link existence. Furthermore, the general problem of feature selection, extraction and discovery is widely regarded as the most important factor in machine learning [9]. Besides pair-dependent features, I propose to explore the role that graph features have in identifying relationships that lack explicit annotation. In my experience, graph features, such as the shortest path distance between candidate vertices, offer the best support (in terms of entropy) for the existence or absence of links. The major problem with this approach is that extracting graph features is computationally expensive for sufficiently large graphs. Although I have begun work on developing fast, approximate search algorithms I will need to formalize and empirically study these methods. Finally, these features will be used by traditional machine learners to derive information about relationships in data sets. In each step, theories would be tested using the aforementioned link mining toolkit in order to efficiently derive empirical results. I plan to advertise and freely share my toolkit, and continue to present and publish results at refereed conferences and in refereed journals on a regular basis. While my research generally aims to expand the theoretical and computational potential of machine learning, the implications of link mining research can already been seen in the biological, physical and social sciences, and many researchers believe that the application of link mining techniques will continue to grow as more research is conducted. With help from the NSF GRF I intend to study at the University of Illinois Urbana-Champaign (UIUC) where the Data Mining Research Group led by Dr. Jiawei Han (reference letter writer) and I already have a working relationship. I believe that Dr. Han and his colleagues at UIUC are among the best researchers in the world, and they would provide the wisdom and expertise necessary for me to continue my work in this fascinating field. References: [1] Lu, Q., Getoor, L., “Link-based Classification”. ICML'03, Washington DC, 2003. [2] Sen, P., Getoor, L., “Link-based Classification”. University of Maryland CS-TR-4858. 2007. [3] Jensen, D., “Statistical challenges to inductive inference in linked data”. In Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics. 1999. [4] Page, L., Brin, S., Motwani, R. & Winograd, T. “The pagerank citation ranking: Bring order to the web”. Technical Report. Stanford University. 1998. [5] Paradesi, M.S.R., Caragea, D. and Hsu, W.H., “Structural Prediction of Protein-Protein Interactions in Saccharomyces cerevisiae”, IEEE-BIBE'07, vol. 2, Boston, MA, Oct. 2007. [6] Witten, I. H. and Frank, E., “Data Mining: Practical machine learning tools and techniques”, 2nd Edition, Morgan Kaufmann, San Francisco, 2005. [7] ECJ: A Java-based evolutionary computation research system, 2006. http://cs.gmu. edu/eclab/projects/ecj/ [8] Han, J., Pei, J., & Yin, Y., “Mining frequent patterns without candidate generation”, International Conference on Management of Data ACM-SIGMOD'00, pp. 1-12. 2000. [9] Caruana, R, Niculescu-Mizil, A., “An empirical comparison of supervised learning algorithms”. ICML'06, pp. 161-168, Pittsburgh, PA, 2006.	0
One of the most important unknowns in high-z extragalactic astronomy is how reionization occurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominates the intergalactic medium (IGM). HI attenuates radiation from early stellar populations, masking galaxies from detection. Understanding how and when reionization occurs can reveal whether or not these young galaxies provided the necessary ionizing radiation to completely reionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the high redshift-space this implies, spectroscopic observations are limited as these galaxies are very faint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths. My background in NIR spectroscopy and observational astronomy has prepared me to assist in addressing this question. I propose using Lyα and CIII] to investigate the properties and ionization state of young galaxies using ground- and space-based telescopes, the structure and distribution of HI in the IGM and the circumgalac- tic medium (CGM) of certain galaxies, and implications for the evolution of the neutral fraction of the IGM throughout the EoR. The individual points proposed will be summarized as follows: (i) small scale testing and building of an analysis technique, (ii) distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. By understanding more about the IGM during the reionization era and of the galaxies within it, we can further constrain the properties of current galaxy evolution and reionization models. Small scale testing & building of analysis technique: In the search for galaxies during the EoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric 0 surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UV bright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring the escape fraction of Lyα many studies have inferred an increasingly neutral fraction of the IGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emission hundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent method uses a complementary spectroscopic tracer not attenuated by HI, with the UV metal line CIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5]. 0 In my current work, I measure CIII] H-band emission of galaxies found via Lyα emission using Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) and attenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGM of these galaxies and surrounding IGM. From my previous work experience, I have developed a proficiency in coding which enabled me to gain a close familiarity with the MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fix bugs often encountered when working with incredibly faint emission lines and less common dithering patterns for standard star observations. I wrote code to optimally extract my 1D spectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage of IRAF and Python, I developed code that can track the photometric variability of my data from a frame-to-frame basis – important when working with faint emission lines. Distribution of galaxies and evolution of neutral fraction: Using the foundation built from my previous work, I will build a statistical sample of galaxies during the last half of the reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escape fraction as a function of redshift. Using my optimized extraction technique to improve Taylor A. Hutchison 2 measurements, I will use this dataset to constrain the offset between these galaxies’ systemic and attenuated redshifts. This work will significantly increase the sample of high-z galaxies with both Lyα and CIII] measurements. In addition, it will provide a more significant comparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses. A current complication for this project is the lack of a complete spectroscopic sample of LAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines or the [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to 0 NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes – it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in the range of NIRCam on JWST. As a first approach to resolving this, I will take the current sample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using my optimized extraction technique to improve measurements. This has already been attempted for some galaxies [4], providing useful lower limits for determining exposure times and potential telescopes for future observations, including JWST. We are planning proposals for the first JWST cycles for this work. I will then take advantage of the deep multi-wavelength imaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I am part of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with early indications that the proposal has so far been successful. Finally, as a scientific collaborator on an ERS JWST proposal, I will prepare for access to that data – understanding what spectra I will be looking for from running binary stellar population models (eg. BPASS), scaled to match expected bandpass magnitudes, through the JWST exposure time calculator. With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will be able to further constrain the amount of hard radiation emitted from these galaxies; as shown with mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5]. By tracking its evolution through the last half of the EoR, I can inform current reionization models. Lastly, through gathering my sample I will map out the distribution of these galaxies, identifying whether galaxies with large escape fractions are tracing over-dense luminous regions, located within large ionized bubbles [3,8]. Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space- based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of the fluxes of the CIII] doublet, when measurable, I can infer an estimate of the electron density of the gas in the CGM. This is closely linked to the metallicity of the CGM, which directly affects the velocity offset of Lyα emission. Not only do recent studies indicate that a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, they also suggest (from mid-z analogs) a strong link between the profile of Lyα emission and the properties of the gas within the CGM [5]. This can be incredibly important as some high-z galaxies have been found to have more symmetric Lyα profiles, contrary to the archetypal asymmetric shape, thought to be indicative of high star formation and galactic winds [6]. Understandingthe rateand distributionof reionization, including thefactors andprocesses responsible for it, remains one of the most important unknowns in extragalactic astronomy. My work will aim to shed more light on this question, enabling more precise modeling of thiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies. References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erb et al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004	0
extraordinarily challenging by the extreme starlight suppression required to image faint planets around brilliant stars. The noise-limited performance of current high-contrast imaging instruments can resolve planets up to 10 million times dimmer than their host star. In order to access Earth-like planets – the highest-priority planetary targets named by the Astronomy and Astrophysics Decadal Survey [1] – sensitivity must be expanded to planets 10 billion times fainter than their star. The primary limitation on increasing contrast is speckle noise, which is scattering of the stellar point spread Fig. 1: Raw image of the function (PSF) that can mimic or obscure planet signals [2]. As observation planetary system HR8799. time increases, the total noise contributions of read noise and photon noise Four Jupiter-size planets attenuate, but speckle noise does not, establishing a high noise floor that are obscured by quasi- cannot be reduced without removing the speckles themselves. I propose to static speckles. Image: Dr. develop a computational method of speckle subtraction for data taken C. Marois with the Gemini Planet Imager (GPI), which will improve the precision of existing data and will enable future higher-contrast observations of exoplanets. In addition to improving the sensitivity of legacy data, the proposed work will provide timely support for the funded GPI upgrade beginning in 2020 and returning to science operations with commissioning of GPI 2.0 in 2023. Study Design: Speckle noise is created as starlight passes through non-uniform atmosphere and optics, producing a stellar PSF that varies with time. Atmospheric speckles can only be corrected by improved adaptive optics hardware. This leaves “quasi-static” speckles caused by non-common path aberrations within the instrument optics. Quasi-static speckles change slowly over timescales of minutes to hours [3], resulting in an effect that varies both chromatically and temporally. This project aims to model, and subsequently remove, these quasi-static speckles using the non-parametric technique of principal component analysis (PCA). PCA identifies “principal components” as linear combinations of input parameters, producing a dimensionally reduced result which identifies the strongest predictors of the features of that data. These results can be used to subtract the quasi-static speckles directly. Previous applications of PCA to high-contrast imaging (e.g. [4, 5]) have focused on subtracting the entire stellar PSF, both atmospheric and quasi-static speckles, which is useful for recovering target signal but results in improvement for only the dataset it’s applied to. The technique of applying PCA to isolated quasi-static speckle noise has never been successfully applied to exoplanet high-contrast imaging, but will result in a more flexible, broad correction for this type of noise. By characterizing quasi-static speckle behavior and evolution over given epochs and wavelengths, corresponding corrections can be applied not only to the training data, but any data of matching instrument, epoch, and wavelength. This method may also reveal stable speckle behavior which is present at all times and wavelengths, and can be universally subtracted. Once applied, the precision of legacy data is expected to improve by one order of magnitude, and this speckle subtraction will allow future GPI observations to achieve greater contrast by approximately two orders of magnitude [6], making important steps towards accessing Earth-like planets. The phases of this project are outlined below. Phase I: First, I will build a training dataset of GPI science images containing isolated speckle noise, which can be achieved by subtracting a noiseless model of the stellar PSF from all training data. This will leave only unexplained noise behind, the primary component of which is quasi-static speckle noise. Phase II: Grouping the training dataset over discrete time increments and wavelength intervals, I will apply PCA to the training data. The results of this analysis can be used to subtract an estimate of the stable components of quasi-static speckle noise from the data. Phases I and II will take place during years 1 and 2 of graduate school, which is well-timed to inform the concurrent GPI upgrade. Phase III: Then, I will quantify the effectiveness of this correction procedure by measuring the improvement of the intrinsic noise present in each subtracted image, as well as by performing injection- recovery tests with simulated planet signals. Phase IV: Finally, once these results have been verified, I will develop and release an open-source codebase for quasi-static speckle subtraction, intended for use by scientists working with GPI data. Phases III and IV will take place during years 3 and 4 of graduate school, which will align with the commissioning of GPI2.0 allow for my results to be folded into its data reduction pipeline. One anticipated challenge associated with Phases I and II is that speckle noise is difficult to isolate in legacy GPI data, either because accurate reference PSFs cannot be generated, or because the data contains systematics which may confuse the subsequent analysis. In this case, since GPI will be present at Notre Dame for upgrades, I will be able to collect data directly from the instrument using the telescope simulator operated by the Chilcote group. By allowing more control over observing conditions and precise knowledge of the input PSF, data taken using the telescope simulator will enable a cleaner first-step analysis, allowing more robust treatment of the legacy data when it is later re-introduced. Another anticipated problem is that GPI2.0 goes on-sky before Phase IV concludes. However, modifications to the processing pipeline can still be made after science operations commence since post- processing can be retroactively applied. Additionally, even if completion of Phase IV lags, the robustness and impact of this technique will be well understood from Phase III, and observations can be planned in anticipation of the correction tools of Phase IV being completed in the future. Intellectual Merit: The importance of increased contrast for high-contrast imaging campaigns is crucial even beyond exoplanets, with implications for the direct imaging of all astrophysical objects at small angular separations from a comparatively bright source — including circumstellar disks, stellar winds, or jets emitted from neutron stars, pulsars, or black holes. This project is a high-impact, far-reaching, and low-cost avenue to increasing the science yield of existing direct imaging instruments, increasing the sensitivity of extant and future data without the need for the expensive and prolonged development of new instrumentation. Additionally, while the proposed solution will be built specifically for GPI, it can be adapted to interface with other ground-based high-contrast imaging instruments, including SPHERE on VLT and CHARIS on the Subaru Telescope. A software-based speckle subtraction method also has strong implications for the development of space-based direct imaging missions, such as the Nancy Grace Roman Space Telescope (formerly WFIRST), as speckle noise is similarly dominant in space-based direct imaging [7]. The open-source release of my work will facilitate broad advancement and collaboration across astrophysics subdisciplines and different instrument teams. Years of previous research experience, including publishing papers, presenting at conferences, giving talks, and directing analysis, have prepared me to effectively execute the proposed work. I have years of experience with data analysis and visualization with Python, and as part of my work with CERN and GPI have worked with large datasets and distributed computing. I currently work on GPI with Professor Jeffrey Chilcote at the University of Notre Dame, so I am familiar with the instrument and am prepared to hit the ground running. I will also benefit from the technical expertise and support of the international GPI collaboration as I develop this project. Given the large volume of legacy data which will be analyzed during this project, as well as the computationally demanding nature of PCA, I will require access to high-performance computing facilities such as the Notre Dame Center for Research Computing. Broader impact: The NSF GRFP will support me to pursue high-impact research alongside community engagement. Alongside this speckle suppression project, I will establish a peer mentorship program at my graduate institution, as well as develop curricula and workshops to engage public elementary students in space science, both of which I detail in my personal statement. I will integrate these engagement efforts with the GPI Outreach team to create science communication materials conveying the excitement of squinting through stellar glare to find new worlds orbiting underneath. [1] National Research Council 2010, New Worlds, New Horizons in Astronomy and Astrophysics [2] Marois, C., Doyon, R., Nadeau, D. et al. 2003, EAS Publications Series, 8, 233-243 [3] Hinkley, S., Oppenheimer, B., Soummer, R. et al. 2007, ApJ, 654, 1, 633-640 [4] Wang, J., Ruffio, J.-B., De Rosa, R., et al. 2015, Astrophysics Source Code Library, ascl:1506.001 [5] Soummer, R., Pueyo, L., Larkin, J. 2012, ApJL, 755, 2 [6] Soummer, R., Ferrari, A., Aime, C. et al 2007, ApJ, 669, 1, 642-656 [7] Brown R., Burrows C. 1990, Icarus, 87, 2, 484-497	0
Unlocking success: Neurobiological correlates of grit in adolescents. Intellectual Merit: During adolescence, the brain undergoes extensive structural and functional development. Specifically, adolescence is characterized by differential development of reward circuitry and cognitive control systems such that cognitive control regions are relatively underdeveloped compared to reward processing regions.1 Although adolescents are able to reason about risky decision making, they are also vulnerable to social influences. In emotionally salient conditions (e.g., the presence of peers), the maturity of adolescent reward circuitry compared to the less mature prefrontal control system appears to exacerbate risk taking that results in negative outcomes (negative risk taking).2 However, adolescent differential brain development and vulnerability to social influences may also lead to greater recruitment of cognitive control processes used to engage in risk taking that results in positive outcomes (positive risk taking), like “grit”. Grit is defined as the determined pursuit of a superordinate goal in the face of failure.3 Higher levels of grit are associated, over and above IQ, with objectively measured successes (educational attainment, GPA)4 and greater well-being.5 Neurobiological investigations of behavior can corroborate and challenge our assumptions regarding the neural mechanisms underlying motivational, cognitive, and affective components of risk taking. Despite the large body of research investigating negative risk taking, there is a gap in knowledge regarding the neural mechanisms of positive risk taking and whether these mechanisms differ from negative risk taking. Novelty: The brain-based mechanisms of positive risk taking remain unknown, and the only empirical investigations of grit are through self-report. This study will address gaps in our understanding of the association between negative and positive risk taking in adolescence, provide the first ecologically valid experimental manipulation of grit, and will determine how grit behavior relates to external measures of success (e.g., GPA). Experimental Design: Participants will consist of 60 adolescents (14-18 yrs).6 Participants will undergo an MRI desensitization procedure in the Galván Lab mock scanner before completing a novel computer task in an fMRI scanner. The fMRI task, the “Grit Task”, is a money-earning paradigm I created that builds on extensive delay of gratification and delay discounting literature. There are two types of trials, each worth a fixed amount, lower-value trials (LVTs) and higher-value trials (HVTs). Participants must choose to perform either LVTs or HVTs before beginning (path selection). Participants who select the HVT path will be considered “delayers” who have higher grit than those who select the LVT path. If LVTs are selected, money earned will be paid at the end of the session up to $10 max. If HVTs are selected, money earned will be paid in 1 week at a min. of $20, max. $30.7 In addition to the delay in payment, the HVT path will require completion of a mental rotation task (MR task; participants must mentally rotate two 3-D figures and determine whether they are identical) between each money-earning trial. Requiring completion of the MR task will improve ecological validity compared to delay of gratification measures that traditionally do not require completion of an effortful task to achieve higher-value rewards. For example, college success requires continued goal-oriented pursuit, not simply an initial decision to delay the receipt of reward for a greater reward. Prior to path selection, all participants will practice the MR task. Participants will be told they must successfully complete (unlimited attempts) the MR task before each money- earning trial if the HVT path is selected. Traditionally, MR tasks are used as a measure of spatial processing, however here the MR task will facilitate manipulation of “failure”, an essential element of grit. Participants will be told, regardless of performance, that they have failed at some MR task attempts (randomized). This will require that participants sustain their choice of the HVT path and continue to attempt the MR task to receive the higher reward. Between each money-earning trial delayers will decide whether they want to continue with the HVT path or switch to the LVT path (reward decisions). Path selection and subsequent Graduate Research Plan reward decisions are proxies for grit. Those choosing to continue on the HVT path and perform the MR task after they have failed will be considered more “gritty” delayers. Delayers who subsequently switch to the LVT path, and participants who select the LVT path at the outset, will remain on the LVT path and will be capped at the LVT max award. Restricting low-to-high switching and setting min/max awards for each path will minimize strategizing. On the LVT path participants will view the MR stimuli before money-earning trials but will not be required to complete the MR task. MR tasks have been successfully used in adolescent fMRI studies and adapted to eliminate gender differences. Validated survey measures will assess (1) supportiveness of adolescents’ home and peer environments,8 (2) grit and impulsivity,9 (3) academic achievement, optimism, IQ, self- esteem, performance anxiety, and well-being.10 The Stoplight Task (ST), a computerized fMRI task in which participants drive a virtual car, will be administered to determine whether gritty individuals are prone to more negative risk taking. In the ST, participants decide whether to brake as the car approaches a yellow light at an intersection. Not braking results in a higher crash risk but also a potentially higher monetary reward for finishing quicker. Anticipated Findings: On the Grit Task, more gritty individuals will exhibit greater: (1) perseverance on the Grit Task, (2) activation in mesolimbic reward circuitry (ventral striatum) at delayed reward presentation, and (3) activation in regulatory control regions (dorsolateral and ventromedial prefrontal cortices; dlPFC, vmPFC) during reward decisions, compared to less gritty individuals. Ventral striatum activation on the ST and Grit Task are expected to be highly correlated. Gritty individuals are expected to exhibit more PFC activation during both tasks resulting in more gritty behavior and less risky behavior (measured by ST yellow light decisions). Feasibility: I will work with Dr. Adriana Galván, a developmental neuroscientist with expertise in adolescent brain development and my advisor, to implement this program of research. Dr. Galván has a database of over 400 ethnically diverse adolescents from which to recruit participants, and her affiliation with the UCLA Center for Cognitive Neuroscience gives me access to state-of-the-art neuroimaging facilities. Scanning fees will be paid by Dr. Galvan’s unrestricted funds. Broader Impacts: Identifying the neural correlates of grit will advance our understanding of positive risk taking and inform efforts to improve positive goal-oriented pursuits (e.g., academic achievement) for adolescents. For disadvantaged adolescents who lack external encouragement to engage in positive risk taking, this research is critical. As part of UCLA Psychology in Action (PIA), I will share with educators and policy makers at interdisciplinary symposia how positive risk taking is beneficial for adolescents. I will also engage with lay audiences about the implications of my research through PIA’s social media platforms and through community outreach at area schools. I will use my findings to encourage educators and community organizations to provide positive outlets for adolescents. I will advance scientific knowledge by presenting my work in published manuscripts and at conferences, and through transdisciplinary collaboration investigating positive risk taking with the UC Consortium on the Developmental Science of Adolescence. I will directly provide opportunities for adolescents to engage in positive risk taking by conducting leadership workshops at area high schools and will expose underrepresented groups to careers in STEM fields by actively recruiting women and minority research assistants. References: 1Casey, B.J., Getz, S., & Galván, A. (2008). Dev Rev, 28, 62-77. 2Crone, E.A., & Dahl, R.E. (2012). Nat Rev Neurosci, 13, 636-650. 3Duckworth, A., & Gross, J.J. (2014). Curr Dir Psychol Sci, 23(5), 319- 325. 4Duckworth, A.L., Peterson, C., ... (2007). J Pers Soc Psychol, 92, 1087-1101. 5Steger, M.F., Kashdan, T.B., ... (2008). J Res Pers, 42, 22-42. 6Sample size calculated using fmripower.org. 7Amounts based on intertemporal choice heuristic calculation; Ericson, K.M.M, White, J.M., ... (2015). Psychol Sci, 26(6), 826-833. 8e.g., NRI-RQV, NRI-SPV. 9 e.g., Grit Scale, DOSPERT, BIS/BAS. 10e.g., LOT-R , WASI-II, Rosenberg Self- Esteem Scale, LSAS-SR, SWLS.	0
www.rachelcsmith.com *All Rights Reserved to Original Author 2010 GRFP Research Proposal Native Bee Reproductive Success in Restored Habitats Introduction: Ecological restoration can rehabilitate ecosystem services, but its success depends upon the ability of the restored site to sustain functional populations.1 Restoration has been proposed as a way to promote conservation of native bee populations that have declined due to habitat loss and fragmentation.2 Native bees are effective pollinators of many economically important crops,3 and drastic crashes in managed, non-native honey bee populations due to colony collapse disorder have highlighted systemic vulnerability, as well as the need to diversify on-farm pollinator communities. Within agricultural systems, hedgerows (linear strips of native flowering shrubs planted in fallow field margins) are the preferred restoration method: In 2007, Congress passed the Pollinator Habitat Protection Act (S.1496), incentivizing the creation of pollinator-friendly hedgerows. However, agricultural landscapes have become increasingly simplified due to intensive farming practices, and potential source habitat may be too distant to provide reliable immigration to hedgerows.4 In addition, recent research5 suggests that hedgerows may be sink habitat, where the death rate is greater than the birth rate.6 This research used species richness as a proxy for reproductive success, which is problematic because it gives no indication of long-term population viability within sites. If hedgerows are sinks, pollination services could be threatened.3 Therefore, I propose to directly measure native bee reproductive success in order to assess the sink hypothesis and the conservation potential of hedgerows. Background: Native solitary bees typically have one generation per year, therefore there are two main components that influence reproductive success: per female fecundity and offspring survival. Fecundity may be influenced by proportion of forage (pollen) available for provisioning of brood cells7 at both the local and landscape level.8 Hedgerows often contain low plant diversity (usually between 8 - 15 species); if these resources are inadequate, bees may need to forage in the surrounding landscape to obtain sufficient pollen to meet larval needs.4,8 Limited or patchy landscape resources could reduce success as fewer nests could be created. Larval mortality can be heightened by increased parasitism, and cleptoparasite and parasitoid abundance is often greater in restored sites than in natural areas.10 Additionally, parasitism rates have been correlated with resource availability: in resource-poor environments, bees compensate for floral scarcity by increasing search time, broadening the window for successful parasitism.11 While exposure to herbicides12 and abiotic factors, such as high in-nest moisture and temperature levels,13 can also be fatal to larvae, their effects are difficult to measure; therefore, I will divide causes of mortality into two categories: parasitism and unknown.10 In order to demonstrate the occurrence of source-sink dynamics it is necessary to compare population demographics in multiple habitats.14 Thus, treatments will be in two habitat types, restored (hedgerow), and un-restored (fallow field margins), situated in either complex (heterogeneous) or simple (homogenous) landscapes (n = 18). Additionally, in order to have baseline data against which gauge the success of the restored sites, fecundity and offspring survival will be recorded in natural habitats (n = 4). I will use trap-nesting bees (cavity-nesters) as my study taxon because ninety percent of the native bee species managed for agriculture are trap-nesters, and they readily occupy artificial “trap-nests,” bundles of hollow reeds, that can be lined with removable straw inserts to facilitate monitoring of nest progress.8 Hypotheses: In order to examine the capacity of hedgerows to sustain viable populations of trap- nesting bees, I will measure fecundity and parasitism in two landscape contexts: 1. Fecundity of trap-nesting bees will decline with decreased resources. I hypothesize that landscape complexity will be more important to fecundity than local-level resources. In simple Native Bee Reproductive Success *Do not Reproduce without Permission www.rachelcsmith.com *All Rights Reserved to Original Author 2010 landscapes, I do not expect to find significant differences in fecundity between hedgerows and fallow field margins. In contrast, I predict that in complex landscapes fecundity will in higher in both treatment types, approaching observed levels in natural habitat. However, if fecundity in hedgerows in simple landscapes is higher than in fallow field margins, it would indicate that the local resources they provide are sufficient, bolstering claims that they are an appropriate restoration method in homogenous landscapes. 2. Parasite pressure on larvae will increase with decreasing resources, negatively impacting reproductive success. In simple landscapes, I expect to observe spikes in parasitism levels in both habitat types. I predict that the additional resources provided in heterogeneous landscapes will buffer larvae against heightened parasitism in hedgerows but not in fallow-field margins. Further, I predict that offspring survival in hedgerows and field margins in both landscapes types will be significantly lower than in natural habitat, signifying that disturbed landscapes subject larvae to increased threats from parasitism and other factors shown to increase mortality. Methods: Study Location: This study will take place in Yolo County, an agricultural region in California’s Central Valley. In the study region, complex landscape is a mosaic of natural habitat, riparian corridors, organic farms, and conventional agriculture; simple landscapes are dominated by intensive agriculture (> 80%). Landscape features will be categorized using GIS landsat data. Each site will contain a 300 m transect with a trap-nest in the center, and will be at least 2 km apart to ensure isolation.15 Floral Resources: Vegetation sampling will commence with nest initiation and terminate when nesting ceases. I will record flowering species and number of inflorescence in 1 m2 quadrats along transects. To determine the proportion of local and landscape resources used, I will collect voucher pollen from all flowering plants within a 1500 m radius of trap-nests, and compare it with sub-samples of pollen from nests.8 Parasitism: Once nests are completed, I will x-ray larvae in the lab to ascertain which are parasitized;8 parasitoids will be identified after emergence by Dr. Robbin Thorp, of the UC Davis Bee Biology Lab. Unparasitized pupae will be stored in optimal conditions at the UC Berkeley insectary and monitored for emergence of cleptoparasites. Broader Impacts: Due to the persistent, damaging effects of colony collapse disorder, restoration of native bees is essential for the maintenance of pollination services in agricultural areas.3 These findings could validate hedgerows as an effective restoration method, or illuminate its short-comings. Worldwide, native bees are the most important pollinators in natural systems, and are therefore necessary for preservation of biodiversity.3,16 The result of this study will help identify factors that could contribute to the success of pollinator restoration at larger scales. I will submit papers to scientific journals, present at conferences, and share my results with farmers at annual workshops put on by the Xerces Society, a non-profit dedicated to insect conservation. References: 1. Ormerod, SJ. J. of Applied Ecology 40 (Dec 2003) 2. Dixon, KW. Science 325 (Jul 2009) 3. Kearns, CW. et al. Ann. Review of Ecology and Systematics 29 (1998) 4. Ricketts, TH, et al. Ecology Letters 11 (May 2008) 5. Ockinger, E, HG Smith. J. of Applied Ecology 44 (Feb 2007) 6. Pulliam, HR. American Naturalist 132 (Nov 1988) 7. Muller, A, et al. Biological Conservation 130 (Jul 2006) 8. Williams, NM, C Kremen. Ecological Applications 13 (Apr 2007) 9. Steffan-Dewenter, I. Ecological Entomology 27 (Oct 2002) 10. Exeler, N, et al. J. of Applied Ecology 46 (Oct 2009) 11. Goodell, K. Oecologia 134 (Mar 2003) 12. Freemark, K, C Boutin, Agriculture Ecosystems & Environment 52 (Feb 1995) 13. Hranitz, JM, et al. Environmental Entomology 38 (Apr 2009) 14. Watkinson, AR, WJ Sutherland, J. of Animal ecology 64 (Jan 1995) 15. Gathmann, A, T Tscharntke. J. of Animal Ecology 71 (Sept 2002) 16. Allen-Wardell, G, et al. Conservation Biology 12 (Feb 1998)	0
PRIMARY RESEARCH OBJECTIVE: The primary objective of my PhD research is to formulate novel statistical methods for accurately quantifying uncertainty and variability in life-cycle assessments (LCA), the primary method used to assess embodied carbon (greenhouse gas emissions associated with the production, transport, construction,andend-of-lifeofbuildingmaterials).Theresultsofmyworkwill advance LCA research by enabling more robust decision-making,moreaccuratesensitivityanalyses,and enhanced comparability between whole-building LCAs. Academics, practitioners, and policymakerswill thus be empowered to more effectively understand, quantify, and ultimately reduce embodied carbon. Immediate embodied carbon reduction is necessary for curbing the catastrophic effects of the climate crisis, yet the current body of research is severely limited, and few policies or industry standards incorporate embodied carbon into emission reduction strategies. JUSTIFICATION: Although whole-building LCA methodology is standardized by an international coalition of technical standard-setting bodies (ISO 14040/14044),thereisconsiderablevariabilityindata quality, impact assumptions, and scope. This variability is particularly appreciable for biogenic carbon, the physical carbon that is stored in biological materials such as wood, hemp, and straw. Negative biogenic carbon emissions due to carbon storage are treated inconsistently across whole-building LCAs because these assumptions are not standardized. In a five-building case study series, embodied carbon normalized by floor arearangedfrom-936to207kgCO e/m2whenbiogeniccarbonstoragewasincluded 2 in theanalysisand132to557kgCO e/m2whenbiogeniccarbonstoragewasexcluded[1].Scientistshave 2 investigated several approaches to incorporate uncertainty in whole-building LCA, including but not limited to: probability density functions to model building element service-life [2], probability density functions to model manufacturing emissions [3], and more advanced, exhaustive methods that involve conducting aglobalsensitivityanalysisacrossparameterspacesdeterminedviaLatinhypercubesampling [4]. Unfortunately, these modeling practices are not yet standardized or integrated into whole-building LCA. From 2014-2019, 44% of published LCA studies did not mention uncertainty and 36%mentioned uncertainty but did not incorporate it in the analysis [5]. In summary, there is no consensus among available methods to incorporate uncertainty in whole-building LCA, and available methods are seldom incorporated in LCA research. I aim to build upon theseavailablemethodsbyintroducingaprobabilistic framework to standardize uncertainty characterization in biogenic carbon accounting, enabling better decision-making and comparability between whole-building LCAs. Objective 1: Literature Review. Intent: To assess sources of uncertainty and uncertainty characterization methods in scientific literature and industry case studies. Methods: I will conduct a literature review of academic LCA studies,whole-buildingLCAs,material-specificLCAs,andembodied carbon benchmarks. I will select the most appropriate sources and characterization methods for LCA uncertainty. I hypothesize that Monte Carlo simulation with probabilistic modeling will most accurately characterize emission uncertainty for a linear model like whole-building LCA. Objective 2: A Probabilistic Framework for Whole-Building LCA. Intent: To increase the potential for synthesis between data sets for whole-building LCA in academic research and in practice. Methods: Based on my findings from the literature review, I will develop a statistical framework for classifying, characterizing, andquantifyinguncertaintyinLCA.Ihypothesizethatrunningwhole-buildingLCAswith my proposed probabilistic approach will more accurately modelthisuncertainty,whichwillallowme(1) to standardize uncertainty quantification in LCA research and (2) to enable more accurate comparisons and better decision-making. Objective 3: StandardizingBiogenicCarbonAccounting.Intentandbackground:Currentapproaches to biogenic carbon accounting are deterministicandquantifyneitheruncertaintynorvariabilitythatcome from widely varying assumptions. The ‘0/0approach’assumesnetcarbonsequestrationfromtreegrowth balances with end-of-life carbon emissions, and thus, the biogenic carbon storage and its subsequent release are ignored [6]. The ‘-1/+1 approach’ considers biogenic carbon as a negative carbon emission during Life Cycle Stage A (the production stage) with carbon released during Life Cycle Stage C (the end-of-life stage) [6]. Dynamic LCA relies on the regrowth of biogenic carbon put into a building by accounting for forest rotation periods over a period leading up to or during the building life [7].Carbon discounting methods calculate a net present value of carbon emissions avoided with biogenic carbon storage [8]. Ton-year accounting converts the time-value of biogenic carbon storage into a carbon offset equivalent that yields anestimateofnegativeemissionsperton-yearofstorage[9].Insummary,thereisa vital need toformulateanaccurateandstandardizedmodelingmethodologythatproperlyaccountsforthe benefits of biogenic carbon storage in buildings while also quantifying variability and uncertainty in the calculation. Methods: I will formulate stochastic models for biogenic carbon that producemoreaccurate and consistent whole-building LCA results. I hypothesize that introducing these stochastic models to dynamic LCA will yield the most accurate and descriptive results but will becomputationallyexpensive and difficult to implement for regular use. Therefore, I posit that a stochastic model based on ton-year accounting will yield a viable, but easy-to-implement approach. My proposed method (likely to be stochastic ton-year accounting) will enable practitioners to circumvent end-of-life assumptions for biogenic carbon, which are often the source of significant uncertainty. INTELLECTUAL MERIT: Despite being a necessary component to solving the climate crisis, embodied carbon reduction is not widely studied in theUS.LCAistheprimarymethodusedtoresearch, understand, and reduce embodiedcarbon,butdisparatedatasetswithwidelyvariedassumptionspreclude comparison ofexistingstudiesandmoreadvancedanalysis.Withmyresearch,Iwillsteerresearchersand practitioners towards a more complete understanding of the most important data needed to describe the total embodied carbon footprint of a building. With support from the National Renewable Energy Laboratory (NREL) and the University of Colorado, Boulder’s Center for Research Data & Digital Scholarship, I will elevate the standards for embodied carbon and LCA research. BROADER IMPACTS: The two most impactful ways to facilitate widespread, effective embodied carbon reduction are (1) establishingembodiedcarbonbenchmarkingmethodologyand(2)disseminating statistically rigorous embodied carbon reduction tools. Benchmarks: Designers need science-based embodied benchmarks to inform effective target setting, though none currently exist. These benchmarks must describe expectedrangesofembodiedcarbonperusablefloorareaandaccountforvariablessuchas lateral design requirements, building geometry, building type, and soil quality. My research will be integrated with ongoing research in Dr. Srubar’s Living Materials Laboratory to establish embodied carbon benchmarking methodology. Our lab group has collaborators at NREL who are particularly interested in integrating this methodology with their analyses of operational carbon (greenhouse gas emissions associated with building energy use). Embodied carbon reduction tools: Industry-standard LCA software programs are proprietary and provide conflicting results with ill-constrained uncertainty. My research will culminate in publishing an open-source, easy-to-implement software packagetoensure broad implementation by researchers and practitioners. I will then distribute my findings through prominent academic and industry organizations (e.g., CLF, AIA, SEI, UNFCCC, USGBC, WGBC). CONCLUSION: I am uniquely suited to conduct this research because of (1) my first-hand experience with LCA, engineering design work, and computational research, (2) the exemplary leadership I have demonstrated in the embodied carbon space, and (3) my position in a leading embodied carbon research groupwithimportantrelationshipssuchasthatwithNREL.Myresearchwillelevatethestandardforhow academic and industry practitioners conductLCA,whichwillbeessentialformoreeffectivelycombating the climate crisis. I will ensure the findings of my research are disseminated to salient technical communities and so that they are applied ubiquitously in industry and academia. [1] TallWood Design Institute CLT Case Studies (2020); [2]K.Goulotiet.al.,BuildingandEnvironment (2020); [3] M.A. DeRousseau et. al., Journal of Cleaner Production (2020); [4] E. Igos et. al., The International Journal ofLifeCycleAssessment(2019);[5]N.Bamberet.al.,TheInternationalJournalof Life Cycle Assessment (2020); [6] Hoxha et. al., Buildings & Cities (2020); [7] A. Levasseur et. al., Environmental Science &Technology(2010);[8]L.Marshall,A.Kelly,WorldResourcesInstitute(2010); [9]P. Moura Costa, C. Wilson,Mitigation and AdaptationStrategies for Global Change(2000)	0
Background: The Bushveld Complex is located in South Africa and was emplaced approximately 2.056 Ga. It is the largest layered mafic intrusion in the world, covering an area of 65,000 km2 with a thickness ranging from 7-9 km1. The Bushveld is an important resource for the world, hosting major quantities of platinum and platinum group elements (PGE), titanium, iron, vanadium, tin, and chromium2. The mafic to ultramafic cumulate sequence of the complex is called the Rustenburg Layered Suite (RLS) and is divided into five zones: Marginal, Lower, Critical, Main, Upper, and Roof Zones. The Upper and Upper Main Zones (UUMZ) are genetically related to each other and are separated from the lower zones by a layer known as the Pyroxenite Marker (PM)3. The UUMZ hosts the most significant Fe, V, Ti, and P deposits2. The UUMZ is dominated by gabbro, anorthosite, and Fe-Ti-oxide rich rocks, which include magnetitite (magnetite and ilmenite) and nelsonite (magnetite, ilmenite, and apatite) mineral assemblages. Fe-Ti-oxide rich rocks make up the smallest proportion in the UUMZ but host the majority of economically significant minerals. The Fe-Ti-rich rocks are typified in 26 magnetite and 6 nelsonite layers documented in the western limb4; the same number of magnetite layers have also been mapped in the eastern limb3. The gabbro layers in the UUMZ are thought to have been formed by cooling and differentiation of the residual magma that produced the Pyroxenite Marker. However, the evolution and relationship between the anorthosite and magnetite/nelsonite layers is still poorly understood. Several models have been proposed to explain the genesis of these layers, including: immiscibility, fractional crystallization/mineral accumulation, disequilibrium crystallization, chamber rejuvenation and magma mixing, and hydrothermal enrichment2-6, 9. However, no consensus has been reached as to which model may most accurately describes the petrogenesis of the Fe-Ti-oxide rich layers because each authors’ interpretation of the data has in turn been challenged by other workers. Objective: To apply new techniques and ideas developing in magma chamber research to the magnetite layers in the Upper Zone of the Bushveld Complex and use the new datasets to test models for development and differentiation of UUMZ layers. The results will be applied to magnetite pipes that have similar mineral assemblages to the magnetite layers, but have contested origins. Hypothesis: Testing different previously proposed models for the origin of Fe-Ti-oxide rich magnetite and nelsonite layers in the Bushveld Complex will result in a new model that combines certain aspects of these end-member processes to more accurately define the petrogenesis of the oxide-rich layers. Methods: The western limb will be studied via samples from the Bierkraal cores and the magnetite layers in the eastern limb will be sampled during field-mapping. Bulk rock geochemistry analyzed with x-ray fluorescence (major elements) and LA-ICPMS (trace elements). Mineral compositions will be analyzed by EPMA and LA-ICPMS; the data will be integrated with photomicrographs collected using optical and scanning electron microscopy. Testing models: Immiscibility. Silicate liquid immiscibility occurs when a homogenous silicate melt separates into two compositionally distinct liquids with identical mineralogy but in differing proportions. In mafic layered intrusions, this process begins after considerable crystal fractionation, resulting in a crystal mush. If the permeability of the crystal mush is high, liquid may separate by gravity, producing nearly monomineralic layers5,6. The presence of Fe-rich silicate inclusions in minerals such as plagioclase and apatite is evidence for liquid immiscibility. If the Fe-Ti-oxide rich layers formed through immiscible processes, then the crystallized Fe-rich melt inclusions will have major and trace element content similar to the magnetite layers. Additionally, the host mineral that crystallized from the Si-rich melt will have lower REE, HFSE, P, Ti, and FeO contents than the conjugate Fe-rich inclusions7. Fractional crystallization/mineral accumulation. Fractional crystallization of magma will lead to a dense residual magma that will begin to crystallize magnetite and accumulate into magnetite-rich layers. The resulting magma after magnetite crystallization will have a lower density and rise buoyantly continuing this process. If fractional crystallization occurred, a fractionation trend will be recorded in minerals with increasing height in the section. Consequently, stratigraphically higher magnetite and anorthosite layers will have increased iron enrichment, lower plagioclase An%, lower Mg# in pyroxene and olivine, lower V content in magnetite, and higher whole rock SiO wt%4. Additionally, in a closed system with continuous 2 fractionation, incompatible elements become more enriched and compatible elements depleted with increasing stratigraphic height. In magnetite, this will be recorded as decreased Ti, V, and Cr (compatible elements) and increased Si and Ca (highly incompatible)8. Disequilibrium crystallization. Another idea proposed for the genesis of magnetite-rich layers is rapid crystallization in disequilibrium conditions in response to increased oxygen fugacity (ƒO ) towards 2 the base of a magma chamber. As magnetite crystallization progresses, ƒO is lowered. Vanadium (V) 2 partitioning between magnetite (mt) and clinopyroxene (cpx) can be used as a proxy for oxygen fugacity, as it is sensitive to changes in ƒO . If V /V increases, this corresponds to decreasing ƒO 2. Additionally, 2 mt cpx 2 if the Fe-Ti-oxide layers crystallized instantaneously in high ƒO , Mg, Al, and Si contents will be relatively 2 enriched in magnetite from these layers compared to disseminated magnetite in anorthosite/gabbro layers9. Chamber rejuvenation and magma mixing. If new magma was injected periodically during the emplacement of the UUMZ to form the distinct Fe-Ti-oxide rich layers, step-like changes in mineral composition through a vertical unit, rather than a smooth fractionation trend, will be observed. Also, if the new magma is more primitive than the final fractionation stages of the previous injection then compositional reversals will be observed moving up-section. Compositional reversals in magnetite will be seen as higher Cr and V contents followed by an abrupt change to lower content4. Hydrothermal enrichment. This model is particularly applicable to the magnetite pipes that have a vertical structure. If the Fe-Ti-oxide rich layers formed by hydrothermal enrichment rather than having a magmatic origin, magnetite will be depleted in Ti, Al, and HFSE, as hydrothermal fluids have generally low concentrations of these minerals due to their relatively low solubility. In contrast, magmatic magnetite will be relatively enriched in compatible elements. Silicon and Ca are two elements that are highly incompatible with magnetite, so if these are enriched in the samples, it suggests hydrothermal activity. Another characteristic of magnetite that is indicative of hydrothermal enrichment is if the Ni/Cr ratio is >1 (in silicate magmas this ratio is always less than one)9. In addition, photomicrographs of magmatic magnetite commonly show concentric compositional zoning in contrast to patchy textures common of hydrothermal minerals9. Intellectual Merit: The UUMZ contains world class deposits of important strategic elements including vanadium, iron, and titanium. There are limited global resources of these minerals and global demand is growing exponentially2. Understanding how these ore deposits form is critical both globally and to U.S. interests, as the country is dependent on foreign sources for many of these important commodities. There is still much debate about the formation of magnetite layers in the Bushveld Complex, even though they have been identified and studied for many years. Results from this research may be applied to other parts of the extensive Bushveld Complex, as well as to other layered mafic intrusions around the world. Broader Impacts: Impacts of this research of magnetite layers of the Bushveld Complex extend beyond Earth. One of the most exciting endeavors of the last century, sending humans to outer space, has propelled scientific curiosity perhaps more dramatically than any other scientific activity. As interest in deep-space exploration and establishing a human presence on Mars and other planets increases, so has the need to understand the processes of ore-deposit formation and exploitation. Lessons learned about the differentiation of magnetite layers in the Bushveld Complex may provide insight and strategies for sourcing iron and titanium (and other metals) that are crucial to permanent infrastructures, but are not cost effective to send from Earth. Results from my research will be shared with colleagues through publications and presentations at conferences, including the national GSA and AGU conferences. Additionally, one of my long-term goals is to provide educators with resources they can implement in their classrooms to encourage curiosity in their students and create lifelong learners who can contribute to scientific advancement. Applying ore- forming processes to ideas of colonization on other planets offers the potential for stimulating STEM activities that can be leveraged into K-12 educations to increase engagement in science. Potential projects topics could include what would be required to mine resources on another planet, requiring students to engage strategy and research, and in the process hopefully garner a lifetime of scientific curiosity. [1] Zeh et al. (2015) Earth Planet Sci Lett, v. 418, p. 103-114 [2] Fischer (2018) PhD Dissert., 129 p. [3] Scoon and Mitchell (2012) S Afr J Geol, v. 115.4, p. 515-534 [4] Tegner et al. (2006) J Petrol, v. 47, p. 2257-2279 [5] Cawthorn (2015) in Layered Intrusions, p. 515-587 [6] VanTongeren and Mathez (2012) Geology, v. 40, p. 491-494 [7] Veksler et al. (2006) Contrib Mineral Petrol, v. 152, p. 685-702 [8] Dare et al. (2014) Miner Depos, v. 49, p. 785-796 [9] Klemm et al. (1985) Econ Geol, v. 80, p. 1075-1088	0
Hypotheses: Molecular dynamics simulations can be used to quantify proton transport capabilities of amphiprotic materials for use in hydrogen fuel cells. Introduction: Vehicular internal combustion engines are responsible for 28% of greenhouse gas emissions in the United States and are the second biggest source of these emissions (1). Proton exchange membrane (PEM) hydrogen fuel cells are a cleaner alternative to the internal combustion engine, emitting only water (2). The high cost of hydrogen fuel cells, however, impedes their success. Current fuel cells operate at low temperatures (85OC) to maintain loading of the membrane with water. Water serves as the proton exchange fluid. Operating at these temperatures requires an expensive platinum catalyst (3). To overcome this problem I will implement cutting- edge modeling in the Scott Auerbach laboratory at UMass Amherst to develop anhydrous proton exchange materials and improve the viability of PEM fuel cells. Among the most promising proton conducting materials are azoles – five-membered carbon and nitrogen rings that both accept and donate protons (4). To increase molecule stability, we tether them to oligomer chains. Oligomers offer a compromise between material stability and liquid-like flexibility, allowing for faster and more efficient proton motion (5). Molecular dynamics (MD) simulations enable the study of hydrogen bonding interactions of these systems. I propose investigating the hydrogen bond lifetime and reorientation rate of amphiprotes tethered to oligomers using MD simulations. I will accomplish the following goals: 1) Run atomistic MD simulations on tethered imidazole; 2) Run coarse-grain MD simulations on tethered imidazole; 3) Benchmark across length scales; 4) Use both models to investigate other azoles. The project focus is on hydrogen bonding networks that govern proton diffusivity. Through modeling we relate microscopic properties to macroscopic performance to design new PEMs. Background: To study hydrogen bonding networks formed by tethered amphiprotes, we run molecular dynamics (MD) simulations, which allow the observation of atomic-level changes. Efficient proton transport takes place via the Grotthuss mechanism, which involves the transport of a proton by the collective motion of many hydrogen bonds (Figure 1). This is reminiscent of bucket brigades to put out fires. The Grotthuss mechanism requires hydrogen bond networks followed by functional group rotation before transport of the next proton (6). Herein lies the challenge of designing efficient proton conductors: extended hydrogen bonding arises in solid-like systems, Figure 1: Grotthus mechanism of while rapid functional group rotation occurs in liquid-like proton transfer in imidazole (6) systems. MD allows us to compare the atomic level trade- offs between extended hydrogen bond clusters and functional group dynamics. Balancing these parameters is vital for designing next-generation PEMs. 1) MD of tethered imidazole: Imidazole is promising because it offers long H-bond lifetime and fast reorientation compared to other azoles (5). I will use MD software DL-POLY to run atomistic simulations on imidazole oligomers. With NPT constant temperature and pressure simulations we extract volume parameters that we input into NVE constant volume and energy models. These simulations enable calculations of hydrogen bond cluster size, lifetime, and reorientation rate. These simulations cover extremely short times – on the order of 10-9 seconds. To study proton transfer over more realistic scales, the next step is coarse-grain modeling. 2) Coarse-grain modeling of tethered imidazole: In MD, the formation and reorientation of hydrogen bonds occur on the order of picoseconds (10–12 s) to nanoseconds (10–9 s), while the MD time step is femtoseconds (10–15 s), thus requiring prohibitively long simulations. Coarse-grained MD allow us to increase the time step by restricting atomistic degrees of freedom (such as vibrational modes between atoms.) Coarse-graining also allows us to consider systems approaching macroscopic dimensions of real membranes. I will build a coarse-grained model in Gromacs software using the Martini force field (7). Martini maps four functional groups (such as CH ) to one coarse-grain bead. These beads imitate the behavior of the functional groups they 2 represent by replicating their dipole moments. Atomistic simulations consist of 300 oligomers, but through coarse-graining we can model up to 4500 oligomers, an order of magnitude increase. This will enhance our understanding of the behavior of proton transfer fluids in an actual PEM. 3) Model benchmarking: We will benchmark coarse-grain models against atomic-level simulations to ensure that hydrogen bond properties agree across length scales. To do so I will work with Qinfang Sun, a graduate student in the Auerbach lab. She has built atomic-level simulations of azole liquids and oligomers, and her expertise will enable me to be successful. I will use her results to build a coarse-grained system to study long range interactions between molecules. By combining the results from our studies, we will have a much more complete picture of proton transfer and potential for use of tethered amphiprotic materials in fuel cells. 4) Expansion of model to other amphiprotes: Once the coarse-grain model is benchmarked, it becomes an effective tool for studying PEM materials. I will investigate how other azoles, including triazole, tetrazole, and pyrazole, change the nature of hydrogen bonding cluster size and lifetime. I will also study how oligomer backbone length affects these parameters, searching for balance between percolating hydrogen bonds and rapid reorientation dynamics. Intellectual merit: My background as a chemical engineer is crucial for this project, combining my knowledge of chemistry with my engineering, problem-solving perspective. I will use my knowledge to build models that accurately represent material behavior. I will implement the models to study hydrogen bond capabilities of proton exchange materials. However, this project will not be as simple as determining which material offers the highest performance. I must find the optimal trade-off between hydrogen bond lifetime and reorientation rate. It is my responsibility to decide which materials are most promising for next generation PEMs. Broader Impacts: Hydrogen fuel cells offer environmentally sustainable transportation. My research will further PEM development by identifying the most promising materials to focus on in lab testing. These materials have the potential to revolutionize PEM fuel cells, reducing cost and increasing cell lifetime. The goal of this research is to make fuel cells competitive with the internal combustion engine, offering an environmentally friendly and cost effective alternative. If successful, my research will give everyone access to affordable, green transportation. References 1. EPA. Sources of Greenhouse Gas Emissions. 2014.http://www.epa.gov/climatechange/ghgemissions/sources.html 2. EPA. Fuel Cells & Vehicles. 2012. http://www.epa.gov/fuelcell/basicinfo.htm#performance 3. Baschuk, JJ, Li, X. Carbon monoxide poisoning of proton exchange membrane fuel cells. International Journal of Energy Research. 2001; 25(8): 695-713. 4. Viswanathan, U, Basak, D, Venkataraman, D, Fermann, JT, Auerbach, SM. Modeling Energy Landscapes of Proton Motion in Nonaqueous Tethered Proton Wires. J. Phys. Chem. A. 2011; 115: 54325-5434. 5. Harvey, JA., Auerbach, SM. Simulating Hydrogen-Bond Structure and Dynamics in Glassy Solids Composed of Imidazole Oligomers. J. Phys. Chem. B. 2014; 118: 7609-7617. 6. Mangiatordi, GF, Laage, D, Adamo, C. Backbone effects on the charge transport in poly-imidazole membranes: a theoretical study. J. Mat. Chem. A. 2013; 1: 7751–7759. 7. MARTINI Coarse Grain Force Field for Biomolecular Simulations. http://cgmartini.nl/	0
The Eccentricity Distribution of Long-Period Brown Dwarf Companions Keywords: brown dwarfs, direct imaging, radial velocity Abstract: I propose to measure the eccentricity distribution of long-period (1-500 yr) brown dwarfs. I will accomplish this measurement by modifying the Orbits for the Impatient algorithm and applying it to calculating posterior eccentricity probability density functions (PDFs) for a statistically large sample of long-period brown dwarfs. I will then aggregate these PDFs to determine the underlying eccentricity distribution of long-period brown dwarf companions. These results will allow direct comparison with the empirical distribution of exoplanets discovered by radial velocity and direct imaging, allowing insights about planetary formation, and will provide opportunities for statistical inferences about individual systems. Background and Motivation: Whether brown dwarfs form like large planets or small stars is key to understanding the process of exoplanet formation. Brown dwarf eccentricities can provide insight into formation scenarios, since brown dwarfs with high eccentricities are likely to have experienced close encounters with other orbiting objects early in their lifetimes (Morbidelli 2014), but in spite of this, little analysis has been done to characterize the eccentricities of individual brown dwarfs, let alone the population of brown dwarfs at a wide range of orbital separations. In order to study the formation scenarios of brown dwarfs and make statistical inferences about the population of brown dwarfs with respect to the population of giant exoplanets, it is therefore necessary to have accurate eccentricity measurements from a large sample of brown dwarfs orbiting stars. Fortunately, several long-period brown dwarfs have already been observed with both the radial velocity and direct imaging methods, but few orbital fits intended to characterize their orbital eccentricities have been attempted, in many cases due to their long orbital periods, for which only short orbital arcs have been observed. In order to provide these empirical constraints, I propose to use the NSF Graduate Research Fellowship to compute eccentricity PDFs for each of a statistically large sample of long- period brown dwarfs using our novel Bayesian technique, Orbits for the Impatient (OFTI; Blunt et al 2016). Since many systems will only have observations covering a fraction of a full orbital period, the uncertainty on the eccentricity of any given system will be relatively large, but with a large sample of systems, the underlying eccentricity distribution of the population can be measured to much higher precision than for any one object. I will combine the eccentricity PDFs to compute the physical distribution of the eccentricities of brown dwarfs, and determine whether this distribution is consistent with or qualitatively different from that for long-period giant planets. Methods: Combining data from radial velocity and direct imaging measurements can result in precise determinations of orbital eccentricities (Nielsen et al 2016), so the first step in my analysis will be to modify the OFTI algorithm, which produces orbital element PDFs from direct imaging data, to fit orbits to data sets composed of both radial velocity and direct imaging measurements. Once I’m prepared with this necessary tool, I will compile NSF Research Statement Sarah Blunt radial velocity and direct imaging data on >40 brown dwarf companions from the literature, and supplement these by reducing and analyzing unpublished data. Stanford is ideally suited for accomplishing this work, since there are many individuals at Stanford and in the Gemini Planet Imager Collaboration (including GPI PI Bruce Macintosh, and collaboration members at Stanford and nearby institutions Eric Nielsen, James Graham, Robert De Rosa, and Jason Wang) who are experienced with reduction of both radial velocity and direct imaging data, and since Stanford has access to the high-performance computing resources necessary for robust and efficient data reduction. Once I have compiled reduced direct imaging and radial velocity data for a sufficient number of brown dwarfs, I will run the modified version of OFTI on each data set to produce a posterior PDF in eccentricity for each brown dwarf. I will then aggregate these posterior PDFs to obtain the physical probability distribution of long-period brown dwarfs, testing the effectiveness of several functional models at reproducing this distribution, and so determine for the first time the functional form of the long-period brown dwarf eccentricity distribution. Anticipated Results: The most direct application of the calculation of the physical eccentricity distribution of long-period brown dwarfs will be a comparison with the eccentricity distribution of exoplanets detected by the radial velocity method (Nielsen et al 2010), and corresponding analysis to determine the probability that the two empirically determined distributions are the same. Additionally, the orbit fits to the selected brown dwarfs will in many cases be the first measurements of eccentricity, semi-major axis, and relative inclinations of these systems, which will be useful for understanding interactions between the substellar companions and circumstellar disks (e.g. Rameau et al. 2016). Orbit fits can also be used to calculate the probabilities of the existence or absence of additional bound exoplanets or brown dwarfs orbiting a given star (e.g. Bryan et al 2016), and guide future observations of these systems. Proposed Timeline: Year 1: Modify OFTI and compile brown dwarf astrometric and RV data. Year 2: Publish early results for individual systems and finish fitting orbits to remaining systems. Year 3: Produce eccentricity distribution of long-period brown dwarfs, publish comparison to planet population. References: Blunt, S., Nielsen, E. L., De Rosa, R. J., et al. 2016, ApJ (submitted) Bryan, Bowler, Knutson, Kraus, Hinkley, Mawet, Nielsen, Blunt (2016). ApJ, 827, 100 Morbidelli, 2014, Phil. Trans., DOI: http://dx.doi.org/10.1098/rsta.2013.0072 Nielsen, E. L. & Close, L. M. 2010, ApJ, 717, 878 Nielsen, E. L., De Rosa, R. J., Wang, J., et al. 2016, AJ, eprint arXiv: 1609.09095 Rameau, J., Nielsen, E. L., De Rosa, R. J., et al. 2016, ApJL, 822, 2	0
The evolution of similar traits in distantly related species is one of nature’s great surprises. Convergent evolution of traits has been widely observed throughout the animal kingdom; however, it remains unclear whether this phenotypic convergence results from convergent evolution of genes (Stern, Nat Rev Genet 2013). On a molecular level, convergent evolution occurs when the amino acids of a specific protein preferentially undergo mutations that produce similar or even identical amino acid sequences within distinct evolutionary lineages. A high level of adaptive convergent evolution – that is, convergence due to positive selection of beneficial mutations – would suggest that some genes have “optimal configurations,” which evolution uses and reuses across species. If such genes exist, then evolution is somewhat predictable, proceeding by one of a small number of possible paths (Stern & Orgogozo, Science 2009). In contrast, the complete absence of adaptive convergence would indicate that protein configurations beneficial to one species are seldom optimal within other species, and that evolution may proceed by a much wider set of paths. Thus, two fundamental questions in evolutionary molecular biology are 1) how often convergent molecular evolution occurs and 2) whether molecular convergence is a primary driver of phenotypic convergence. To adequately answer these questions, genome-wide analyses are needed; however, most previous analyses of convergent evolution have been limited to single genes (Li et al., Curr Biol 2010) or small taxa (Bazykin et al., Biol Direct 2007). Recently, the advent of whole-genome sequencing has opened new opportunities for exploring molecular convergence. Published genomes now exist for over 100 animal species, and 177 more are currently underway (Koepfli et al., Annu Rev Anim Biosci 2015). Additionally, phenotypes for 4,541 mammalian traits have been documented in a public database (O’Leary et al., Cladistics 2011). Together, these data enable a genome- wide search for convergent mutations throughout mammalian species followed by a test for association between convergent genes and specific convergent phenotypes, which might achieve strong statistical power for detecting adaptive convergence. I plan to develop and apply a statistical model to test for convergent evolution among 61 sequenced mammalian species. I hypothesize that convergent molecular evolution occurs at a higher rate than has been previously observed, and that this genetic convergence drives convergence of observable traits. This model will extend the boundaries of biological knowledge by measuring the frequency of adaptive convergence, and will augment existing statistical methods in a broadly applicable framework that can be extended to other genomes in the future. Aim 1: Develop a statistical framework for inferring convergent evolution between genomes. I will make my framework broadly generalizable to any data set with the following inputs: 1) a phylogeny containing the accepted evolutionary relationships within a set of species, and 2) the pairwise sequence alignments of all proteins in those species for which unique human orthologs exist. My analysis will integrate these data as follows:  Step 1: Infer ancestral sequences. For every amino acid position in every sequence, I will employ a linear-time approach to compute probability distributions of DNA and amino acid sequences at each ancestral node in the phylogeny (Yang, Mol Biol Evol 2007).  Step 2: Infer patterns of adaptive convergent evolution between species pairs. At the protein sequence level, I will apply a statistical analysis adapted from the CONVERGE algorithm described by Zhang & Kumar (Mol Biol Evol 1997). For every pair of species within our analysis, I will examine each amino acid and compute the probability that this amino acid converged between the two species, using the ancestral sequences from the previous step as reference. I will then determine the likelihood of convergence within each exon and within the gene. Because rapidly evolving genes often converge by random chance, called neutral convergence, I will control for gene-specific mutation frequencies in order to distinguish adaptive convergence from neutral convergence (Thomas & Hahn, Mol Biol Evol 2015).  Step 3: Evaluate performance on simulated data. I will simulate evolution of protein sequences in which a small pre-selected subset of sequences evolve under a non-neutral convergence pattern and the rest evolve neutrally. I will then test my model’s precision and recall in inferring which of the sequences evolved under the convergent model, correcting for false discovery. If my model is able to detect evidence of convergent evolution in the simulated data at a low false discovery rate, but I observe no signs of convergent evolution in my analysis of the biological data, then these results will cast doubt on the hypothesis that non-neutral convergence is a significant driving force in molecular evolution. Aim 2: Quantify the levels of convergent evolution within mammalian genomes, and identify functions enriched within convergent genes.  Step 1: Identify specific genes that have evolved in convergence across species. Using public alignment tools (Kent et al., PNAS 2003), the Bejerano Lab has obtained cross-species sequence alignments and a phylogenetic tree for 61 mammalian species. I will apply my statistical framework to all pairs of sufficiently diverged mammalian species and identify particular genes within each species-species pairing that exhibit non-neutral convergence. I hypothesize that I will find widespread evidence of adaptive convergent evolution.  Step 2: Identify potential phenotypic effects of convergent genotypes. If evolutionary convergence is truly adaptive, this would suggest that genes converge when they undergo similar selective pressures within different species (Castoe et al., PNAS 2009). Therefore, I hypothesize that genes facing similar pressures within independent species will be more likely to evolve in convergence within those species. For example, in aquatic mammals such as dolphins, manatees, and walruses, we might expect high convergence of skin- expressed genes involved in thermoregulation. I have identified a set of 20 convergent phenotypes that arose independently in mammals. For each of these phenotypes, I will conduct Gene Ontology enrichment analysis on genes that show elevated levels of convergence among the animals possessing the phenotype in question. As a null model, I will apply the same enrichment tests within groups of species that do not exhibit phenotypic convergence. If my hypothesis is correct, this analysis will suggest functions for which similar selective pressures across species have necessitated similar courses of protein evolution across these species. Importantly, in order to confirm these putative links between convergent genotypes and convergent phenotypes, future experiments will be necessary. The specific amino acid substitutions that I identify will be prime targets for further examination in functional assays, as described by Liu et al. (Mol Biol Evol 2014). My experience developing statistical models to quantify bias in genome-wide DNase-seq experiments (Gloudemans, 2015 Honors Thesis) makes me well-qualified to develop a genome- wide statistical model of evolution. This project will help us to understand whether phenotypic convergence is a direct result of genotypic convergence. To further facilitate broader impacts, I will create a user-friendly visualization that highlights hotspots of convergent evolution, and I will make this tool publicly available in the UCSC genome browser. This tool will allow biologists to visually explore instances of molecular convergence and to ask even deeper questions about the specific functional roles of these convergent mutations, ultimately increasing our understanding of how these proteins shape the lives of humans and all other species.	1
Investigating morphological variation in Siphonophores Background & Proposal: Historically, Siphonophores have been mistaken for jellyfish due to their transparent bodies, long tentacles, and stinging nematocysts [1], [2]. Like many other cnidarians (e.g., corals), Siphonophores are colonial animals and are made up of multiple animal bodies, calledzooids, which arise from the same embryoand function together as one organism. Within the Siphonophore, zooid types are arranged in a specific pattern, which is repeated across the organism and determined at the growth zones of the Siphonophore [4]. Siphonophores have a high degree of functional specialization and precise organization within the colony, which sets them apart from most other animal species [3]. Though Siphonophores are a diverse group, we lack an understanding of how the organizational pattern of zooid type differs across species, and to what degree this morphological variation of patterns is conserved. Understanding the conservation of pattern type, will inform us of the functional specialization structures that are indicative of their survival. To answer these questions, I will use geometric morphometric methods to compare differences among Siphonophore species. This approach builds on recent work done in the Casey Dunn lab at Yale, draws directly from myexperience in Dr. Dean Adams’s lab,and is motivated by my own interest in complex trait evolution. Last year, the Dunn lab published a transcriptome-based Siphonophore phylogeny and used it to reconstruct the evolutionary history of changes in Siphonophore sexual systems, life history traits, habitats, and zooid types. In their comparisons of zooid type, Munro et al. [1] used only the binary characterization of presence/absence of each zooid type, making this study void of any zooid organizational pattern classification. Previous studies have also suggested that organizational patterns of zooid type are species-specific [5].These organizational patterns have never been examined from a phylogenetic perspective. I am interestedin extending the work done in Dr. Dunn’s lab by quantifying morphological variationof zooid types to determine their evolutionary history and organizational pattern within the Siphonophore colony.Understanding the evolution of zooid types is key to unraveling the mechanisms behind coloniality and functional specialization. Broadly, this study will improve our understanding of complex traits in non-model organisms from which we lack critical information about their basic biology. The aim of my study is to determine the evolution of organizational patterns and variation of zooid specialization in Siphonophores by applying novel methods to quantify three-dimensional data.To quantify the morphologicalvariation of zooid type beyond presence/absence descriptions, I will use micro-computerized tomography (CT) scans to characterize organizational patterns. This project will analyze traits of Siphonophores that is currentlynot understood within the scientific community. Methods:I will collect at least three specimens for each of the 33 species analyzed in the phylogeny produced by Munro et al [1] to quantify zooid morphology. Specimens will be collected via blue water SCUBA diving or remotely operated vehicles from the Monterey Bay Aquarium Research Institute. Collected samples will be stored and preserved in solutions of formaldehyde, as standard procedure [6]. In the Dunn lab at Yale, I will stain samples using osmium tetroxide to enhance the visualization of body structures and then use the micro-computerized tomography scanner available on site to scan collected specimens. Using CT scans, I will obtain images of x-rays for every species. To quantify zooid structures from these scans, I will develop a novel landmark scheme appropriate for use in Siphonophores and obtain these data using the programAvizo™ (Fig. 1). I will then perform multivariate computational analysis for these landmarks using thegeomorph package[7] in R [8]. To test for correlations between zooid morphology and phylogenetic history, I will perform a phylogenetic regression for Procrustes shape variables, which will identify patterns of zooid shape variation across the Siphonophore phylogeny. I will then determine the rate of evolution for zooid shape and organizational pattern by performing morphological disparity tests. These results will indicate the tempo and mode by which these morphological structures have evolved and the degree of conservation across species. Feasibility: In the Dunn Lab, I will work with experts in evolutionary and Siphonophore biology. At Yale, having access to the largest Siphonophore collection to date would allow me to assess all preserved specimens to incorporate into my research. My past research experience in preparing and maintaining museum specimens, as well as operating and analyzing data from a CT scanner will allow me to successfully complete this project. In the Casey Dunn Lab, I will apply methods from my work with Dr. Dean Adams’s lab including geometric morphometrics, biostatistics, and phylogenetics to complete this project. Intellectual Merit: The collections from this study will illuminate our understanding of the diversity across the Siphonophore phylogeny. It will also aid in the development of new techniques to maintain and preserve non-model specimens for theYale Peabody Museum. I will use morphological data to reveal how Siphonophore phenotypes have dispersed throughout their evolutionary history. Applying computational approaches to compare morphology has been an ongoing limitation for research in evolutionary biology. This study will further develop these approaches by using a novel combination of techniques such as multivariate analyses and CT scanning. Major outcomes of this study will be the identification of species-specific organizational patterns, as well as a greater understanding of phenotypic plasticity of zooid types. These results will inform biologists on the evolution of coloniality, functional specialization, and the morphological specificity of zooids. All CT scans, specimens, and codes from these analyses will be openly accessible to scientists via data sharing platforms. Broader Impacts: Throughout my dissertation, I willparticipate in public outreach and the education of young scholars in science by giving a series of presentations about my experience as a scientist, research methods, and results at theYale Peabody Museum. At Yale, I will continue to participate in the Society for the Advancement of Chicanos/Hispanics and Native Americans in Science and begin working with Pathways to Science programs to help low-income, first-generation, and underrepresented students pursuing science. I plan to engage students in these programs and the public by using the unique context of museums. I will work jointly with curators to help build interactive exhibits by providing field video blogs, preserved specimens, and topics that expand upon the direct implications of my work into more generally societally relevant fields such as declining biodiversity and global climate change. Importantly, these proposed exhibits not only give a diverse public face to scientists, but also use the museum to help spark scientific curiosity in the public. References:1) Munro et al.Molecular Phylogeneticsand Evolution127(2018):823-833. 2) Cooke et alClinical Toxicology3(1970):589-595. 3) Mackie, G.O.LowerMetazoa(1963)329-337. 4) Goetz FE.Nanomia bijugawhole animal and growth zones from http://commons.wikimedia.org/wiki (2018). 5) Dunn, C.W., Wagner, G.P.,Dev Genes Evol216(2006):743-754 6) Holst et al.Journalof Plankton Research38(2016):1225-1242. 7) Adams et al. Geomorph(2018) R package version 3.0.6 8) R CoreTeamR(2013).	1
Background. How does motivation drive learning? Evidence abounds that reward, motivation, and curiosity can all enhance learning and memory1; these findings have far-reaching implications for education. However, a fundamental problem undermines our ability to apply this research in classrooms: extrinsic reinforcement can actually decrease intrinsic motivation2. In other words, although rewards like candy, stickers, and money are often used as incentives for students, these secondary reinforcers may decrease internal motivation, curiosity, and fulfillment. In the brain, dopamine pathways are strongly implicated in reward and motivation1. Dopaminergic cells in the ventral tegmental area (VTA) project to the hippocampus and surrounding medial temporal lobes3, influencing memory by enabling the brain to prioritize and remember important information4. Moreover, high-reward contexts increase sustained VTA activation and memory encoding5. Past studies of intentional encoding strategies have shown the importance of elaborative and self-referential processing6, but have yet to link these methods to dopaminergic modulation. Importantly, it remains an open question whether cognitive strategies can act upon dopaminergic pathways to enhance memory, either immediately or over time. Cognitive neurostimulation, the volitional modulation of one’s own brain activity through mental imagery and thoughts, offers a promising method of enhancing motivation. However, past research has found that without guidance, individuals struggle to self-motivate and modulate VTA activity7. Neurofeedback provides individuals with real-time information about their own brain activity. Past studies in the Adcock lab have successfully used neurofeedback to train participants to self-activate the VTA; this activation is associated with self-reported motivation7. A day later, trained participants retained the ability to self-activate, even without neurofeedback. Intellectual Merit. Thus far, no study has shown that self-activation of the VTA can influence memory encoding or consolidation. This missing link is essential for elucidating neural mechanisms of motivation and memory, as well as extending cognitive research to education. The proposed research will take a novel approach to address this gap in the literature by embedding neurofeedback training within a memory task. The present study seeks to: (1) Train participants to drive intrinsic motivation and self-activate the VTA, (2) Test whether self- activation enhances memory encoding and consolidation, and (3) Identify effective motivational strategies with a data-driven approach. I hypothesize that with neurofeedback, participants will learn to self-motivate and drive VTA activation, thus enhancing subsequent memory. Methodology. Fifty healthy participants will be recruited to participate in a study at the Duke University Brain Imaging and Analysis Center, which houses a GE Premier 3T MRI scanner. First, participants will complete two trait motivation surveys, the Motivational Trait Questionnaire8 and the Behavioral Inhibition-Approach System9 (BIS/BAS). In the scanner, I will collect fieldmap, T1-weighted structural, and functional scans (TR=1s, voxels=2x2x2 mm3). Participants will complete three kinds of tasks: Activate task. Participants will be instructed to self-motivate by using personally-relevant thoughts and mental imagery (e.g., one past participant reported success with visualizing a cheering crowd7). Using PYNEAL software, previously developed in the Adcock lab, I will calculate real-time VTA activation and inform participants with a dynamic thermometer display. Watch task. Participants will passively view the thermometer display, but will be informed that fluctuations are random, not neurofeedback. On these trials, the thermometer display serves as the control task, equating visual input with the Activate task. Encode task. On each trial, participants will try to memorize a series of 7 object images (2 sec. each), sampled randomly without replacement from a set of 336 images. In total, participants will complete 8 runs of 6 trials each. A random half of the trials will begin with the Activate task (20 sec), and the other half will begin with the Watch task (20 sec). NSF-GRFP Graduate Research Plan: Alyssa H. Sinclair 2 The Encode task (20 sec) will conclude every trial. Between runs, participants will verbally describe the motivational strategies employed on preceding trials. After the MRI scan, participants will be randomly assigned to either the Same-Day group (memory test immediately after the scan, n=25) or the Next-Day group (memory test 24-hours later, n=25). In a behavioral testing room, participants will complete a recognition memory test of the images from the Encode task (336 old images, 168 novel images), and rate confidence on a 5-point Likert scale. Analyses. In summary, I will employ a 2x2 design (Task: Watch, Activate X Time: Same-day, Next-day) to address the following questions: 1. Does VTA self-activation enhance memory? I expect that within-subjects, average memory accuracy (d', signal detection theory) and event-related VTA activation will be greater on Activate than Watch trials. Moreover, trial- wise VTA activation will be parametrically related to memory for the stimuli encoded on a given trial. Previous work in the lab has detected similar neuromodulatory effects on single trials10. 2. Does self-activation of the VTA influence memory encoding and/or consolidation? If VTA-activation enhances consolidation, then the Next-Day group will exhibit greater disparity in memory accuracy between Activate and Watch trials (relative to the Same-Day group), because consolidation is time- and sleep-dependent. Within the Next-Day group, I will compare memory accuracy for Activate and Watch trials to control for sleep-consolidation effects that are independent of VTA-effects. An alternative hypothesis is that VTA-activation will improve memory equally in both groups, reflecting selective effects at encoding. 3. What cognitive factors drive motivation and enhance memory? Using the trait motivation survey data, I will test whether individual differences in personality (e.g., higher scores on trait motivation and the BIS/BAS Reward Responsiveness subscale) predict success on the Activate task and subsequent memory accuracy. Moreover, I will employ a data-driven approach to identify the self-reported motivational strategies that most effectively increased VTA activation (e.g., verbalizations, various types of mental imagery). Importantly, the existing literature on motivation and reward is constrained by a limited set of strategies that are imposed by experimenters. Informed by my fMRI findings, I will develop future behavioral studies that test the efficacy of the diverse motivational strategies that participants intuitively employ. Broader Impacts. Motivation is a core component of learning. Critically, low-income and disadvantaged students exhibit low intrinsic motivation, which predicts poor academic outcomes11. In 2016, a staggering 29.7 million American children (41%) lived in low-income families12. In classrooms, fostering intrinsic motivation can improve learning outcomes and student retention9. Extrinsic reinforcers, such as monetary rewards, can have restricted and short- lived effects; in contrast, intrinsic motivation predicts long-term student success10. The proposed research seeks to empower individuals to drive intrinsic motivation and self-activate motivational brain systems, thus engaging the brain to improve learning. With a novel neurofeedback approach, I will identify cognitive strategies that effectively act upon dopamine systems to enhance memory. In future behavioral studies, I will directly test whether these strategies can successfully bolster motivation and memory without neurofeedback. The present program of research seeks to uncover accessible and non-invasive methods of fostering intrinsic motivation and improving memory, thus broadly benefiting learning and education. References: 1. Wise, R. A. Nat. Rev. Neurosci. 5, (2004). 2. Butler, R. Br. J. Educ. Psychol. 58, (1988). 3. McNamara, C. G. & Dupret, D. Trends Neurosci. 40, (2017). 4. Lisman, J. E. & Grace, A. A. Neuron 46, (2005). 5. Murty, V. P. & Adcock, R. A. Cereb. Cortex 24, (2014). 6. Kirchhoff, B. A. Neurosci. 15, 166–179 (2009). 7. MacInnes, J. J., Dickerson, K. C., Chen, N. & Adcock, R. A. Neuron 89, (2016). 8. Heggestad, E. D. & Kanfer, R. Int. J. Educ. Res. (2000). 9. Carver, C. S. & White, T. L. J. Pers. Soc. Psychol. 67, (1994). 10. Adcock, R. A., et al. Neuron 50, (2006). 11. Schultz, G. F. Urban Rev. 25, (1993). 12. Koball, H. & Jiang, Y. NCCP, (2018).	0
"Today almost 2 million people in the U.S. face functional impairment due to limb loss and amputations with more than 185,000 amputations occurring every year. Neural prostheses are devices that aim to return full functionality to patients through brain-machine interfaces (BMIs). Emerging neural prostheses decode the user’s intention from neural signals recorded directly from the brain and create a closed-loop sensory feedback system through neural stimulation as shown in figure 1. The primary components of this technology are the neural signal interpretation algorithms and the neuromodulators (implanted neural recording and stimulation hardware). Recent advances in neural decoding techniques indicate the achievable high accuracy of BMIs. Previous work by Dr. Rajesh Rao demonstrates the viability of an unsupervised hierarchical k-means clustering algorithm to predict human behavior from brain recordings [2]. Figure 1: Closed-loop Feedback System [1] Advances in neuromodulators include recent work by Dr. Jan Rabaey on OMNI a wireless, low- power modular and distributed closed-loop neuromodulation device for chronic use [3]. Proximity of neuromodulators to neural tissue results in stringent power restrictions; any neural decoding algorithms must occur on external machines. Neuromorphic computing systems, hardware designed to function like biological neurons, are highly capable of enhancing brain- machine interface functionality. Classification algorithms on neuromorphic processors use 2 or more orders of magnitude less energy than on existing digital hardware. Implementing a neuromorphic processor in neural prostheses has the potential to improve BMI power consumption and mobility by eliminating data transmission and external hardware. A need for the consolidation of these 3 components - neural decoding, neuromorphic computing and neuromodulation - in hardware for BMI implementation has been clearly identified [4]. Proposition Throughout my undergraduate degree, I have conducted research on neuromodulators’ hardware- software interface, gained experience in neural signal processing and learning algorithms (such as SVMs, PCA and hyperdimensional computing), and worked on and gained an understanding of neuromorphic computing for adaptive learning hardware. I had the experience of working with three distinguished professors - Dr. Jan Rabaey, Dr. Rajesh Rao and Dr. Hugh Barnaby - on these distinct yet complementary areas. If I have the honor of receiving the NSF GRFP, I propose to fuse my knowledge in each to implement complex adaptive learning algorithms on neuromorphic hardware integrated with a neural recording and stimulating system. Research Plan Year 1: Evaluation and integration of neuromorphic hardware models with neural recording and stimulation devices. Cutting-edge neuromorphic chips such as, but not limited to, Intel’s Loihi, IBM’s True North, and the commercially-available Intel Curie module (if unable to obtain the former two) will be evaluated on power consumption, learning capabilities, applicability to neural decoding and accessibility. Consequently, an interface between the neuromorphic processor and the neuromodulation system including data transmission to and from each module will be designed. Year 2: Implementation of adaptive learning algorithm on neuromorphic hardware for neural signal decoding/interpretation. Various filters and learning algorithms will be evaluated including, but not limited to, k-means clustering, support vector machines, and spiking neural networks given potential performance on neural data and constraints of the selected processor. This involves research, design, implementation and testing of the algorithms on the neuromorphic processor to determine functionality and competence. Year 3: Testing, validation, improvement and finally study of system to advance knowledge of possibilities/limitations of neuromorphic applications in neural engineering. This process will involve testing of the system as a whole, ensuring data transmission between the neuromodulator and neuromorphic processor and functionality of the algorithm on the processor. Once the system is finalized, studies will be conducted to determine accuracy, speed, and power consumption of the system signifying operability in brain tissue. Further study: Research beyond the initial 3 years will include 2 years of in vivo studies to test and validate the system’s recognition of motor intention from neural data in animals. Facilities To conduct this study, I will work with Dr. Jan Rabaey at UC Berkeley. He has expressed considerable interest in working with me if I receive the NSF GRFP. I will integrate the neuromorphic hardware model with his group’s previous work on OMNI, the neuromodulation device. The neuromodulator is the primary component of my proposal on which the neuromorphic processor and learning algorithm are built. Working with Dr. Rabaey in UC Berkeley and continued mentorship from Dr. Barnaby and Dr. Rao on the neural decoding and neuromorphic modules will provide the resources I need to meet the goals of this proposal. Intellectual Merit and Broader Impacts Previous work claims that a neuromorphic neural interface will eliminate the need for external machines and significantly reduce power consumption enabling the possibility of a fully- implanted system. Potential for success has been demonstrated through simulations and modeling but the lack of a direct hardware implementation limits understanding of the true feasibility and impediments of utilization of neuromorphic processors in a closed-loop feedback system particularly regarding accuracy, speed and power consumption. This project will develop enabling technology that will advance the knowledge of the capabilities and limitations of neuromorphic applications in neural engineering. The work produced from this project will be presented at national and international conferences. Medically, the highly competent neural interface this research addresses has the potential to change lives through applications in bypassing spinal cord injury, deep-brain stimulation, and engineering plasticity for neurorehabilitation. Socially, this interdisciplinary project promotes collaboration between academic institutions. My experience with TYE taught me that the drivers of interest in engineering are role models and incredible technology. When I talk to middle and high school girls about my research - mind-controlled prosthetics - their eyes light up with possibilities. Throughout my graduate career, I will continue mentoring young girls to pursue engineering and will use this research to spark excitement for engineering in young minds. [1] S. J. Bensmaia et al., “Restoring sensorimotor function through intracortical interfaces: progress and looming challenges,” Nature Reviews Neuroscience, vol. 15, no. 5, pp. 313–325, 2014. [2] N. X. R. Wang, R. P. N. Rao et al., “Unsupervised Decoding of Long-Term, Naturalistic Human Neural Recordings with Automated Video and Audio Annotations,” Frontiers in Human Neuroscience, vol. 10, 2016. [3] A. Moin, J. Rabaey et al., ""Powering and communication for OMNI: A distributed and modular closed-loop neuromodulation device"", 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2016. [4] F. Broccard et al., “Neuromorphic neural interfaces: from neurophysiological inspiration to biohybrid coupling with nervous systems,” Journal of Neural Engineering, vol. 14, no. 4, p. 041002, Feb. 2017."	0
Graduate Research Plan Statement Reconstructing Morphologies of Distant Galaxies with the JWST Mock Catalog Keywords: Galaxy Evolution, Early Galaxy Structure, Near Infrared Imaging Abstract: In today’s universe, galaxies have simple morphologies (i.e. shapes and structures). They are ellipsoidal and symmetrical, often with an additional spiral component like our own Milky Way. However, there is strong evidence to suggest that galaxies in the early universe were clumpy and distorted, possibly driven by their assembly. The question then arises: how and why did galaxies change over time from asymmetrical structures to the objects we see today? The James Webb Space Telescope (JWST), NASA’s upcoming flagship space observatory, will revolutionize astronomers’ ability probe from the formation of the first galaxies over time until today. As a member of the JWST NIRCam team at the University of Arizona, I propose to create an analysis framework for the extraction of morphological information from the unique shapes and structures present in young galaxies. My proposed study will provide the necessary tools to understand the changing look of galaxies over cosmic time, fulfilling one of the primary aims of the JWST mission: understanding galaxy evolution from the first galaxies until today. Background and Motivation: The morphologies of galaxies are tied to the overall evolution of galaxy populations over cosmic time. For example, in today’s universe we use the shapes of galaxies to infer their recent histories; i.e. large spheroidal galaxies have undergone a merger with a galaxy of similar mass, whereas spirals have not (e.g. Mihos & Hernquist 1996). However, the deepest observations from the Hubble Space Telescope reveal that galaxies eleven billion years ago exhibit asymmetries and clumps in their images (e.g. Elmegreen et al. 2007, 2009). With few detections of distant galaxies, astronomers have been unable to fully characterize the evolution of galaxy morphology over cosmic time. Instead, the community has turned to complex hydrodynamical simulations, such as Illustris (Genel et al. 2014), to model galaxy evolution. While simulations can prove valuable for understanding certain astrophysical processes, they require tens of millions of CPU hours and can be difficult to compare to observations. Therefore, there exists an opportunity to provide methods to generate images of early galaxies with physical shapes that can be used to develop tools to extract morphological information from upcoming JWST data. Methods: The need to accurately simulate the data of the upcoming JWST Advanced Deep Extragalactic Survey (JADES) has led to the development of the JAdes extraGalactic Ultradeep Artificial Realizations (JAGUAR) mock catalog package (Williams et al. 2018). The package uses an empirical model for galaxy properties across cosmic time, including morphologies. However, the JAGUAR morphologies are modeled by simple symmetrical spheroids, an unrealistic assumption for early galaxies as shown in Figure 1. My first task will be to assign realistic morphologies to each mock galaxy. Here I will couple the results of hydrodynamical simulations, similar to Illustris, with existing observations of distant galaxies to inform the shape, structure, and composition of morphological components in each mock JAGUAR galaxy. By folding these in with JWST data simulation tools, I will produce physically motivated images of early galaxies with colors and luminosities that agree with observations of the distant universe. Following my creation of mock galaxy images, I will test techniques to extract morphological information from the data. Astronomers have attempted to tackle the problem of accurate characterization of distinct features in images with an ever growing suite of machine learning techniques. I will robustly examine existing techniques and find the approach that is best suited for categorizing asymmetric NSF Graduate Research Fellowship Plan Raphael E. Hviding Graduate Research Plan Statement and distorted morphologies of early galaxies. These include ensemble classifiers, such as a random forest decision trees employed in classifying galaxy mergers, e.g. Goulding et al. (2017), or deep learning algorithms trained on existing data sets, e.g. Sánchez et al. (2018). While both have been used to great effect, these procedures have only been tested in the local universe. I can therefore adapt and test candidate algorithms on my simulated images, measuring the method’s ability to recover input parameters. I will evaluate the efficacy of a broad set of techniques to find those that extract morphological parameters accurately for early galaxies over a wide range of cosmic times. The Steward Observatory at the University of Arizona is the ideal host institution for this endeavor, hosting the Principal Investigator of the NIRCam instrument on JWST (Professor Marcia J. Rieke) and the primary authors of the JAGUAR project (C. C. Williams and K. N. Hainline). I am therefore confident I can integrate my enhanced morphological modelling with current mock catalog efforts. Ultimately, I will produce a data analysis framework optimized for extracting morphological information from early galaxies validated through observations and simulations which will help determine why galaxies look the way they do today. Fig. 1: Young galaxies from current observations on the left (Bowler et al. 2017) compared with current models in the JAGUAR mock catalog on the right (Williams et. al 2018). Note the discrepancy in the level of structure. Conclusions and Broader Impact: My simulations of early galaxies and corresponding extraction algorithms will have immediate implications for the characterization of complex morphologies of young galaxies. These will be useful for any large-scale extraction of galaxy parameters in the early universe and essential for the success of the extragalactic component of the JWST mission. My tools will be made publicly available to the astronomical community to further the understanding of galaxy evolution to one day understand the nature and fate of all galaxies. In addition, I aim to bring the results of my research to the general public in Tucson. Given the intuitive nature of galaxy shapes, I aim to create lesson plans for the elementary school children I work with through Project ASTRO. By using my simulated images of galaxies, I will present a realistic view of the scientific capabilities of JWST in a fun and informative manner to inspire curiosity about extragalactic astronomy. Furthermore, simulated JWST observations of realistic images of galaxies are ideal for public lectures. By presenting a compelling visual narrative of watching galaxies change shape over time, I can communicate the main outcomes of my study that uniquely engages an audience. I plan to present my work at public talks that are held throughout Tucson, including the Steward Observatory weekly lectures and the Astronomy On Tap series. With support from the NSF Graduate Research Fellowship, I can develop useful and relevant tools for the astronomical community while taking advantage of the intuitive nature of galaxy shapes to bring high level cutting edge science questions to the general public. References: Agertz+, 2009, MNRAS, 397, L64 Elmegreen+, 2009, ApJ, 692, 12 M i h os+, 1996, ApJ, 464, 641 Bowler+, 2017, MNRAS, 466, 3612 Genel+, 2014, MNRAS, 445, 175 Sánchez+, 2018, arXiv: 1807.00807 Elmegreen+, 2007, ApJ, 658, 763 Goulding+, 2017, PASJ, 70, S37 Williams+, 2018, ApJS, 236, 33	0
that microbes constantly pass through the human gastrointestinal (GI) tract. During transit, physical and chemical barriers such as peristaltic action, pH gradients, and intestinal enzymes protect the gut from microbial colonization. However, select bacteria can overcome these barriers by adhering to specific niches in the GI tract using surface proteins called adhesins (1). Adhesins locate bacteria in environments conducive to their growth, granting them a selective advantage. Therefore, both the surface properties and environment of a microbe are essential to their survival. While structural research regarding adhesins and their targets is expanding, little is known about the parameters governing microbial binding within the GI tract; describing these parameters therefore can lead to a fundamental understanding of bacterial attachment. Previously, complex microfluidic devices that simultaneously incorporate physiological characteristics, mechanical forces, and mammalian cell co-cultures have been used to study microbes in the GI tract (2, 3). While these are valuable models of in vivo activity, their complexity obscures analysis of how individual environmental conditions and the surface characteristics of microbes affect their attachment. To address this challenge, I have developed a modular platform to engineer the adhesive properties of live microbes and investigate their attachment under various conditions. This platform uses adhesive ligands conjugated to the bacterial surface as a proxy for adhesins, allowing tunable ligand density and specificity on bacteria. Using this system, the influence of bacterial surface properties on attachment can be analyzed in more detail than is possible with unmodified bacteria. Furthermore, by incorporating factors (either individually or in combination) such as bile salts, pH variation, fluid flow, and enzymatic activity, the influence of environment on bacterial attachment can be determined. I hypothesize that bacterial adherence in the GI tract is a consequence of both bacterial surface characteristics as well as environmental factors. By using a modular platform to control the microbe surface and its environment, the proposed approach can be used to study attachment of diverse bacterial species to biotic or abiotic surfaces under varying conditions. Aim 1: Determine how the specificity and density of adhesive ligands on microbial surfaces affect attachment. During my first year as a graduate student, I have developed a platform based on avidin and biotin binding to mediate the attachment of bacteria to targeted surfaces. Bacteria are first functionalized with biotin using N-hydroxysuccinimide (NHS) chemistry, which forms an ester bond between biotin and primary amines on the bacteria surface. Adhesive ligands (such as antibodies, as shown) are conjugated to streptavidin using amine-based chemistry and are then linked to biotin groups on bacterial surfaces following mixing (Figure 1A). To date, I have demonstrated that biotinylation of bacteria enhances binding to an abiotic streptavidin-coated surface (Figure 1B) and that attachment of Intracellular Adhesion Molecule (ICAM-1) antibodies enhances binding to live Caco-2 cells, a model cell line of intestinal 4000 3000 Biotin Targeting Ligand 2000 Bacteria 1000 0 0.0 0.2 0.4 0.6 Streptavidin Cell Density (OD600) ).u.a( ecnecseroulF 4000 TaTargrgeetteedd 3000 CCoonntrtrooll 2000 1000 0 Cell Density (OD600) ).u.a( ecnecseroulF A 5000 4000 3000 2000 1000 0 0.2 0.4 0.6 0 Control Targeted Figure 1. (A) Schematic of engineered microbe. (B) Concentration-dependent attachment of biotinylated GFP- expressing bacteria to a streptavidin-coated well plate. (C) Attachment of ICAM-1-targeted, GFP-expressing microbes to live Caco-2 cells compared to an unmodified control. ).u.a( ecnecseroulF C 5000 4000 3000 2000 1000 0 Control Targeted ).u.a( ecnecseroulF B enterocytes, in comparison to a non-targeted control (Figure 1C). To optimize this system, I will adjust the density and specificity of antibodies on bacterial surfaces. Antibody density is controlled by the extent of bacterial-surface biotinylation and will be varied using the ratio of biotin mass to bacterial density. The density of biotin sites on the bacteria surface will be quantified following fluorescent streptavidin probe attachment using flow cytometry. Next, the effect of antibody specificity will be determined with three anti-ICAM IgG antibodies (clones R6.5, 1A6, and 1A29) with varying specificity for ICAM on Caco-2 cells. Monolayers of Caco-2 cells will be grown in tissue culture treated well plates and incubated with GFP-expressing microbes that have varying antibody density and specificity. Bacterial attachment will be quantified using the fluorescence signal from attached bacteria, measured using a plate reader and analyzed for their spatial localization using microscopy. By enabling precise control over the surface characteristics of microbes, this aim provides clear insight into how these characteristics influence microbial binding. The antibody configuration (density and specificity) that provides the highest levels of attachment in Aim 1 will be further analyzed in Aim 2. Aim 2: Use Design of Experiments to analyze and optimize environmental parameters for microbial attachment. Design of Experiments (DOE) is a statistical 8000 approach to determine the sensitivity to and interactions between individual parameters that affect a system response (4). This approach 6000 will be used to screen four environmental factors for their relationship 4000 to microbial binding: bile salt concentration, pH, intestinal enzyme 2000 activity, and fluid flow rate. The factors will be tested at two levels that reflect the upper and lower limits experienced along the GI tract. 0 Microbial attachment will be determined using a GFP-expressing strain in well plates (for static studies) or a straight-channel microfluidic chip (for fluid flow studies) that I have previously validated for targeted microbial attachment (Figure 2). Attachment of microbes will be quantified both through automated particle counting in ImageJ and with fluorescence intensity in the well plates or microfluidic channels. Results from the screening experiments will be used to identify which of the factors significantly influence microbial attachment. A secondary study with the identified factors will be designed to determine a response surface model and a desirability profile for microbial binding in varying environmental conditions. This will be used to estimate the conditions that optimize microbial attachment, which will be validated experimentally. By optimizing the environmental conditions, this model can determine the location of the GI tract that is most conducive for a bacteria’s attachment. Broader Impacts. This interdisciplinary proposal applies techniques from materials science, engineering, microbiology, and biochemistry to analyze parameters that influence microbial attachment and represents a practical, tunable system for modifying the adhesive properties of microbes. Successful completion of this project will provide valuable insight into the mechanisms of microbial colonization of the human GI tract. More broadly, due to the ubiquity of microbes in the biomedical sector, modular platforms that can be used to provide mechanistic insights into a variety of microbes on both biotic and abiotic surfaces are sorely needed. As research is most impactful when communicated directly to the public, I will teach middle school students the profound role bacteria play in our lives and our environments through Morehead Planetarium’s SciMatch program. Additionally, I will broaden the reach of this research with a novel art/science collaboration designed to increase public scientific literacy in Chapel Hill. [1] Stacy et al. Nature reviews. Microbiology 14, 93-105 (2016). [2] Kim et al. Lab on a chip 12, 2165-2174 (2012). [3] Bhatia and Ingber. Nature Biotechnology 32, 760 (2014). [4] Anderson et al. Productivity Press (2007). slleC dehcattA Control Targeted Figure 2. Attachment of biotin-conjugated bacteria to streptavidin-coated channels under flow (1μL/min)	0
Vehicles Background and Motivation: Solar-assisted electric vehicles (SEVs) are the ultimate zero-emission vehicles since they do not contribute to greenhouse gas emissions. However, due to the low power output of the photovoltaic array, widespread use of SEVs can only be realized if the vehicle system is highly energy efficient. Such rigorous optimization for limited power applications will lead to universal efficiency increases in vehicles of all kinds. One major boost in vehicle efficiency is to utilize in-wheel motors. For example, Protean Electric’s in-wheel motors were found to increase the 244 mile range of the Tesla Roadster by 14%.1However, this ​ ​ pivotal technology also increases unsprung mass; in Protean’s case, 30 kg at each wheel. The addition of unsprung mass causes more energy losses in the shock absorbers due to increased vibrational forces in the suspension.2 It also adds complications traditional vehicle control for ​ ride comfort and safety.3 I would like to propose a comprehensive approach to maintain ride ​ comfort and vehicle safety while harvesting suspension energy loss so that in-wheel motors can become the enabling technology for SEV’s. Research Method: While working at the Advanced Transportation Energy Center (ATEC) and ​ Future Renewable Energy and Electricity Distribution Management (FREEDM) Center at North Carolina State University (NCSU), I will develop multi-level control for an adaptive regenerative shock absorber (ARSA), which will lead to improvements in electric vehicle efficiency. An existing high efficiency ARSA will be optimized to make it comparable in weight to traditional shock absorbers. The novelty of the design will be the adaptive control strategy, which adjusts ARSA parameters to changing road conditions to optimize safety, comfort, and efficiency. The first layer of the control strategy will determine if an energy consumptive active or energy regenerative semi-active control should be used. The second layer will use an adaptive algorithm to adjust ARSA parameters based on vehicle response to road excitation. At the third level, an energy optimization strategy will ensure that the most energy is captured based on the corresponding mode of operation. The dynamic nature of the suspension will require quick response from the control scheme so the controller will be implemented in a closed-loop with negative feedback. Intellectual Merit: The SEV is a highly interconnected system and understanding the ​ relationships between its subsystems can increase its overall efficiency. This work will result in more intelligent suspensions that allow in-wheel motors to make better electric vehicles, including those assisted or driven by solar-power – the pinnacle of sustainable transportation. The potential of the ARSA is recognized among automotive suspension engineers worldwide.4,5,6 Previous work has resulted in the development of suspension control algorithms ​ optimized for specific conditions or objectives.7 An adaptive, multi-level control scheme will ​ allow all benefits of the ARSA in dynamic situations. Facilities: NCSU is the ideal place for this research due to the extensive resources available and ​ established collaborations. At FREEDM, I will have access to the dSPACE vehicle chassis simulator, which serves as the vehicle model, to quickly simulate the ARSA and develop control logic via software-in-the-loop (SIL). Once my design is constructed, I can integrate it with the ​ Lab Rapid Assessment Tool (LabRAT), a bare-skeleton vehicle made for bench testing components, at ATEC. ATEC also has an all-wheel drive dynamometer which I will use to get control data for comparison to simulation and road tests. The solar car team, SolarPack, will provide access to the prototype SEV for full integration of the prototype ARSA. Connections through SolarPack, such as the Haas Formula 1 racing team, can be leveraged to arrange premier testing facilities for accurate practical experiments. Timeline: 1. Year One - Simulation and Design ​ ​ ​​ Objective: Design and simulate a highly efficient regenerative shock absorber with ​​ semi-active and active controls Methodology: Literature review and optimization of existing ARSA topology. Use ​​ dSPACE simulator to implement active and semi-active controls to prototype which will be integrated in future multi-layer control. Simulation using stochastic road models will be used to guide design. 2. Year Two - Active Regenerative Shock Absorber Experimentation ​​ ​​ ​​ Objective: Build ARSA and experiment with control parameters to optimize both active ​​ and semi-active logic. Methodology: Integrate the ARSA into the LabRAT to iterate the control parameters in ​​ each control strategy. 3. Year Three - ARSA Integration, Controls Assimilation and Testing ​ ​ ​​​ Objective: To evaluate and improve robustness of control scheme which will result in an ​​ ​ ​ ​ ​ ​ ​ ARSA that is ready for integration to the SEV. Methodology: The ARSA and multi-layer control will be integrated to the SEV for ​​ experimentation. Accurate road experiments will be done to iterate to robust control. Robustness will be assessed by designing to International Organization for Standardization requirements. Broader Impact: With the anticipation that societal and economic growth will require increased ​ movement of people and things, we must ensure that our methods of transportation are clean and efficient. ARSAs are a step towards a better future where transportation is not only sustainable, but more intelligent. The SEV represents the most efficient vehicle topology and will benefit from the development of the ARSA because it will enable the in-wheel motor. Using the in-wheel motor will result in improved vehicle range by decreasing energy usage of the drivetrain. The proposed control strategy is not constrained to passenger vehicles and can be adapted to all ground vehicle applications. The findings will be communicated in IEEE and SAE conferences and journals, and the technology will be demonstrated in solar car outreach activities, such as the EV Challenge, the Science Olympiad, and minority engineering summer transition programs to engage early college and high school students in automotive engineering research. SolarPack has already arranged to show our SEV at future North Carolina Science & Engineering Fairs, which will be a great way to inspire K-12 students to break barriers in the automotive industry through research. The NSF Graduate Research Fellowship will allow me to engage sustainable transportation research on a deeper level and focus on contributing unique insights to the automotive engineering field. References: 1 ​Watts. A et al., SAE International, 9-10, 2010 ​ ​​​ 2 ​Anderson M. et al., Advanced Vehicle Control, 2-3, 2010 ​ ​ 3 ​Rojas. A.E. et al., SAE International, 14, 2010 ​ ​ 4 ​Shi D. et al., Smart Materials and Structures, 8-9, 2015 ​ ​ 5 ​Zuo L. et al., Smart Materials and Structures, 2010 ​ ​ 6 ​Sabzehgar R. et al., IEEE/ASME Transactions on Mechatronics, 2013, ​ ​ 7 ​Huang K. et al., International Journey of Automotive Technology, 2011 ​ ​	0
Game Ranching in Botswana: Effects on Wildlife and Rural Communities Keywords: wildlife conservation, resource tenure, community-based natural resource management (CBNRM), Botswana Objective: To develop an interdisciplinary understanding of the effects of game ranching on wildlife conservation and CBNRM in a livestock-wildlife conflict area Research Focus: In the mid-1990’s, “Use it or lose it” emerged as a controversial wildlife policy slogan, indicating that wildlife would have to pay its way, through consumptive and non-consumptive use, if it were to survive.1 This major shift from existing colonial protectionist strategies is a critical part of today’s African land-use planning discourse. Game ranching, the focus of my research, is the intentional management and maintenance of wild animal populations for subsequent human use (i.e. meat, trophy hunting).1 Touted by its proponents as a sustainable use of land that both conserves biodiversity and enhances livelihoods,2 ranching already is an established industry in South Africa and Namibia. Studies show that game ranching has less impact on land than large-scale cattle ranching,3 yet its viability for wildlife conservation continues to be debated. Furthermore, game ranching’s implications for community- based management of natural resources (CBNRM) has yet to be explored. CBNRM aims to devolve management of and benefits from natural resources to communities so as to create incentives favoring sustainable use.4 However, rights granted under CBNRM do not guarantee that communities will benefit from a given resource.5 In Botswana, communities do not have full control over the key determinants of resource conservation and economic development—hunting quotas, market prices, robustness of wildlife populations, macro-economic/political conditions, and ownership over the land and wildlife itself.6 Therefore, communities rarely invest in natural resource infrastructure and conservation.7 Competition from private game ranches may also threaten CBNRM viability; however, the development of game ranching on communal lands could provide new opportunities for CBNRM projects, as game ranching by definition involves intense management of natural resources. Although game ranching on communal lands is in its infancy in Botswana, a country noted for both conservation and CBNRM initiatives, it merits study given its potential to affect the current community-based conservation model. Social and ecological aspects of environmental phenomena have repeatedly been shown to be interdependent;8 thus, rigorous study of game ranching requires an interdisciplinary approach. The ecological component of my research will take place on private game ranches because there are few community-managed game ranches in Botswana. I will address the question of whether game ranches promote overall conservation of wildlife species at levels similar to that of nearby protected areas (PAs), or merely conserve harvestable species with clear economic value. My sociological research on the implications of game ranching for CBNRM will examine how resource management capabilities and decision-making authority of communities change when game ranching is incorporated as a community-managed program. If game ranching on communal lands increases community security of tenure over wildlife, do communities then invest more in wildlife management? Research Hypotheses: A) Relative to PAs, game ranches (i) maintain similar densities of economically valuable wildlife species (ii) show smaller densities of species with zero or negative economic value. B) Game ranching allows for more management over natural Copyright © 2007 by original author. All rights reserved. resources than do other forms of wildlife use. C) Community-managed game ranches increase security of tenure over wildlife. D) Increased community management and secure wildlife tenure leads to community investment in wildlife management. Methods: My research will combine standard ecological sampling and field methods with the sociological extended case method, which examines interacting effects of external forces on a particular case in order to modify wider theoretical assertions.9 Table 1. Integrated Ecological and Sociological Research Methodology Ecological sampling on private game farms In-depth case study at Dqãe Qare10 (target sample size = 12-14 ranches in central Kalahari) (community-run game ranch in central Kalahari) • Determine distribution & abundance of species with (+), (-), • Interview key informants to determine if and no economic value to game ranches game ranching leads to increased community • Survey methods: a) detection rates along foot transects for control over natural resources compared to direct sightings, track and scats11 b)‘capture’ rates at remote other CBNRM ventures photographic stations12 (to ↑ chance of detecting species, ie. • Indicators of control: a) extent of legal rights elusive carnivores) over land & wildlife b) ability to self- • Compare with parallel data collected from: adjacent determine hunting quotas c) stability of livestock ranches & nearby Central Kalahari Game Reserve revenue (CKGR) to determine game ranching’s impact on local • Conduct structured household surveys & wildlife biodiversity relative to other land uses key informant interviews with community • Other data sources: a) Dept. of Wildlife wildlife population participants in the game ranch on: a)perceived census data in CKGR b) interviews with ranch managers levels of control over natural resources b) about nature and level of ranch management practices (i.e. willingness to invest in wildlife management control strategies for predators, bush clearing, fencing and c) actual levels of investment in wildlife veterinary care) and land-use history management Expected Results: 1) Game ranching’s effects on species’ populations vary depending on the species’ economic value to the ranch 2) Community game ranches have increased level of control over natural resources, stimulating investment in wildlife management. Significance: This research will contribute novel interdisciplinary knowledge that is meaningful to both Botswana and the broader field of conservation science. My study site is ideal because: 1) it encompasses a matrix of land-use types across a continuous landscape, enabling assessment of game ranching’s impact on biodiversity with few confounding factors; 2) I am already familiar with Botswana’s ecology, economics, and socio-politics and have good working relations with key stakeholders; and (3) game ranching is new in Botswana so my results can influence future policy. (I certify this proposal represents my own work and ideas—ACG) 1 Kock, R. A. 1995. Wildlife utilization: use it or lose it—a Kenyan perspective. Biodiversity and Conservation 4: 241-256. 2 Luxmoore, R. 1985. Game farming in South Africa as a force for conservation. Oryx 19 (4): 225-234. 3 Smet M. and D. Ward. 2006. Soil quality gradients around water-points under different management systems in a semi-arid savanna, South Africa. Journal of Arid Environments 64(2): 251-69. 4 Murphree, M. and Hulme D. eds. 2001. African Wildlife and Livelihoods. Cape Town: David Philip. 5 Ribot, J.C. and Peluso, N.L. 2003. A Theory of Access. Rural Sociology 68(2): 153-181. 6 Barnes, J.I. 1999. Economic potential for biodiversity use in southern Africa: empirical evidence. Environment and Development Economics 4: 203–236 7 du Toit, J. et al. 2004. Conserving tropical nature: current challenges for ecologists. Trends in Ecology & Evolution 19(1): 12-17. 8 Blaikie, P. and Brookfield, H. 1987. Land Degradation and Society. New York: Methuen and Co. 9 Burawoy, M. 1991. The Extended Case Method. In Ethnography Unbound. UC Press: Berkeley. 10 I am taking Setswana lessons (national language) and will augment this with a San language course while in Botswana 11 Stander, P. E. 1998. Spoor counts as indices of large carnivore populations: the relationship between spoor frequency, sampling effort and true density. Journal of Applied Ecology 35: 378-385. 12 Carbone, C., S. Christie, K. Conforti, et al. 2001. The use of photographic rates to estimate densities of tigers and other cryptic mammals. Animal Conservation 4: 75-79.	0
Introduction: Externally actuated micro/nanorobots have generated considerable excitement over the last decade due to their potential to carry out controllable microbiological tasks.7 Specifically, microrobotic swarms,containingtensofthousandstomillionsofindividualrobotsthesizeofbacteria,havethecapacity to perform diagnostics and directed drug transport within deep tissues and microvasculars hitherto inac­ cessiblebyconventionalmeans.7 Unlikeindividualmicrorobots, swarmsleveragethecoupledinteractions between constituents to form large­scale collective motions that far exceed the speed, strength, and func­ tionalityofa singlemicrorobot. Because theseswarms areexternallyactuated bymagneticor opticfields, actuation schemes can be programmed to control a swarm’s collective motions and morphology. Specific examplesincludeclustering,swirling, dispersion,orribbonformation, whichtogetherallowforhighenvi­ ronmentaladaptivity.4 Unfortunately,experimentationaloneisinsufficienttoproperlydesignmicrorobotic swarms for real­world applications; instead, efficient computational tools are needed to augment experi­ ments by enabling rapid investigation into the effect of design parameters. However, modeling swarming microrobotsisinherentlychallenging—owingtothetheoreticalandcomputationalcomplexityofresolving the many­body hydrodynamic effects and short­ranged collisions. No state­of­the­art method is currently capable of accurately simulating real­world microrobotic swarms. This issue is further exacerbated by the generallackoffundamentalunderstandingofhowtobestgenerateandcontrolaswarm’scollectivemotions for specific tasks, especially within confined microfluidic environments. I aim to overcome these chal­ lengesby(1)creatingthefirsthigh­fidelity,scalablecomputationalframeworkabletosimulatedense suspensions of complex­shaped, microrobots and (2) numerically investigating how key parameters, likerobotshape,actuationscheme,andgeometricconfinementaffectaswarm’scollectivemotions. IntellectualMerit: Accurate simulation of microrobotic swarms within real­world microfluidic environ­ ments is essential if these systems are to be designed for practical biological applications. Previous work toward modeling these systems has primarily focused on dilute suspensions, where the long­range hydro­ dynamic interactions dominate the system dynamics, allowing the effect of near­body dynamics to be ap­ proximatedbyrepresentingcomplex­shapedparticlesintermsofsimplegeometrieslikespheres,ellipsoids, or rods. However, as the number of particles per unit volume increases, near­body interactions become increasinglyimportantcausingtheseapproximationstobreakdown. Withindensesuspensions,evenseem­ ingly insignificant modifications to robot shape, like using spherical robots vs cubic robots, will result in drastically different close­to­contact dynamics, which directly impacts internal pattern formations. There­ fore,fordensemicroroboticswarms,itisimperativethatthenear­bodydynamicsbetweencomplex­shaped particlesbeaccuratelycapturedtocorrectlypredict/controltheircollectivemotions. Task1­Isogeometricanalysis: Toaddresstheseissues,Iproposetodevelopahighfidelitymodelcapable ofaccuratelyresolvingthehydrodynamicinteractionsbetweencomplex­shapedmicrorobots. Guidedbythis goal,IhavebeenworkingindirectcollaborationwithProf. B.Shanker(anelectromagnetsexpert)andProf. H.M.Aktulga(ahigh­performancecomputingexpert)todevelopanisogeometricboundaryintegralmodel, which I am implementing within Python. The fundamental principal of isogeometric analysis is to utilize smoothbasisfunctionstorepresentparticlegeometriesandthephysicsontheirsurfaces,therebyproviding higherorderdescriptionoffieldsandenablingaccurateresolutionofnear­bodydynamics. Towardsthisend, Ireformulatedanexistingboundaryintegralsolver3 basedontheassumptionthatmicrorobotsaretypically genus­zeroshapes,allowingmetopullbacksurfacequantitiestotheunitsphereandthendiscretizeinterms of spherical and vector spherical harmonic basis functions. I then solve the governing boundary integrals through Galerkin’s method. One of the challenges when solving hydrodynamic boundary integrals is the evaluation of the nearly­singular integrals that arise when solving particle to self and particle to nearby particleinteraction. Toaddressthisdifficulty,Iderivedasingularity­freemethodforevaluatingparticleself­ interaction through established techniques of singularity subtraction/isolation. My next step is to integrate adaptivequadraturetechniquestohandletheinteractionbetweenclose­to­contactparticles. Oncecomplete, Iwillresolvetheeffectofno­slipconfinementsbyaddedadditionalconstraintstomylinearsystembasedon well­establishedmethods.6 Myfirstmilestonewillbetobenchmarkthismodelagainstanalyticalsolutions fortheflowbetweensphericalparticlesbothwithandwithoutconfinement. Task 2 ­ High performance software development: High fidelity simulation of microrobot swarms is computationally intensive and requires fast, scalable numerical methods to make modeling real­world sys­ temsfeasible. Thekeybottleneckistherapidcomputationofpairwisehydrodynamicinteractionsbetween 𝑁 particles, which scales as 𝒪(𝑁2). To overcome this issue, I will integrate my hydrodynamic solver into theparallelcomputingframeworkdevelopedbyProf. Shanker. ThisFORTRAN­basedframeworkcenters aroundtheAcceleratedCartesianExpansions(ACE)algorithm,whichreducesthecomputationalcomplexity ofevaluatingthepairwiseinteractionsfrom𝒪(𝑁2)to𝒪(𝑁).1Toaccomplishthistask,Iwillfirstconvertmy currentPythonimplementationintoFORTRAN.Iwillthendevelopsuitabledatastructuresfortheefficient computationandcommunicationofsphericalharmoniccoefficientsandcreatecustomMPIcommunication schemes for the tonsorial kernels that arise in our calculations. These implementation details are vital for ensuring that our framework remains computationally tractable and will be validated based on scalability. TheNSFGRFPwillsupplementourcomputationalresourcesbyprovidingaccesstoXSEDE,enabling metofullyharnessthecapabilitiesofthishigh­performanceframework. Task 3 ­ Dense microrobotic swarms under rigid confinement: The simulation of dense microrobotic swarms requires simultaneously resolving the hydrodynamic interactions and short­ranged collisions be­ tween particles. Unfortunately, traditional collision resolution algorithms become numerically unstable when applied to dense assemblies. To overcome this limitation, my collaborator Dr. W. Yan (a compu­ tationalbiologist)developedacollisionresolutionalgorithmusinggeometricallyconstrainedoptimization.5 Throughthiscollaboration,IwillexpandDr. Yan’sexistingopen­sourcecode­basetoincludefastmethods forevaluatingthedistancefunctionsandsurface­normalsbetweencollidingnon­convexparticlesbasedon advances within the computer graphics community.2 I will then couple this code­base with the framework developed in Task 2. Once complete, I intend to leverage the speed and flexibility of this computational tool to analyze how key parameters like robot shape, robot actuation type, and confinement geometry af­ fectaswarm’scollectivemotionsandpatternformations. Bysystematicallyperformingsimulationswithin thisexpansiveparameter­space,Iintendtoprovideexperimentalistswithacomprehensivepictureofhowto bestdesigntheirmicroroboticswarms. Toquantifytheeffectstheseparametershaveonaswarm’scollective motions,Iwillutilizemyexistingpost­processingtoolkit,whichIhaveappliedtoactivemattersystemsfor extractingtheirlarge­scaletopologicalstructuresandensembleaveragedstatistics. Basedontheseresults,I williterativelydesignloadingschemesandrobotshapestostreamlinethetransportandcapturingof large­scalecargowithinsimulatedmicrovascular­likeenvironments. BroaderImpacts:Controllablemicroroboticswarmshavethepotentialtoprofoundlyimprovepublichealth by facilitating novel treatment methods like the transport of chemotherapy drugs directly to cancer sites.7 My work’s efficient computational framework will augment existing experimental techniques by enabling researchers to virtually prototype their swarm designs within realistic environments. To facilitate the use of this framework by others, I will open­source and thoroughly document all software I develop. In doing so, Ihopetohaveafar­reachingimpactonthefieldsofsoftcondensedmatter, robotics, andmicrofabrica­ tion, whichcouldbenefitsignificantlyfromamodelfordenseparticulatesuspensions. Furthermore, Iwill disseminatethisresearchtonon­engineersbyparticipatingintheAlliancesforGraduateEducationandthe Professoriate’sChalkTalks,whichseektodistillcomplexscientificworksforgeneralaudiences. References:[1]Baczewski,A.D.,etal. 2010,JournalofComputationalPhysics,229[2]Bender,J.,etal. 2014,Comput. Graph. Forum,33,246–270[3]Corona,E.,etal. 2017,JournalofComputationalPhysics, 332, 504 [4] Xie, H., et al. 2019, Science Robotics, 4, eaav8006 [5] Yan, W., et al. 2019, The Journal of ChemicalPhysics,150,064109[6]Zhao,H.,etal. 2010,JournalofComputationalPhysics,229,3726[7] Zhou,H.,etal. 2021,ChemicalReviews,121,4999	0
duplicate or reproduce without permission. www.rachelcsmith.com Seasonal Migration Within Aseasonal Tropical Rainforests: A Phenomenon With Immense Implications INTRODUCTION: Tropical rainforests (TRF) are often considered aseasonal, however every TRF studied shows seasonal phenological variations corresponding to precipitation and solar irradiance1. Flushing, flowering, fruiting, and invertebrate biomass have general community- wide peaks during a region’s wet season, with large-fruited trees exhibiting the strongest phenological clumping1,2. Due to local precipitation regimes, phenology peaks vary in different geographical locations, creating spatio-temporal resource shifts1,3,4. Migration, the large-scale seasonal range shifts that occur in response to disparities in regional resources, has not been studied as a faunal survival adaptation within tropical rainforests1. Uncovering how animals move in response to seasonal resource shifts is critical to the conservation of migrating species and the ecological processes they perform5,6,7,8. Furthermore, species dependent on variable resources are the first to face local extinction after forest fragmentation. Moreover, migrating species are particularly threatened by current global climatic changes5,6. I will create a spatio-temporal model of fruiting shifts in SE Asia, then track hornbill movements to test my hypothesis that migration exists in TRF to follow resource shifts. BACKGROUND: Current research on migration as a response to seasonal resource shifts is focused on temperate and highly seasonal tropical regions5. While altitudinal migration that follows seasonal phenological changes does exist in TRF, large-scale seasonal migration that follows regional climatic differences is completely unreported2,6,7. Newton’s comprehensive textbook on migration argues that the increased movements required to cross climatic gradients and the limited resource inequalities between TRF negate the returns for intra-TRF migration7. However, highly mobile TRF frugivores like hornbills can traverse hundreds of kilometers per week, and in SE Asia, seasonality is sufficient to create resource disparities4,8,9. SE Asia is the optimal location to test for migration because monsoons create localized weather patterns in lowland TRF. Variations in wet seasons form a matrix of adjacent landscapes with offset phenologies1,4. Consumers depend on these spatio-temporal rhythms in the food supply1,2,3,8. Local seasonal resource disparities are extreme, exceeding six fold increases in fruiting species during months of peak rainfall. This provides incentive for migration6. Hornbills are large frugivorous birds that are highly mobile and capable of migration. They favor large, ripe, oily fruits in rare canopy/emergent tree species that fruit seasonally2,3,8,10. Hornbills are keystone seed dispersers and the SE Asian equivalent of toucans8,9. Hornbills track resources throughout their home ranges and juveniles are known to roam until they obtain territories, however hornbills are not known to migrate8. A seasonal flock of 3000+ plain pouched hornbills, Aceros subruficollis, has recently been discovered around Lake Tasek Temengor in Peninsular Malaysia11. The hornbills fly north after staying in the region during the two month period of peak rainfall and fruiting3,11. The destination of A. subruficollis is unknown, however, it has never been recorded as a breeding in Malaysia8,11. The seasonal presence of this flock in Malaysia for purposes other than breeding suggests that A. subruficollis is migrating outside of Malaysia, most likely into Thailand. If A. subruficollis is migrating, it would constitute the first documented migration by a TRF species7. Altitudinal migration can be refuted because A. subruficollis vacates from the Lake Tasek Temengor region where elevation changes exceed 1000m within a 30km zone11. A. subruficollisis is currently listed as a vulnerable species due to the rapid decline in small total population. In addition, the details of its range, life history and ecology are unknown8,12. NSF Proposed Research 2010 All Rights Reserved to original author. Do not duplicate or reproduce without permission. www.rachelcsmith.com HYPOTHESIS: i) Rainfall-driven local phenology differences have resulted in significant seasonal resource disparities across space within TRF. ii) A. subruficollis will migrate in order to exploit seasonal resources. Null: i) Resources are homogeneously distributed in time and ii) A. subruficollis movements do not correlate with resource abundance. OBJECTIVE 1: Create a spatio-temporal resource model using GIS mapping techniques to test the relationship between rainfall and phenology of fruiting trees. Then, model optimal migration paths for A. subruficollis based on distance and temporal resource abundances. METHODs: 1) Create regional monthly rainfall/fruiting species database. Precipitation data is available from the Malaysian and Thai Hydrological departments, phenological data is available from the literature1. 2) Model month-by-month rainfall and fruit abundance by region in ArcGIS. 3) Use large-scale layered models to quantify resource disparities across time and space. 4) Model A. subruficollis optimal movements to exploit spatio-temporal resource peaks. OBJECTIVE 2: Test if A. subruficollis migrates to exploit spatio-temporal resource abundances. METHODS: 1) Radio-track 15 A. subruficollis individuals for two years9. Capture birds with pulley-mounted canopy mist-nets at roost in Malaysia and attach satellite-transmitters at the base of the tail feathers9. Monitor their movements with a receiver9. 2) Input A. subruficollis movement data into a GIS spatio-temporal resource model. 3) Determine if there is causal relationship (using spatial auto-correlation) between movements and spatio-temporal resources. CONSEQUENCES: Migrants and species dependent on seasonal resources are particularly vulnerable to climatic changes and forest fragmentation5,7,8. Moreover, concerns about climate change stress the importance of keystone seed dispersers, like hornbills, to help move the trees to more suitable climates8,9. A positive feedback response could develop where keystone migrants disappear from disturbed forests, decreasing ecosystem functioning and future forest resilience. TRF migration also directly challenges Rapaport’s Rule of decreasing animal range size with latitude, a theory based on decreased resource variability in the tropics. The seasonal resource models I will create will bring the degree of variability into question. Migrating frugivores also provide rapid long-range seed dispersal along distinctive corridors and back to roosts, shaping the spatial regeneration patterns and diversity of forest trees3,10. BROADER IMPACTS: I will partner with the Forestry Research Institute of Malaysia (FRIM). Malaysian researchers will aid in all aspects of this project including anticipated co-authorships on publications, and becoming fully trained in the methods and analyses. FRIM helps to manage Malaysia’s natural resources, making it optimal to immediately bridge my research with policy and action. Additionally, this research will locate movement corridors that are critical to conservation efforts for this vulnerable species, which benefits future human generations of all nations12. A. subruficollis is also a charismatic species and important tourism draw in the region8. Finally, Dr. Poonswad at the Mahidol University in Bangkok has enlisted master’s students working on Thailand Hornbill Project (THP) to help track A. subruficollis in Thailand. Working with FRIM and THP will bring together an international team and facilitate the local and broad dissemination of results in English, Malay and Thai. BIBLIOGRAPHY: [1]van Schaik, C.P.,Terborgh, J.W. and Wright, J.S. 1993. Annun. Rev. Ecol. Syst. 24; [2]Walker, J.S. 2006.Biol. Conserv.130; [3]Medway, F.L.S. 1972.Biol. J. Linn. Soc. 4 [4]Kumagai, T. et al. 2009.Water Resources 45; [5]Both, C., et al. 2006. Nature 441; [6]Levey, D.J. 1994. The Auk 111;[7] Newton, I. 2008. Academic Press,London; [8]Kinnaird, M. F. & T. G. O'Brien. 2007. [9]Holbrook, K.M. & T.B. Smith. 2000. Oecologia125; [10] Hardesty, B.D., Hubbell, S.P., et al 2006. Ecology Letters 9.[11] Chew, H.H., & S. Supari. 2000. Forktail 16; Univ. Chicago Press; [12]IUCN 2009. Version 2009.1;	0
Life Sciences: Systematics & Biodiversity Background and proposal: Complex body plans evolve through the acquisition of heterogeneous material properties. In the late 18th century, architects began to combine rigid and flexible materials in order to achieve new levels of structural complexity in response to the limited space within urban areas. An analogous process happened in animals throughout the Cambrian explosion. During this time, complex animal morphologies diversified with the appearance of both rigid and flexible materials, such as skeletons, muscles, and fluid-filled cavities. The relationship between rigid and flexible materials and the production of diverse complex morphologies in skeletonized animals, such as arthropods and vertebrates, has been extensively studied. However, little is known about this relationship in gelatinous animals. In jellyfish (Cnidaria: Medusozoa), the underlying material properties are not well understood but still thought to play a major role in the evolution of diverse morphologies. Jellyfish have a free- living medusa stage (medusoid) with a bell that is responsible for locomotion1. Much of the medusa bell shape appears to be constrained to a concave ellipsoid, conserved across >2,000 species of jellyfish1. In stark contrast to this relatively simple medusa morphology, a unique subclade, the siphonophores (Cnidaria: Hydrozoa), exhibits morphologies well beyond this norm. Siphonophores are colonial animals that use asexual reproduction to produce physiologically integrated chains of individuals, called zooids. Siphonophores swim using retained, specialized medusoid zooids (nectophores), which propel the colony. Nectophores display astonishing diversity of extremely complex morphologies2,3 (Fig. 1). In order to achieve this diversity, siphonophores have something typical medusae do not: ridges and facets. In other systems, the development of ridges and facets is dependent on the distinct underlying material properties4. Like free living medusae, nectophores are composed primarily of mesoglea, an expanded extracellular matrix, which is a mesh network comprised mostly of collagen fibers located between epidermal and gastrodermal epithelia5,6. Anecdotal evidence of physically handling siphonophores indicates that the mesoglea has a wide variety of elasticity and stiffness both within a nectophore and across species depending on the presence of ridges and facets. It is unknown to what extent the ridges and facets of nectophores depend on the material properties of the mesoglea, such as the density, elastic modulus, and viscous modulus. Typical medusae, without ridges and facets, have homogenous mesogleal material properties. I hypothesize that complex nectophore morphologies with ridges and facets require heterogeneous mesogleal deposition within the nectophore. If supported, the nectophores with complex ridges and facets will have both regions with elastic properties and regions with viscous material properties. In contrast, the nectophores that lack ridges and facets will have homogenous mesoglea with material properties comparable to typical medusae. If the core hypothesis is not supported, it would mean that material properties and nectophore morphology can vary independently. If this is the case, I propose two alternative hypotheses: (1) heterogenous mesoglea evolved first facilitating the evolution of ridges and facets, and (2) ridges and facets evolved first, and were secondarily reinforced by A: Phylogeny & Morphology heterogeneous mesoglea. To Ridges/Facets N N Y N Y N Y (Y/N) test these hypotheses, I will characterize nectophore morphology and material Other Rosacea Vogtia Sphaeronectes Lensia Stephalia Agalma properties to test for Medusozoans phylogenetic correlations. I will also use the phylogeny to model trait evolution of the Siphonophorae Figure 1: Phylogeny and medusa morphology. Adapted from Totton (1932). Research Proposal Lauren Mellenthin morphology and material properties. This study will help understand not only how diversity in morphology arises, but how it evolves. Are mesogleal ultrastructure and correlated material properties phenotypically integrated with nectophore morphology? Answering this question recognizes the role material properties have in achieving morphological diversity in gelatinous animals. This work will contribute to the growing interest in the scientific community that realize diverse material properties and their critical role in the evolution of complex body plans. Methods: I will use museum specimens from the Yale Peabody Museum (YPM), which has ~180 species with ~2-3 specimens per species to describe the ridges and facets of nectophore morphology. Using literature and YPM specimens, I will delineate the presence and absence of ridges and facets across siphonophore species. Freshly collected specimens will be used for mesoglea and material property analysis. I will use scanning electron and differential interference contrast microscopy to image the mesoglea. To analyze material properties, I will use small angle x-ray scattering and rheology. These techniques characterize the structure of the mesogleal mesh at sub-micron lengthscales, density, and viscous and elastic moduli. All tools are available at Yale. Using a well resolved phylogeny7, I will test if morphology changes in congruence with mesoglea and material properties using phylogenetic mixed models in R statistical software8 with the pglm package. I will also test the order at which morphology and material properties arose via ancestral state estimation with the ape package. Feasibility: A risk of this project is the unpredictability of fieldwork to obtain fresh material for measurements of material properties. However, working with experts in siphonophore biology in the Dunn lab and collaborators will alleviate this risk. My previous research experience with Dr. Dean Adams in comparative morphometrics is directly applicable for questions of trait evolution. Former work at the Field Museum and current involvement in YPM has prepared me for handling museum specimens and contributing to collections for future generations. Collaborating with Dr. Alison Sweeney of Yale will contribute to the biophysical aspects of this project and her mentorship reduces radical differences between fields of physics and organismal biology. Intellectual Merit: Using and adding to YPM specimens leverage an important collection for a novel interdisciplinary project. Nectophores are a diagnostic trait of siphonophores and are used to characterize species, therefore this work will enhance what we know about siphonophore identification using gross morphology. Nectophore biology has broader implications for understanding how different zooid types contribute to colony integration. This work will also inform how extracellular matrix development has influenced metazoan evolution at early nodes in the metazoan tree, potentially with implications for body plan evolution. The emerging field of soft robotics currently use animal biomaterial properties to understand efficient fluid mechanics. A major breakthrough was a medusa-like robot. However, nectophores utilize heterogeneous soft materials to integrate a variety of functions beyond self-propulsion, allowing engineers to design robots exclusively of soft materials with extensive functional repertoires. Broader Impacts: I plan to bridge the disparate fields of physics and evolutionary biology, by sharing this work. Additionally, by being a graduate affiliate of the Yale Peabody Museum, I am able to share my research and educate about natural history to a broader audience. I will take advantage of this unique platform that encourages global curiosity about ocean exploration and overall scientific curiosity to put my work in perspective. Importantly, these opportunities excite both the scientific and public communities about current interdisciplinary research. References: Megill, W.M., PhD diss., McGill University (1991)1, Carré, C., Carré, D. Ordre des siphonophores (1995)2, Totton, A.K. Siphonophora. (1932)3, Gibson, L.J. The Royal Society (2012) 4, Bergheim, B.G. Essays in Biochemistry (2019)5, Gambini, C. Biophys. J. (2012)6, Munro, C. Molecular Phylogenetics and Evolution. (2018)7, R Development Team. R Foundation for Statistical Computing (2008)8	0
The nature of dark energy is one of the biggest mysteries in physics and astronomy today. To quote Michael Turner, dark energy is “a problem for the 22nd century discovered by accident in the 20th century.” The Dark Energy Spectroscopic Instrument (DESI) is a massive next- generation survey that will attempt to constrain the dark energy equation of state. DESI will measure the spectra of over 30 million galaxies and determine their redshift. These redshifts, combined with measurements of Type Ia supernovae (SNe Ia), provide the strongest measurements of cosmological distances, our way of knowing the expansion rate of the universe. DESI offers the potential to spectroscopically observe ~105 supernovae (SNe) including many SNe Ia [1]. Interestingly, while SNe Ia are used as standard candles, their origins are not fully understood. Evidence exists for two types of progenitors: a degenerate white dwarf accreting matter from a giant companion star, or two coalescing white dwarfs [2]. Identifying a large population of SNe Ia will help answer the progenitor question and provide constraints on dark energy. My background in handling large astrophysical datasets (see: Personal Statement) has prepared me make valuable contributions in this area. I propose to develop an efficient computational procedure to identify SNe Ia in the DESI survey. In order to obtain accurate, unbiased distance measurements from SNe Ia, spectroscopic measurements are crucial in calibrating corresponding photometric observations. In particular, DESI will be able to spectroscopically complement future large-sky surveys such as the Large Synoptic Sky Telescope (LSST), which will begin its science observing in 2021 but collect immensely more data (expected ~1 million transient alerts per night with ~1 million SNIa observed over a decade [2]), and the Zwicky Transient Factory. In doing so, we can understand just how “standard” these standard candles are and provide complementary redshift coverage. Further, it is to our advantage to identify SNe Ia in real-time and send out alerts for followup observations. Moving forward, generalized data-analysis pipelines that are able to handle enormous quantities of data will be fundamental to the success of future experiments. I propose the following analysis and timeline to develop such a computational procedure: 1. (year 1) Identify SNe Ia in galactic spectra. 2. (years 2-4) Extend the identification algorithm to detect and classify galactic spectral anomalies (outliers) in general. 3. (final year) Develop an automated pipeline that will run in real time to identify transients. In step 1. I will focus on identifying SNe Ia in the DESI catalog. This will require construction of spectroscopic models of galaxies and applying a statistical test to a large sample of galaxy spectra to look for deviations from the model expectations. Spectra with significant deviations (anomalies) will be tested further by fitting a SN Ia spectral template to see if the anomaly is in fact a supernova. I propose to develop several complementary tests to identify SNe Ia spectra. Previous studies have searched for SNe Ia in the Sloan Digital Sky Survey (SDSS) catalog using singular-value-decomposition of a large sample of galaxy spectra to construct a basis of eigenspectra. The eigenbasis is used to fit galaxy spectra, and the residual spectrum is searched for features corresponding to a SNe Ia (note that the model in this case, i.e. the individual eigenspectra, does not represent a physical object - only the linear combination has a physical interpretation). Using this method, Graur & Maoz [3] report 90 SNe Ia in SDSS data release 7. A first alternative approach involves constructing a χ2 statistic (or likelihood test) of SNe Ia templates with the observed spectra, defining anomalies based on the χ2 goodness of fit. The Ryan Rubenzahl NSF Research Proposal University of Rochester advantage is that no unphysical basis is being used. As a sanity check, I will cross-correlate my sample of SNe Ia with those found in previous studies [3]. With this sample, I will calculate an estimate of the SNe Ia rate to help answer the progenitor question and make first-order distance estimates using SNe Ia redshifts to build a framework for future photometric calibrations. While useful for cosmology, this method should be capable of identifying more than just SNe. In step 2. I will generalize this procedure to allow the classification to include any number of other astrophysically interesting phenomena. As a first step, this will involve adding templates for each source of interest, for example a two-galaxy-spectrum model to represent sources of strong gravitational lensing, which offer excellent tests of the general theory of relativity. Once the algorithm is efficient at detecting several different classes of objects, I will drop all a priori assumptions. In this case, the proposed algorithm will be general and unassuming of any particular class of object, allowing the potential of discovering new phenomena. A successful algorithm will also identify bad spectra either due to instrumental issues or other errors, learn what those patterns look like, and avoid or even correct for them in the future. At this stage, a plethora of astrophysical phenomena beyond just SNe may be observed and studied. Previous studies have tried general approaches to anomaly detection using predictive learning algorithms such as a random forest (decision tree), and have even found larger yields than targeted identification algorithms [4, 5]. I will study these approaches and develop a machine-learning algorithm to classify spectroscopic anomalies with DESI. I will work to boost the efficiency as well as the yield of outlier spectra while minimizing false-positives. In step 3. I will maximize the efficiency of the algorithm so that it can be used in real-time in a pipeline. These approaches will each be tested and trained on existing spectroscopic data obtained by SDSS. I will also test simulated spectra generated for DESI. Simulated data in particular will give an estimate of the classification purity of the pipeline for the various phenomena we are aiming to observe. Once DESI is online, the learning algorithm will be applied to the real data, being continuously trained as the dataset grows. An integral part of my work on this project will be engaging the public in active participation. The citizen-science project Galaxy Zoo Supernovae demonstrated that members of the general public are remarkably good transient-spotters: 93% of supernovae were correctly identified by the public with no false-positives [6]. I will make the results of my pipeline, including sample data, available to existing citizen-science platforms which have a proven track record of engagement and popularity with the public. Participants will enjoy hands-on involvement in the data analysis alongside lessons detailing the qualitative astrophysics of supernovae and how their signature can be seen in galaxy spectra through the heavy elements produced in the explosion. The data collected by citizen scientists can support actual results by providing confirmation or rejection of suspected outlier spectra. My findings, combined with the citizen-science results, will lead beyond DESI and into future LSST analysis and other data- intensive astronomy projects. References [1] DESI Collaboration, 2016, Final Design Report I [4] Baron & Poznanski, 2017, MNRAS, 465, 4530 [2] Maoz & Mannucci, 2012, PASA, 29, 447 [5] Buisson et al., 2015, MNRAS, 454, 2026 [3] Graur & Maoz, 2013, MNRAS, 430, 1746 [6] Smith et al., 2011, MNRAS, 412, 1309	0
in coral reef environments and providing ecosystem services that are intrinsic to the longevity of society. The diverse microhabitats provided by the elaborate morphologies of corals function as predation refuge and are essential for supporting the low trophic level (LTL) fish community.1 Specialist fish species will live within one coral colony (or others of similar morphology) for much of their lives, whereas generalist fish can associate with a wider variety of microhabitats. Trophic cascading of the LTL fish community results in flourishing commercial fisheries, which are estimated to be globally valued at $5.7 billion USD annually.2 Yet, the existence of coral dominated tropical reefs is largely threatened by global scale, anthropogenic warming-induced coral bleaching events—which has in part contributed to a 50-75% decline in worldwide coral cover over the last ~35 years.3,4 The loss of microhabitat often leads to drastic declines in the reef fish community5 and can crash commercial fishery markets. To mitigate against the further decline of coral reefs and the fisheries they support, restoration strategists in-part rely on large- scale coral propagation and outplanting—involving the artificial fragmentation of reef-obtained donor colonies and returning the clonal population back to the reef.6 Often, studies attempting to describe coral reef environments solely focus on percent coral cover and fail to capture the complex nature of coral reef ecosystems.7 It remains unclear how reef fish community assemblages are directly affected by bleaching- induced changes in microhabitat availability. Understanding fish-microhabitat associations is essential for devising targeted, efficient fisheries restoration efforts. The proposed research aims to elucidate the unique fish-microhabitat associations to better inform outplanting-based fisheries restoration efforts. Revealing fish-microhabitat associations would lead to the development of a comprehensive Coral Outplanting for Fisheries Guide (COFG) to be leveraged by coral restoration and fisheries managers. This research also aims to capture changes in the population levels and spatial distributions of commercially valuable high trophic level (HTL) populations while under the presumed pressure of depleted LTL prey populations following a bleaching event. Hypotheses: I hypothesize that (1) bleaching events will induce the largest decreases in specialist, LTL fish populations relative to generalist fish populations (H1), (2) bleaching events drive HTL predator populations to relocate to less-affected regions of the reef where food sources are sufficient, or in more extreme cases, recruit to nearby reefs owing to the reductions of LTL fishes associated with H1 (H2), and (3) outplanting of fisheries-specific coral taxa will facilitate the recovery of the fishery stock (H3). Experimental Approach: Timeseries Density Maps: The framework of one entire reef would be imaged before and after a single bleaching event, which would be scheduled according to existing local degree heating week (DHW) data. DHW is a measure of accumulated thermal stress obtained by the 12-week time-integration of sea surface temperature data exceeding the local bleaching threshold and is a reliable bleaching predictor.8 The onset of bleaching is expected when DHW values reach 4 °C-weeks, whereas mass bleaching and mortality is expected at 8 °C-weeks.9 The framework would be characterized by generating high taxonomic resolution photomosaics10 of the benthic coral community coupled with ArcGIS-generated density maps of coral colony microhabitat volume approximated using structure-from- motion (SfM). SfM is a computationally intensive software that would allow me to digitally reconstruct the reef and extract microhabitat volume data from each colony. The movements of lower and higher trophic level fish populations would be continuously monitored utilizing size-specific acoustic telemetry transmitters and receivers to create 3D population density maps in ArcGIS. It is imperative to implant size-specific transmitters to minimize potential adverse health impacts to best isolate for tracking the natural movements of the fish.11 Fish populations would be estimated for all implanted fish taxa using well-established tag-recapture techniques and the appropriate stock assessment model according to the species-specific life history traits. Statistics: To determine specific fish-microhabitat associations and build the COFG, colony location and taxonomic classification will be tested against the time-based location density of the tagged LTL fish. To test for potential reef-level population reduction differences in LTL fish species (specialists vs. generalists) (H1), I would linearly model the tag-recapture-obtained population abundance data and evaluate whether species, time, and the interaction of species and time are significant predictors of mean abundance. To test for potential significant changes in the movements of HTL predator populations (H2), I would model the time-based location density of tagged LTL fish populations paired with the time-based location density of tagged HTL predators. I would encourage future studies to utilize the COFG produced by this research to answer H3. These studies would require quantifying the background recruitment rates of LTL fish populations and the new recruitment rates following a large-scale outplanting effort. I would also overlay the microhabitat volume density maps with the fish population density maps to allow for better visual interpretation of the data. Resources: I am applying to be advised by Dr. Sandin who is a leading expert in coral reef ecology at the Scripps Institution of Oceanography (SIO). Dr. Sandin’s team is comprised of many individuals with years of experience who would assist me in reliably imaging the reef and identifying coral colonies. I hope to also receive guidance from Dr. Brice Semmens (SIO), who often uses telemetry techniques in his research, to safely implant fish with acoustic transmitters and reliably track their movements. This study would rely on the resources available to SIO, especially the use of custom-framed, study-optimized cameras12 and SfM to digitally reconstruct the reef from imagery and perform the colony microhabitat volume calculations. The spatial monitoring of LTL populations would require surgically implanting fish with Juvenile Salmon Acoustic Telemetry System (JSATS)13 microacoustic transmitters and installing JSATS N201 receivers around the perimeter of the reef. The spatial monitoring of HTL populations would follow the same methods but require Vemco™ V16p-4H transmitters and VR2 receivers. Intellectual Merit: This research would provide valuable insight to ecologist’s holistic understanding of successional reef fish communities. Although previous work has evaluated the role of decreased reef framework on fish community composition using transect based methods,14 it remains unknown how shifts in specific coral species alter the population level and distribution of specific fish species. This research aims to reveal these mysteries with the increased quantized framework resolution from SfM and the paired monitoring of fish movements. This project would provide great predictive value to infer what reef-associated fish communities may look like in the future if the current frequency of bleaching events continues. Particularly, which fish species we might expect to decline at a given reef site if targeted conservation efforts, such as those that would be made possible by the COFG, are not enacted. Broader Impacts: The COFG developed from this research could guide the decisions of coral restoration and fisheries managers by detailing which coral species serve as primary microhabitat for a particular LTL fish population—enabling managers to easily identify which coral species to outplant to maximize the available microhabitat for LTL prey species for a specific fishery. In theory, increased microhabitat availability and trophic cascading would result in increased LTL prey population(s) and the HTL fishery stock. However, the world’s leading coral outplanting organization, the Coral Restoration Foundation (CRF), has only optimized the large-scale propagation and outplanting of 4 coral species. The fisheries- relevant coral species identified by this research that are not currently being outplanted would provide reason to increase funding for the development of new programs working to optimize the large-scale propagation of these species. The realization of this optimized restoration strategy would require a collaborative effort between SIO, CRF, and fisheries managers around the world to outplant the specific coral taxa that provide the most relevant microhabitat for prey of target fisheries. This innovative restoration optimization would be a valuable strategy to help work towards sustainable fisheries and maintaining their incredible economic value for future generations. I intend to disseminate the findings from this project via publications in peer-reviewed journals to drive similar studies that would build upon my findings and broaden the geographic relevancy of the COFG. I will present my findings to students at nearby institutions to inform aspiring ecologists of the problems our worlds reefs are facing, hopefully inspiring them to pursue related careers and research. References: [1] Bellwood et al (2004) Nature 429:827-833. [2] Cesar et al (2003) Cesar Environ Econ Consul, NLD. [3] Goreau & Hayes (1994) Ambio 23:176-180. [4] Bruno et al (2019) Ann Rev Mar Sci 11:307-334. [5] Jones et al (2004) PNAS 101:8251-8253. [6] Rinkevich (1995) Restor Ecol 3:241-251. [7] Brito-Millán et al (2019) Mar Ecol Prog Ser 630:55-68. [8] Liu et al (2014) Remote Sens 6:11579-11606. [9] Liu et al (2003) Eos 84:137-144. [10] Gracias et al (2003) IEEE J Ocean 28:609-624. [11] Lefrancois et al (2001) Mar Biol 139:13-17. [12] Kodera et al (2020) Coral Reefs 39:1091-1105. [13] McMichael et al (2010) Fish Res 35:9-22. [14] Richardson et al (2018) Glob Change Biol 24:3117-3129.	0
rates of gun-related homicides and emergency department visits are notoriously higher in the U.S. than in other developed countries. Gun violence disproportionately affects low-income and minority residents; homicide is the first (second) leading cause of death for young black (Hispanic) men [1]. Early death and incarceration contribute to staggering numbers of “missing” black men (83 black men per 100 black women), creating other social problems in minority communities [2]. Therefore, decreasing gun crime would reduce inequality by improving outcomes for disadvantaged minority groups. In addition, reducing gun violence would benefit taxpayers by decreasing Medicaid and Medicare spending on gun-injury related health care.1 Therefore, governments have several reasons to examine potential policy solutions to reduce gun crime. Background: Revitalizing urban areas with green spaces could be part of a policy solution to reduce gun violence, although the effects of vegetation on crime are theoretically and empirically ambiguous. On the one hand, trees and shrubbery could provide hiding places for criminals and inhibit neighborhood surveillance by obstructing views of the streets. On the other hand, greener spaces could deter crime (1) by providing community gathering spaces, thereby placing more eyes on the ground and (2) by dampening criminals’ sense of aggression through the physiologically calming effects of nature [3]. Understanding how green spaces affect criminal activity has important consequences for safety in urban neighborhoods. Intellectual Merit: A naïve comparison of gun crime between more and less green areas potentially includes selection bias, since more affluent, less crime-ridden areas also tend to be greener. Even a comparison across time for areas that become green can produce biased estimates if a confounding factor like gentrification of a neighborhood simultaneously increases green spaces and decreases crime. To circumvent such endogeneity, my research would exploit vacant lot renovation programs as close-to exogenous variation in green spaces, thereby identifying the causal impact of greening spaces on gun crime rates: a policy-relevant parameter. By combining this policy-induced variation with objective, detailed data on gunshots and geographic imagery and data across several American cities, my proposed research seeks to uncover how greening urban spaces affects gun violence: one important category of crime. Studying this effect is challenging with typical, reported crime data since renovating lots might affect reporting rates (if more people are present to report crime, for instance) and actual amounts of criminal activity. My study will address this issue by using new data from ShotSpotter gunshot sensors (described below). Furthermore, existing studies on the effects of greening vacant lots on crime rely on relatively sparse crime data in a single locality [4] [5]. In contrast, I would utilize data on a long time horizon, with more frequent observations,2 across several counties and municipalities.3 Therefore, I could contribute longitudinal and more precise, generalizable estimates of the effect of greening spaces on gun crime to the existing literature. Data: ShotSpotter is a technology that captures incidents of gunfire using audio sensors, providing comprehensive data on these events including precise geographic coordinates and timestamps. A key advantage of these data is that they are not subject to reporting bias or underreporting, thereby providing a more objective measure of gun crime: my outcome measure [6]. In order to measure 1 Medicaid and Medicare picked up nearly half of the costs of caring for Chicago survivors of gunshot wounds among costs between 2009 and mid-2016 analyzed by the Chicago Tribune. 2 According to Carr & Doleac, 911 reports of shootings (reports of assault with a dangerous weapon) capture just 12% (2-7%) of all gunfire incidents recorded by ShotSpotter. 3 ShotSpotter has been employed in at least 90 U.S. counties or cities since 2000, of which I currently have access to over 1,500 locality-months’ worth of data representing 27 unique localities. the treatment (greening of vacant lots), I will utilize cities’ databases on vacant lot renovations where readily available.4 To allow for analysis in cities where such databases are not available, I will use machine-learning algorithms on high-resolution digital aerial imagery and LIght Detection and Ranging (LIDAR) data to classify areas as green spaces, trees, or other objects of urban spaces, akin to methods described by Zhou & Troy [7]. While I have not worked with these kind of geographic data before, my background in machine learning coupled with support from GIS experts at my university’s library would allow me complete this step of my project. I will combine these data in ArcGIS to create a lot-time level panel dataset.5 Methods: The first part of the empirical analysis involves an event study, difference-in-differences (DD) approach before and after greening of vacant lots occur. Control observations will be vacant lots that were not renovated. The primary outcome measure will be the number of ShotSpotter- detected gunfire incidents within a specified radius from each lot. I will check the results using different radii from lots both (1) as a robustness check and (2) to determine how close to a green lot one should live to experience its effects: a question that remains underexplored in the current literature. Regression analyses will include year and month fixed effects (FE) and lot-, Census block-, or city-level FE. In specifications with city FE, I will control for Census block-level covariates available from the American Community Survey (ACS) including median income, home ownership rates, racial demographics, and other variables that could be associated with crime levels. I will cluster standard errors at the lot level: the level of treatment. Further analyses would adjust for spatial correlation. In addition to basic DD estimates, I will also employ methods of synthetic control to establish comparison groups that better demonstrate common pre-trends. A potential concern with this quasi-experimental approach is that greening spaces might just push the same amount of crime to other areas of a city not-yet renovated. However, this phenomenon would attenuate my estimates of the treatment effect since crime would increase in control areas relative to treatment areas.6 To further investigate the prevalence of this phenomenon, I could look at citywide impacts before and after periods of lot restoration. Assuming encouraging findings, I would also complete a cost-benefit analysis. I would gather information on the average cost to turn a vacant lot green and maintain it, and associated administrative costs. On the benefits side, I will use estimates of the social costs of gun crime (criminal justice claims, loss of life, and other costs [8]) to estimate costs avoided because of decreased gun violence. More broadly, I will translate my findings into units like dollars and lives saved that are salient and easily interpretable for policy-makers. I will also submit my findings to peer-reviewed publications and present at crime, urban policy, and economics conferences. This research has important implications for determining the potential of urban renewal policies, such as vacant lot restoration programs, to reduce crime. If vacant lot restoration programs are effective at curbing gun violence, minority populations would disproportionately benefit. References: [1] CDC data. [2] Wolfers, J., et al. (2015, April 20). The New York Times. [3] Kuo, F. E., & Sullivan, W. C. (2001). Environment and Behavior, 33(3). [4] Branas, C. C., Cheney, R. A., MacDonald, J. M., Tam, V. W., Jackson, T. D., & Ten Have, T. R. (2011). American Journal of Epidemiology, 174(11). [5] Garvin, E. C., et al. (2013). Injury Prevention, 19(3). [6] Carr, J., & Doleac, J. L. (2016). Brookings Research Paper. [7] Zhou, W., & Troy, A. (2008). International Journal of Remote Sensing, 29(11). [8] Gani, F., Sakran, J. V., & Canner, J. K. (2017). Health Affairs, 36(10). 4 For example, Branas et al use such a database to study Philadelphia, PA. 5 A lot-day dataset is possible for cities in which the exact renovations dates are known. Otherwise, I will create a dataset using whatever courser time level lot greening could be detected using aerial imagery and LIDAR data. 6 To determine the effects of spillover effects more exactly, future randomized trials could consider varying the proportion of lots renovated within a city among a sample of several cities.	0
"Key Words: Proteins, Electrostatics, Folding, Stability, Interactions, 3D Modeling, pK , Small a Angle Neutron Scattering, NMR, ab-initio calculations, molecular dynamics Hypothesis: The combination of experimental methods and computational modeling of the properties of proteins will enable a more fundamental understanding of biophysical principles and allow for innovative and practical solutions to problems in medicine, computational modeling, and bio-engineering. Introduction: The role played by electrostatics on protein folding, stability, and interactions is a crucial element in our understanding of the cell and its functions. Particularly, charged amino acids influence the stability and interactions of proteins, but their charge state depends on their pK . a Therefore, accurate predictions of pK values are critically important for successfully modeling a the pH-dependence of protein folding, stability, and interactions. Accurately modeling these characteristics of proteins can improve the development of practical solutions to problems that arise such as when certain bacteria render an entire set of antibiotics useless (Norris 2007). However, calculating pK values of amino acids in proteins and modeling protein characteristics a is a difficult task and a constant challenge in biophysics. Research Plan: To calculate the pK s of proteins, one must calculate the energy difference between a protein ensembles with ionized and neutral (non-ionized) amino acids to determine the pK shift, a i.e. the change of pK induced by interactions within the protein. Common methods used in these a calculations include the finite difference Poisson-Boltzmann (FDPB) method, the Generalized Born (GB) method, molecular dynamics (MD), empirical methods, and combinations of these techniques. However, with each of these methods there are strengths and weaknesses in predicting experimental data. Problems arise because protein structures are given by X-ray crystallography, and there are various structural artifacts due to the crystallization process. The calculations mentioned above must average over an ensemble of the multiple protein conformations. Since proteins do not have a static structure, the flexibility of proteins introduces more complexities as the charges will fluctuate due to this flexibility. Therefore, successful modeling of energies and pK values requires either (a) explicit modeling of these a conformational ensembles or (b) the development of approaches that can mimic the effect of ensembles. My previous research focused on the second task, as described in my previous essay. My future research will focus on the explicit modeling of conformation ensembles. The problem of modeling these complicated systems can utilize experimental methods as well. By complementing the computational modeling with experimental methods such as NMR spectroscopy and small angle neutron scattering, a stronger understanding of the properties of folding and conformation changes in protein interactions will be possible. At the University of Tennessee, there is continuous collaboration with Oak Ridge National Laboratory, and Oak Ridge has the premier Spallation Neutron Source (SNS). Neutron scattering allows for better spatial resolution of protein structures since neutrons interact with nuclei and the scattering can distinguish hydrogen from deuterium. Also, my work on the Palmetto cluster at Clemson enables me to work with larger supercomputers such as Jaguar and Kraken which are hosted at Oak Ridge. With the resources of NMR spectroscopy at the University of Tennessee and the SNS at Oak Ridge, there is a great potential for more accurate models of proteins and a better understanding of protein interactions. Anticipated Results: The use of ab-initio principles and molecular dynamics will accurately model changes made to ionization sites within proteins as I have demonstrated through the pKa cooperative initiative. With small angle neutron scattering, I will be able to use protein crystals to create more accurate 3D images of the protein. I will use this data to measure the accuracy of the proposed theoretical models and improve these models. Because of prior success, I am hopeful that the availability of new information from the scattering will further improve the accuracy of the models and aid in our understanding of protein folding, stability, and interactions. During my research with Dr. Serpersu, I learned how important it is to utilize various experimental methods such as ITC and NMR to understand properties of proteins. These are not the only experimental tools available, and I will learn and use all of the necessary methods. The use of experimental methods will be used to further verify and understand protein characteristics. It is important to investigate whether or not the addition of ab-initio calculations correctly models pK s. If the initial ab-initio calculations incorrectly model the pK , I propose that ab- a a initio calculations are completed for all sites that are the proximal to the charged amino acid. This will allow for more flexibility in the MD calculations. Using the method proposed above, it is expected that the results obtained will align closely with the experimental results. I have successfully modeled charged amino acids within alpha helices of proteins. In this case, ab-initio calculations rearrange the structure so that the side chain of the titratable group is modeled as facing the surrounding water or other medium. If the modeling of proteins using the aforementioned methods is indeed accurate, then the results of this research will allow scientists to predict the pK of proteins and to model the 3D structure of proteins more accurately. a Broader Impacts: A more fundamental understanding of the properties of proteins will also allow us to take steps towards the development of methods to combat antibiotic resistant bacteria. The results of my research will be published in peer-reviewed scientific journals, and I will present my research to the public to cultivate an interest in science among the general public. My local presentations of research have struck many chords with young students. I mentor high school students throughout the year, and they consistently ask me how they can get involved with research. Many of these students have gone on to research in their undergraduate careers. Through encouraging and communicating with younger students, there is a great potential that they will also become interested in how they can help change the world through the progress of science. Works Cited: Norris, A. and Serpersu, E. “Interactions of coenzyme A with the aminoglycoside acetyltransferase (3)-IIIb and thermodynamics of a ternary system.” Biochemistry. (2010). 49(19): 4036-42 Talley, K. and Alexov, E. “On the pH-optimum of activity and stability of proteins” Proteins: Structure, Function, and Bioinformatics. (2010) Talley, K., Ng, C., Shoppell M., Kundrotas P.J. and Alexov, E.""On the Electrostatic Component of Protein-Protein Binding Free Energy"" PMC Biophys. (2008)"	0
Regardless of signs of recovery from the economic crisis of 2007-2008, economic issues remain salient, and many Americans continue to experience stress due to their economic situation. Economic stress consists of both subjective and objective evaluation of one’s financial and employment-related stress1. My research will focus on four forms of economic stress: income inadequacy, financial fragility, underemployment, and job insecurity. Financial stress occurs when individuals perceive their personal financial situation to be insufficient to afford their needs and wants (perceived income adequacy) and are unable to cope with unexpected expenses (financial fragility), consequently leading to financial strain. While the unemployment rate in the United States is relatively low (below 5%), many workers are still experiencing employment-related stress due to underemployment and job insecurity. Underemployed workers hold jobs that insufficiently use their skills, abilities, education, or qualifications and may also receive less hours and pay than desired. Job insecurity is an individual’s subjective evaluation of the “perceived threat to the continuity and stability of employment as it is currently experienced”3. Employee perceptions of job insecurity have been empirically shown to increase stress, reduce mental, physical, and work-related wellbeing, and predict organizational commitment and turnover intention at work2-6. Perceived job insecurity may induce more stress than actual job loss or unemployment because the anticipation of job loss may prevent coping strategies to manage the stress. Along with unemployment, job insecurity has been a popular focus of economic stress research. However, there has been less concentration given to occupational health impacts of broader financial issues (e.g. 19). Economic stressors linked to the Great Recession, such as job insecurity, are associated with increased somatic symptoms experienced by individuals and a greater likelihood of alcohol abuse as a potential coping mechanism7. Relationships have also been found between perceived job insecurity of men and psychotropic drug use8. Additionally, empirical evidence suggests that illicit drug use may be a coping strategy for recessions and unemployment9. To my knowledge, research has not examined opioid use as a mediator of the relationships between economic stress and occupational health. For decades, the United States has struggled with an opioid epidemic. Opiates are drugs derived from the opium poppy plant that chemically interact with opioid receptors on nerve cells in the brain and in the nervous system to produce pain relief and pleasurable effects10. Opioid overdoses are driven by synthetic opioids (i.e. fentanyl), semi-synthetic opioids (i.e. oxycodone), and heroin and have led to four times more of American opioid-related deaths in 2015 compared to 199911-12. Long-term opioid use frequently starts with the treatment of acute pain13. Prescribed opioid pain relievers are the prescription drugs most often misused, resulting in a 72.2% increase in deaths due to synthetic opioids – not including illegally produced fentanyl – from 2014 to 201514. The number of opioid prescriptions written in 2012 would have been enough for every adult in the United States to have one bottle of opioid painkillers11. This longstanding problem has led to many societal consequences. The overuse and misuse of opioid substances costs the United States 80 billion dollars each year in healthcare, criminal justice, and productivity costs15. While the consequences of opioid use have been well-researched in other social domains16- 17, the relationships between opioid use and the workplace have received less attention. Current research shows that opioid use is related to absenteeism and work productivity18. My research will bridge this important gap in the literature on economic stressors, opioid use, and work-related outcomes. Gwendolyn Watson NSF Research Proposal Broadly speaking, my goal is to establish a stream of research that will answer the following questions: (1) What is the nature of the relationship between multiple economic stressors (job insecurity, fragility, underemployment, and income adequacy) and opioid use? (2) What are the causal mechanisms linking opioid use and economic stress? (3) How does opioid use relate to occupational health outcomes? Particularly I am interested in the impact on employee work engagement such as organizational commitment and turnover intentions. Opioid Use Work engagement Economic Stressors Retention Organizational Job insecurity Commitment Financial fragility Occupational Health Outcomes Performance Underemployment Other outcomes Income inadequacy This project will be a short-term investment with long-term goals. My short-term goals are to focus on the first two questions to establish the relationship between economic stressors and opioid use, as well as to identify causal factors to help explain the relationship. I will be analyzing a series of archival datasets and in-process data collection to test these relationships. Currently, my advisor Dr. Bob Sinclair is administering a longitudinal study online through Amazon’s Mechanical Turk (MTurk) that includes the variables of interest for my research (economic stressors, opioid use, and multiple occupational health outcomes). The first wave of this data collection is complete with over 700 participants. I also have access to multiple additional sources of survey data including other unpublished MTurk data sets, studies of retail employees, nurses, and larger population surveys. To test these initial relationships, I will conduct Multiple Regression Analyses and Structural Equational Modeling as applicable to each dataset. Having access to many existing and in-progress datasets will facilitate my short-term productivity and provide me with experience so that I can collect my own data in the future My long-term goals are to continue expanding on economic stress and opioid use to link their relationship to organizational and occupational health outcomes. I would like to further explore the workplace implications and potential interventions if opioid use is found to be a coping mechanism for managing both employment-related and financial economic stress. My current resources for data sets and collection will allow me to pursue my research interests and become a scholar in this area of this research. The NSF Graduate Research Fellowship will help me establish a program of work that will advance knowledge and have broader social impact. The proposed program of work will advance knowledge by bridging gaps in the literature concerning the causal links among economic stressors, opioid use, and work-related outcomes. In the long run, I hope to establish a program of multidisciplinary collaborative research connecting my work in applied psychology with scholarship in economics, public health, and business. Understanding the intricacies of the relationships between the variables of interest will have broader social impact as we will be able to identify potential antecedents of opioid use to create social or organizational interventions. This knowledge will help organizations better understand how to address issues of economic stressors and opioid use and ultimately mitigate negative occupational health outcomes. REFERENCES: [1] Voydanoff, P. (1990). Journal of Marriage and the Family.[2] Probst, T. (2004). Sage Publications Inc. [3] Shoss, M. (2017). Journal of Management [4] Cheng, G. H.-L., & Chan, D. K. S. (2008). Applied Psychology: An International Review [5] De Witte, H., Pienaar, J., & De Cuyper, N. (2016). Australian Psychologist [6] Probst, T. M., Stewart, S. M., Gruys, M. L., & Tierney, B. W. (2007). Journal of Occupational and Organizational Psychology [7] Vijayasiri, G., Richman, J. A., & Rospenda, K. M. (2012). Addictive Behaviors [8] Lasalle, M., Chastang, J. F., & Niedhammer, I. (2015). Journal of Psychiatric Research [9] Nagelhout, G. E., Hummel, K., de Goeij, M., de Vries, H., Kaner, E., & Lemmens, P. (2017). International Journal of Drug Policy [10] American Society of Addiction Medicine. (2016). [11] Heavey, S. C., Chang, Y., Vest, B. M., Collins, R. L., Wieczorek, W., & Homish, G. G. (2018). International Journal Of Drug Policy [12] Sarpatwari, A., Sinha, M., & Kesselhei, A. (2017). Harvard Law and Policy Review [13] Shah, A., Hayes, C. J., & Martin, B. C. (2017). MMWR: Morbidity & Mortality Weekly Report [14] Rudd, R., Seth, P., David., F., & Scholl, L. (2016). MMWR: Morbidity & Mortality Weekly Report, 65(50): 1445-1452. [15] Florence, C.S., Zhou, C., Luo, F., & Xu, L. (2016). Medical Care [16] Centers for Disease Control and Prevention (2014). [17] Zibbell, J. E., Asher, A. K., Patel, R. C., Kupronis, B., Iqbal, K., Ward, J. W., & Holtzman, D. (2018). American Journal of Public Health [18] van Hasselt, M., Keyes, V., Bray, J., & Miller, T. (2015) Journal of Workplace Behavioral Health. [19] Leana, C., & J. Meuris. (2015). The Academy of Management.	0
