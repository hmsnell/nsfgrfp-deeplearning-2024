text	Success
humanscalestructuresoutofmodularcomponentsinrealworldenvironments. IntroductionAutonomousunderwatervehicles(AUVs)fittedwithspeciallydesignedgripperswill build concrete retaining walls and artificial reefs with modular blocks. Blocks will be designed to providepassiveerrorcorrectiononbothpickupandplacement,makingthesystemrobusttonoise fromlocalizationandcontrol. Localizationwillbeachievedwithspeciallydesignedinfrastructure partiallyembeddedintheblocksthemselvestoprovidecertaintynearassemblyareas. Underwater construction is both dangerous and expensive because specially trained human divers must build and maintain structures by hand. At moderate depths, extreme measures must be taken to protect divers including the use of hyperbaric chambers to treat decompression sick- ness. Small mistakes or equipment failures during a dive are disastrous, often resulting in death. Atextremedepthsbuildinginpersoniseffectivelyimpossibleduetothereducedcapacityofpres- surized air tanks at high pressure. The substantial cost and risk of using divers on site leads us to favor assembly on land and then transportation to the final location which constrains the types of structuresthatcanbebuilt. Though autonomous underwater construction technology could open a frontier of possible ap- plications, no autonomous system exists to date. Teleoperation based solutions have received at- tention in the literature including a rubble leveling robot [2] and a back hoe [1]. Teleoperation eliminates the need for accurate localization, but it requires that large amounts of data are passed back to the human operator at a high rate. Communication underwater is often achieved using either a physical tether or acoustic networking. Managing long tethers is difficult and acoustic communicationislowbandwidth,limitingthepossiblerangeofteleoperationbasedsolutions. Research Plan My initial system will be built around a BlueROV2 underwater robot which is availableintheDartmouthroboticslab. TheBlueROV2isa10kilogram,0.5meterlongrobotwith 6degreeoffreedommotion. Itisanattractiverobottobasethesystemonbecauseitincludesawell maintainedcodebaseandseveralsensorsforlocalizationincludingtwoinertialmeasurementunits, pressuresensors,twocompasses,alowlightHDcamera,andashortbaselineacousticpositioning system. IwillutilizevisualmarkerscalledARTagsforvisualpositioninginformation. Research Question 1 How can we best exploit accurate localization information when it is avail- able, and smoothly switch to coarse grained techniques when it is not? Approach My preliminary experiments on localization suggest that sensor accuracy varies depending on the robot’s position and velocity. I will empirically model the noise and accuracy of the sensors under relevant cir- cumstances, then design a sensor fusion algorithm which exploits the most reliable information at every instant. Research Question 2 Given some desired structure, how can we design a feasible and robust build plan for the structure? Approach To design a feasible build order for a structure, itwillbenecessarytoconsiderthephysicalconstraintsofboththebuildingblocksandtherobot’s maneuvering capabilities. I will model the build ordering constraints induced by the structural andmaneuveringconstraintsasaconstrainedoptimizationproblem. Theobjectivefunctionofthis problemwillencodetheabilityoftherobottoexploitlocalizationinformationthroughoutthebuild. My initial work on build order optimization in 3D printing can be generalized to suit this applica- tion[3]. ResearchQuestion3Howcanweautomaticallyreacttobuilderrorscausedbychanging environmental circumstances? Approach To gain insight into the failure modes of the system, I willtestitrepeatedlyonaselectedfewbuildplansinboththepoolandLakeSunapee,NH.During each test, I will record and categorize the failures that occur. Based on the most common types of errors, I will develop automatic recovery mechanisms. For example, if a block is not fully seated onanotherblock,therobotcouldgentlynudgethemintoplace. Iftherobotpassesthroughacloud ofsilt,itcouldstopandwaitforthecloudtosettlebeforecontinuingthebuild. ExperimentPlanFirst,Iwillisolateeachsensoranddetermineitsaccuracylimitations. Toestab- lishtheaccuracyoftheIMUandARTagreadings,Iwillutilizeanindustrialroboticarmavailable inthelabtocollectaccuracyandnoisedatabasedonknownprogrammedmotions. Toevaluatethe positional accuracy of the sensor fusion algorithm on land, I will utilize a highly accurate VICON positioning system as ground truth. With the sensor quality models in hand, I will begin iterating on the sensor fusion algorithm. Preliminary debugging style experiments can be conducted in a 6 foot diameter water tank in the lab. Frequent field tests in the athletic pool and nearby lakes such as Lake Sunapee will inform further iterations. Simple tests such as repeatedly moving a block back and forth on a platform in calm clear waters will help isolate the quality of localization data. Build order and error recovery algorithms will be tested by selecting several simple build plans and repeatedly testing them in varying environmental circumstances. The ultimate goal will be to successfullyexecuteabuildplanneartheshoreoftheCarribeanseaduringoneofthelab’syearly tripstotheBellairsResearchInstituteinBarbados. Intellectual Merit Underwater construction will require the co-development of localization in- frastructure and sensor fusion strategies to rapidly instrument a build site. Rapidly deployable localization infrastructure will have applications in any autonomous robotic system attempting to achieve manipulation tasks in a remote or harsh environment. Rather than attempting to jump to unaided manipulation in totally unstructured environments, I will be taking the realistic approach of exploring the minimal amount of additional infrastructure required to complete manipulation tasks. This work will also advance the evaluation of the trade offs between computational and physical complexity in autonomous robotic systems. For example, designing blocks which more successfullycorrectforerrorcouldallowmanipulationbehaviorstobemoresimplewhilerequiring amorecomplexblockgeometry. Developingtechniquestorigorouslyevaluatetradeoffsbetween computationalandphysicalcomplexityforcomputational-physicalsystemsisanimportantstepin designingrobustroboticsystemsforrealworldenvironments. Broader Impacts A system which can robustly place modular components on one another could enable the mostly autonomous creation of retaining walls or artificial reefs. As ocean levels con- tinuetorise,retainingwallswillbeincreasinglyimportantindevelopedcoastalareas. Bystacking componentsofartificialreefs,wecouldenablelargerscalereefrestorationactivities. Asthetech- nology advances, it could enable more subtle applications as well. It could enable us to more efficientlybuildoffshoreenergyinfrastructureorenableustoscaleunderwateragriculture. This project is a valuable opportunity for young researchers to gain hands on experience with robotics and software development, preparing them for a productive career in an exploding field. During the conduction of the preliminary study, I worked with two high school students who con- tributed directly to this research. The students gained exposure to robotic software design and mechanical modeling. I will continue using this work as an opportunity to mentor driven young researchers. [1]TaketsuguHirabayashietal.“Teleoperationofconstructionmachineswithhapticinformationforunderwaterappli- cations”.In:AutomationinConstruction15.5(2006).21stInternationalSymposiumonAutomationandRobotics inConstruction. [2]T.S.Kimetal.“Underwaterconstructionrobotforrubblelevelingontheseabedforportconstruction”.In:2014 14thInternationalConferenceonControl,AutomationandSystems(ICCAS2014). [3]S.LensgrafandR.R.Mettu.“Animprovedtoolpathgenerationalgorithmforfusedfilamentfabrication”.In:2017 IEEEInternationalConferenceonRoboticsandAutomation(ICRA).	0
Introduction: Development of personalized medical treatments and diagnostics is limited by our ability to engineer tools that can keep up with demand. Exosomes are a type of nano-sized extracellular vesicle (EV) naturally produced by cells and released via endosomal fusion with the plasma membrane. Although primarily utilized in the detection of diseases such as cancer, exosomes are increasingly being investigated as a means of drug delivery due to their potential for multi-functional targeting and inherent biocompatibility [1]. A longstanding setback in the study and application of exosomes for therapeutic purposes is the lack of standardized methods with which to isolate, concentrate, and characterize them [2]. Although scientists can functionalize exosomes with targeting molecules and drug payloads, scale-up is limited by the ability to quickly identify favorable processing conditions and thus good manufacturing practices. In this project, I propose creating a system that will reduce the burden of the screening process by developing a tool to rapidly assess exosome production and functionality. This study will help others engineer EVs as a tool for medical and non-medical applications. Traditional methods quantify exosomes via nanoparticle tracking analysis (NTA) which relies on light scattering of particles to judge size and concentration via analysis of Brownian motion. However, NTA also counts non-exosome particles such as protein aggregates, leading to large discrepancies in actual determination of exosome quantity in solution. Moreover, NTA does not account for functionality of engineered exosomes, which typically display targeting molecules on their surfaces. Current exosome purification methods are labor intensive, requiring multiple days of centrifugation and gradient separation to remove the crude EV material prior to quantification and analysis of the exosome product [2]. Therefore, developing a method to rapidly quantify and assess exosome functionality with targeting molecules would enable scientists to focus their attention on scaling up and purifying only the most promising therapeutic exosome processes. This development would bypass the current time-consuming roadblocks associated with engineered exosome production, advancing the field. Objective: I will establish a high-throughput method to screen processing conditions for engineered exosomes. Protein microarrays, which have previously been used in diagnostic exosome assays [3], have the potential to quantify functionalized exosomes in an efficient and accurate manner. Therefore, I hypothesize that protein microarrays can be utilized as a high-throughput method to screen processing conditions for engineered exosomes. I plan to (1) investigate optimal microarray conditions by engineering spot formulations and process steps and (2) evaluate ideal processing specifications for exosome production by running protein microarrays in tandem with cholesterol-based quantification standards. Aim 1: Investigate optimal microarray conditions to create a high-throughput screening tool for functionalized exosomes. Protein microarrays can be modified to include different antibodies in each spot. The ideal formulation would be one that binds only targeted exosomes. Exosomes express the surface ligands CD9, CD63, and CD81 as unique identifiers from other EVs [2,3]. Antibody cocktails for these ligands, as well as the expressed targeting molecules, would enable binding of only the desired exosomes— even in the presence of a crude EV sample—while washing away all other materials in solution. I will determine the concentration of each antibody for optimal exosome binding kinetics, which is a function of the desired targeting molecule antibody. The formulation must also be modified to include a solvent material that is suitable for microarray printing but that does not interfere with the antibody-exosome interactions. Once I find the ideal antibody cocktail ratios, I will test the formulation to ensure that it prints appropriately on the microarray slides. The solution’s fluid properties must enable it to flow easily for spotting on the slides, dry quickly, and spread evenly. Different additives noted in literature would be investigated to find those that work best with the solution. I expect that a 5% glycerol content will produce the desired results, as it was used in a previous publication illustrating a method for exosome phenotyping using protein microarrays [3]. Various factors affect microarray accuracy, including sample application times, wash and blocking buffer identities and concentrations, number and duration of washes, and drying times. Each of these factors will be explored systematically using a factorial design of experiments (Fig.1a). If the microarray cannot be optimized for EVs, a column-based affinity separation method could be used to purify and confirm functionalization of exosomes. Completion of this aim will optimize the microarray as a tool for engineered exosome analysis. Aim 2: Evaluate ideal process conditions for engineered exosome production. Having identified the most favorable microarray conditions, I will compare processing conditions for engineered exosomes to prove the rigor of the quantification method. With this high-throughput method, dozens of different processing specifications—such as days in culture, feed conditions, pH setpoints, and centrifugation steps—could be analyzed simultaneously with a minimal demand on sample volume. The first step in engineering a suitable high-throughput method is to create a reliable standard for comparison. While working at Codiak Biosciences, I pioneered a cholesterol assay that circumvented time-consuming NTA by quantifying exosomes based on their cholesterol content and presented a poster on this work at the International Society for Extracellular Vesicles (ISEV) 2020 Annual Meeting. The fluorescence plate-based Amplex Red Cholesterol Assay was quick and accurate within a prescribed range of 0-8 µg/mL. Employing it also enabled me to quantify exosomes without the need for purification, reducing cost and time spent. I will utilize this preliminary work to develop a standard method for exosome quantification for this project. Once the standard is established, exosome samples conjugated with fluorescent tags will be applied to the microarray spots. After washing, only the exosomes with the desired surface ligands will bind to the spot and be fluorometrically detected (Fig.1b). The relative fluorescence of each spot will be compared to a standard where only the exosome detecting antibodies, not the targeting molecule antibody, are present. This relative fluorescence describes the number of functionalized exosomes in the sample. These signals will be compared to the cholesterol-based assay to obtain a quantitative readout of the number of engineered exosomes and their relative protein loading. If fluorescent tags are not feasible, dyes, luminescent substrates, or other types of markers could be conjugated to the exosomes to create a measurable readout. The results of this aim will elucidate the ideal conditions for engineered exosome screening. Intellectual Merit: This work will generate a deeper understanding of protein microarrays as high- throughput screening tools and can be applied to development of new exosome or nanoparticle-based technologies, which could be used in applications ranging from drug delivery to water treatment. It will expand the foundational knowledge surrounding EVs and support future endeavors to optimize the production of different types of engineered exosomes, aiding in the discovery and development of EV therapies. As a member of the Leonard Lab, which has expertise in exosome production and engineering, I am well positioned to complete this project. Broader Impacts: This proposal integrates chemical engineering, bioengineering, and biochemistry principles to create an interdisciplinary project that advances dynamic drug delivery platforms through high-throughput screening. Successful completion of this project will enable accurate exosome quantification, reducing labor and time investments. Rapid identification of improved processing conditions will support efficient and sustainable production of therapeutic exosomes, increasing manufacturing feasibility, and thus increasing their eventual accessibility on a global scale. I will leverage my connections at Codiak Biosciences to establish a collaboration to facilitate and support the project. I will disseminate the knowledge gained from this research to the scientific community for feedback and further development through conferences and publications, such as the ISEV Annual Meeting. Providing students with opportunities to learn about STEM fields and to participate in projects directly will foster the next generation of research scientists. I plan to direct my outreach toward programs encouraging underrepresented students to consider graduate school, such as REUs and the Northwestern Morning Mentors and Mentorship Opportunities for Research Engagement (MORE) programs, where I will mentor students and encourage their participation in STEM research. References: [1] Wang, J. et al. (2017). ACS Applied Materials & Interfaces, 9(33), 27441-27452. [2] Chia, B. S. et al. (2017). TrAC Trends in Analytical Chemistry, 86, 93-106. [3] Jørgensen, M. et al. (2013). Journal of Extracellular Vesicles, 2(1), 20920.	0
conserved protein belonging to the ribonuclear binding protein family.[1]It is involved in diverse, essential cellular functions including RNA transport and alternative splicing. TDP-43 is also the primary component of cytoplasmic aggregates that are the hallmark of amyotrophic lateral sclerosis (ALS), a neurodegenerative disease with very limited treatments and an average survival-after-diagnosis of 2-3 years.[2]Some of these aggregates are amyloid—aggregatescharacterized by a fibrillar morphology and β-sheet-rich structure as well as a prion-like ability to propagate the amyloid structure.[3]Amyloid proteins are closely linked to human disease, including many neurodegenerative diseases like Alzheimer’s and Parkinson’s. One role TDP-43 performs in the cytoplasm is protecting RNA during cellular stress by forming membraneless organelles known as stress granules (SGs).[4]SGsare concentrated droplets of RNA and RNA-binding proteins that are formedvialiquid-liquid phase-separation(LLPS), the spontaneous de mixing of a solution into a metastable concentrated droplet phase and a dilute phase driven by transient, multivalent interactions. Phase-separated membraneless organelles have recently been discovered to play critical roles throughout the cell, but the complete composition and internal protein conformations of these droplets are not fully understood.[5]Long-lived SGs (the resultof prolonged cellular stress) have also been shown to lose fluidity and become proteinaceous aggregates, suggesting a connection between protein aggregation and phase-separation.[4]A link between LLPS and amyloid aggregation is likely, as many amyloidogenic proteins can phase-separate without the need of other proteins or RNA.[6]In fact, it has been shown that LLPS droplets made from the C-terminal domain (CTD) of TDP-43 act as an intermediate for amyloid aggregation in some conditions (Figure 1).[7] The study of protein conformation inside any LLPS droplet has been very limited. Better understanding of the basic composition of droplets (i.e. water content) as well as the secondary structure of protein in LLPS droplets will help Figure characterize how it may serve as an aggregation 1. intermediate. Investigating whether either of these TDP-43factors change with droplet age will add further to states. the story. For example, is droplet aging and TDP-43 in (a) soluble (b) phase-separated, and accompanying loss of droplet fluidity due to CTD (c) aggregated states as viewed by brightfield changes in protein conformation, water confocal microscopy. Scale bars are 10 µm. exclusion,oranotherfactor?Doessecondarystructureofconstituentproteinschangeasdropletsage,ordo proteins remain disordered throughout droplet lifetime? Do aggregates formed via a phase-separation intermediate differ substantially from those formed in non-phase-separating conditions? Being able to address these questions will help 1) better understand LLPS and how itcarriesoutitsmanycellularroles, and 2) identify possible drug targets if TDP-43 LLPS proves to be linked to pathological amyloid aggregation. This work aims to understand secondarystructureofTDP-43inphase-separateddropletsand track changes that may occur in droplet hydration and protein conformation during the transition from LLPS to aggregate. Methods and Experimental Design. I propose to utilize Ramanmicro-spectroscopy—asensitivetypeof vibrational spectroscopy which utilizes Raman scattering—coupled to a microscope, which allows for spatial resolution (Figure 2a).[8] Water scatters strongly in the 3000–3600 cm−1 range, allowing for an approximation of hydration in droplets based on the intensity of the water bands versus a standard. The Raman fingerprint region (1000–2000 cm−1) informsonthesecondarystructureoftheprotein(Figure2b). Specifically, the formation of β-sheet-rich structures upon amyloid aggregation results in a characteristic peak at ~1665 cm−1.[9]Sidechain packing can be analyzedviaC-H deformation modes at 1300-1500 cm−1.[9]One of the strengths of Raman spectroscopyis that a single droplet can be observed over time with Raman spectra taken at multiple time points. Raman data from Murthyet al.of another phase-separated amyloid protein similar to TDP-43 suggests that protein within the droplets resembles soluble protein at early time points, but it is unknown if this structure is sustainable or if it naturally proceeds to a more amyloid-like secondary structure.[10]By followingthe lifetime of TDP-43 droplets, I can determine changes in hydration and protein structure as the droplets age and lose fluidity, filling the gaps left by Murthyet al.and expanding the work to a proteinmore relevant to ALS pathology. Aim One: Collect Raman spectra inside of LLPS can act as a mechanistic intermediate during TDP-43 LLPS droplets TDP-43 amyloid formation. TDP-43 will be used to prototype the experiment Aim Two: Characterize Differences between CTD because it reliably forms β-sheet-rich amyloid, and I Aggregates have previously characterized its aggregation Figure 2. Raman spectroscopy kinetics in a variety of conditions. TDP 43 can CTD phase-separate on its own and has been reported to drive aggregation of the full-length protein.[11]After proof-of-concept, full-length TDP-43 will be expressed and purified. The protein will be placed in phase-separating solution conditions (high salt, neutral pH), and phase-separation will be confirmed viabrightfield microscopy (Figure 1b). I will create a catalog of Raman data from each time point giving a detailed view of the structural changes during droplet solidification. I hypothesize that: 1) the conformation of TDP-43 will differ between its soluble, LLPS, and aggregated forms, 2) water content in droplets will (a) Diagram decrease as droplets age and water becomes excluded illustrating the set-up of a Raman microscope, from the stacked β-sheets formed by proteins in the adapted from [12] (b) Representative Raman amyloid conformation, and 3) as the phase-separated spectra of TDP-43 fibrils with evident C-H droplet ages, the secondary structure band around CTD deformations an amide I band reporting on β-sheet 1600-1700 cm-1will narrow and sharpen, indicatinga content. transition to β-sheet that mimics amyloid aggregates. Taken together, this data would demonstrate that TDP-43 can aggregate even when phase-separation is not present. I will use Raman spectroscopy to analyze the secondary structure of aggregates formed with and without the phase-separation. This will reveal any polymorphism in structure as Raman spectra are sensitive to not just secondary structure, but also side chain packing as seen in the C-H deformation bands. I will also use Raman to examine samples propagated from the brains of ALS patients (patient samples are used to ‘seed’ recombinant protein and, due to the prion-like nature of TDP-43, structure is preserved). By comparing the Raman spectrum of the in vitrophase-separated and non-phase-separated aggregatesto the spectra of the patient-propagated samples, insight into whether disease-related aggregation stems from a phase-separated intermediate will be gained.Resources and Suitability.I have significantexperience using Raman micro-spectroscopy in Dr. Jennifer Lee’s lab at the NIH. Paired with my prior work with TDP-43 and strong record of independence and publication, this makes me uniquely well-suited to pursue this project. The work will be conducted utilizing the Raman micro-spectrometer at the University of Wisconsin Centers for Nanoscale Technology. Intellectual Merit.By characterizing the generalhydration and composition of LLPS droplets, this project will provide foundational information on LLPS, a process of interest to fields ranging from polymer chemistry to cell biology. Additionally, if LLPS are shown to be intermediates in amyloid aggregation, this opens new doorways for drug development targeting the proteins in droplets. Broader Impacts. This project illustrates the value of an interdisciplinary approach in studying human health and disease processes. Specifically, it demonstrates how physical chemistry methods—the most micro scale—can offer foundational, useful information on emergent processes in complex biological systems. I hope that demonstrating the utility of physical chemistry, which can often feel hopelessly far removed and theoretical for students, will pique the interest of the next generation of chemists as I work with them as a teaching assistant and research mentor. References. [1] Y. Sun, Biochemistry 2017. [2] M. Neumann, Science 2006. [3] J. L. Robinson, Acta Neuropathol 2013. [4] C. M. Dewey, Brain Res 2012. [5] S. Boeynaems, Trends Cell Biol 2018. [6] S. Elbaum-Garfinkle, J BiolChem2019.[7]W.M.Babinchak,JBiolChem2019.[8]R.R.Jones,Nanoscale Res Lett2019. [9] Z. Movasaghi,Appl Spectrosc Rev2007. [10] A. C. Murthy, Nat Struct Mol Biol2019. [11] A. E. Conicella, Structure2016. [12] S. Lohumi,Appl Sc.2018.	0
More than 120,000 people in the United States are currently on the waiting list for life- saving organ donations; over 6,000 of these people die annually without receiving needed treatment. Tissue engineering may help address issues of donor organ shortage and transplant rejection. However, applications have been limited by the ability to design complicated scaffolds that reproduce the architecture present in the cellular microenvironment. 3D bioprinting (3DBP) holds promise for addressing these shortcomings by producing biologically-inspired structures based upon computer-generated models. Unfortunately, low cell viability due to lack of nutrient transport through thick scaffolds is a current barrier to clinical use. Without blood vessels to facilitate nutrient and oxygen transport, cells at the center of the constructs die. There is an urgent need to improve methods such as 3DBP to develop implants to treat these patients. One method to address this problem involves artificial blood vessels created by physical channels through the scaffolds, seeding with endothelial cells, and adding growth factors. By this method, scaffold design must be further complicated by including vasculature. However, introducing oxygen-releasing polymers directly into the scaffolds may be a simpler way to address the need for oxygen transport.1 My research objective is to investigate the ability of oxygen-releasing microspheres to decrease cell necrosis for adipose-derived stem cells in printed implants. Using oxygen-releasing microspheres to reduce cell death rates for tissue engineering applications has only been referenced in one publication at the time of this application. This system involves a two level approach. First, core-shell microspheres are created with the shell consisting of poly(lactide-co-glycolide) (PLGA) and the core containing H O -modified 2 2 poly(vinyl pyrrolidone) (PVP). This shell system allows for slow release of H O -modified-PVP 2 2 for up to two weeks.3 Second, the microspheres are then encapsulated in a hydrogel with catalase enzyme and cardiosphere-derived cells (CDCs). Catalase reacts with the H O bound to PVP to 2 2 generate oxygen, which is then free for use by the cells. This system has been shown to eliminate significant CDC death.4 This promising result may also be replicable in other cell types. Among the most propitious lineages of adult stem cells are the adipose-derived stem cells (ASCs) due to their ease of acquisition and ability to be chondrogenic, osteogenic, and adipogenic. Despite these advantages, ASC survival rates following in vivo implantation are low. Reduced viability may be due to lack of oxygen at the implant or inject site, which could be addressed by oxygen-releasing polymers.2 The steps involved in making a clinically relevant printed scaffold include material choice, cell type choice, printer type choice, scaffold material characterization, in vitro testing, preclinical animal testing, and clinical testing. Using this approach, I will complete the first six steps to adapt the PVA-H O system for use in 3DBP and test its ability to prevent necrosis in 2 2 adipose-derived stem cells using various hydrogel formulations. I hypothesize that using oxygen-releasing polymers will increase ASC viability in engineered tissue systems. To do this, I must address tissue engineering concerns such as refining 3D printing parameters, determining whether microsphere addition alters the mechanical properties of the gel, and selecting the gel type by printability and cell viability. 2 RESEARCH PLAN 2.1 Gel manufacture: For 3DBP applications, inks must solidify quickly, have appropriate mechanical properties, be non-immunogenic, and promote proliferation. Before manufacturing and encapsulating the microspheres, I will test several gel formulations including poly(ethylene glycol) diacrylate to determine an appropriate method for 3DBP. Printability will be measured by the degree to which the print matched the design specifications by calculating the percent error of the printed scaffold compared to the 1 cm2 design. I will select the weight percent of each hydrogel by testing printability. 2.2 Manufacture of microspheres and gel encapsulation: To create the H O -modified 2 2 polymer, H O will be mixed with PVP in multiple molar ratios. The core-shell microspheres 2 2 will be electrosprayed using coaxial electrohydrodynamic atomization using a protocol described by Nie et al.3 Both the flow rate and voltages will need to be optimized to create particles of uniform size and morphology. I will use the previously determined weight percentage of hydrogel to encapsulate the ASCs, catalase, and microspheres. The optimum concentration of catalase will be determined by studying oxygen release kinetics in a hypoxic, acellular environment and measuring which concentration of enzyme sustains oxygen release for the longest time and at the highest levels. This will be determined over a 21-day period by using Ru(Ph Phen )Cl , a luminescent molecule sensitive to O concentration, while using rhodamine 2 3 2 2 b, a fluorescent molecule insensitive to O to correct for background absorbance. 2 2.3 Testing printed materials: Ensuring that scaffolds are safe for cells in vitro prior to further testing is essential. Cell viability in the scaffolds printed with H O /PVP will be tested in vitro in 2 2 a hypoxic environment using Live/Dead, MTS, DNA content, and IHC assays; they will then be compared to control scaffolds without microspheres and containing microspheres with no incorporated H O . Should cell mortality persist, I will attempt to adjust the gel porosity, printing 2 2 methods, and O release system to increase viability. If viability improves with the microspheres 2 in vitro, I will test their efficacy during a subcutaneous study in mice. 2.4 Timeline and Proposed Laboratory: To conduct this study, I would like to work in Dr. Warren Grayson’s lab at Johns Hopkins. Due to the close alignment of our research interests, when I met him at the BMES conference this year, he expressed considerable enthusiasm in working with me and funding my project should I be awarded the NSF GRFP. I anticipate this project to take five years: two for gel and microsphere manufacture and testing and three for in vitro and in vivo material studies. 3 INTELLECTUAL MERIT AND BROADER IMPACTS Currently, only Li et al. have used microspheres to deliver O to implant sites to prevent 2 cell morbidity. Instead, I will develop and test a rapid, accurate, and programmable method to fabricate these tissues using 3DBP. There have been no published papers that utilize microspheres, oxygen release, hydrogels, and 3DBP in conjunction to print biomaterials; integration of these techniques could greatly increase versatility of 3DBP in tissue engineering applications. As cell death is one of the primary concerns of tissue engineering in general and 3DBP specifically, finding a solution to this problem could advance the field from constructing thin tissue sections, to larger tissues, and eventually organs. Developing a standard method for incorporation of oxygen-releasing microspheres into a hydrogel-based printed scaffold is a novel approach that has potential applications in areas such as cardiac, bone, and cartilage tissue engineering. Osteoarthritis treatment is a particular challenge because of the hypoxic environment, but the H O -polymer complex can be a source 2 2 of oxygen for implanted stem cells while they heal the native tissue. Should this method prove effective, it could be used to encapsulate other materials such as growth factors or nutrients. To eventually reach clinical application, I will collaborate with surgeons at Johns Hopkins to develop materials that have clinical utility. Finally, I will present the results of my work at national and international conferences. [1]Camci-Unal, G., et al. (2013) Polym Int. [2]Tsuji, W., et al. (2014) World J Stem Cells. [3]Nie, H., et al. (2010) J Biomed Mater Res A. [4]Li, Z., et al. (2012) Biomaterials.	0
Keywords: coral reef; resilience; bleaching; climate change; ecosystem based management Introduction: Coral reefs shelter 25% of all marine species1 and provide food, protect coastlines, and support economic opportunities for over 1 billion people worldwide.2 However, coral reefs face multiple threats. Chronic stressors, including overfishing, coastal development, pollution, and ocean acidification, slowly degrade coral reefs by undermining vital ecological processes, such as grazing by reef fish and coral growth.3 Coral reefs are also threatened by acute stressors, such as cyclones and coral bleaching events, that may severely damage or restructure coral reef ecosystems.2,3 Climate change is predicted to compound these stressors and has already increased average ocean temperatures by 0.9°C globally.4 This increase has triggered three global bleaching events to date, and most coral reefs are projected to face annual bleaching by the 2050s.4 Adaptive and innovative management approaches are needed to ensure the longevity of coral reef communities in an era of global change.5 One potential approach supported by an emerging body of research3, 5-13 is resilience-based management (RBM).7 Under RBM, scientists and coral reef managers use a variety of biotic and abiotic indicators to assess coral reef resilience, where resilience is defined as the capacity of an ecosystem to resist and recover from stress without shifting to a less desirable ecosystem state.6 However, several knowledge gaps related to coral reef resilience hinder application of RBM theory.5-12 Scientists have not reached consensus on which biotic and abiotic indicators are the strongest drivers of coral reef resilience.6, 8 Most assessments of coral reef resilience are predictive, and few studies exist that test the accuracy of these predictions following major stress events.6, 13 Furthermore, these assessments are conducted across a range of geographies using varying methodologies, 5-12 which makes it difficult determine if observed variability in resilience is due to context-specific factors or broad biogeographic patterns. In addition, assessments of coral reef resilience are still evolving to incorporate emerging science on ecosystem thresholds and phase shifts between coral- and algal-dominated states.3, 11, 13 Proposed Research: The destructive 2014–2017 global bleaching event (Event), which impacted 51% of the world’s coral reefs,14 presents a unique opportunity to examine the impacts of major stress events on coral reef resilience. By evaluating changes in coral reef condition across an array of sites before, during, and after the Event, I will (1) assess the relative importance of select biotic and abiotic factors in determining coral reef resilience, (2) examine how the importance of these factors varies spatially among reefs in the central Pacific, and (3) ground-truth existing methods used to predict coral reef resilience. Experimental Design: An effective analysis of coral reef resistance to, and recovery from the Event will require extensive monitoring data that documents biotic and abiotic conditions on coral reefs before, during, and after the Event. As a graduate researcher at the Scripps Institute of Oceanography (SIO), I will be well positioned to access comprehensive monitoring data from past studies and collect data through current monitoring programs. To evaluate coral reef condition before the Event, I will analyze benthic and reef fish monitoring data collected as part of an extensive SIO study that monitored coral reefs across 56 islands in the central Pacific from 2002–2009.15 To assess coral reef resilience during and after the Event, I will analyze photo surveys and fish transect data collected as part of the 100 Island Challenge (Challenge), which is currently surveying 100 islands across a range of environmental and anthropogenic gradients in the Pacific and Caribbean.16 Photo surveys collected as part of the Challenge are orthoprojected to generate comprehensive 3D models of the coral reef benthos that document species composition and spatial arrangement to a resolution of 1cm2. The Challenge surveys sites (many of which were affected by the Event)14, 16 at regular intervals to document ecological changes, and has been monitoring numerous islands since 2013. I will select study sites by cross-referencing monitoring locations from the 2002–2009 study15 with locations currently being monitored by the Challenge (I anticipate this will reveal several dozen sites with sufficient monitoring data). I will evaluate the historic exposure of study sites to chronic stressors using the World Resources Institute’s Reefs at Risk spatial dataset of anthropogenic impacts.9 Similarly, I will evaluate historic exposure to acute stressors before the Event, and bleaching alert levels during the Event, using spatial data produced by NOAA’s Coral Reef Watch. Based on this evaluation, I will classify sites into eight experimental groups (Fig. 1). I will build off existing meta-analyses of RBM studies6, 12 to identify priority resilience indicators to measure, such as coral cover/diversity, herbivore biomass/diversity, coral recruitment, coral disease, macroalgae cover, bioerosion, substrate availability, and topographic complexity.3, 5-12 I will monitor these indicators as a member of the Challenge research team and extract relevant data from the 2002-2009 SIO monitoring dataset.15 Using this data, I will generate site-level averages for each indicator before the Event to determine baseline conditions and analyze indicator variance to assess pre-bleaching trends in ecosystem health. I will compare these values to indicator averages and variance after the Event to identify indicators with a statistically significant impact on coral reef resistance to and recovery from bleaching. I will also analyze spatial variability in site resilience to detect regional or context specific patterns (i.e., which indicators determine resilience and where), a knowledge gap prior studies have not addressed. Lastly, I will examine areas of overlap and divergence between my findings and those of prior resilience assessments. Fig. 1: Sites will be Anticipated Results: I hypothesize that reef resilience will vary classed into 8 groups based on past exposure. depending on each site’s history of chronic and acute stress exposure. In addition, I anticipate that the accuracy of resilience predictions and the relative importance of biotic and abiotic drivers of resilience will exhibit spatial variability at regional and local scales. Intellectual Merit and Broader Impacts: This study will ground-truth RBM theory using empirical data to analyze patterns of coral reef resilience before, during, and after a major environmental disturbance. As a co-author of an RBM study,9 I recognize the potential of this research to address knowledge gaps, and thereby enable scientists to further refine RBM methods to reflect observed patterns of coral reef resilience. This continued refinement will be critical6 as reef managers, communities, and governments move to adopt RBM to inform marine protected area design, fisheries regulations, and other conservation measures7-10 that would have broad impacts for millions of people who depend on coral reef ecosystems.2 In addition, the methods outlined in this study could be modified to test RBM assumptions for other ecosystems threatened by climate change.7 As I conduct this research, I will use connections I have cultivated in the marine non-profit community by collaborating with researchers and practitioners, and draw from my communications experience as an environmental blogger to widely disseminate my findings. References [1] Plaisance et al. 2011. PLoS one, 6(10): e25026. [2] Hoegh-Guldberg et al. 2007. Science, 318.5857: 1737-1742. [3] Anthony et al. 2014. Glob Change Biol, 21: 48–61. [4] Van Hooidonk et al. 2016. Scientific Reports, 6: 39666. [5] Nyström et al. 2008. Coral Reefs, 27: 795–809. [6] McClanahan et al. 2012. PloS one, 7.8: e42884. [7] Maynard et al. 2015. Biological Conserv, 192: 109-119. [8] Maynard et al. 2010. Coral Reefs, 29.2: 381- 391. [9] Harris et al. 2017. Aquatic Conserv, 27.S1: 65-77. [10] Weeks & Jupiter. 2013. Conserv Biol, 27.6: 1234- 1244. [11] Mumby et al. 2013. Conserv Letters, 7.3: 176-187. [12] Lam et al. 2017. PloS one, 12.2: e0172064. [13] Jouffray et al. 2014. Philosph Trans B, 370.1659: 20130268. [14] Eakin et al. 2017. Reef Encounter, 32(1): 33-38. [15] Smith et al. 2016. Proc R Soc B, 283(1822): 20151985. [17] 100islandchallenge.org/study-design/	0
"Energy Agency estimates that by 2025, 70 million electric vehicles (EVs) will be on roads worldwide. As a result, research on increasing efficiency, reliability, and practicality of EVs and associated systems will be of increasing importance. Described herein is a wireless power transfer (WPT) approach for the dynamic charging of EVs. The goal is to address two challenges; reduction of component stress during high-power, high-frequency operation and electromagnetic field containment. Literature Review: Dynamic charging, also known as roadway charging, consists of embedded roadway coils that transfer power to receiving coils on vehicles in motion (see figure) and has been demonstrated to increase the range of EVs. However, power transfer levels of 50-100kW are required to maintain the EV battery state of charge while the vehicle is in motion [2]. Another challenge is the health implications of electromagnetic fields emanating from the coils. The Society of Automotive Engineers (SAE) J2954 standard sets limits on stray electromagnetic fields [3]. Therefore, their reduction must be addressed if dynamic charging is to achieve mass adoption. Objectives: To develop a high efficiency, high power multi-coil WPT system that will allow for localized electromagnetic field production between source and receiving coils. Hypothesis: A reflexive WPT system comprised of: coils with embedded capacitors, saturable inductors and custom magnetic geometries, will enhance field containment capability and allow for efficient, high power dynamic EV charging with minimal electromagnetic field emissions. Research Plan: Stage I-Initial Investigations: A WPT testbed based on a reflexive field containment compensation approach [4] will be fabricated. This topology achieves field containment by exploiting the receiving coil’s reflected capacitive reactance to neutralize the source coil’s inductive reactance. The source coil’s current is thus attenuated when the coils are uncoupled, thereby reducing emanated fields. Based on my recent findings [5], saturable inductors will be used to enhance the field confinement effect. The goal of this stage is to achieve a power transfer of 10kW at an efficiency above 90% while maintaining a 30-fold field attenuation factor between uncoupled and coupled conditions. Stage II-Coil Embedded Capacitors: At power transfer levels of 50-100kW, the high voltages at the terminals of the inductor coils stress the resonant tank capacitors making the SAE J2954 mandated high frequency (~80KHz) operation challenging to implement. Previous studies have demonstrated this power transfer level only at lower frequencies [6]. Self resonant coils have been shown to eliminate the need for capacitors [7], however these are limited to use in MHz level resonant frequencies and lower powers. To address this, coil enclosures will be designed and built with polyethylene dielectrics spacers that allow for embedding large parasitic capacitances to reduce the voltage stress on the external resonant tank capacitors and allow for efficient high frequency operation. Stage III-Additive Manufacturing Based Magnetic Structures: Custom magnetic structures will be implemented in the coils to allow for optimum channeling of flux during operation to further reduce stray fields and improve coupling. This will be preceded by extensive finite element analysis of the desired magnetic structure geometry. The magnetic structures will be fabricated using an additive manufacturing process recently demonstrated at Oak Ridge National Laboratory [8]. A supplier for the required magnetic nano-alloy powder (FeNbSiCuB) and polyphenylene sulfide mixture has already been identified. Stage IV-Optimization: This stage will focus on the challenge of optimizing system performance at ~100kW for high efficiency operation, conformance to SAE standards and EV testing. Soft switching will be employed on the Silicon Carbide based inverter while processing minimal reactive power to improve efficiency. The test EV will be modified to carry the receiving coil and necessary power electronics to interface with its rechargeable batteries. Based on the SAE standards, shielding for the receiving coil will be designed to prevent fields from penetrating the EV cabin. An array of segmented source coils will be embedded on equal intervals of test track. As the EV drives along the track, the power transfer and field containment capability of the system will be characterized and compared to simulation figures. Timeline and Collaboration: The proposed duration of the project is three years and will be performed under the supervision of Dr. Srdjan Lukic at North Carolina State University’s (NCSU) NSF funded Future Renewable Electric Energy Delivery and Management Engineering Research Center (FREEDM ERC) in collaboration with Dr. Tim Horn of the Center for Additive Manufacturing and Logistics (CAMAL). Being one of the nation’s leading centers for additive manufacturing, CAMAL’s facilities have specialized equipment capable of fabricating magnetic structures from magnetic nano-alloy powders. The systems level integration will be performed on the EcoPRT EV mentioned in the personal statement. Anticipated Results: Through the use of saturable inductors, custom magnetic structures, and coil embedded capacitors, high frequency power transfer at 50-100kW power levels with high field attenuation factor between uncoupled and coupled conditions will be demonstrated. Noting that a WPT system is a transformer with a large air gap, this project strives to meet a theoretical system efficiency of 98+%, or comparable to plug-in charging. Intellectual Merit: This research plan focuses on developing novel WPT systems that enable dynamic charging and, more generally, on pushing the boundaries of high power WPT by developing better models of complex coil designs with integrated passives. Additive manufacturing will allow for novel magnetic structures that are optimized for specific applications, and this methodology can be utilized in other high power high frequency applications. Broader Impacts: The mass proliferation of EVs is essential in addressing both climate change and the dependence on fossil fuels. Through the implementation of WPT systems for EV roadway use, the proposed project will assist in making feasible a system that will allow for the efficient and reliable increase in range and thereby practicality of EVs. I will regularly present findings from this research effort at scientific conferences and high profile journals. With NCSU being located in the Research Triangle area, I will share findings with industry to facilitate industrial proliferation of the technology. (1) Clean Energy Ministerial, Electric Vehicle Initiative & International Energy Agency (2017). Global EV Outlook 2017. (2) Z. Pantic, et. al., ""Inductively coupled power transfer for continuously powered electric vehicles,"" 2009 IEEE Vehicle Power and Propulsion Conference, pp. 1271-1278. (3) Wireless Power Transfer for Light Duty Plug- In Electric Vehicles and Alignment Methodology, no. SAE Standard J2954, 2016 (4) K. Lee, et. al., ""Reflexive Field Containment in Dynamic Inductive Power Transfer Systems,"" in IEEE Transactions on Power Electronics, vol. 29, no. 9, pp. 4592-4602, Sept. 2014. (5) A. Dayerizadeh, et. al., ""Saturable Inductors for Superior Reflexive Field Containment in Inductive Power Transfer Systems,"" 2018 IEEE Applied Power Electronics Conference and Exposition (APEC), Accepted. (6) G. Jung et al., ""High efficient Inductive Power Supply and Pickup system for On- Line Electric Bus,"" 2012 IEEE International Electric Vehicle Conference, Greenville, SC, 2012, pp. 1-5. (7) A. Kurs, et. al., “Wireless power transfer via strongly coupled magnetic resonances,” Science, vol. 317, no. 5834, pp. 83–86, Jul. 2007. (8) U.S. Department of Energy (2015). Electric Drive Technologies: 2015 Annual Report."	0
Graduate Research Plan Statement Title: Exploring critical zone structure and function in a tropical urban watershed through concentration-discharge relationships Introduction: Streams are recognized as integrators of the surrounding landscape. Stream water chemistry is thus an excellent indicator of broader critical zone (CZ) processes. The CZ is the space from the top of the vegetative canopy down to bedrock and lowest extent of freely circulating groundwater.1 The CZ framework provides a holistic approach to develop predictive models that describe processes at the earth’s surface, including the constraints on material export from the continents to fluvial networks.2 Concentration-discharge (C-Q) relationships in streams provide an integrated signal of sources and transport processes to examine how solutes and sediment respond to changing patterns of runoff.3 Studies of the CZ are mostly limited to pristine systems; however, C-Q relationships in urban streams may be more complex due to altered hydrology, impaired water quality and heterogenous subcatchments.4 With increasing pressures on urban landscapes, there is an urgency to understand hydro-biogeochemical processes of the urban CZ that govern water quality and quantity. Research in urban systems will be highly valuable to cities and communities and can better inform management practices and help improve urban infrastructure.4 I propose to characterize and compare C-Q relationships across two stream networks with differing levels of urban development through a series of whole-network sampling efforts capturing baseflow to storm events. I also propose to study the impacts of hurricane disturbance on C-Q trends through analysis of long-term water chemistry records. I will test three hypotheses on variability in C-Q relationships across stream networks associated with watershed urbanization (H1-H3): H1: Watershed urbanization drives greater variability in C-Q relationships across stream networks associated with increased impervious surface area. H2: C-Q relationships during storm events are more variable in the urban network due to heterogenous hydrologic signals, and ultimately depend on storm intensity. H3: Both urban and pristine stream networks show increased variability in C-Q relationships after hurricane disturbance, but the magnitude of variability is higher in urban watersheds. Study sites: This work will be conducted in two watersheds: the urban Río Piedras in metropolitan San Juan, Puerto Rico and the mostly undeveloped Río Espíritu Santo in El Yunque National Forest in Río Grande, Puerto Rico, as a reference site. The Río Piedras flows through the metropolitan area with highly modified channels and significant impacts from failing sanitary infrastructure.5 In contrast, the Río Espiritu Santo originates in the Luquillo Mountains and is mostly undeveloped except on its coastal plain with significant changes in water chemistry evident only after hurricane impacts.6 The University of New Hampshire´s (UNH) Water Quality Analysis Lab (WQAL) group, including myself, has extensive experience working in these two stream networks. These watersheds are also a part of the Luquillo Long-Term Ecological Research (LTER) site and Luquillo Critical Zone Observatory (LCZO), which have generated multi-decadal records of stream chemistry and discharge. The lab also has ongoing collaborations with research groups at the University of Puerto Rico (UPR), who have worked in the Río Piedras: Jorge Ortiz- Zayas’s Tropical Limnology Lab and Alonso Ramírez’s Aquatic Ecology Lab. Proposed approach: (H1) I will establish a synoptic sampling regime of 20 sites in each stream network, ranging from small headwater sites to larger mainstem and coastal sites. I will collect water samples as well as measure physicochemical parameters with a handheld multiparameter instrument at least 15 times in the span of 2 years. Samples will be analyzed for nitrate, ammonium, phosphate, dissolved organic carbon, anions, cations, dissolved greenhouse gases, and total suspended solids (TSS) at the UNH WQAL Lab. I will also take discharge measurements at each 1 Carla López-Lloreda NSF Graduate Research Fellowship Graduate Research Plan Statement site and date either using dilution gauging or acoustic velocity measurements, depending on stream size. I will target sampling dates that capture a range of flow conditions by monitoring the US Geological Survey (USGS) gauging stations at two sites within each stream network. This sampling will allow me to characterize spatial and temporal variability in C-Q relationships across the stream network. (H2) I will conduct targeted storm sampling at one USGS gauged site in each stream network with ISCO automated samplers. These samples will be analyzed for the same solutes as in H1, with a focus on TSS to calculate sediment flux during storm events. Real-time discharge data for these events will be obtained online through the USGS´s National Water Information System Interface. I will capture events of different magnitude to explore effects of storm magnitude and intensity on C-Q relationships and solute and sediment transport. I will use these results to evaluate specific patterns of hysteresis, which characterize the rising and falling limbs of a storm event, providing additional insights into material reservoirs within watersheds. (H3) I will use long-term chemistry data from sites in the Río Piedras and the Río Espíritu Santo, which have been sampled weekly for approximately 10 years for multiple solutes and are USGS gauged sites. I will quantify C-Q relationships before and after Hurricane María in September of 2017 which will allow me to examine the resilience of these watersheds to a major perturbation. Intellectual Merit: This work will provide insight into the interactions and mechanisms of sediment and nutrient production, pathways, and transport in a tropical urban watershed. Studying storm events and hurricanes is particularly important because they are “hot moments” of increased hydrological activity, which transport disproportionate amounts of solutes and sediments to streams and oceans compared to baseflow conditions.7 Understanding high flow processes is crucial, as major storm events and hurricanes are expected to increase in intensity and frequency with climate change.8 And though much C-Q work has been done on temperate systems, tropical streams supply disproportionate amount of sediments and solutes to the ocean and studying these systems in a global context is becoming increasingly important.9 This work also leverages research done by the Luquillo LTER and LCZO research networks. Broader Impacts: During this work, I will continue developing collaborations with local research groups in ways that will engage local underrepresented undergraduate students in fieldwork through my network and storm sampling efforts. I will work with UPR professors to offer independent research opportunities to students. This research will provide a useful framework for local government agencies to use in their nutrient and sediment management plans across the island, including restoration projects aimed to help reduce pollution and sedimentation and regain critical zone services in urban ecosystems.7 For this, I will engage local researchers examining sediment and nutrient loading and the local Puerto Rico offices of the Department of Natural Resources. I will also collaborate with local organizations working in the Río Piedras such as the San Juan Bay Estuary and the ENLACE project of the Caño Martín Peña. These projects have longstanding efforts in the Río Piedras watershed and extensive connections with local communities. In collaboration with these organizations, I will use this opportunity to teach communities about water quality in their watershed through a series of roundtables and by providing educational resources to disadvantaged groups that have greater exposure to poor water quality. References: [1] National Research Council (2001). Basic Research Opportunities in Earth Science. Natl. Acad. Press. [2] Brantley, S. L. et al. (2017). Earth Surface Dynamics, 5(4), 841–860. [3] Chorover, J. et al. (2017). Water Resources Research, 53, 8654–8659. [4] Kaushal, S. S. et al. (2014). Biogeochemistry, 121, 1–21. [5] Potter, J. D. et al. (2014). Biogeochemistry, 121, 271–286. [6] McDowell, W. H. et al. (2013). Biogeochemistry, 116, 175–186. [7] Kaushal, S. S. et al. (2018). Biogeochemistry, 141(3), 273–279. [8] Zimmerman, J. K. et al. (2018). Ecology, 99, 1402–1410. [9] Schlesinger, W.H. & Bernhardt, E. (2013). Biogeochemistry (3rd. ed.). Academic Press. 2	0
Eukaryotic post-translational modification of bacterial effectors Keywords: asparagine hydroxylation, Legionella pneumophila, Yersinia pestis, bacterial effectors Legionella pneumophila, the causative agent of Legionnaire’s disease, has only recently become a human pathogen. Its intracellular lifecycle in amoeba, the natural host, has primed the bacteria for invasion into human alveolar macrophages. Co-evolution within amoeba and horizontal gene transfer has helped shape the near 300 effectors produced by Legionella that are injected into the host cell by the Dot/Icm type IVB translocation system1. A majority of these effectors have eukaryotic-like domains such as: F-box, U-box, Sel-1, ankyrin repeats, leucine- rich repeats, and CaaX motifs, which were likely acquired by horizontal gene transfer2. These domains aid in the hijacking of host processes by L. pneumophila in order to promote growth and replication. Many injected bacterial effectors are modified by the host through various post- translational modifications, however asparagine hydroxylation modification has never been observed. Post-translational asparagine hydroxylation of proteins in mammalian cells is mediated by Factor Inhibiting HIF (FIH). FIH is most commonly studied for its role in asparagine hydroxylation which regulates the Hypoxia inducing Factor (HIF), responsible for the transcription of around 100 hypoxia-related genes. FIH recognizes the amino acid sequence Lxxxx(D/E)ϕNϕ3. This motif can be found in 11 of the injected effectors of L. pneumophila, designated as Hydroxylated Effectors of Legionella (HEL). This motif can also be found in other injected bacterial effectors such as, the Outer Protein M (YopM) of Yersinia pestis, IpaH4.5 ubiquitin ligase of Shigella flexeneri, and an uncharacterized ankyrin protein of Rickettsia felis. It is likely that this motif is present in many other bacterial effectors that have yet to be described. Exploitation of host post-translational modification plays an important role in bacterial pathogenesis by further tuning it with the host, allowing it to manipulate and modulate host functions. Our hypothesis is that pathogens hijack host FIH in order to hydroxylate effector proteins, making them biologically functional. To test this hypothesis, three aims are proposed. Aim 1-Hydroxylation of effector proteins In order to determine if the HEL proteins of L. pneumophila, YopM of Y. pestis, and IpaH4.5 of S. flexeneri are hydroxylated in human cells, HEK293 cells will be transfected with plasmid containing FLAG-tag fusion proteins. Purified proteins will be analyzed by matrix- assisted laser desorption/ionization (MALDI) Mass Spectrometry (MS), to identify a 16 dalton mass shift in the fragment containing the hydroxylation motif. This will be done in collaboration with Dr. Michael Merchant. To confirm the role of FIH in asparagine hydroxylation, FIH inhibitors and FIH silencing by RNAi will be utilized. We have tested and confirmed hydroxylation in this manner for one of the HEL proteins, AnkH. This gives us reason to believe that others may be hydroxylated as well and supports our reasoning for these studies. 1 Ashley Richards Dissertation Proposal 2013 Aim 2- Protein-protein interactions of FIH and effectors Our preliminary studies have shown colocalization of the host FIH and some of the HEL proteins to the Legionella containing vacuole (LCV). Therefore, interaction between FIH and effectors is likely to occur. Due to the transient enzymatic nature of interactions with FIH, Bimolecular Fluorescence Complementation (BiFC) will be used to show the interaction between the proteins of interest and FIH. This system utilizes two plasmids each harboring half of a fluorescent molecule that emits light when brought together by interacting proteins fused to either half. If fluorescence can be detected by confocal microscopy in cells transfected with two plasmids, containing the N-terminal portion of the fluorescent molecule fused to either FIH or effector protein and the C-terminal portion of the fluorescent molecule fused to an effector protein or FIH, then interaction between of the two proteins can be suggested. Aim 3- Role of asparagine hydroxylation motif in the biological function of effectors Generating point-mutations in the asparagine of the hydroxylation motif for each protein will elucidate how hydroxylation of this residue is important to the function of the protein. This will be in comparison to the knock-out mutant, lacking the gene, which will also be generated. These mutants will be used in functional studies in a variety of species and cells such as human derived macrophages, mice, and amoebae. Because L. pneumophila has a plethora of hosts, it is possible that a mutant has an effect in one species or type of cell but not another. This will also be done with Y. pestis YopM mutant in human macrophage cell line, in collaboration with Yersinia researcher Dr. Matthew Lawrenz. Hydroxylation of bacterial proteins has never been shown before. This post-translational modification could be the key to more refined modulation and regulation of the host. This motif seems to be abundant in human pathogens and has implication in convergent evolution of bacterial effectors to better survive in its mammalian host. Not only will this educate us on bacterial host interactions but also provide more insight on FIH, as little is known about the nature of FIH hydroxylation outside of its role in hypoxia. Broader Impacts: These studies would lead into knowledge about effector proteins in the study with unknown function. Ultimately better understanding bacterial effectors and their role in the host could result in potential targets for novel treatments. My research will provide new insights into bacterial protein post-translational modification, and be added to publically accessible databases designed to predict protein structure and function. This will allow others to use this information to elucidate novel functions or regulatory mechanism for proteins in other species. References [1] de Felipe, K.S., Glover, R.T., Charpetier, X., Anderson, O.R., Reyes, M., Pericone, C.D., and Shuman, H.A. (2008) Legionella eukaryotic-like type IV substrates interfere with organelle trafficking. PLoS Pathog 4, e1000117 [2] Al-Quadan, T.P., Price, C.T., and Abu Kwaik, Y. (2012). Exploitation of evolutionarily conserved amoeba and mammalian processes by Legionella. Trends Microbiol 20, 299-306 [3] Wilkins, S. E., Karttunen, S., Hampton-Smith, R. J., Murchland, I., Chapman-Smith, A., & Peet, D. J. (2012). Factor Inhibiting HIF (FIH) Recognizes Distinct Molecular Features within Hypoxia-inducible Factor-α (HIF-α) versus Ankyrin Repeat Substrates. Journal of Biological Chemistry, 287(12), 8769-8781. 2	0
"MULTI-MODAL DATA ! Keywords: Alzheimer’s Disease, Structural MRI (sMRI), Functional MRI (fMRI), Mild Cognitive Impairment (MCI), GPU computing ! Summary: I propose to use multiple imaging systems, such as structural and functional MRI imaging to provide a temporal evolution of Alzheimer’s disease (AD) with multi-modal data. It will include patients that show no symptoms and patients that have MCI. This temporal evolution will show how different regions of the brain changes and how AD evolves. The hippocampus region will be a main region of interest, but other regions like the temporal lobe will be examined [2,3]. In addition with the imaging system, looking at the cerebrospinal fluid will provide great insight to how AD affects the body [1]. I will also incorporate GPU computing to make it efficient. ! Motivation: AD is ranked as one of the leading diseases in increasing deaths. AD plans to increase among the world, which will have an effect on the economy. It is proposed that the expected cost of AD will rise to $1.2 trillion by 2050 [2]. Some methods have shown how some patients who have MCI evolve into AD [4]. If AD is not detected early, it will lead the patients to be in the latter stages of AD. The latter stages, such as stage 5 create severe cognitive decline and require the AD patient to need assistance in performing routine tasks [2]. Thus, detecting AD early is very crucial. ! Hypothesis: Detecting AD in the early stages could be very beneficial to future patients, but has been a challenge. One group has said that validating imaging biomarkers for AD has brought controversial findings [5]. Now, using multi-modal data, such as fMRI, sMRI, and other imaging systems, could capture how AD progresses through time. With seeing different areas of the brain and CSF, it could provide insight to how AD evolves with patients that show no symptoms and patients that have MCI. ! Research Strategy: To develop an extensive model of what makes AD develop between patients that show no symptoms at all and patients that have MCI. Objective 1: Identify the critical parts/aspects that could lead to Alzheimer’s developing. Objective 2: Examine the different imaging systems that have been used to look for AD, such as fMRI, sMRI, PET, and etc. Objective 3: Extract the regions from the different imaging systems that show where AD could develop and run pattern recognition methods to classify which are likely to develop. Objective 4: Identify which interconnections between regions to show how the disease progresses. ! Research Methodology: This research methodology is based on several pattern recognition approach. Other people have used different image processing techniques to extract regions of interest that show where Alzheimer’s could develop and use pattern recognition to classify the regions. Overall, biomedical image processing and pattern recognition will be the foundation in developing more information about detecting AD in the early stages. ! Anticipated Results: The anticipated result will show how different areas of the brain can show how Alzheimer’s progresses throughout the patient’s age. It will also show evolution between patients with no previous symptoms and patients with MCI. Plus, I plan to incorporate GPU computing so the data could be computed faster because some of the imaging systems, such as fMRI is computationally expensive and extensive [1]. The research that is conducted will be submitted to a journal paper. A journal publication could be sent to IEEE Transactions or Computer Vision and Pattern Recognition. ! Institution: Dr. Alan C. Bovik from University of Texas at Austin is a great professor to conduct research for this project. One of his research areas is biomedical engineering. His work has done detection and diagnosis of breast cancer. The Laboratory for Image and Video Engineering would be a great place to conduct research for this project. ! Intellectual Merit and Broader Impacts: This research could be valuable to the medical field. It would help multiple people in trying to understand more about AD. The major problem with AD is trying to diagnose it earlier in the beginning stages because there people could prepare. When AD is detected at the latter stages, the damage is done and the patients diagnosed with AD will need to be cared for the rest of their life. If a new patient is experiencing common trends compared to a previous patient where it showed the patient’s temporal evolution, then the temporal evolution can provide a model to show if a new patient with similar patterns will develop AD. ! ! 1 2 3 4 5! ! !1 Mason, Emily J., Manus J. Donahue, and Brandon A. Ally. ""Using Magnetic Resonance Imaging in the Early Detection of Alzheimer's Disease."" (2013). !2 Bukhari, Ijaz. ""Early Detection of Alzheimer's-A Crucial Requirement."" arXiv preprint arXiv:1305.2713 (2013). !3 Ahmed, Olfa Ben, et al. ""Alzheimer Disease detection on structural MRI."" Proceedings of ESMRMB 2013 Congress. 30th annual meeting. 2013. !4 Douaud, Gwenaëlle, et al. ""Brain Microstructure Reveals Early Abnormalities more than Two Years prior to Clinical Progression from Mild Cognitive Impairment to Alzheimer's Disease."" The Journal of Neuroscience 33.5 (2013): 2147-2155. !5 Dukart, Juergen, et al. ""Generative FDG-PET and MRI Model of Aging and Disease Progression in Alzheimer's Disease."" PLoS computational biology 9.4 (2013): e1002987."	0
mate. The majority of research regarding the signal function of visual traits has concerned color or patches of color. This focus on charismatic coloration has left unexplored the potential signaling function of achromatic patterns. Some patterns, such as the white check patch of great tits (Parus major) or the black facial spots of female paper wasps (Polistes dominula), have been suggested to function as assessment signals1. Yet, there is little empirical evidence that females consider patterns when selecting a mate. In the songbird family Estrildidae, patterns or certain features of patterns may be dimorphic, which suggests that these may be sexually selected traits. This idea is only beginning to be explored, with some studies linking estrildid patterns to individual quality2,3. Little is known, however, about how females perceive these signals, including the extent to which features that stand out to human observers draw the attention of the birds themselves. The zebra finch (Taeniopygia guttata) is an ideal species in which to study the role of achromatic patterns in estrildids. This species exhibits four notable sexually dichromatic traits, two being related to color and two being related to pattern. These include red beak color, which is more pronounced in males than females, and an orange cheek patch, barred bib, and white flank spots that are only exhibited by males. Although the function of male beak color has been well studied, little attention has been paid to the other three dimorphisms, aside from one study showing that female zebra finches prefer males with more symmetrical barred bibs5. There also is little known about how these dichromatic patterns have evolved, and to what extent these patterns vary in wild populations. Furthermore, zebra finches come from the subfamily Poephilinae6, which exhibit a wide range of chromatic and achromatic patterns, allowing for a tractable phylogenetic comparison of perceptual abilities. I hypothesize that dimorphic achromatic patterns function as signals of quality independent of color, and that species with these patterns are able to perceive and assess variation in this signal better than those without. Aim 1a: Quantify the range of natural variation of zebra finch patterns. Using museum specimens from the University of Michigan’s ornithology collection, I propose to measure the degree of variation of plumage patterns within zebra finches. After photographing the ventral and lateral sides of specimens, I will use ImageJ software to determine the degree of variation in male bar and spot pattern. This is an important analysis because in order for sexual selection to operate, there must be existing variation for it to act upon. After determining the range of this variation, I will categorize the types of plumage variation that may exist among males. Likely, these will include differences in regularity, contrast level, and density of patterns, though other features may also differ. Finally, I will test if certain aspects of these patterns covary with body size, beak color, or other indicators of individual quality. Aim 1b: Determine the extent to which females can discriminate natural pattern variation. Using methods developed to test for categorical color perception in this species (i.e. training birds to flip discs of different colors for a food reward)7-9, I propose to ask how female zebra finches perceive variation in patterns of bars and spots. To do this, I will make discs using paper printed with images of male chest or flank plumage as determined in Aim 1a. I will place these discs on wells in which a food reward is found and train birds to select the pattern that varies the most from the others. Using the range of variation determined in Aim 1a, I will test for the extent to which two varied patterns are indistinguishable to zebra finches. Given that zebra finches court at close range, this approach matches how close females would be to these patterns when assessing males. Additionally, I will test for sex differences in contrast perception to ask whether dichromatic bar/spot patterns could function in female choice, male competition, or both. If dichromatic patterns serve as signals of quality, I predict that female zebra finches will have the visual acuity necessary to discern variation in this signal. Aim 2: Test female preference for pattern quality. I will test for female preference of male pattern variants using digitally manipulated images of male conspecifics. Using a television screen separated by a partition (hereafter “Bird TV”), I will observe how long females spend near one of two videos of male zebra finches as evidence of their mate preference. I will first record videos of male zebra finches and then digitally alter certain aspects of their achromatic patterning that have been positively correlated with body condition and dominance hierarchy in other estrildids: regularity of barred plumage2 and number of white spots10, respectively. Bird TV, a novel method for mate choice experiments, is currently being developed by my advisor, Dr. Steve Nowicki, to test female preference for male beak color, which is a well-supported preference. After this approach is validated, Bird TV will be used in my proposed experiment. I predict that females will prefer the aforementioned pattern variants that have been previously suggested to serve as signals of quality (regularity of barred plumage and large spot size), but have not been explicitly tested. Aim 3: Compare the perceptual abilities of closely related estrildids: The zebra finch is a unique estrildid in that it simultaneously exhibits dimorphic carotenoid coloration, barred plumage, and spotted plumage. As a result, it is possible that zebra finches are adept at both color and contrast perception. Other closely related species, such as the long-tailed finch (Poephila acuticauda) or double-barred finch (Taeniopygia bichenovii), exhibit fewer dimorphic traits. Therefore, I will compare the visual abilities of zebra finches to its closest relatives to see if there are trade-offs associated with color and contrast perception. For example, the double-barred finch does not exhibit carotenoid coloration, but both males and females have bars and spots, and so may have been selected to specialize in perceiving patterns. In contrast, the long-tailed finch exhibits carotenoid coloration in both male and female beaks, but lacks bars or spots, suggesting that it may have been selected to specialize in color perception. I hypothesize that there is a trade-off between color and contrast perception—specifically, that exclusively colorful birds are better at color perception, and that exclusively patterned birds are better at contrast perception. I propose to test the color and contrast perception of the long-tailed finch and double-barred finch using the methods from Aim 1, then test female choice using methods from Aim 2, and compare these results to those of the zebra finch. An inconspicuously colored and more distant relative of the zebra finch, the Bengalese finch (Lonchura striata domestica), was recently shown to discriminate colors differently than zebra finches11. My work will follow up on this investigation to test the effect of phylogenetic relatedness on color and pattern perceptive abilities in Estrildids. Intellectual Merit: In studying the potential signaling function of certain visual traits, it is critically important to first understand how these signals are perceived. If such signals are not being perceived or differentiated at all, then we need to re-evaluate what other function they could possibly serve apart from intraspecific communication. While patterned plumage has been previously proposed to function in intraspecific signaling, no study has actually examined how adept animals are at perceiving variation in these patterns. Therefore, my work will be a necessary first step in determining if patterned plumage should continue to be explored as a signal of quality. Broader Impacts: As an NSF fellow, I will ensure that my research, at every stage, contributes to my personal goal of retaining as many students as possible in the sciences. At Duke, I will invite undergraduates to not only assist in collecting data, but also in contributing intellectually so that they may share authorship on future publications. During the summer, I will train undergraduates through Duke’s paid Biological Sciences Undergraduate Research Fellowship. This program not only introduces students to biological research, but also provides professional development opportunities and a campus-wide showcase to present their research. I benefited greatly from summer research programs like this, particularly the NSF REU, so I know firsthand the impact they can have. I want to help Duke undergraduates feel comfortable in a research environment through close mentorship and reassurance that questions and mistakes are part of the learning process. Beyond Duke, I plan to share my research with scientists and science enthusiasts in the broader “Triangle” (Durham. Raleigh, and Cary, NC) area. The North Carolina Museum of Natural Sciences in nearby Raleigh hosts opportunities for science communication and outreach that I plan to fully engage in. For one, they host the Scientific Research and Education Network (SciREN) Networking Event, which gives an opportunity for scientists to adapt their research into K-12 lesson plans. They also host a Darwin Day event, in which people of all ages are invited to learn about Darwin and his legacy. I will share my findings at these events, and adapt them so that they are appropriate and accessible to all ages and backgrounds. Finally, I will participate in Skype-a-Scientist to connect with students globally. 1Pérez-Rodríguez et al. 2017. Proc. Royal Soc. B. 2Marques et al. 2016. Roy. Soc. Open Sci. 3Soma & Garamszegi 2018. Behav. Ecol. 5Swaddle & Cuthill 1994. Proc. R. Soc. Lond. B. 6Olsson & Alström 2020. Mol. Phylogenet. Evol. 7Caves et al. 2018. Nature 8Zipple et al. 2019. Proc. Royal Soc. B. 9Caves et al. 2020. Behav. Ecol. Sociobiol. 10Crowhurst et al. 2012. Ethology 11Caves et al. (in press) Am. Nat.	0
25% of all marine life.1 They provide invaluable services by protecting shorelines from storm surge, supplying food sources, and promoting eco-tourism.2 However, coral populations around the world are exhibiting an alarming decline due to climate change, specifically from ocean warming (OW). OW has severely diminished coral health through increased disease susceptibility and bleaching.3 To fully understand how corals might fare under future OW conditions, the potential for coral acclimatization over generations and life history stages must be assessed. Previous research on coral acclimatization has focused primarily on intra-generational acclimatization (IGA), which investigates if corals can adjust to new conditions within their lifetime. For example, Brown et al. (2002) found that when G. aspera were exposed to higher levels of solar radiation, they were less susceptible to bleaching.4 While IGA is crucial in elucidating coral resilience, the study of trans-generational acclimatization (TGA) in corals is essential to understanding the persistence of coral reefs in a warmer future. TGA occurs when the phenotype of the offspring is influenced by the environmental conditions experienced by the parents and/or previous generations.5 Epigenetic modifications, or heritable alterations in gene expression and cellular functions that do not involve changes to the original DNA sequence, are thought to play a role in TGA.6 Most studies on epigenetic modifications and TGA have focused on exclusively DNA methylation; for example, Strader et al. (2019) found that parental environments of S. purpuratus affected patterns of DNA methylation in offspring.7 However, other epigenetic markers, such as histone modification and chromatin remodeling, may be relevant to TGA in marine invertebrates, but have been seldom studied. I will address this knowledge gap by investigating multiple epigenetic mechanisms and outcomes of TGA in corals, over multiple generations and life history stages, in the context of OW. Additionally, I will examine coral physiological processes to better understand all aspects of coral TGA. Discerning the influence of epigenetics on TGA will contribute to the knowledge of coral resilience and susceptibility in an evolutionary and ecologically relevant context. The coral Pocillopora damicornis was chosen for this study, as it is an important reef-building coral in the Indo-Pacific region and is commonly used in laboratory experiments as a model coral. Aims and Hypotheses: Aim 1: Assess physiological effects of TGA in offspring at several developmental stages (larval, juvenile). Hypothesis 1: Both larval and juvenile offspring whose parents were exposed to OW conditions will have a higher tolerance to OW conditions than larval and juvenile offspring whose parents experienced ambient conditions. Aim 2: Compare the epigenetic modifications, specifically DNA methylation patterns and histone modifications, of coral parents and offspring during several developmental stages (larval, juvenile) to evaluate the acquisition and stability of TGA. Hypothesis 2: Parents exposed to OW conditions will produce offspring with differentially methylated genes and modified histones compared to offspring whose parents experienced ambient conditions. Research Methods: I will collect reproductively viable adult P. damicornis from the fringing reefs of Kaneohe Bay, Hawaii and experimentally expose them to OW conditions. To simulate current and future environmental parameters, experimental ambient/high temperatures will be defined as 26/30°C.8 Exposures will take place in mesocosm tanks with flow-through experimental treatment water in the Hawaiian Institute of Marine Biology’s seawater system for one month. Following the adult exposure, I will evaluate the photosynthetic efficiency (F/F ) of v m adult corals to assess their capacity to photosynthesize. Additionally, I will preserve adult tissue samples for later epigenetic analysis (see below). After the exposure and physiological assessment, I will induce adults from all treatments to spawn. I will collect larvae post-spawn 1 and experimentally expose them to ambient/OW conditions for one week. At the beginning and end of the larval exposure, I will measure lipid content and oxygen consumption from a subset of larvae in each treatment to determine energy reserves needed for metamorphosis and metabolic rate, respectively. Following the exposure, I will preserve another subset of larvae from each treatment for epigenetic analysis (see below). I will transfer the remaining larvae from each treatment into 10 L tanks with ambient flow-through seawater and plugs for settlement. After 6 months, I will experimentally expose the now-juvenile offspring to ambient/OW conditions for one month. At the end of the exposure, I will measure juvenile F/F ; following photosynthetic v m analysis, I will preserve juvenile tissue for epigenetic analysis (see below). Epigenetics: At the end of all exposures, I will collect tissue from juveniles/adults and a subset of larvae for DNA methylation and histone modification analyses. Using extracted genomic host DNA, I will assess whole genome DNA methylation using the MeDIP-seq approach. This method utilizes DNA immunoprecipitation and next-generation sequencing to estimate methylation levels of specific DNA regions. I will also use genomic host DNA for histone modification analysis. Histone modifications will be analyzed through the ChIP-seq method, which combines chromatin immunoprecipitation and next-generation sequencing to identify regions of the genome associated with these modifications. Intellectual Merit: Not only will this research considerably enhance knowledge of physiological and epigenetic processes in coral biology, but it will be one of the first studies to provide a deeper understanding of coral resilience over multiple generations and life history stages. The utilization of cutting-edge epigenetic analyses will help to define the contribution of DNA methylation and histone modifications to TGA, which is currently understudied in corals. Additionally, the cognizance of coral TGA potential in the face of anthropogenic stressors will allow scientists and reef managers to make more informed predictions about future reef health and population evolution. An increased knowledge of epigenetic mechanisms in corals will also supply a starting point to investigate TGA potential in other marine invertebrates who may be susceptible to climate change. Broader Impacts: The Graduate Research Fellowship will enable me to pursue important research opportunities and will equip me with the knowledge and abilities needed to succeed as a future governmental or non-profit research scientist. Moreover, the GRF will enhance my skills as a scientific educator and mentor to younger students. I intend to partner with local high schools in the greater University of Hawaii area to connect students with marine science and research. Through this partnership, I will provide opportunities for students to undertake independent projects within the context of my research. I will guide students through an integrated overview on how to conduct research projects from the initial proposal to the final written product. More specifically, I want to include low-income high school students during the research partnership. Low-income students can often be excluded from fully pursuing their interests in science, due to lack of financial and academic support. I hope to provide those students with research opportunities and support, so that they can receive an enriching experience. References: 1Reaka-Kudla (1997) Biod. II. 2Constanza et al. (2014) Glob. Env. Chan. 3Hoegh- Guldberg et al. (2007) Sci. 4Brown et al. (2002) C. Reefs. 5Torda et al. (2017) Nat. Clim. Chan. 6Eirin-Lopez and Putnam (2019) Ann. Rev. Mar. Scie. 7Strader et al. (2019) J. Exp. Mar. Bio. Eco. 8IPCC (2013) AR5. 2	0
KEYWORDS: digitallogic,evolutionarycircuitdesign,geneticmodels,inversion,recombination, systemsbiology,transcriptionalnetworks. Thisprojectisoriginalandofmyowndesign. BACKGROUND: Biologicalsystemsself-regulatevianetworksofinteractingtranscriptionfactors. Such networks produce complex behavior; some theorists have argued that they can produce any behaviordesired.1 However,existenceproofsdonotguaranteethatpractical solutionsexist. One method to test if a desired behavior can be achieved is to run a computer simulation. The computer can test thousands of designs quickly, converging on the best matches. This process has beenusedtoevolvedesignsforoscillators,latches,andotherinterestingbehaviors.2,3 To demonstrate and extend the power of simulated evolution for biological network design, I willevolveageneticallyencodedbinarycounter. Syntheticbiologyhaslongusedbinarycounting as a model system. It is perhaps the simplest behavior which requires a full implementation of digitallogic;thisissignificantbecauseoftherobustnessandsimplicityofdigitalcircuits. Counters have inspired many entries in the iGEM design contests4 as well as high profile experimental attempts,suchastheunary(linear)counterinlastyear’sScience.5 However,tomyknowledge,all thesedesignshaveusednonstandardelementssuchasrecombination-basedgeneticswitches. RESEARCH PLAN: I will perform simulated evolution and compare counter designs with and without switches. Viable designs will be refined by hand and ultimately become physical DNA constructs for testing in vivo. I predict that only designs evolved with switches will be viable. These results will be significant in themselves; they will also inform the synthetic biology design processandimprovethemodelinginfrastructureforfutureresearch. ON COUNTERS: Briefly, a counter element changes state every nth time it is triggered. For a binary counter, n = 2. One could imagine using a binary counter to produce yeast that turn green for every 2nd cell division. More seriously, one could imagine a researcher in aging or cancer connectingfivesuchcounterstomakecellsturngreenwhentheyhavedivided25 =32times. AIM 1: Performselectionforbinarycountinginsimulatedgeneticnetworks. Existing software can simulate evolution in genetic networks. This work will use Genetdes,6 freeopen-sourcesoftwarewhichnativelyusesSBMLformat7todescribethenetworksitactsupon. Asaformermetabolicengineer,IamproficientinSBMLandwithnetworkmodeling. HYPOTHESIS 1: Electronic circuits will evolve into binary counters. The field of evolutionary circuit design began inelectronics, and a mature literature exists on the topic.8,9 Counter design is a common test case, and has been demonstrated a number of times. This will serve as a positive control;Iwilltestandrefinetheselectionmethodsonatargetknowntoexist. HYPOTHESIS 2: Under validated selection conditions, networks of interacting transcription factors will NOT evolve into binary counters. Selection will be performed with Genetdes using a proven fitness function. However, success appears unlikely. No human designer has produced a counternetworkfrompurelytranscriptionallogic,nordoknownexamplesexistinnature. AIM 2: Improvesimulatednetworksbyincludingrecombination-basedgeneticswitches. To test these switches in the context of a modeled network, I will make several changes to the Genetdes simulator and its underlying SBML representations. Current SBML standards permit embeddedtriggersforfunctionsanddiscontinuousevents;thesearekeyfeaturesforimplementing recombination. InthisAim,IwillthereforeextendGenetdestofullSBML-2compliance. Toupdatethemodel,Iwillfirstcreatenewparts: matchedpairsofrecombinaseandtargetsite (Rec and INT), using available kinetic data for the fim system.10 Rec binds INT, then complexes with another bound Rec. This complex triggers a discontinuous event, in which the paired INTs exchange locations. INTs will be context aware, storing the connections made at their genetic location (cis interactions). I will also create a new cell compartment where active INTs and their neighborswillbehiddenduringthisexchange,mimickingDNAblockedbyReccomplex. HYPOTHESIS 3: Networks containing switches will outperform transcription factors alone. Recombination is the favored mechanism for natural behaviors with periodic state changes, such as E. coli virulence10 and S. cerevisiae mating.11 The switches are discrete, leak-free, and fairly efficient-goodtraitsforacounter.5 Iexpectthatswitchingwillimprovethecounterdesigns. AIM 3: Convertevolvedcountermodelstophysicalformandconfirmactivityinvivo. GOAL: Producefunctionalnetworksinalivingcellwithsimulatedevolution. Forhigh-scoring designs, I will perform stochastic simulations on their network to assess noise tolerance. Designs whichsurvivethistestwillbesubjecttosensitivityanalyses,determiningtheirrobustnesstokinetic parameters. TheseanalyseswillbeperformedwiththeSimBiologytoolkitinMATLAB. For the most promising designs, I will manually ensure that each model element has a corre- sponding physical part with the right kinetics. I will also choose the most informative elements to tagwithfluorescentreporterproteins. Designswillbebuiltwithcharacterizedpartslibraries.12,13 Testing and debugging the designs will require dynamic analysis of multiple reporter proteins, ideally in single cells. I plan to work with Prof. Hasty of UCSD, a pioneer of this technique. His grouphasperformedsimilarworkforoscillatorsandcircadianclocks,showingfeasibility.14 AIM 4: Produceeducationalsoftwareusingideasfromnetworkevolutionmodels. ThealgorithmsthatevolveSBMLmodelsinGenetdescanbegeneralizedtouseanystructured input.6 I will refactor existing software to develop a general-purpose evolution simulator; this simulatorwillbeusedtocreateprogramsinTurtle,asimplegraphicslanguageforchildren. ThefitnessofaTurtleprogramwillberatedbyhumanscomparingitsoutputtoatargetimage. Thiswillbeawebgame,whichwillincorporategamedynamicssuchasallowinguserstocompete on how well their scores match the consensus. I will create time-lapse videos of the evolutionary process;thesewillbepostedonYouTubetoprovidevisuallyappealingtoolsforeducation. RESOURCES: Pilot studies for Aims 1 and 4 will be conducted on a small Beowulf cluster15 planned for DIYbio-Boston. Next fall, I plan to enroll in graduate school; I will use university resources(orideally,TeraGrid)forAim2. LabsforAim3canbefoundatUCSDandelsewhere. BROADER IMPACTS: I have dedicated a full Aim to public outreach. For the Turtle project, the tools will be developed in partnership with amateur scientists in DIYbio. Data will be collected throughpublicparticipation,andresultswillbepackagedtoreachthewidestpossibleaudience. I take scientific impacts just as seriously. I will continue to publish and give talks, making suretoreachthebroadergroupwhichcouldexpandonmywork. Forinstance,thepartsdesigners wouldwanttoknowifIwaslimitedbyaspecificgapinthepartslibraries,andthemodelerswanta formalismforrecombination. Withinmysubfield,Ialsohopetouseinterestingresultstomakethe casefordesignpracticeslikesimulatedevolution. Syntheticbiologywantstobetransformative;it couldbe,ifwewerebetteratit. Bylearningtodesignbiologicalsystemsbetter,smarter,andfaster, andbysharingthatknowledge,webuildtheinfrastructurethatwillallowittoreachitspotential. REFERENCES: (1)Buchleretal.2003.ProcNatlAcadSciUSA.(2)RodrigoG&JaramilloA.2007.SystSynth Biol.(3)CaoHetal.2010.SystSynthBiol.(4)igem.org(5)FriedlandAEetal.2009.Science.(6)RodrigoGetal. 2007.Bioinformatics.(7)sbml.org(8)BeielsteinTetal.2002.IEEE-CEC.(9)ShanthiAPetal.2005.IEEE-EH. (10)HamTSetal.2008.PLoSONE.(11)HaberJE.1998.AnnuRevGenet.(12)partsregistry.org(13)biofab.org (14)BennettMR&HastyJ.2009.NatRevGenet(15)beowulf.org	0
Recollection and neurophysiological correlates of fictional memories Keywords: autobiographical memory, fiction, episodic memory, cortical potential This project aims to understand the differences between experienced and fictional memories, from brain processes to behavioral effects. Episodic memories are characterized by a sense of re-living and visual imagery, and form the basis for developing an autobiographical self. Rubin et al. assessed the qualities of autobiographical memories by measuring variables including degree of reliving, visual and auditory imagery, emotions, setting, and belief1. Recent investigations have begun to probe the shared processes of remembering (“I went to the science museum 2 years ago”) versus imagining (“I imagine myself graduating from college in the future”). Absent from the literature is thorough behavioral data on fictional memories: the memory of an imagined experience without an explicit reference to self, derived from fiction (“I can visualize Atticus Finch standing in a Southern courtroom”). Fictional memories are encoded and retrieved with subjective characteristics similar to veridical memories, and can be source of integrated knowledge about the world2; as such, they occupy an interesting and largely unexplored niche in memory research. Conway et al. used electroencephalography (EEG) to record the dynamic process of retrieving true memories from the past3. Using the excellent temporal resolution of EEG, he established a distinct neural signature for what retrieving and maintaining autobiographical memory broadly looks like in the brain. First, there is activation in the prefrontal cortex, followed by additional temporo-occipital activation once the memory is formed. In a different task, subjects constructed future, imagined scenarios that were plausible and involved the self. Conway found that the real and imagined conditions relied heavily on the same brain regions. However, one difference that left prefrontal activation was highest during active maintenance of plausible imagined memories, presumably because this construction task is more effortful. Secondly, he found that temporal and occipital lobe activation is greater in the recall of real autobiographical memories, suggesting that imagined memories elicit stored sensory data, they do so less than real autobiographical memories. In order to gain a theoretical and practical understanding of fictional memories, both behavioral and neurophysiological data are important; my proposed study will investigate these perspectives. I expect that many fictional memories can be vividly re-lived, they may not be associated with a particular time or place. I also expect fictional memories to evidence the dynamic localization of autobiographical memory. And if memories that are explicitly understood to be not real are incorporated into autobiographical memory, then they can influence identity and behavior. Developing a clearer picture of the neurophysiology of fiction and memory could illuminate how fiction-reading contributes to cognitive and affective development, or how fictional sources could be used intentionally by educators. If fictional memories do not show activity aligned to real and imagined autobiographical memories, then we must begin to explain how any episodic-like memory can exist without these networks. I propose a research project to be carried out in two phases. Since there is not existing research on how to cue a fictional memory and it is critical to have reliable and controlled protocols for in the next, EEG-centered phase, I will first establish this protocol, as well as gather behavioral data through surveys. This first phase will also allow me to find and address any unanticipated sources of error in this novel process, and yield data to modify the design of the next phase. I will limit the study to fictional memories generated through the written word (e.g. novels and short stories). Subjects will be given a cue to recall a scene from a written work of fiction that can be strongly recalled; I will seek to gather 30 observations per subject. To gather Brenda Yang Graduate Research Statement NSF GRFP 2015 pilot data, EEG will be used to record cortical scalp potentials (more detailed methods are below). I anticipate that these memories—like veridical episodic memories— will differ in many ways within and between participants, including time since the last experience, personal interest, and amount of rehearsal. These qualities will be assessed via a questionnaire modified from Rubin et al.1, which asks participants to rate their experience on a scale of 1 to 7 for questions that address recollection (like reliving), component processes (like visual imagery, spatial layout and emotions), and reported properties (like importance, rehearsal, and age of memory). The questions will be delivered after each cue through a provided keyboard. In the second phase, I will use an EEG paradigm to examine the temporal dynamics of fictional memory construction. In two conditions, I will record scalp potentials with EEG while subjects recall and maintain (1) true memories of the past and (2) plausible imagined scenarios of the self; these are the scenarios studied by Conway, and will be used as controls. In the third (3) experimental condition, I will elicit the retrieval and maintenance of fictional memories using the protocol established in phase 1. I will seek to gather 5 observations for each condition per participant to balance the need for statistical rigor with maintaining a reasonable length for the study. Each trial will begin with the memory instruction “Real Memory,” “Imagined Memory” or “Fictional Memory” on screen for 3s. A fixation stimulus will be presented for 3s, followed by the cue for one of the three scenarios. The cue will remain on screen until subjects indicate with a bimanual joystick pull that a memory has been successfully retrieved or generated. Participants will communicate that they were unable to retrieve a memory via a keyboard instruction, which will lead to a new trial. After the memory is retrieved, subjects will fixate on the screen and be instructed to hold in mind the memory for 7.5s. Then, the participant will then type a brief description of their memory using the keyboard provided. In designing the cues and trials, it will be critical to balance cues and trials across participants. Subjects with high shifts in voltage throughout the trials will not be analyzed. Statistical significance will be assessed using a 3-way ANOVA involving the electrode levels and the 3 conditions of memory instruction. If fictional memories are experienced as episodic memories, I would expect to find patterns of cortical potential for fictional memories that are similar to that of the imagined future events. That is, the activation of posterior brain regions should be reduced compared to remembrances of real events. Of interest is the degree to which the prefrontal cortex is activated in the absent of a scenario that does not explicitly involve the self. Behaviorally, I expect to find that re-living of fictional memories to be comparable to veridical ones and primarily visual in nature. Of interest are differences in how the event comes to the subject “a coherent story,” the perspective of the experience, and whether the memory comes back “in words” for a fictional memory that was, after all, delivered through language. To support this work, I am seeking graduate programs which would allow me to combine behavioral and brain measures. I have established contact with several institutions where this would be possible and where training and facilities for EEG is available. For example, Elizabeth Marsh at Duke University studies fiction, false memories, and applications to educational practice. Conditional on my acceptance to the program, she has offered guidance for collaboration between her lab and others in the psychology and cognitive neuroscience departments. I am confident that with these supports, my experience designing novel experiments, and solid conceptual background, I can carry out this research within three years. 1Rubin, D.C., Schrauf, R.W., Greenberg, D.L. (2003). Belief and recollection of autobiographical memories. Memory & Cognition, 887-901. 2Marsh, E.J., Meade, M.L., Roediger, H.L. (2009) Learning facts from fiction. Journal of Memory and Language, 519 –536. 3Conway, M. A., Pleydell-Peace, C. W., Whitecross, S. E., & Sharpe, H. (2002). Neurophysiological correlates of memory for experienced and imagined events. Neuropsychologia, 1-8.	0
Improving Bounds on the Entropy of Odd Cycle Graphs Keywords: graph entropy, independent set, indistinguishability Introduction The entropy, also known as the Shannon capacity, of a graph is an important quantity in information theory, and can be used to study the zero-error capacity of a noisy communication channel. This channel can be represented as a cycle graph G in which each vertex represents a transmitted symbol and each edge indicates indistinguishability between symbols. A cycle graph is a graph which consists of a single cycle, i.e. a series of vertices connected in a loop. For instance, the cycle graph C (shown in 5 Fig. 1 with one of its independent sets in blue) represents a communication channel with five distinct symbols (herein called a, b, c, d, and e) in which adjacent symbols can be mistaken for each other due to noise in the channel. Figure 1. The The question posed is to determine the most efficient communication schema graph C with an 5 independent to transmit data with no errors and maximize precious band-width, and this indicated in blue. information density is encapsulated by the quantity known as graph entropy. Background Due to their graph theoretic properties, the entropy of all even cycle graphs is known. The same quantity is far more elusive for odd cycle graphs, however. In 1979, Lovász famously determined the entropy of C to be √5, but the entropy of C , for all odd p ≥ 7, is unknown. In a 5 p 2017 paper, Mathew and Östergård [1] used a stochastic search of independent sets guided by possible symmetries to establish the current best bounds on the entropies of C for p up to 15. p Even in the few short years since their research, computers have increased significantly, presenting the opportunity to further improve these bounds by using new algorithms, high performance computing, and theoretical results. Proposal To further improve the known bounds on odd cycle graph entropy, I propose using today’s increased computing power to run a variety of stochastic independent set search algorithms in parallel on high-performance computing clusters. Determining the entropy of a graph involves maximizing the size of its independent set, and the hope is that this search will yield at least a slight improvement in the previous bounds found in [1], particularly on the entropy of C . 7 Another source of potential untapped by Mathew and Östergård is the algorithmic Lovász local lemma, proven to succeed by Moser and Tardos in 2010 [2]. Because there is a natural family of local modifications to be made to a graph’s independent set, the lemma gives an algorithmic way to explore the space of independent sets. A third approach is to turn the problem of finding a graph’s entropy by constructing a maximal independent set into a boolean satisfiability problem (abbreviated SAT) and apply a SAT solver. Over the last decade, the field of SAT-solving has produced numerous sophisticated and effective methodologies, yielding a variety of strategies for approaching the entropy problem [3]. Methods I plan to focus initially on the entropy of C , in three stages: stochastic independent set search 7 algorithms, application of the algorithmic Lovász lemma, and use of multiple SAT-solving strategies. Noemi Glaeser Graduate Research Plan NSFGRFP 2018 In the initial stage, I will attempt to improve the bounds on entropy by writing code to probabilistically constructing independent sets. With various start configurations (an empty set, a random set, or a simple suboptimal construction, for instance), I will allow an iterative program to run for a limited time span, attempting to add points to the set. The method of adding points can be varied, e.g. allowing replacement of one point for another, or two points for another, but never removing more than two points at a time. Another approach is to simulate physics in the search, for instance by favoring the addition of points that produce more rigid configurations, which would result in the lattice structure suspected in optimal packings. Each algorithm will be optimized and modified to run in parallel on a high-performance computing cluster. Once the improvements from the initial stage have been exhausted, I will move on to applying the algorithmic Lovász local lemma to the problem. This involves a similar construction of the independent set, but one that allows the insertion of illegal points and adjusts the existing structure to restore a legal configuration. These changes propagate outward from the point of origin and are guaranteed by Moser and Tardos to eventually stabilize. Finally, the problem can be redefined in terms of a Boolean expression to be satisfied by the largest possible independent set. At this point, several free and open source third-party SAT solver algorithms, some of which are highly parallelizable, can be applied to the expression. The expression may also be rewritten in various ways, and the algorithms again applied, to improve the chances of a favorable result. The practice of applying SAT solvers is becoming increasingly effective in addressing well-known problems, notably in [4]. Conclusion Intellectual Merit Should these approaches prove successful, they can be applied to similar problems, particularly the entropies of C with p ≥ 9. The algorithms developed may also prove to be useful for other p problems in graph and information theory, in particular the outline of the iterative stochastic program, the physics-inspired approach to packing problems, and the novel application of the algorithmic Lovász local lemma. Broader Impact A more accurate understanding and estimate of the entropy of these odd cycles has direct implications for the definition of error-correcting codes. Knowing the entropy of C , for instance, 7 offers a constructive proof of the existence of a specific optimal information density, also yielding the construction of a 7-symbol encoding mechanism that realizes this density. This will lead to more efficient but still error-free communication through noisy channels, which could impact all digital communications, but in particular unreliable modes such as the satellite communication by phones, internet, and even space probes. [1] Mathew, K.A., & Östergård, P.R.J. (2017). “New lower bounds for the Shannon capacity of odd cycles”. Designs, Codes and Cryptography, 84: 13-22. doi:10.1007/s10623-016-0194-7. [2] Moser, R.A., & Tardos, G. (2010). “A constructive proof of the general Lovász local lemma”. Journal of the ACM, 57(2): 1-15. doi:10.1145/1667053.1667060. [3] Gong, W., & Zhou, X. (2017). “A survey of SAT solver”. AIP Conference Proceedings, 1836(020059): 1-10. doi: 10.1063/1.4981999 [4] Heule, M.J.H., Kullmann, O., & Marek, V.W. (2016). “Solving and Verifying the Boolean Pythagorean Triples problem via Cube-and-Conquer”. Lecture Notes in Computer Science: 228–245. doi: 10.1007/978-3-319-40970-2_15.	0
Do not duplicate or use without permission www.rachelcsmith.com Making Optimal Decisions for an Uncertain Future: Quantifying the Effects of Anthropogenic Disturbance on Biodiversity and Ecosystem Services Key Words: Disturbance Effects, Biodiversity, Ecosystem Services, Ecosystem Management Introduction: Anthropogenic disturbances negatively impact species, genetic and functional diversities of ecosystems, reducing the essential services they provide. While we know that the presence of diverse functional traits in an ecosystem is directly linked to the successful provisioning of essential services1, it is often easier to quantify an ecosystem’s genetic diversity than to determine its functional diversity. Unfortunately, we lack a fundamental understanding of the interrelationships between these forms of diversity and whether or not one can be used as a proxy for another. This gap in our knowledge impedes our ability to make management decisions that maximize future ecosystem services. My proposed research will: Research Objectives: 1. Determine if and how genetic diversity and functional diversity are related. 2. Determine if and how natural and anthropogenic disturbances alter the relationship between genetic diversity and functional diversity. 3. Create a predictive management tool that allows us to maximize genetic and functional diversity, and thus the provisioning of ecosystem services, in the uncertain future. To accomplish these objectives, I will develop a database of the species present before and after anthropogenic disturbances. The database will contain data from a global range of ecosystems, as well as a subset of data that will be provided by Sierra Pacific Industries (SPI), the largest private landowner in California. I will analyze these data to determine how genetic and functional diversities change after disturbance and then create an easy-to-use online management tool that predicts how future anthropogenic disturbances will alter biodiversity and ecosystem services. Background: Phylogenetic Diversity (PD) is the length of evolutionary pathways connecting taxa, and it is a well-known index used to measure genetic diversity2. When managing an area to conserve overall biodiversity, maximizing PD is likely the best way to hedge our bets and increase the probability of having the right extant species at our disposal in a future of unknown environmental, economic, and medical needs3. Functional diversity (FD), the total branch length of a tree of functional traits, measures the diversity of functional traits in an area4. Maximizing the FD of an area is important because essential ecosystem services are directly tied to the value, range, and abundance of an ecosystem’s functional traits1. It is quickly becoming more practical and economical to determine the PD rather than the FD of an ecosystem because DNA barcoding can reliably complete large taxonomic surveys, while quantifying the functional traits present in an area is still a large undertaking. It is possible that PD can serve as a proxy for FD, which would allow us to assess an ecosystem’s function without quantifying functional traits. However, no large-scale study has ever demonstrated the relationship between genetic and functional diversities. Additionally, we currently have no way to accurately predict the effects of anthropogenic disturbances on PD and FD. SPI owns approximately 1.7 million acres of land and must routinely make management decisions without knowing the exact consequences of their management practices. SPI is interested in learning if their management causes changes to the biodiversity and ecosystem services of their land. Therefore, in addition to studying disturbances in a range of global ecosystems, I will collaborate with SPI to determine the effects of harvesting-related disturbances. NSF-GRFP Proposed Graduate Study Copyright 2010 to Original Author All Rights Reserved. Do not duplicate or use without permission www.rachelcsmith.com Methods: I am currently collecting data from published articles that document changes to the species diversities of a range of organisms after anthropogenic disturbances (e.g. fire, timber extraction) in ecosystems subject to diverse natural disturbance regimes (e.g. fire, flooding). SPI has already compiled a dataset that details the plant species observed before and after clearcutting, replanting, and applying herbicides in 200 forest patches. With this data, I will: 1. Build separate phylogenetic and functional trees of species found before and after human disturbance, using known phylogenies, programs such as Phylomatic and TreeBASE, and lists of functional traits, such as USDA PLANTS and Jepson Herbarium’s Flora Project. 2. Analyze relationships between disturbance, PD, and FD. Using a likelihood and Bayesian approach, I will compare changes to PD and FD after disturbance and changes in the K score of trees, which is the difference in the relative branch lengths and topologies of phylogenetic trees5. If PD and FD change in correlated ways after disturbances, then PD can serve as a proxy for FD. I will also determine if the changes in PD, FD, and K score can be explained by specific disturbance regimes. 3. Organize data and results in an SQL database. Create an accessible online tool that will be available to researchers, landowners, government, and NGOs. It will formulate ecosystem management plans and allow users to predict how their actions will change their land’s biodiversity and ecosystem services. I will also develop a California version for use by SPI. Expected Results: 1. Ecosystems with a high PD will have a correlated high FD. 2. If an ecosystem faces an anthropogenic disturbance that mimics its natural disturbance regime, PD and FD will change in correlated ways, and PD can serve as a proxy for FD. Broader Impacts: Large-scale biodiversity loss directly threatens ecosystem stability and reliability by impairing ecosystem services, such as primary production, carbon storage, and pollination1. Gaining a better understanding of the effects of our ecosystem management decisions will allow us to avoid the irreversible loss of biodiversity and ecosystem functions or to at least limit the scale, frequency, and intensity of anthropogenic disturbances in the future. My research findings will elucidate the relationships between genetic and functional diversities. I will provide the information and tools we need to make economically efficient and socially optimal resource management decisions, ensuring that we conserve the organisms that will provide essential ecosystem services in an uncertain future. I have already started data collection for this project, and I am currently mentoring three undergraduates, including two women who also belong to ethnic minorities, who are aiding in the development of my dissertation project. After developing the management tool, I will hold workshops for landowners throughout California. I will teach them to use the tool, and I will speak on the benefits of managing land to maximize biodiversity and ecosystem function. My research will inform the long-term management decisions of landowners in California. References: [1] Diaz, S et al. 2007. Incorporating plant functional diversity effects in ecosystem service assessments. PNAS 104:20684. [2] Faith, D. 1992. Conservation evaluation and phylogenetic diversity. Biol Cons 61:1–10. [3] Forest, F et al. 2007. Preserving the evolutionary potential of floras in biodiversity hotspots. Nature 445:757-760. [4] Petchey, OL & Gaston, KJ. 2002. Functional diversity (FD), species richness and community composition. Ecol Letters 5:402-411. [5] Soria-Carrasco, V et al. 2007. The K tree score: quantification of differences in the relative branch length and topology of phylogenetic trees. Bioinformatics 23:2954.	0
Introduction: To better understand the impact of global climate change and the effects of increased biogenic and anthropogenic emissions, more research is needed into the unique photochemical processes that take place in the Arctic atmosphere. Although geographically remote, the Arctic has a significant impact on globally important feedbacks to climate change. Since ozone (O ) is the precursor for most of the oxidizing, or self-cleaning, capacity in the 3 troposphere, it generally controls the oxidation potential of the atmosphere. Similar to the discovery of stratospheric ozone depletion, observations of ozone depletion events (ODEs) in the polar boundary layer (BL) were surprising. Springtime episodic depletions of tropospheric O , and the characteristic photochemistry 3 involved, are current areas of considerable research. These sudden and recurrent ODEs during late winter and spring are associated with elevated concentrations of halogenated radicals. The conversion of inert halide salt ions into reactive halogenated species has been shown to deplete O following the onset of Polar Sunrise [e.g. 1,2]. While it is widely accepted that bromine is the 3 primary driver of ODEs via photochemical reactions, the role of iodine chemistry in ozone destruction and oxidizing strength of the atmosphere is not well understood [e.g. 3,4]. The dominance of bromine in Arctic ODEs appears to be a function of its abundance only, as even small perturbations in the iodine concentration significantly impact the rate of ozone depletion in recent models [5]. These models have also shown that interactions with iodine may double the efficiency of bromine in ozone depletion [6]. Problem Statement: To date, few successful in-situ measurements of iodine compounds have been achieved in the Arctic due to the lack of analytical methods. As a result, iodine chemistry is often omitted from current climate models that simulate Arctic ODEs, which may significantly underestimate the rate of ozone depletion. Research Objectives: This study proposes the exploration of new technologies and methods for the selective, quantitative chemical detection of iodine compounds that may play an important role in Arctic ODEs. In this study, I propose to investigate new analytical procedures and instrumentation to observe and quantify I and IO 2 during springtime Arctic ODEs. This work will then allow iodine chemistry to be incorporated with reduced uncertainty into kinetic analyses and computer model simulations. Method Development: To complete these objectives, I will work with Prof. Paul Shepson of Purdue University, a leading scientist in the field of atmospheric halogen chemistry in the Arctic. I will design new laboratory techniques to study the reaction mechanics of Arctic iodine species using chemical ionization mass spectrometry (CIMS), and inductively coupled Argon plasma mass spectrometry (ICP-MS). CIMS has not yet been used to detect I /IO in the High Arctic, 2 however, the Shepson lab has adapted a CIMS for the field and I plan to expand the capabilities of that instrument to detect I and IO in-situ, using SF gas as the ion source. 2 6 Study Site and Field Observations: Field observations of I and IO using CIMS and filter 2 sampling will take place in Barrow, Alaska where Beaufort meets the Chukchi Sea in the Arctic Ocean. This site is ideal for studying an Arctic maritime environment as it is coastal, surrounded by first-year sea ice, and the predominantly northeasterly winds come from clean, undisturbed snow over the sea ice. This ensures that our measurements include only halogenated compounds from the natural environment. Filtered samples from the field site will be transported and analyzed using ICP-MS at the Mass Spectrometry Center at Purdue University. Mallory Ladd Proposed Research 11/14/12 Modeling: Initial models established in the Shepson lab will be expanded to include observations of reactive iodine species (I, IO, HI, HOI, I O , INO ) significant to ODEs in the Arctic. 2 2 x Improvements will be made to a multiphase, zero-dimensional model used previously to study Br and Cl radicals [6], in order to predict the chemistry of iodine species and their involvement during ODEs. Developments to the model needed to expand on the heterogeneous iodine production mechanism and provide a better representation of snowpack and aerosol chemistry are as follows: nitrate and sulfate chemistry will be included; pH, temperature and the availability of oxidants will be varied; and finally, vertical mixing rates will be updated to include a better parameterization of the transfer of halogenated compounds between snow, interstitial air and the boundary layer. This model will aim to quantify the fraction of O depleted 3 by iodine during ODEs, the impact of iodine on the rate and timescale of O depletion, and the 3 effect of iodine on other important atmospheric oxidants such as HO and NO . x x Intellectual Merit: Significant opportunities exist in this proposal to discover fundamental knowledge related to the kinetics of Arctic iodine species and their relationship to ODEs. In addition to the development of a sensitive and selective method for quantifying reactive iodine species from the High Arctic during an ozone depletion event, this research will produce a model of Arctic ozone photochemistry that incorporates the effects of I /IO chemistry in addition to 2 those of Br and Cl. This will lead to a better understanding of the role of iodine chemistry in ODEs and other tropospheric chemical cycles important to the future of global climate change. My previous experience with analytical instrumentation has provided me with a strong foundation for success in developing this method. Undergraduate research and my current position as a lab and field technician have prepared me well for conducting research in an academic setting as well as in the field (see previous research essay). Working in the Shepson lab will be central to the growth of my knowledge about environmental modeling. Having presented multiple research projects, in addition to preparing a manuscript and following it through submission and revision, I am trained in effectively communicating the results of my research. Broader Impacts: I will share this research via publications in peer-reviewed journals for general and specialized audiences, and presentations at both national and international conferences. I will continue my involvement with the ACS and AXΣ (see personal statement) via the ACS Student Affiliates and the AXΣ-Beta Nu chapter at Purdue. In each phase of this project, I will actively recruit high school and undergraduate students from underrepresented groups to gain valuable research experience in our lab. I will encourage students from the ACS Project SEED and the NSF REU program to apply for these positions. The interdisciplinary nature of this research will significantly broaden their scientific experience and enhance their understanding of the importance of chemistry in the Arctic to global climate change. I also plan to be in close contact with the PolarTREC program so that I may host a high school teacher during my field campaign in the Arctic. I plan to partner with them to connect directly to their students back home via a cyber-based platform so that we may share our experiences in the field. Through digital storytelling, I will also be able to communicate the importance of this research to the broader public via a website that Dr. Shepson has previously developed (www.arcticstories.net). Literature Citations: [1] Bottenheim, J.W., et al. (2002) Atmos. Environ., 36, 2535-2544 [2] Helmig, D., et al. (2012) J. Geophys. Res., 117, D20303 [3] Barrie, L.A.; Platt, U. (1997) Tellus 49B,450-454 [4] Mahajan, A., et al. (2010) J. Geophys. Res., 115, D20303 [5] Calvert, J.G.; Lindberg, S.E. (2004) Atmos. Environ., 38, (30), 5087-5104 [6] Stephens, C.R. (2012) PhD Dissertation, Purdue University	1
"between a treatment and control group – has come to be understood as the gold standard for scientific settings where the end goal is intervening in some process to achieve a desired end, as in genetic engineering, clinical trials or public policy design. This is for good reason: causal inference is a technique for identifying the precise impact of a given intervention on the target outcome, which is essentially impossible otherwise due to issues of confounding. Nonetheless a number of issues still plague causal inference as a method of inquiry. Perhaps the most important is failure to generalize. We see this everywhere from MPRA estimated gene expression not predicting measured expression in cells [1] to the numerous nudges that work well in laboratories and fail when scaled [2]. Often these issues of generalization are related to a shift in the underlying population tested in the lab and the actual population intervened on. This means there is a deep connection between understanding when we should expect treatments to translate to results and out-of-distribution prediction problems in the machine learning literature. The second major problem with causal inference it is not integrative: information from one experiment rarely if ever informs our understanding of another experiment on a similar population. A clear consequence is that causal inference has produced a fractured landscape of treatment effects without real theoretical connections, especially in the social sciences. Using latent representations of experimental units would allow for multitask learning which would effectively share information on treatment responses across treated units. Related Work Machine Learning and causal inference is an emerging intersection with tremendous promise. Most work in the literature is focused on understanding treatment heterogeneity within a given study. Usually this is done by building a model to predict the outcome for treated and control units, then using that model to predict counterfactual treatment or control outcomes for each unit and taking the difference. The distribution of these differences captures the degree of treatment heterogeneity, which is often of interest especially in medical contexts where it is important to know if treatment effects are driven by broad effects or much higher than average effectiveness in some sub-group. Within this literature the closest work to my proposal is [3], which attempts to learn representations to improve the quality of these counterfactual predictions but does not focus either on out-of-distribution predictions for understanding generalization or learning representations for multiple experimental treatments. Proposal Toward extending the literature on machine learning and causal inference to address the generalizability of treatments and allow sharing of information across treatment effects I propose to use learned representations of experimental units to allow for out-of-distribution prediction with calibrated uncertainty estimates and multi-task learning. Calibrated uncertainty in individual predictions should allow extrapolating from the experimental setting to the population of interest and looking at the confidence intervals to understand the expected range of outcomes. Multi-task learning, and in particular using a shared latent representation across experiments should enforce information sharing across different experimental settings, formally allowing the results of treatment in one experiment to inform the analysis of other experiments. Research Plan Much of my work up to this point has been on representation learning of regulatory DNA and of political beliefs. In the political context I have found that even without tuning, representations can provide more robust out-of-distribution prediction. Along similar lines I have found that multi-task learning of latent representations also improves the out-of-distribution predictions. In the genetics context my work has shown that Gaussian processes offer well calibrated uncertainty estimation on samples far from the training distribution. Aim I: Before digging deeper into method development I want to confirm these insights hold across other contexts. Does multi-task learning improve the quality of latent representations for out-of-distribution prediction in genomics as well as politics? Do Gaussian processes still provide well calibrated uncertainty if treated units are companies instead of basepairs? More generally I plan to build simulations to explore the dimensions of when and why these ideas hold up and hopefully to develop supporting theory. Aim II: After confirming the results from my past work I want to extend these insights to develop a framework for embedding causal inference in deep learning. To build the learned representations I plan to explore Variational Auto-encoders, Auto- Encoding Generative Adversarial Networks, and comparing to a baseline using multitask learning directly and taking the last shared layer as the latent representation. The core idea is to use these latent representations to predict the outcomes for each unit in the control and treated conditions. Because of their high-quality uncertainty estimation, I plan to use Gaussian processes to make these predictions. Of course, if the learning of the latent representations is completely independent of outcome prediction there will be no information sharing across tasks. So, I am going to leverage another property of Gaussian processes: their differentiability. I plan to split training into two phases. The first unsupervised phase will just focus on training the auto- encoder for learning representations. The second phase will optimize the latent space for the predicting the outputs for all experimental settings simultaneously, updating the auto-encoder by back-propagating through the Gaussian processes (in the figure these are the yellow arrows pointing to outcomes). Aim III: This framework is only useful if it actually works in practice. I want to conduct replications of several randomized trials that were first tested in the lab and then scaled. In particular I want to examine deworming studies from development economics [4], fixed/growth mindset work from the education literature [5], and α-1 adrenergic receptor antagonists for COVID-19 treatments [6]. The first of these failed to scale, and the second succeeded with limited effectiveness, and the third is an example where the experiment only involves older men but the target population for intervention is the general public. If my method correctly recovers the average treatment for these experiments, it would confirm the value in robustly extrapolating before taking the costly step of scaling treatments. Intellectual Merit Should my approach to out-of-distribution confidence intervals prove successful it would have significant implications for the machine learning literature. Similarly, if integrating information across experiments proves useful for estimating treatment effects that will be very significant for work in causal inference, transforming the way we think about randomized trials. Instead of one-off experiments we could engineer large models that integrate as many effects as possible to mutually improve our understanding. Even if my main approach does not work as expected, in the process of completing this research, I will certainly be able to contribute to our understanding of when out-of- distribution prediction is easy and when it is hard, and to the literature on learning representations. Broader Impacts Understanding when and how treatments effects will generalize when scaled up significantly is a crucial question in clinical settings and in public policy. If I am able to establish a framework that allows for more precise estimation of treatments when scaled it could greatly improve our understanding of who drugs are effective at treating, allowing greater patient understanding of expected outcomes and uncertainty. It would also improve the design of government programs, and the cost of designing government programs if extrapolation could substitute for running full scale experiments. References [1] de Boer, Carl G., et al. ""Deciphering eukaryotic gene-regulatory logic with 100 million random promoters."" Nature biotechnology 38.1 (2020): 56-65. [2] Rai, Tage S. ""Honesty “nudge” fails to replicate."" Science 368.6488 (2020): 279-280. [3] Johansson, Fredrik, Uri Shalit, and David Sontag. ""Learning representations for counterfactual inference."" International conference on machine learning. 2016. [4] Miguel, Edward, and Michael Kremer. ""Worms: identifying impacts on education and health in the presence of treatment externalities."" Econometrica 72.1 (2004): 159-217. [5] Yeager, David S., et al. ""A national experiment reveals where a growth mindset improves achievement."" Nature 573.7774 (2019): 364-369. [6] Konig, Maximilian F., et al. ""Preventing cytokine storm syndrome in COVID-19 using α-1 adrenergic receptor antagonists."" The Journal of Clinical Investigation 130.7 (2020)."	0
first confirmed physics beyond the standard model, but has proven impossible to measure in the lab. However, neutrinos make up a significant fraction of the mass in the universe (up to 1%). Due to their very low, but non-zero, mass, they have a distinct effect on the growth of large scale structure. At early times, neutrinos behaved as “hot” dark matter but became “cold” dark matter as the universe expanded. Hot dark matter is able to stream freely through potential wells created by massive objects, pulling them apart while cold dark matter becomes trapped and enhances the structure. The formation of galaxy clusters, which are the most massive gravitationally bound objectsintheuniverse,ishighlysensitivetotheneutrinomass. Moremassiveneutrinosbecomecold earlier, leading to a larger number of high-mass clusters. Through the Sunyaev-Zel’dovich Effect, high-resolution microwave telescopes are able measure the abundance of clusters over cosmic time. The SZE is the result of photons in the Cosmic Microwave Background (CMB) interacting with high-energy electrons in a galaxy cluster. When CMB photons scatter off the electrons, they are shifted in frequency. At low frequencies (below ∼220 GHz), this results in a reduction of the CMB intensity. These relatively small “shadows” are used to detect galaxy clusters in high resolution CMB surveys. The amplitude of the effect can be used to estimate the cluster mass. Current cluster cosmology studies are limited by systematic biases of the cluster masses, some of which comes from emissive sources in the clusters themselves. Dust emission due to start formation in the member galaxies is a potentially important bias of cluster mass. In 2015, the Planck Collaboration released a paper exploring the correlation between the SZE and the Cosmic Infrared Background (CIB) [1]. The CIB is emitted by hot gas in star forming galaxies. Unfortunately, the Planck cluster catalog only extends to redshift of ∼1.0, and the CIB primarily comes from redshift >1.0. For cluster surveys with higher angular resolution (such as those produced by data collected from the South Pole Telescope and the Atacama Cosmology Telescope), this effect has not been quantified for most of their clusters. I will probe the emission from dust in galaxy clusters out to redshift ∼1.5, using clustersfoundindatafromtheSouthPoleTelescope(SPT)andCIBmapsfromPlanck. Following [1], there are two methods for performing this correlation. The first is a stacking analysis. I will make small cutouts of the Planck CIB maps and SPT CMB maps at the locations of SPT-selected clusters. Stacking the cutouts and using aperture photometry to extract the signal strength will reveal the average correlation between the SZE and CIB in each frequency band. The second method uses Fourier techniques to account for both the clusters detected at high signal to noise and lower significance clusters that are below the detection threshold. I will create an angular power spectrum of the correlation of each band with the SZE signal. These two methods are complementary. The first is a very direct probe of known clusters. The second includes lower significance sources of SZE signal, which leads to higher signal to noise. Both of them require careful modeling of the SZE and CIB in order to distinguish the two terms. ExtendingthisworkbythePlanckteamwillprobearegionofredshiftthatismoreimportantto the systematic bias of cluster mass. The star formation rate is increasing over the range that I will probe,sotheemissionfromdustismoresignificant. Furthermore,mappingoutthecorrelationasa functionofredshiftwillallowmetoprobehowitchangesoverthehistoryoftheuniverse. Ofcourse, this comes with additional analysis challenges, most of which have to do with the extreme distance of the high redshift clusters in the SPT catalog. Since the CIB is emitted by hot compact objects, its amplitude decreases with distance (unlike the SZE, which is a spectral distortion of the CMB). Pushing to higher redshift will mean some loss in signal, simply from the distance. On the other 1 hand, the surface brightness (amplitude at the source) of the CIB is higher in the redshift range I willprobe. Thisisduetotheincreasedstarformationrate. Theexpansionofspaceeffectivelyshifts thePlanckbandsintoahigherintensityregionoftheemissionspectra,furthercounteringthiseffect. My previous work with SPT has prepared me well for this project. I have been running a search for clusters using data collected by the SPT in 2012 and 2013. Through this work, I have become accustomed to working with CMB maps through both my low level analysis tasks, and the high level work to produce a cluster catalog from SPT data. Furthermore, I have been exposed to many more analysis techniques through collaborative work with my colleagues. Finally, as part of the SPT collaboration, I will have access to all of the resources I need to complete this project: SPT CMB maps filtered appropriately to isolate the SZE, Planck CIB maps (which are publicly available) and minimal computing resources. This work will address a significant unknown in the systematic error budget of cluster masses. Even if I find significant contamination of the SZE by the CIB, that is simply the first step in accounting for the error. Since cluster cosmology is currently limited by systematic errors on cluster mass, caused by effects like this, understanding and eliminating them will have lead to significant improvements in cosmological constraints from cluster surveys. Beyond the impact on future cluster cosmology surveys, the results of this correlation probe several other astrophysical phenomena. The Butcher-Oemler Effect [2] predicts that star formation is suppressed in galaxy clusters, relative to a similar galaxy outside a cluster. Since the CIB traces star formation, this correlation would determine if there is statistical evidence for the Butcher-Oemler Effect. Furthermore, thecorrelationcanbebinnedinredshifttodetermineifthestarformationinclusters is time-dependent. The stacking method also includes spatial information, which allowed Planck to determine that the star formation in low redshift clusters is primarily in the outskirts of the cluster. This analysis would extend our knowledge to older clusters. The results of this work also have appeal outside of the scientific community. They help us to understand star formation in the largest objects in the universe. Using SPT-selected clusters allows us to look back in time, when the star formation rate was highest. I will present this knowledge to the general public through several outlets. There are multiple local astronomical societies (the Eastbay Astronomical Society and the Mount Diablo Astronomical Society, for example) that are very interested in having graduate students speak about their work. I will be giving talks to some of them in the near future on my current work with the SPT, so when this work is done, it will be easy for me to return and present our new findings. My research group is also starting a collaboration with the Chabot Space and Science Center. I will work with then to create a standalone exhibit detailing my work, and arrange talks for the public. References [1] PlanckCollaborationetal. Planck2015results.XXIII.ThethermalSunyaev-Zeldovicheffect–cosmicinfrared backgroundcorrelation. ArXiv e-prints,September2015. [2] H. Butcher and A. Oemler, Jr. The evolution of galaxies in clusters. I - ISIT photometry of C1 0024+1654 and3C295. Astrophysical Journal,219:18–30,January1978. 2	0
Unraveling the process of polysaccharide utilization in complex bacterial ecosystems Intellectual Merit – Introduction: Bacteria do not exist in isolation in nature; they form complex communities in which they must recognize, compete over, and, share nutrient resources.1,2 The gut microbiome is an ideal model to study this type of ecosystem, as we know what nutrients these bacteria are exposed to and have access to powerful tools to study taxonomy and metabolism. Gut bacteria break down polysaccharide nutrients and convert them to health-beneficial end products of metabolism called short-chain fatty acids (SCFAs) that are taken up and used for energy by host intestinal cells.1,2 However, there is a gap in our understanding of what factors give rise to the emergent phenomenon of SCFA production. The leading hypothesis in the field is that communities that fail to respond to a given polysaccharide lack certain “keystone species” that are integral to metabolism.1 The search for keystone species has focused on primary polysaccharide degraders,1 but findings in our lab indicate that these are abundant even in samples otherwise characterized as non-responders (Fig. 1). In contrast with the notion of a single species as a keystone, I hypothesize that community-level polysaccharide metabolism is driven by the presence of core communities of multiple species that function as an assembly line. This would explain how samples can fail to produce SCFAs despite the presence of primary degraders. By shifting the focus to species that consume degradation byproducts, I will reveal novel cross-feeding interactions and develop a model to predict polysaccharide response from community composition. Our lab has a collection of stool samples from multiple healthy donors that have been characterized for SCFA production (Fig. 1), making me well situated to conduct this research. I will address my hypothesis in the Figure 1: Bacterial communities derived from following aims: stool samples from 9 donors (A-I) contain primary degraders of all tested polysaccharides.3 Intellectual Merit – Research Plan: Aim 1: Isolate the core communities from multiple stool donors. I will grow bacterial communities from stool samples on minimal media with inulin as the sole carbon source in our lab’s “artificial gut,” a set of eight bioreactors.4 Inulin is a well-studied polysaccharide, a polymer of fructose with a single terminal glucose residue. The species that persist after three weeks of growth in this media, as measured by 16S sequencing, will include all species that consume inulin or inulin byproducts. By reducing complex communities to only those species involved in inulin metabolism, I will test the hypothesis that core community composition is directly related to SCFA production. I will next quantify expression of genes encoding select carbohydrate active enzymes and transporters for all species present using RT- qPCR. I hypothesize that degradation will occur in waves, with primary degrader species upregulating genes of interest at early time points, followed by secondary degraders. While I expect all samples to exhibit a robust first wave of upregulation, I hypothesize that the magnitude of the second wave will correlate with SCFA production. If there is no correlation, this would support the alternative hypothesis that strain-level variation drives differences in polysaccharide response. In this case, I would expect the gene expression experiments to reveal an increased number of species that upregulate the genes of interest in SCFA-producing samples. Graduate Research Plan Statement Jeffrey Letourneau Aim 2: Differentiate primary, secondary, and tertiary degraders. Our lab has developed a microfluidics growth assay that allows for the isolation of single cells in droplets (Fig. 2). Species that grow in this assay are defined as primary degraders, since cross-feeding interactions are prevented. I will identify secondary and tertiary degraders by taking conditioned media from primary degraders, which will contain partially broken down polysaccharides and byproducts of their breakdown, and using this media as the sole carbon source in a subsequent assay. Conditioned media will be characterized using the lab’s GC and HPLC to measure SCFAs and polysaccharides, respectively. I hypothesize that while stool samples from all donors will contain some primary degraders, samples unable to produce SCFAs will lack key secondary and/or tertiary degraders. Growth will be determined by 16S sequencing to measure relative abundance and flow cytometry to calculate the total number of cells. The use of 16S will also allow me to determine which bacteria are involved at which stage of polysaccharide utilization. I hypothesize that the primary degraders will comprise of both generalists (Bacteroides ovatus) and specialists (Roseburia inulinivorans, for inulin), secondary degraders will include acetate producers (Bifidobacteria and Lactobacilli), and tertiary degraders will include acetate- and lactate-utilizing Figure 2: Droplets containing bacterial butyrate producers (Eubacterium and Anaerostipes). cultures each derived from single cells.5 Broader Impacts – Disseminating Research: To date, I have shared my research findings with diverse audiences from kindergarteners to faculty and will continue to use the results of this project as a means of promoting science education. I am making my research accessible to middle and high schoolers by teaching for a Saturday program called Duke Splash, where I have previously taught Intro to Microbiology and Bread Science. I am in the process of developing two new classes to debut this November, Nutrition Science and Data Science, both of which incorporate data from my own research on polysaccharide metabolism. At Duke, I have many opportunities to share my research, such as the monthly Duke Microbiome Center seminar, where I recently presented my findings on transcriptional memory of polysaccharides in gut bacteria. To reach undergraduates, I will be giving a talk at the annual University Scholars Program (USP) Symposium, the theme of which will be “(in)dependence.” With this theme in mind, I will be presenting on how humans are dependent on gut bacteria to help metabolize dietary fiber as it relates to my research. Previously, I spoke at the USP Graduate Research Seminar on my work as a rotation student. Beyond Duke, I plan to present my research at conferences such as the 2019 Keystone Microbiome Conference. Broader Impacts – Importance and Innovation: Emergent properties of complex systems can be difficult to deconstruct, and this project provides a methodology for doing so in a novel, high-throughput manner that can be applied in future studies. By determining how emergent polysaccharide metabolism phenotypes arise, I will address a key gap in our understanding of how microbial ecosystems respond to nutrients. These results may be applied in human diet research to predict individual polysaccharide response and in bioremediation to develop co-culture methods for effective degradation of pollutants. Moreover, my findings will raise exciting new questions related to evolutionary biology as to how polysaccharide utilization came to be an assembly line process. References: 1. Makki et al. 2018. Cell host & microbe. 2. El Kaoutari et al. 2013. Nature reviews Microbiology. 3. Villa. 2018. Unpublished. 4. Silverman et al. 2018. bioRxiv. 5. Bloom. 2018. Unpublished.	0
Motivation: Virus purification and concentration is critical for the production of vaccines and gene therapy vectors. Today, vaccines exist for 26 viral diseases1, but low yields and high costs of production prevent access to many vaccines. Similarly, production limitations will reduce access to viral gene therapy, which may provide cures to single-mutation genetic diseases. Although many technologies exist for virus particle purification, they each face unique roadblocks to developing rapid, high-yielding, and ultimately continuous processes. Chromatography is widely-used for virus purification, but suffers from low binding capacities, frequently inactivates viruses, and cannot operate continuously. Ultrafiltration is readily scalable and provides high throughput and recoveries, but fouling reduces flux and removal of contaminating proteins is often ineffective. Ultracentrifugation is difficult to scale up and has low yield, even though purities are high. Aqueous two-phase systems (ATPS), most commonly constructed with two polymers or a polymer and a salt, are proposed as a replacement to simultaneously purify and concentrate virus. Biologically-gentle purification is achieved by choosing the identity and concentration (described by tie-line length, TLL) of phase-forming components, ionic strength, polymer molecular weight, and pH so that the desired bioparticle partitions to one phase and contaminants to the other. Many viral particles have been purified in ATPS, with yields as high as 79%2, a significant result in an industry that often accepts overall downstream yields of 30%. Still, the vaccine industry is reluctant to adopt ATPS in part because each bioparticle requires a unique system for optimal recovery and predictive models are lacking to fill this critical gap. So far, attempts to develop a predictive model for bioparticle partitioning in ATPS can be grouped into three approaches. A first approach, the Collander equation, uses known partitioning of bioparticles in ATPSs to predict partition coefficient (K) in new systems3. Because of its reliance on data, this model cannot predict K for new bioparticles. A more robust method combines density gradient theory with thermodynamic and association models. It successfully predicted the mass transfer of an amino acid in ATPSs4, but cannot model partitioning of large bioparticles like viruses. A better solution by Chow, et al. uses surface and interfacial properties to predict partitioning5. Particle diameter, net charge, and contact angle with the ATPS components can predict K. No one has yet successfully combined a theoretical model with measurements of ATPS and particle surface properties to predict bioparticle partitioning. Chow, et al. verified their model empirically by successfully comparing K to TLL and pH, but did not attempt predictions with surface measurements. I propose combining novel methods developed in my lab to measure virus surface characteristics with Chow’s theoretical model to together predict virus partitioning in ATPS, a task which was not previously possible. Hypothesis: Partitioning of virus particles in aqueous two-phase systems can be predicted using surface chemistry properties measured at the single-particle level. Experimental Plan: I propose extending Chow, et al.’s models to predict partitioning of virus particles in polymer-salt ATPS. Chow’s model requires phase properties of ATPS and surface properties of the viral particle to predict K. Methods for characterizing ATPS are rapid and well- known. Turbidity measurements will be used to determine the binodal points and tie lines. Characterization of viral particles is much more difficult. While virus diameters and isoelectric points (pI) (related to surface charge of viral particles) may be found in the literature, no method to measure the contact angle between ATPS and viral particles currently exists. 1 Historically, virus characterization has depended on bulk solution measurements or amino acid sequencing. Instead, my lab has developed a novel single-particle method to measure virus surface chemistry using chemical force microscopy (CFM), shown in Fig. 1. In a recently submitted article6, we determined the pI of two model viruses using atomic force microscopy probes chemically functionalized to carry positive or negative charges. Adhesion to the viral particle was measured in varying solution pH and the pI determined6, giving a direct characterization of the viral surface and avoiding error from contaminants. A similar single-particle Fig. 1. CFM with characterization will be used to determine the parameters of Chow’s virus particles partition model. The first stage of this work will focus on how virus surface chemistry changes in the presence of ATPS components individually before combining them. First, virus-sized gold nanoparticles (AuNPs) coated in BSA or lysozyme will be immobilized on a gold surface. Then adhesion force between the AuNPs and probes modified with charged or hydrophobic ligands will be measured by CFM. These proteins are known to partition differently in ATPS, and since contact angle between these proteins and ATPS may also be determined by traditional sessile drop methods, comparison will confirm that CFM can be related to surface tension for well- defined systems before extending the method to viruses. Similar determinations of surface tension by CFM for non-biological surfaces have already been reported7. Then, two enveloped (surrounded by a lipid bilayer) and two non-enveloped viruses will be used to explore varying viral surface chemistry. Once a relationship between CFM and contact angle is established, the second stage of this work will characterize multiple ATPS and adapt Chow’s model to predict virus partitioning. To verify the model as a function of component concentration, three TLLs for three common polymer (PEG) and salt (citrate, phosphate, and sulfate) ATPS will be evaluated to show the model is robust for varying chemistries. Similarly, the pH of ATPS will be varied over the range of stability for each virus, typically 4.5<pH<7.5. The model will be complete when it predicts the K of a virus as a function of TLL or pH. Once this work establishes a reliable model for virus partitioning between the bulk phases in ATPS, partitioning to the interface, which frequently results in the irreversible aggregation and inactivation of virus particles, can be explored. Intellectual Merit: Developing predictive models for virus partitioning in ATPS will add to the understanding of viral surfaces and the driving forces behind ATPS, reducing the experimental cost of developing ATPS to purify new bioparticles in the future. Broader Impacts: By filling a critical gap in the literature, making ATPS research faster, and making ATPS more accessible to industry, this work will speed the development and production of vaccines and gene therapy, ultimately reducing outbreaks of viral and genetic disease and saving lives. In addition, I will mentor undergraduate researchers who will have the opportunity to develop contact angle measurements with protein-coated AuNPs. References: 1. WHO. Vaccines and diseases. 2019 2. Joshi, P.U., et al. Journal of Chromatography B, 2019. 1126. 3. Madeira, P.P., et al. Journal of Chromatography A, 2008. 1190(1-2): p. 39-43. 4. Chicaroux, A.K. and T. Zeiner. Fluid Phase Equilibria, 2019. 479: p. 106-113. 5. Chow, Y.H., et al.. Journal of Bioscience and Bioengineering, 2015. 120(1): p. 85-90. 7(38): p. 21305-21314. 6. Mi, X., et al.. Under review. 7. Drelich, J., et al. (2004). Journal of Colloid and Interface Science 280(2): 484-497. 2	0
One of the most important unknowns in high-z extragalactic astronomy is how reionization occurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominates the intergalactic medium (IGM). HI attenuates radiation from early stellar populations, masking galaxies from detection. Understanding how and when reionization occurs can reveal whether or not these young galaxies provided the necessary ionizing radiation to completely reionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the high redshift-space this implies, spectroscopic observations are limited as these galaxies are very faint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths. My background in NIR spectroscopy and observational astronomy has prepared me to assist in addressing this question. I propose using Lyα and CIII] to investigate the properties and ionization state of young galaxies using ground- and space-based telescopes, the structure and distribution of HI in the IGM and the circumgalac- tic medium (CGM) of certain galaxies, and implications for the evolution of the neutral fraction of the IGM throughout the EoR. The individual points proposed will be summarized as follows: (i) small scale testing and building of an analysis technique, (ii) distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. By understanding more about the IGM during the reionization era and of the galaxies within it, we can further constrain the properties of current galaxy evolution and reionization models. Small scale testing & building of analysis technique: In the search for galaxies during the EoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric 0 surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UV bright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring the escape fraction of Lyα many studies have inferred an increasingly neutral fraction of the IGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emission hundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent method uses a complementary spectroscopic tracer not attenuated by HI, with the UV metal line CIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5]. 0 In my current work, I measure CIII] H-band emission of galaxies found via Lyα emission using Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) and attenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGM of these galaxies and surrounding IGM. From my previous work experience, I have developed a proficiency in coding which enabled me to gain a close familiarity with the MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fix bugs often encountered when working with incredibly faint emission lines and less common dithering patterns for standard star observations. I wrote code to optimally extract my 1D spectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage of IRAF and Python, I developed code that can track the photometric variability of my data from a frame-to-frame basis – important when working with faint emission lines. Distribution of galaxies and evolution of neutral fraction: Using the foundation built from my previous work, I will build a statistical sample of galaxies during the last half of the reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escape fraction as a function of redshift. Using my optimized extraction technique to improve Taylor A. Hutchison 2 measurements, I will use this dataset to constrain the offset between these galaxies’ systemic and attenuated redshifts. This work will significantly increase the sample of high-z galaxies with both Lyα and CIII] measurements. In addition, it will provide a more significant comparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses. A current complication for this project is the lack of a complete spectroscopic sample of LAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines or the [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to 0 NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes – it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in the range of NIRCam on JWST. As a first approach to resolving this, I will take the current sample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using my optimized extraction technique to improve measurements. This has already been attempted for some galaxies [4], providing useful lower limits for determining exposure times and potential telescopes for future observations, including JWST. We are planning proposals for the first JWST cycles for this work. I will then take advantage of the deep multi-wavelength imaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I am part of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with early indications that the proposal has so far been successful. Finally, as a scientific collaborator on an ERS JWST proposal, I will prepare for access to that data – understanding what spectra I will be looking for from running binary stellar population models (eg. BPASS), scaled to match expected bandpass magnitudes, through the JWST exposure time calculator. With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will be able to further constrain the amount of hard radiation emitted from these galaxies; as shown with mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5]. By tracking its evolution through the last half of the EoR, I can inform current reionization models. Lastly, through gathering my sample I will map out the distribution of these galaxies, identifying whether galaxies with large escape fractions are tracing over-dense luminous regions, located within large ionized bubbles [3,8]. Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space- based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of the fluxes of the CIII] doublet, when measurable, I can infer an estimate of the electron density of the gas in the CGM. This is closely linked to the metallicity of the CGM, which directly affects the velocity offset of Lyα emission. Not only do recent studies indicate that a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, they also suggest (from mid-z analogs) a strong link between the profile of Lyα emission and the properties of the gas within the CGM [5]. This can be incredibly important as some high-z galaxies have been found to have more symmetric Lyα profiles, contrary to the archetypal asymmetric shape, thought to be indicative of high star formation and galactic winds [6]. Understandingthe rateand distributionof reionization, including thefactors andprocesses responsible for it, remains one of the most important unknowns in extragalactic astronomy. My work will aim to shed more light on this question, enabling more precise modeling of thiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies. References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erb et al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004	0
Intellectual Merit How do you study something you cannot see? Dark matter (DM) is responsible for shaping the large-scale structure of the universe we see today, comprising 80% of all matter.Decadesof direct and indirect searches for annihilation radiation have notyieldedanysignals. InordertostudyDM astrophysically, we must use the luminous parts of the universe — the galaxies and galaxy clusters that reside inside DM halos — as tracers. The DM halo relationship is cleanest near the outskirts of these structures, where non-gravitational physics, such as AGN feedback, have the least effect. Whiledifficult to observe due to their low density, new and upcoming advancements in instrumentation are detecting observational tracers in halo outskirts for the veryfirsttime.SubsequentmeasurementsofDMhalomass and dynamical quantities, such as accretion rate, will constrain not only large-scalestructurebutalsothe nature of the DM particle itself. My projectexplorestheoutskirtsofbothindividualgalaxiesandgalaxyclusters:twostructuresat different cosmological scales but subject to fundamentally similar dynamics. Starting on the scale of galaxies, the brightest cluster galaxies(orBCGs)haveextendedfaintstellarhalosthatarenowthoughtto have physically significant edges [1]. Moreover, in recent optical images from the Hyper Suprime-Cam Subaru Strategic Program (HSC survey), it has been shown that the stellar mass in the outskirts (10-100kpcradii)oflow-redshiftBCGsisanexcellentproxyforDMhalomass[2].Theoutskirtsofthese massive galaxies are dominated by stars accretedduringmergerswithprevioussatellitegalaxiesandthus provide an estimate of “historical richness” for their DM halos. While promising for its potential of measuring DM halo mass, this result leads to more questions: Why is the 10-100kpc region significant? Could an even tighter relationship to DM mass exist with stellar profiles past 100kpc? On a larger scale, the outskirts of galaxy clusters (>1Mpc) also provide a laboratory for the effects of DM. At these distances, dark matter particles turn around at their apocenter, called the splashback radius, the location of which depends on the halo’s mass accretion rate(MAR)[3,4].Recent simulation work suggests this radius coincides with an analogous “stellar splashback” radius of the cluster’s stellar distribution (or intracluster light, ICL), which would also vary with MAR and reveal intriguing DMhalodynamics[1].Thesmallsampleanduseofzoom-insimulationsinthisstudywarrants afollowupinvestigationwithcosmologicalboxsimulationsandalargersample.HowobservablethisICL edge will be with future instruments is also unclear and requires further modeling. There are many other unexplored phenomena at theseclusteroutskirts,notonlyinDMbutinthe hydrodynamics of gas. In particular, an accretion shock must beproducedwhencoldgasfallsintoahalo and experiences a drastic jump in temperature. According to the self-similar collapse model [5, 6], the radius at which this shock appears should be almost identical to the cluster’s splashback radius [7], making it another potential observational tracer of DM. However, the model has not held up in simulations, in which the shock radius has been found to be 20-100% larger than the splashback radius [8]. This disagreement is a fundamental theoretical gap that needs to be understood to interpret current high-resolution observations of the Sunyaev-Zeldovich effect in clusters (from the South Pole Telescope and Atacama Cosmology Telescope) as well as near-future radio observations of accretion shocks. Cosmological simulations offer the opportunity to understand the connections between baryonic physics and underlying DM halos inordertobothinterpretobservationsandmakepredictionsouttohalo radii we cannot yet observe. While many processes in simulations are implemented via subgrid models, the processes in halo outskirts are mostly-first principle physics producible even with these limited modeling conditions. I will use cosmological simulations of dark matter and galaxies includingthelarge volumecosmologicalboxsimulationsuiteIllustris-TNG(TNG)[9].IproposetouseTNGtoinvestigate multiple baryonic physics phenomena at theoutskirtsofgalaxiesandofgalaxyclustersaspotential tracers of dark matter halo properties.My projectconsists of 3 scientific goals: 1. Develop robust techniques to measure BCG light profiles out to large radii to find optimal estimators of DM halo mass 2. Test the connection between ICL edges and splashback radius 3. Analyze the relationship between the accretion shock radius and splashback radius 1. Galaxy Stellar Outskirts andDMHaloMassTheHSCsurveyisamajorstepinobservers’abilityto optically image BCG outskirts out to 100kpc given its high-quality seeing conditions and a depth 3-4 magnitudes deeper than the SDSS. In collaboration with the HSCteam,Iwillusethesedatatodevelopa new technique for extrapolating stellar mass profiles of high-mass BCGs beyond their observable radius by testing them against a sample of simulated, mock-observed TNG galaxies of similar mass. A crucial consideration in simulation-observation comparisons is how to recreate the conditions used by observers. To achieve this science, we need to overcome technicalhurdles.Forexample,thefile ordering of the TNG output data is by friends-of-friends groups, suchthatoverlappinggroupscouldlead to one group missing particles near its outskirts. Forgoingthisdataorganizationtoinsureallparticlesare accounted for makes extracting complete profiles out to large radii in large simulations is a non-trivial task. I recently implemented a function in a simulation-extraction software, Hydrotools, that overcomes this challenge and allows users to extract all particles within a given radius. This function lays the foundation for any project relying on large-scale radial profiles. 2. ICL and Mass Accretion Rate Satellites fall into a halo with various velocities, orientations, and internal energies, so it is not immediately clear that disrupted stars in the ICL are faithful tracers of the DM halo potential. I will use Hydrotools to extract complete stellar and DM profilesfromhalosinTNG to investigate this connection. The splashback radius is signified by a caustic in the DM density profile whereparticlespileupaftertheirfirstorbit,sothe“stellarsplashbackradius”canbefoundanalogouslyin thestellardensityprofile.Iwillcalculatethedifferencebetweentheseradiiandtheirdifferentcorrelations with MAR for a sample of various mass halos, predicting the use of the ICL as a DM halo tracer. 3. Shock and Splashback Radii While TNG does not outputshocksurfacesspecifically,wecaninstead detect a sharp drop in gas entropy profiles to signify the accretion shock radius. I will analyze the relationship between the shock and splashback radii in massive halos over different stages of cluster merger events, using complete gas and DM profiles. These results will provide context to interpret the rapidly growing amount of Sunyaev–Zeldovich, and soon radio, accretion shock observations. Future Directions The outskirts of galaxies and of galaxy clusters are a clear next place to look for potential tracers of DM halos. Upcoming instruments and surveys will revolutionize our constraints on DM features such as the splashback radius. Moreover, the low surface brightness of BCG and ICL outskirtsarealreadydetectablewiththeHSCsurveyandwillbeevenmoresowiththeupcomingVera.C. Rubin Observatory and the Nancy Grace Roman Space Telescope. In the radio regime, the Square Kilometer Array (SKA) will vastly increase the number and resolution of observed accretion shocks. However, to interpret any of these observations we need to improve our theoretical understanding. My project will build off of current observational work and provide the foundation for interpreting future data. Broader Impacts I will continue my work to limit barriers for underrepresented minorities (URM) in astronomy. Particularly, I will use the remainder of my term serving on the AAS SMGA (sexual orientation and gender minorities in astronomy) committee to develop a mentorship program for LGBTQ+ early career astronomers. This program will match graduate students and postdocs with more experienced mentors who share a LGBTQ+ identity. Mentors will guide mentees through navigating the field, especially challenges specific to LBGTQ+ individuals in the workplace. I will also continue to co-lead the BANG! (Better Astronomy for the Next Generation) seminar series in my departmentwhich covers alternate career paths and EDI issues in Astronomy. I will focus on planning seminar sessions covering previously under addressed topics, such as accessible teaching strategies and equitable workplace practices. Finally, I will continue to work with the GRADMAP (Graduate resources for advancing diversity with Marylandastronomyandphysics)teamtoprovideexternalresearchexperiences for students at minority serving institutions by teaching workshops in career skills and serving as a summer research mentor. [1] Deasonetal(2020)[2]Huangetal(2021)[3]Diemer&Kravtsov(2014)[4]Adhikarietal(2014)[5] Bertshinger et al (1985) [6] Shi etal(2016a)[7]Shietal(2016b)[8]Aungetal(2021)[9]Pillepichetal (2018)	0
Motivation: Functional magnetic resonance imaging (fMRI) allows non-invasive measurement of real-time brain activity in humans. The success of this technology has been evidenced by its rapid growth in popularity in the 25 years of its existence, resulting in nearly 40,000 research papers1. A large portion of these studies investigates the correlational structure of the brain signal, known as functional connectivity (FC). FC studies are most often implemented in resting state fMRI (RS-fMRI). Historically RS-fMRI has been especially useful in clinical and developmental imaging because it requires no task demands, avoids performance confounds and measures network connectivity in largely the same way that task fMRI does2. These methods have led to many groundbreaking findings in brain science that were previously inaccessible. Recent evidence3 has shown that many of these findings may be spurious and insidiously riddled with artifactual patterns of connectivity created by head motion. This is most often evident in clinical and developmental populations because head motion is confounded with the group effect of interest. Even small movements, on the scale of .1 mm, have been shown to cause structured patterns of spurious variance, enhancing short-range connectivity and decreasing long- range connectivity3. These findings have caused many groups to entirely reevaluate previous FC findings3 and attempt to develop ways of overcoming this major problem. Current motion correction methods summarize head motion as a rigid body transform with 6 parameters (motion in the 3 spatial dimensions as well as rotations along each of these axes). The amount of motion at any single time point can be estimated by the change in each of these 6 motion parameters from the previous time point. Many methods have been developed to characterize and correct for motion-related signal. Common motion correction pipelines model the relationship between each direction of motion and fMRI signal changes linearly in confound regression. However, this linear assumption may be inadequate4. Higher order expansions of this model4 that allow for temporal offset and nonlinear relationships between motion and fMRI signals have been shown to perform better than standard methods in high motion populations, however, these models still fail to entirely remove motion related signal. To remove the remaining nuisance signal it is common practice to censor the problematic timepoints5. While this method works, it requires removal of valuable time points and often removal of entire subjects from an analysis. Regularly these removed subjects are patients whose data collection cost thousands of dollars and many person-hours. This leaves the field in a tenuous position in which previous findings require reevaluation and future studies must employ burdensome censoring techniques. If lasting, valid progress will be made with FC-fMRI a thorough understanding of motion and better motion correction techniques are required. The proposed research will apply established methodologies to a unique dataset, which will shine light on this important issue in fMRI. Aim 1: Characterize and model head motion artifacts in a single highly sampled subject. The proposed project will begin by thoroughly characterizing motion related signal changes in the MyConnectome dataset6. This publicly available dataset consists of 88 ten minute RS-fMRI scans of a single healthy adult male, resulting in over 45,000 whole brain images. Originally collected to establish the reliability of FC-fMRI methods, this dataset provides a unique opportunity to understand fMRI signals related to motion, an endeavor previously overlooked. Compared to a typical RS-fMRI dataset this sample has no variance related to individual differences or sex effects, and minimal variance related to age, brain size, or vasculature. From the wealth of time points in this dataset, I will construct smaller datasets out of time points that have motion primarily in a single direction and/or magnitude. Each will be the size of a typical RS-fMRI analysis. I will then apply previously described methods5 to characterize the influence of this highly controlled motion on fMRI signal change and FC. The unique flexibility of the MyConnectome dataset will allow me to further describe the nonlinearity and heterogeneity of motion related signal changes. To do this I will use established multiple regression methods 4, testing models of the linear and nonlinear effects of motion on the fMRI signal and time-course while penalizing for model complexity to avoid overfitting. I hypothesize that this approach will allow a precise description of the influence of directionality, magnitude and the time-course of head motion on fMRI signal and connectivity that will surpass current models in the amount of motion related variance explained. Aim 2: Determine generalizability of head-motion model. A major issue with this model may be that its utility is specific to a single subject’s brain and lacks generalizability to developmental or clinical populations with a larger amount of movement. I will use another unique and publicly available dataset, the Philadelphia Neurodevelopmental Cohort (PNC), to overcome this limitation. Since the PNC consists of a pediatric, demographically diverse, developing population, it is well suited to test the performance of the model defined in Aim 1 with a dataset most prone to the previously identified motion confounds of FC-fMRI. I hypothesize that the model identified will significantly outperform current state-of-the art processing methods, significantly reducing measurable motion related confounds5 and the reliance on censoring. Broader Impacts: This research has the potential to contribute crucial information to the growing discussion of motion in FC-fMRI. In depth investigation and rigorous control of motion in a single highly sampled subject has not previously been achieved and will demonstrate the upper limit of our ability to describe and correct motion artifact. With this information, generalizable gains in fMRI processing will follow, especially in mental illness and developmental research where human fMRI is especially important. Following the global push for openness in research and collaborative science, I will use data that is publically available and openly share all analytic programming code necessary to complete these analyses on GitHub so that the entire neuroimaging community may use and expand upon this work. This type of detailed fMRI artifact investigation is crucial for its validity and without it, progress may be dampened and slowed by confounds that are not adequately managed by current processing methods. Receiving support from NSF would allow me to develop these important motion correction methods during graduate school. Feasibility and Support: The proposed research is to be completed with Dr. Satterthwaite, a leader in the fMRI literature relating to motion artifacts and an investigator for the PNC4 .This will streamline access to the PNC data and its growing longitudinal child dataset. My experience with motion correction in a high motion population (see personal statement) and the tools I have previously developed to do this will immediately lend itself to this project. Access to NSF’s XSEDE computing resources, made possible by this fellowship, will be indispensable for parallelizing the computationally-intensive analysis of the large datasets in this proposal. References: (1) PubMed Search “fMRI OR functional MRI OR functional magnetic resonance imaging”. (2) Cole MW, Bassett DS, Power JD, Braver TS, Petersen SE (2014) Intrinsic and task-evoked network architectures of the human brain. Neuron 83: 238 –251. (3) Power, J.D., Barnes, K.A., Snyder, A.Z., Schlaggar, B.L., Petersen, S.E., 2012. Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. NeuroImage 59, 2142–2154. (4) Satterthwaite, T.D., Elliott, M.A., Gerraty, R.T., Ruparel, K., Loughead, J., Calkins, M.E., Eickhoff, S.B., Hakonarson, H., Gur, R.C., Gur, R.E., Wolf, D.H., 2013. An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data. NeuroImage 64, 240–256. (5) Power, J.D., Mitra, A., Laumann, T.O., Snyder, A.Z., Schlaggar, B.L., Petersen, S.E., 2014. Methods to detect, characterize, and remove motion artifact in resting state fMRI. NeuroImage 84, 320–341. (6) http://myconnectome.org/	0
Catalyzed by Amino Acids in Enzymes Background: Proton-coupled electron transfer (PCET) reactions refer to the sequential or concerted transfer of both a proton and an electron. PCET reactions are indispensable to life, occurring throughout cellular respiration and photosynthesis, and they even drive the photocycles of many light-sensing photoreceptor proteins. These proteins often accomplish this chemistry through the use of tyrosine and tryptophan residues, which are oxidized to their corresponding radical species that subsequently play a role in the reaction mechanism. PCET reactions involving tyrosine and tryptophan are of particular importance to the biological processes mentioned above. Motivation: Despite the emerging prevalence of tyrosine (Y) and tryptophan (W) mediated PCET reactions, much of the basic science remains unknown. The local environment of these residues varies from coordination to a metal ion, such as iron or manganese, to being buried in a hydrophobic core. How does the local protein environment stabilize and direct the reactions of these radicals without causing oxidative damage to the protein? Are there any common trends in the existing enzyme systems that utilize tyrosine or tryptophan residues for facilitating catalysis? These questions are difficult to study experimentally due to turnover times dictated by conformational changes, high redox potentials, and the reactive nature of the radical intermediates. To overcome these limitations and isolate a single, well-defined active site, a small (cid:68)-helical protein with a buried Y or W residue (denoted (cid:68) X, where X refers to the 3 catalytic amino acid), has been created. Without significant perturbation of the protein structure, the native Y/W residues can be reversibly oxidized, and the redox potential can be tuned by incorporating Y/W chemical derivatives.2 Proposed Study: I will use the well-characterized (cid:68)X systems of our experimental collaborators 3 as a base to build an extensive computational model, with the goal of understanding the role of the solvated protein environment and electronic effects on amino acid mediated PCET reactions. To address these issues, I will use two main approaches, which will provide the necessary information for the culminating study on PCET kinetics. In my first approach, I will investigate the effect of the protein environment by computing the redox potential of Y/W derivatives using density functional theory (DFT) in both implicit solvent and with electrostatic embedding to include the (cid:68) X protein environment. In my second approach, I will use molecular dynamics (MD) 3 to probe the role of water and protein conformational changes in this system. These two approaches will provide the information needed to compute the PCET rate constant.3 Aim 1: I will assess how the protein environment influences the redox potential by computing the redox potentials of a series of amino acid derivatives with implicit solvent and in the protein environment through electrostatic embedding. The partial charges representing the protein environment will be obtained from the MD simulations in Aim 2. The experimentally measured relative redox potentials from our collaborators on tyrosine and its Table 1. Mean unsigned errors (MUE) of the derivatives in the (cid:68) X scaffold will be used to 3 relative redox potentials with respect to p-cresol. benchmark our methods. The influence of the As shown, the mean unsigned error (MUE) is protein has been viewed as a constant shift below the accepted error threshold of 1 kcal/mol. compared to aqueous values, and finding theoretical Data includes a series of fluorinated tyrosine evidence to support or disprove this assumption, as derivatives. The calculations were performed at well as an explanation, would be valuable. the DFT B3LYP/6-31++G** level with the PCM solvation model.1 Preliminary data (Table 1) for implicit solvation studies reproduces the ordering and magnitude of the experimental redox potentials in the protein very well, suggesting that the protein is uniformly affecting each chemical species and cancels out when computing relative redox potentials. Aim 2: I will characterize the role of protein conformational motions and water accessibility in radical formation and decay. Water serves as the proton acceptor in many PCET systems, and access to the often-buried redox-active residues may be gated by conformational changes. MD studies will be used to build a hypothesis of the mechanism of water entrance into the hydrophobic interior of the (cid:68) X proteins. The water exchange dynamics and probability distribution will be 3 assessed from calculation of the radial distribution function and the average residence time of water in the hydrophobic pocket. In addition, the protein conformational changes associated with water entering and leaving the pocket will be analyzed. If water is found at a high occupancy, the representative conformations for water will be examined in Aim 1. Aim 3: I will predict the rate constants for these PCET reactions using a vibronically nonadiabatic PCET theory and will probe the electronic effects as the substituent on the Y or W is changed. The information calculated from the previous aims will be used as input to the analytical rate constant expression for vibronically nonadiabatic PCET. The underlying PCET theory is related to Marcus theory for electron transfer reactions,4 but the transferring proton is also treated quantum mechanically to include hydrogen tunneling.3 In the past, this theory has been applied to enzymes such as soybean lipoxygenase,5 illuminating the role of protein motions on the large kinetic isotope effect observed experimentally. Application to the (cid:68) X system will provide an unprecedented 3 opportunity to conduct a systematic study on a series of substituted Y and W residues in a controlled protein environment. These results will allow a much deeper understanding of biological PCET reactions. Intellectual Merit: The proposed work aims at advancing our fundamental understanding of biologically relevant PCET reactions through theoretical examination. The (cid:68) X protein system 3 will allow the study of the protein environment and electronic effects in tyrosine and tryptophan residues, which can serve as a model for understanding important radical chemistry in DNA synthesis, cellular respiration, and photosynthesis. The knowledge gained from a well-constructed computational model can motivate the synthesis and incorporation of unnatural tyrosine and tryptophan analogs into biological systems, thereby tuning their energy transfer capabilities. Additionally, the incorporation of unnatural amino acids into proteins for use as fluorophores or mechanistic probes is growing rapidly. With these advancements, an understanding of these probes and their chemical reactivity in the protein environment will be crucial to successful protein design. Broader Impacts: To aid experimental collaborators and non-specialists in understanding this work, I will develop a biological PCET module for our webPCET Java server.6 The webPCET server is freely accessible and provides an overview of PCET and related research by illustrating the theoretical underpinnings through interactive calculations. By construction of this module, I will be improving the accessibility of theoretical chemistry to biochemists and enzymologists, who may otherwise shy away from mechanistic and chemical studies. 1a) Becke, A. D. Phys. Rev. 1988, 38 (6), 3098-3100; 1b) Miertuš, S.; Scrocco, E.; Tomasi, J. Chem. Phys. 1981, 55 (1), 117-129; (c) Becke, A. D. J. Chem. Phys. 1993, 98 (7), 5648-5652. 2) Martínez-Rivera, M. C.; Berry, B. W.; Valentine, K. G.; Westerlund, K.; Hay, S.; Tommos, C., J. Am. Chem. Soc. 2011, 133 (44), 17786-17795. 3) Soudackov, A.; Hammes-Schiffer, S., J. Chem. Phys. 2000, 113 (6), 2385-2396. 4) Marcus, R. A. J. Chem. Phys. 1956, 24 (5), 966-978. 5) Li, P.; Soudackov, A. V.; Hammes-Schiffer, S. J. Am. Chem. Soc. 2018, 140 (8), 3068- 3076. 6) webPCET Application Server, Yale University, http://webpcet.chem.yale.edu (2009); Hammes-Schiffer, S.; Soudackov, A. V.	0
Stomata are small pores located on the epidermis of aerial plant tissues necessary for CO 2 assimilation and O release. Stomata are also the primary sites of transpiration, accounting for 2 nearly 90% of all water loss in rice. Plant species can modulate their stomatal density and conductance on newly emerging leaves in response to environmental stimuli such as CO , water 2 status, and temperature1–3. Despite many recent studies in dicots, limited attention has been paid to characterizing the genetic underpinnings of stomatal development and physiology in monocots, namely grasses. Grass stomata exhibit specialized anatomical and physiological attributes, such as subsidiary cells that flank the guard cells allowing for faster stomatal aperture rates4. Concerted efforts could provide insights into environmental adaptation mechanisms exclusive to monocots. The few studies that have characterized aspects of monocot stomatal biology have relied on mutant screens to identify key genes or have attempted to characterize homologs from the model plant organism Arabidopsis thaliana4–6. The use of quantitative genetic approaches as an alternative might reveal quantitative trait loci (QTLs) associated with this important trait. I will complete a genome-wide association study on a rice (Oryza sativa) diversity panel to characterize stomata-mediated drought response in rice. Currently available high density single nucleotide polymorphism (SNP) data allows for the resolution of discrete QTLs that are relevant in extant rice variation7. Further investigation of the most significant SNPs will enable the characterization of genetic variation involved in rice stomatal physiology and development in response to water deficit. Aim 1: Genome wide association study of stomatal traits in drought simulation A hydroponic platform incorporating polyethylene glycol 6000 (PEG)-induced drought stress will be used to yield a high-throughput and uniform assay of plant stomatal density and physiology in response to drought. The stomatal density differences will be measured in normal watering conditions and drought stress lines for each accession. A Li-6400 XT will be used to measure stomatal conductance of individual replicates. The optimized phenotyping platform will be applied to a rice diversity panel of 300 accessions. I will use principal component analysis to maximize coverage of total genetic diversity in the selected lines. Stomatal density and conductance differences between the two treatments will be used as the phenotypic parameters in the GWAS alongside a high-density rice array containing nearly 700k SNPs. Association mapping will be conducted using a custom python script that can account for subpopulation structure as a potential covariate. All loci above the significant p-value threshold will be further analyzed to identify likely candidate genes associated 1 with stomatal density and conductance modulations responsive to drought. I will then use haplotype analysis to determine the haplotypes and frequencies at the most significant QTL. Aim 2: Characterize candidate genes using CRISPR/Cas9 mediated knock-outs and ectopic overexpression The candidate genes most closely associated with the highest significance SNPs will be further characterized using CRISPR/Cas9 to induce mutations 8.Targeted mutagenesis will be executed in the Kitaake genetic background, in which I have already successfully produced knockouts. The short generation time of this accession makes it ideal for high-throughput research. Additionally, I will ectopically overexpress candidate genes with a strong promoter in the Kitaake background. The stomatal density and conductance of mutant overexpression lines will be measured to determine if these candidate genes play a role in stomatal development and physiology. Aim 3: Multiplexing knock-ins of drought adaptation alleles CRISPR/Cas9 and geminiviruses will be used to produce knock-ins of advantageous alleles in the homologous native location in the Kitaake background. Alleles selected will belong to the haplotype associated with the highest significance SNPs from the most drought tolerant accessions. Useful variation may exist in promoters, genes, or non-coding sequences. This variation will be leveraged using the high replication rates of geminivirus replicons to increase rates of homology-directed repair, with precise positioning enabled by CRISPR/Cas9 mediated double-stranded breaks9. Quantitative traits are governed by numerous QTL that contribute collectively to a phenotype10. Simultaneous knock-ins of alleles from drought adapted accessions into the Kitaake background will be confirmed and assayed for performance in drought. This approach highlights an avenue to leverage natural variation using targeted genome editing for allele swapping. Intellectual Merits: Grass stomatal adaptations are currently understudied. Rice can serve as a model for other monocot species in investigating novel environmental adaptations that have evolved relative to dicots. Access to high density marker sets coupled with transformation facilities can enable biological investigations that go beyond the scope of model plant organisms. Results may be translated to species such as hexaploid wheat, where GWAS and transformation is more challenging, to explore the conservation of adaptive mechanisms among grasses. Furthermore, multiplexing adaptive allele knock-ins could bypass the high time investment and linkage drag inherent to traditional breeding approaches, and be broadly applied to a range of traits for which there is existing GWAS data11. Broader Impacts: Improved understanding of drought tolerance mechanisms in monocots can enable eventual crop improvements. These advancements will be necessary to improve plant performance in the face of impending global climate changes. International collaborators will assist with field testing of the most promising edited lines, integrating a broad community of plant scientists. I will leverage my connections in NPR Scicommers and local science museums to share findings of this study with the public and discuss data at conferences, thereby engaging with all individuals about plant-environmental interactions in the context of climate change. References: [1]Hamanishi, E. T., Thomas, B. R. & Campbell, M. M. J. Exp. Bot. (2012). [2] Gray, J. E. et al. Nature (2000). [3].Zhu, J. et al. Forests (2018). [4].Raissig, M. T. et al. Science (2017). [5] Raissig, M. T. et al. Proc. Natl. Acad. Sci. (2016). [6] Hughes, J. et al. Plant Physiol. (2017). [7] McCouch, S. R. et al. Nat. Commun.(2016). [8]. Doudna, J.A. & Charpentier, E. Science (2014). 9.Wang, M. et al. Molecular Plant (2017). [10] Crowell, S. et al. Nat. Commun. (2016). [11]. Jacobsen, E. & Schouten, H.J. Trends Biotechnol. (2007). 2	0
"LeeAnnM.Sager MazziottiGroup,DepartmentofChemistry,TheUniversityofChicago Goal: Iaimtodevelopamethodologytoexplorethedegreeofexcitoncondensationonaquan- tumcomputer,determinethepreparation(s)thatobtainmaximumexcitoncondensationforagiven numberofqubits,andprobethepropertiesofsaidexcitoncondensatestates. Introduction: Condensation phenomena has been an active area of research since 1924 when EinsteinandBosefirstintroducedtheirideal“Bose-Einstein”gas.1Theidenticalparticlescompris- ingthisgas(bosons)wereproposedtobeabletoaggregateintoasinglequantumgroundstatewhen sufficiently cooled.1 Later, London and Tisza attributed Bose-Einstein condensation (BEC) to be thesourceofsuperfluidity—thefrictionlessflowofzero-viscosityfluids—thathadbeenobserved in low-temperature liquid helium.2 In 1940, Pauli established the relationship between spin and statistics, demonstrating that particles with integral spin values obey Bose-Einstein statistics—are bosons—and hence may form a condensate.2 Extrapolating further, pairs of fermions—particles with half-integer spins—may interact such that the overall two-particle system has integral spin and is hence bosonic. In fact, recent experimental and theoretical investigation has particularly centeredaroundthecondensationsofonesuchclassofbosons: excitoncondensates.3 Excitoncon- densation is defined by the condensation of particle-hole pairs (excitons) into a single quantum state to create a superfluid. The superfluidity of electron-hole pairs involves the non-dissipative transferofenergy,whichhasapplicationsinenergytransportandelectronics.3 Motivation: While excitons form spontaneously in semiconductors and insulators and while the binding energy of the excitons can greatly exceed their thermal energy at room temperature, theyrecombinetooquicklytoallowforformationofacondensateinasimplemanner. Tocombat recombination,thecouplingofexcitonstopolaritons,whichrequiresthecontinuousinputoflight,4 andthephysicalseparationofelectronsandholesintobilayers,whichinvolvesimpracticallyhigh magneticfieldsand/orlowtemperatures,5areemployed. Thus,anew,more-practicalavenueforthe creationandstudyofexcitoncondensationisdesired;quantumcomputingofferssuchanavenue. A qubit is the basic unit of quantum computing (analogous to the classical bit); the qubit itself is a quantum system whose most-general state is a linear combination of its two basis states ( 0 | ⟩ and 1 ,theclassicalbitstates)withanappropriatephasefactor(eiφ)givenby6 | ⟩ θ θ cos θ Ψ = cos 0 +eiφsin 1 = 2 . | ⟩ 2 | ⟩ 2 | ⟩ eiφsin θ ! "" ! "" ! # $2 "" Ifaqubitisconsideredtobeaone-fermion,two-levelsysteminwhichth#er$eisaprobabilitypofthe fermionbeinginthelower-levelstate( 0 )andaprobability1 pofthefermionbeingintheupper- | ⟩ − level state ( 1 ) where p = cos θ 2, then a single qubit can represent two particle-hole paired | ⟩ | 2 | orbitals. Assuch,asystemofN qubitscanbeviewedasN fermionsinanN-folddegenerate,two- # $ level, particle-hole paired system. As explored in Ref. 7, such a model can demonstrate exciton condensationinsystemswithasfewas3fermionsin6orbitals(i.e.,athreequbitsystem). Resources: InthisworktheIBMQuantumExperiencedevices—whichareavailableonline— willbeused. Qiskitopen-sourcequantumcomputingsoftwarewillbeemployedforanalysis. Aim-1, Probe the Extent of Exciton Condensation of an Arbitrary Qubit State: In or- der to computationally probe the presence and extent of condensation behavior, I aim to mea- sure the largest eigenvalue of the 2G˜ matrix—a calculable, characteristic property of exciton condensation—on the quantum computer.8 The elements of the 2G˜ matrix are given by 2G˜i,j = k˜,˜l LeeAnnM.Sager Page1 ⟨Ψ |ψˆ i†ψˆ jψˆ ˜l†ψˆ k˜ |Ψ ⟩ − ⟨Ψ |ψˆ i†ψˆ j |Ψ ⟩⟨Ψ |ψˆ k˜†ψˆ ˜l|Ψ ⟩ where |Ψ ⟩ is the N-fermion wavefunction; i,j,˜ l,k˜ representspinorbitals;andψˆ andψˆarethecreationandannihilationoperators. Duetotheparticle- † hole pairing of each qubit, the spin orbitals denoted by i and j and the spin orbitals denoted by k˜ and˜ l must correspond to the same qubit to be non-zero, simplifying the matrix. In order to obtain the 2G˜ matrix on a quantum computer, these elements must be translated into the basis of Pauli matrices. The expectation values of the Pauli matrices can be obtained through direct measure- mentfromaquantumcomputer,andthematrixelementscanthenbecalculatedthroughuseofthe appropriateconversion. Thelargesteigenvaluecanbecomputedfromthematrixobtained. Aim-2, Determine Preparation(s) for State(s) with Maximum Exciton Condensation: A quantum state of qubits can be prepared on a quantum computer by the application of a unitary transformation,Uˆ ,suchthat Ψ (1,2,...,N) = Uˆ Ψ (1,2,...,N) describesthepreparationof i i i 0 | ⟩ | ⟩ anN-qubitstatefromtheinitialstate Ψ = 00 0 . Iaimtodeterminetheappropriateunitary 0 | ⟩ | ··· ⟩ transformation for a given number of qubits that corresponds with the maximum condensation of excitons—the largest eigenvalue. One particular N-qubit state that may be of interest on this searchistheGHZstate—thestateinwhichallqubitsareinthe 0 stateorthe 1 statewithequal | ⟩ | ⟩ probability (i.e., Ψ = 1 0 N + 1 N );6 this state is highly entangled and is hence an | GHZ ⟩ √2 | ⟩⊗ | ⟩⊗ idealcandidateforexcitoncondensation. # $ Aim-3, Probe Properties of Exciton Condensates: Any physical, measurable property of a system corresponds to a Hermitian matrix (Aˆ) such that the eigenvalues of Aˆare the possible out- comes of measurement of said property. The elements of these Hermitian matrices can be written intermsoftheexpectationvaluesofPaulimatricesandcanthereforebeobtainedforagivenqubit preparation. Fromthismatrix,theprobabilityofagivenmeasurement( Ψ a )andtheexpectation i n ⟨ | ⟩ valueofthatproperty( Ψ Aˆ Ψ )canbeobtainedwhere a istheeigenstatecorrespondingtoa i i n ⟨ | | ⟩ | ⟩ givenmeasurementa and Ψ isanN-qubitstatepreparedbytheunitarymatrixUˆ . Forexample, n i i | ⟩ the energetics of a prepared qubit state can be probed by obtaining the eigenvalues/eigenstates of thetwo-fermionreducedHamiltonianmatrixgivenby2K = 1 1 2 Zj + 1 1 . N −1 −2∇1 − j r1j 2r12 Intellectual Merit: This project aims to expand our understa%nding of exciton’condensation & phenomena. Shouldtheseapproachesprovesuccessful,areliableandfacilepreparationforexciton condensatestateswillbeachieved,andpropertiesofsuchcondensateswillbeabletobeprobedin astraightforwardmanner. Broader Impacts: The superfluidity of excitons in a condensate allows for the frictionless transportoftheexcitationenergy,releaseduponrecombinationoftheparticleandhole. Addition- ally, such superfluidity in a bilayer—with electrons in one layer and holes in another—allows for the frictionless transfer of charge as long as current is directed in opposite directions in the two layers—a phenomenon known as counterflow superconductivity.3 Understanding and exploiting the superfluid properties of exciton condensates may hence be instrumental in the effort to design wiresandelectronicdeviceswithminimallossofenergy,decreasingoverallenergyconsumption. 1 Einstein,A.KöniglichePreußischeAkademiederWissenschaften1924,261––267. 2 Vilchynskyy,S.I.;Yakimenko,A.I.;Isaieva,K.O.;Chumachenko,A.V.LowTemp.Phys.2013,39,724–740. 3 Fil,D.V.;Shevchenko,S.I.LowTemp.Phys.2018,44,867–909. 4 Fuhrer,M.S.;Hamilton,A.R.Physics2016,9. 5 Kellogg,M.;Eisenstein,J.P.;Pfeiffer,L.N.;West,K.W.Phys.Rev.Lett.2004,93,036801. 6 Kaye,P.;Laflamme,R.;Mosca,M.Anintroductiontoquantumcomputing;OxfordUniversityPress,2010. 7 Lipkin,H.J.;Meshkov,N.;Glick,A.J.Nucl.Phys.A1965,62,188–198. 8 Garrod,C.;Rosina,M.J.Math.Phys.1969,10,1855–1861. LeeAnnM.Sager Page2"	0
S.Moore Background Let L be a finite dimensional semisimple Lie algebra. A subset H ⊂ L is said to be a Car- tan subalgebra if H is a maximal toral subalgebra (a subalgebra in which all elements are ad- diagonalizable). In particular, H will be abelian, implying that every h ∈ H is simultaneously ad-diagonalizable. We call α ∈ H∗ a root of L if α (cid:54)= 0 and there exists nonzero v ∈ L such that [h,v] = α(h)v ∀ h ∈ H. The set of roots, R , is finite. Let S = {α ,α ,...,α } ⊂ R be a φ φ 1 2 n φ basisofH∗ suchthatanyα ∈ R canbewrittenasα = (cid:80)n c α withallc eithernonpositiveor φ i=1 i i i nonnegative integers. We call elements of S simple roots. If α ∈ R has all c nonnegative, then φ φ i αissaidtobeapositiveroot. WedenotethesetofpositiverootsbyP . ThesesetsS ⊂ P ⊂ R φ φ φ φ (together with some more data) are called the root system φ of L. For example, the root system of sl (C)(denotedA )hassimpleroots{α ,α }andpositiveroots{α ,α ,α +α }. 3 2 1 2 1 2 1 2 A(Bridgeland)stabilityfunctiononarootsystemisamap Z : P → H = {z = x+iy ∈ C | y > 0} φ satisfying Z(α + β) = Z(α) + Z(β) for all α,β ∈ P . As such, Z is uniquely determined by φ Z | . We also typically require that Z be generic, meaning that Z(α) (cid:54)= cZ(β) for any c ∈ R S φ wheneverα (cid:54)= β ∈ P . SeeFig. 1forexamplesofstabilityfunctionsonA . φ 2 For α ∈ P , the phase of α under Z α +α φ 1 2 is the angle from the positive real axis to α +α 1 2 α Z(α). For generic Z, this induces a combi- 1 α 2 natorial ordering of the elements of P via φ α α 2 1 decreasing phase. Two stability functions Z,Y : P → H are said to be combinatori- φ allydifferent ifZ andY inducedifferentor- Z Z 1 2 derings of P . In particular, Z and Z (see φ 1 2 Fig. 1) are combinatorially different stabil- Figure1: TwostabilityfunctionsonA . 2 ityfunctionsonA . 2 Our goal is to understand simple wall crossings of stability functions, which are defined as follows: Let Z and Y be stability functions of a root system φ. If the induced combinatorial orderingsofP underZ andY arethesameexceptthataconsecutivetripleτ,τ +ω,ω underZ is φ rearrangedtoω,τ +ω,τ underY,thenY issaidtobeobtainedfromZ byasimplewallcrossing. For example, Z is obtained from Z by a simple wall crossing. Intuitively, a simple wall crossing 2 1 arisesfromtakingapaththatconnectsZ andY inthespaceofstabilityfunctions. Suchapathwill necessarily cross through a non-generic stability function in which τ,ω, and τ +ω have the same phase. Notethatthespaceofstabilityfunctionsissimplyconnected,soanytwostabilityfunctions maybeobtainedfromoneanotherviaafinitenumberofwallcrossings. The aim of this project can be broken into two main goals: First, given a root system φ, we aimtocombinatoriallydescribethegraphofcellsofstabilityfunctionsonφseparatedbysimple wallcrossings. Second,wewishtodeterminetherelatedidentitiesamongmotiviccharacteristic classes of geometrically relevant spaces (see below). To accomplish these aims, we will use a 1 combination of methods from combinatorics and rational function identities for neighboring cells (see[RR]). IntellectualMerit Such wall crossings have impacts beyond the study of Lie algebras. In particular, to a root system φ, we may associate a Cohomological Hall Algebra C (defined by [KS]). This will be an φ infinite-dimensional, non-commutative, associative algebra whose product is denoted by ∗. Such algebras are motivated by string theory. To each α ∈ P , [RR] assigns a motivic characteristic φ class c0 ∈ C . The element c0 is a class in an equivariant cohomology (K-theory) algebra of a α φ α geometrically relevant space (such as the Grassmannian, or flag manifold). Such c0 have various α interestinginterpretations,suchasmotivicChernclasses,Chern-Schwartz-MacPhersonclasses,or stableenvelopesinOkounkov’snewtheoryrelatinggeometrytophysics(see[MO]). Supposethatwehaveasimplewallcrossingwhichpermutesτ,ω+τ,andω. By[RR],thisgives risetoanidentityc0 = [c0,c0]intheCohomologicalHallAlgebra. Notethatthisresultissimilar ω+τ τ ω in nature to wall-crossing formulas (also known as quantum dilogarithm identities) in Donaldson- Thomas theory [KS]. Such an identity gives a convenient way to calculate c0 for α ∈ P \ S , α φ φ as the commutator [c0,c0] is well understood for τ,ω ∈ S . For example, in A we can calculate τ ω φ 2 (cid:16) (cid:17) (cid:16) (cid:17) c0 = [c0 ,c0 ] = c0 ∗ c0 − c0 ∗ c0 = 1 + yb − 1 − b = (1 + y)b. That is, the α2+α1 α1 α2 α1 α2 α2 α1 a a a (cid:16) (cid:17) class c0 is obtained as the difference of the K-theoretic total Chern class 1 + yb and the α2+α1 a (cid:16) (cid:17) K-theoreticEulerclass 1− b . a DisseminationofResults I will present the results of this project in a variety of settings. Locally, I plan to present at the Triangle’s annual Assocation for Women in Mathematics conference (established last year) and at UNC’s Graduate Student Seminar. On a larger level, I plan to return to a national conference to presentaswell. Furthermore,resultswillbepublishedinarelevantmathematicaljournal. BroaderImpacts After graduating, I plan to become a professor. As described in my personal statement, a large focus of my career will be in mentoring undergraduates in research projects. I have previously mentored a high school student in a research project where she explored various non-Euclidean geometries. If awarded the NSF GRFP, I will continue building my mentoring capabilities by creatingaprojectforUNC’sDirectedReadingProgram. Thisprogramallowsgraduatestudentsto mentorundergraduatesthroughasemester-longreadingproject. MyprojectwouldbebasedinLie algebra,culminatinginanunderstandingoftherootsystemsassociatedtoeachsl (C). Iwillalso n becomeinvolvedintheMcNairScholarsProgramatUNCbyhelpingwiththeirresearchprogram overasummer. Thiswillallowmetheopportunitytohelpminorityundergraduateresearchersina variety of fields (not just mathematics) by providing them with critical feedback at various stages intheresearchprocess. References [RR] R. Rimanyi. Motivic characteristic classes in cohomological Hall algebras (Preprint). Available at http:// rimanyi.web.unc.edu/research1/,2018. [KS] M.KontsevichandY.Soibelman.Stabilitystructures,motivicDonaldson-Thomasinvariantsandclustertrans- formations.Availableathttps://arxiv.org/abs/0811.2435,2008. [MO] D.MaulikandA.Okounkov.QuantumGroupsandQuantumCohomology.Availableathttps://arxiv. org/abs/1211.1287,2018. 2	0
Background: Cancer is among the leading causes of deaths worldwide with approximately 38% of men and women diagnosed with cancer at some point in their lifetime. With the rising cancer 1 epidemic and the need for cheaper and more accessible drugs for people in developing countries, it is crucial to find new ways to develop pharmaceuticals. One sustainable method is engineering metabolic pathways of microorganisms such as yeast (Saccharomyces cerevisiae) or Escherichia coli to produce the precursor of a drug. One of most successful applications of this technique was achieved in Dr. Jay Keasling’s lab at the Joint BioEnergy Institute by producing the precursor of the antimalarial drug artemisinin, reducing the cost 30-60% and ensuring a continuous supply. 2,3 Engineering microorganisms to produce pharmaceutical products more efficiently can be applied to cancer drug development. Production of terpenoid and polyketide by engineered microorganisms would be particularly beneficial as small amounts of the molecules are produced via natural pathways and yields vary widely based on environmental and epigenetic factors. 4 Paclitaxel (brand name Taxol) is one of the most successful cancer drugs, and was first listed on the World Health Organization Model List of Essential Medicines in 2013. However, there was concern regarding the high cost of the drug , as it initially required the bark of six 100-year-old 5 Pacific yew trees to treat a single patient with breast cancer. Although alternate methods have 6 been developed to extract paclitaxel from needles of the European yew, synthetic biology tools can be used as a more sustainable alternative. Since paclitaxel is a highly decorated diterpenoid (Fig 1), its complicated structure makes it a good fit to be engineered from yeast due to the highly versatile DNA transformation system and well-defined genetic system of yeast. The objective of this project is to engineer a yeast strain capable of synthesizing paclitaxel which can be later optimized for commercial production. This will have an essential impact by reducing the cost of a crucial anticancer drug and providing valuable insight into the pathways required for the production of terpenoid and polyketide natural products from yeast. This project will focus on engineering taxadiene biosynthesis in yeast by utilizing glucose as the hydrocarbon source, studying and identifying cytochrome P450 oxygenation reactions in the pathway, and integrating these components to produce paclitaxel (Fig 1). Aim 1: Engineering of Taxadiene Biosynthesis in Yeast Taxadiene (taxa-4(5),11(12)-diene) biosynthesis in yeast is crucial to the production of paclitaxel but the levels of various precursors these organisms produce naturally are insufficient to make the process feasible. The diterpenoid precursor for taxadiene, geranylgeranyl diphosphate (GGDP), is necessary for a heterologous taxoid biosynthetic pathway but is produced in insufficient quantities in yeast due to competition for steroid synthesis with farnesyl diphosphate. I will introduce heterologous genes from the Sulfolobus acidocaldarius GGDP synthase instead of the native GGDP synthase from Taxus for improved production of GGDP A T axa-4(5),11(12)-d ien e (T P 450 O xygen ation R eaction s P aclitaxel axad ien e) B A im 1E n g in eerin g o f T ax ad ien eB io sy n th esis in Y east A im 2S tu d y C y to ch ro m e P 4 5 0 -D ep en d en t O x y g en atio n R eactio n s in P ath w ay A im 3In teg ratio n o f T ax ad ien eP ath w ay an d O x y g en atio n R eactio n s to P ro d u ce P aclitax el Figure 1. A: Taxadiene intermediate structure catalyzed via various cytochrome P450 oxygenation reactions to produce paclitaxel.7 B: Identified route for production of paclitaxel. and taxadiene as there is no competition for steroid synthesis. Introduction of genes from S. 4 acidocaldarius also results in substantial production of geranylgeraniol, further increasing taxadiene yields. Yeast will be transformed by the lithium acetate method on SC minimal 8 medium agar plates via CRISPR/Cas9 and select plasmids (pVV200 (tryptophane), pVV214 (uracil), pRS313 (histidine) and pRS315 (leucine)). Yeast will be cultivated for 48 hours with glucose as the carbon source, lyophilized, extracted with pentane, and analyzed by GC-MS. Aim 2: Study Cytochrome P450-Dependent Oxygenation Reactions in Pathway The oxygenation steps in the biosynthesis of paclitaxel have yet to be fully studied and identified. After taxadiene biosynthesis, oxidative modification of the olefin and the elaboration of side chains are needed to transform taxadiene into paclitaxel. After the taxadiene skeleton is formed, the olefin must be modified by several P450-mediated oxygenations and coenzyme A dependent acylations. Candidate genes for all but one of the seven steps after taxadiene synthesis are postulated, but the entire pathway has yet to be confirmed. Uracil-specific-excision-reagent 9 (USER) cloning will be utilized for site-directed mutagenesis of the identified genes and cytochrome P450s and USER primers will be designed using the online AMUSER tool. All intermediates will be characterized by GC/LC-MS. Aim 3: Integration of Taxadiene Pathway and Oxygenation Reactions to Produce Paclitaxel Once the taxadiene pathway and oxygenation reactions are identified and characterized, the pathways will be integrated via the lithium acetate method, CRISPR/Cas9, and site-directed mutagenesis to produce paclitaxel. Glucose will be used as the starter carbon source in yeast and will follow the native mevalonate pathway. The established genes to produce GGDP synthase will be introduced and the previously identified taxadiene synthase gene that has been codon optimized for improved yeast expression will be incorporated to produce taxadiene. Then, the 4 pathway developed with the cytochrome P450 oxygenation reactions will be introduced to produce paclitaxel. All intermediates will be evaluated using GC-MS. It is possible that some proteins synthesized in the complete pathway are insoluble or inactive in yeast, thus similar proteins will be determined or engineered to be active. Intellectual Merit: My knowledge from chemical engineering coursework and research with developing cell cultures, DNA analysis, and molecular modification of chemical structures gives me an essential, well-rounded training for fulfilling this project. This project will be the first time a polyketide synthase (PKS)-terpene hybrid has been produced in yeast and will mark an imperative step in the industrial production of PKSs and thus, in the field of synthetic biology. I will collaborate with members of the Joint BioEnergy Institute to learn the genetic technique of integrating genes using CRISPR/Cas9 and use my knowledge of DNA sequencing and GC to confirm the genes responsible for paclitaxel and determine the developed molecules at each step. Broader Impacts: Developing pharmaceuticals from microorganisms is an efficient and cost- effective way to produce the same high-quality compounds obtained from natural products. By engineering yeast to produce paclitaxel, a lower-cost, more sustainable drug could be developed for people suffering from lung, breast, or ovarian cancers who would otherwise not have access to the medicine. If successful, this project will provide a framework for synthesizing other PKS- terpene hybrid chemicals and pharmaceuticals from microorganisms. References: 1. NIH, NCI. 2018. https://www.cancer.gov/about-cancer/understanding/statistics 2. Hale et al., Am Soc Trop Med. 2007. DOI: 10.4269/ajtmh.2007.77.198. 3. Ro et al., Nature. 2006. DOI: 10.1038/nature04640. 4. Engels et al., Met Eng. 2008. DOI: 10.1016/j.ymben.2008.03.001. 5. Mendis. WHO Model List of Essential Medicines. 2011. https://www.who.int/selection_medicines/committees/expert/18/applications/Mendis.pdf?ua=1 6. Horwitz, SB. Nature. 1994. DOI: 10.1038/367593a0 7. Chang et al., Nature. 2006. DOI: 10.1038/nchembio836. 8. Kaiser et al., Cold Spring Harbor Laboratory Press. 1994. ISBN: 0-87969-451-9. 9. Jennewein et al., PNAS. 2004. DOI: 10.1073/pnas.0403009101.	0
One of the most important unknowns in high-z extragalactic astronomy is how reionization occurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominates the intergalactic medium (IGM). HI attenuates radiation from early stellar populations, masking galaxies from detection. Understanding how and when reionization occurs can reveal whether or not these young galaxies provided the necessary ionizing radiation to completely reionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the high redshift-space this implies, spectroscopic observations are limited as these galaxies are very faint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths. My background in NIR spectroscopy and observational astronomy has prepared me to assist in addressing this question. I propose using Lyα and CIII] to investigate the properties and ionization state of young galaxies using ground- and space-based telescopes, the structure and distribution of HI in the IGM and the circumgalac- tic medium (CGM) of certain galaxies, and implications for the evolution of the neutral fraction of the IGM throughout the EoR. The individual points proposed will be summarized as follows: (i) small scale testing and building of an analysis technique, (ii) distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. By understanding more about the IGM during the reionization era and of the galaxies within it, we can further constrain the properties of current galaxy evolution and reionization models. Small scale testing & building of analysis technique: In the search for galaxies during the EoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric 0 surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UV bright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring the escape fraction of Lyα many studies have inferred an increasingly neutral fraction of the IGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emission hundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent method uses a complementary spectroscopic tracer not attenuated by HI, with the UV metal line CIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5]. 0 In my current work, I measure CIII] H-band emission of galaxies found via Lyα emission using Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) and attenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGM of these galaxies and surrounding IGM. From my previous work experience, I have developed a proficiency in coding which enabled me to gain a close familiarity with the MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fix bugs often encountered when working with incredibly faint emission lines and less common dithering patterns for standard star observations. I wrote code to optimally extract my 1D spectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage of IRAF and Python, I developed code that can track the photometric variability of my data from a frame-to-frame basis – important when working with faint emission lines. Distribution of galaxies and evolution of neutral fraction: Using the foundation built from my previous work, I will build a statistical sample of galaxies during the last half of the reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escape fraction as a function of redshift. Using my optimized extraction technique to improve Taylor A. Hutchison 2 measurements, I will use this dataset to constrain the offset between these galaxies’ systemic and attenuated redshifts. This work will significantly increase the sample of high-z galaxies with both Lyα and CIII] measurements. In addition, it will provide a more significant comparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses. A current complication for this project is the lack of a complete spectroscopic sample of LAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines or the [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to 0 NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes – it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in the range of NIRCam on JWST. As a first approach to resolving this, I will take the current sample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using my optimized extraction technique to improve measurements. This has already been attempted for some galaxies [4], providing useful lower limits for determining exposure times and potential telescopes for future observations, including JWST. We are planning proposals for the first JWST cycles for this work. I will then take advantage of the deep multi-wavelength imaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I am part of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with early indications that the proposal has so far been successful. Finally, as a scientific collaborator on an ERS JWST proposal, I will prepare for access to that data – understanding what spectra I will be looking for from running binary stellar population models (eg. BPASS), scaled to match expected bandpass magnitudes, through the JWST exposure time calculator. With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will be able to further constrain the amount of hard radiation emitted from these galaxies; as shown with mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5]. By tracking its evolution through the last half of the EoR, I can inform current reionization models. Lastly, through gathering my sample I will map out the distribution of these galaxies, identifying whether galaxies with large escape fractions are tracing over-dense luminous regions, located within large ionized bubbles [3,8]. Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space- based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of the fluxes of the CIII] doublet, when measurable, I can infer an estimate of the electron density of the gas in the CGM. This is closely linked to the metallicity of the CGM, which directly affects the velocity offset of Lyα emission. Not only do recent studies indicate that a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, they also suggest (from mid-z analogs) a strong link between the profile of Lyα emission and the properties of the gas within the CGM [5]. This can be incredibly important as some high-z galaxies have been found to have more symmetric Lyα profiles, contrary to the archetypal asymmetric shape, thought to be indicative of high star formation and galactic winds [6]. Understandingthe rateand distributionof reionization, including thefactors andprocesses responsible for it, remains one of the most important unknowns in extragalactic astronomy. My work will aim to shed more light on this question, enabling more precise modeling of thiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies. References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erb et al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004	0
Understanding the Role of the Cytoskeleton on Intracellular Particle Dynamics Introduction – Intellectual Merit: The dynamics of the cytoplasm, which includes the cell chromosome and other intracellular particles is relevant to many biological processes, including cell replication and genomic control. However, these systems exist out of equilibrium and show complex dynamic behavior, not easily explained by classic dynamic theory. Recently, new fluorescent labeling techniques have allowed intracellular particles to be tracked as they move though the cell1,2. These experiments, done on E. coli, have shown that that these particles show sub-diffusive behavior. It seems that the cell environment confines the movement of these particles, and blocks them from fully exploring their surroundings. Various theories have been proposed to explain this behavior.3,4. These depend mostly on studying polymer models for the chromosomes and treating the cytoplasm as a viscoelastic medium with a simple memory kernel. This continuum approach has shown some success in reproducing the sub-diffusive behavior of particles. However, there is no need to introduce an artificial memory kernel, the cytoplasm can be modeled explicitly using colloidal models. Then, the particles are treated as colloids moving in a Newtonian medium (water). Then the constraints that induce the caging on these particles and lead to sub-diffusive behavior can be inserted explicitly. There are two factors that could lead to this diffusive behavior. The first is the fact that the particles are confined in the cell. This confinement could lead to a reduced ability to explore certain parts of the cell. Additionally, the cell cytoskeleton would induce an additional major hindrance to their movement. The existence of the bacterial cytoskeleton has only recently been recognized5. However, by now it is well understood that prokaryotes have analogues to most components of the eukaryotic cytoskeleton. These help give the cell it’s structure, and connect different parts of the cell with each other. However, they are also major sources of hindrance in the movement of the cell particles. It is appropriate to use a colloidal model of the cytoplasm, since many large biomolecules easily fall in the colloidal size regime. The hindered diffusive behavior is analogous to that seen in classic colloidal glassing6, and could be explored using the same fundamental theory. Goal: We wish to develop a simple colloidal model to study particle dynamics within a cell, including the role of confinement and the cytoskeleton. This model will demonstrate that this confinement, along with the additional interference of the cell cytoskeleton leads to the sub- diffusive behavior shown in experiments. To achieve this, we will first use existing polymer models to properly study the behavior of the bacterial cytoskeleton. Then Brownian Dynamics studies of a simple cell model would be realized, and the confined movement of the particles in the model cell would be studied. Finally, the role of hydrodynamics would be explored, using newly developed theory and computational methods. Objective 1: Match Cytoskeleton Dynamics to Polymer Models: The first step is to develop a good model for the cytoskeleton, which is a complex network of polymer chains that connect different parts of the cell. A convenient way to model the behavior of these polymers is to use the shearable stretchable Worm Like Chain (ssWLC) model developed by the Spakowitz Group to study general semi-flexible polymers7. This general model allows one to study a wide range of polymers at many timescales. Using their methodology7 to match the known chemical structure of the components of the bacterial cytoplasm5, we can obtain a simple polymer model applicable for Brownian Dynamics, which we will use in the following simulations. Objective 2: Study Particle Diffusion in Cellular Environment: The cell can be modeled as a sphere, and the cytoplasm can be modeled as a colloidal solution inside this sphere. The sphere would have polymers, whose dynamics follow the ssWLC model in a network analogous to the bacterial cytoskeleton5. Then the dynamics of the colloidal solution would be explored using Brownian Dynamics, a classic simulation methodology in colloidal physics. Various factors can affect the overall dynamics of the solution. The first is the concentration, which would be kept neat to cellular concentrations. Additionally, the exact structure of the cytoskeleton is likely to be very relevant. Various randomized structures would be used to study this effect. Finally, a single particle would be used as a probe, and its movement though the cell would be studied to determine if it shows sub-diffusive behavior. Objective 3: Explore the Role of Hydrodynamics: An important factor in the dynamics of a colloidal system is hydrodynamics. These can be included in the Brownian Dynamics simulation through the use of the Accelerated Stokesian Dynamic methodology8. This methodology includes the full effects of hydrodynamics. A challenge is exploring the role of the confinement in the hydrodynamics. Fortunately, the relevant mobility functions have recently been published9.. These would be used along the classic particle-particle mobility, which are well known8, and can be extended to the polymer model10. Since these computations are likely to require significant computational power, they would be parallelized using newly developed methods11. Broader Impacts: Understanding the dynamics of the intracellular environment could lead to increased understanding of genome expression. This in turn could lead to new understanding of many genetic diseases and their mechanism of action. Every effort would be undertaken to undergraduate students in this project. Several parts, including the managing of simulations, can be easily performed by a student new to the field, and could serve as a great learning opportunity. This would be done through REU and other programs for underrepresented students. All papers published from this project will be made available to the wider public using Open-Access publication models. References: 1. Weber, S. C. et al. 2010. Physical Review Letters 104, 238102. 2. Kuwada, N. J. et al. 2013. Nucleic Acids Research 41 (15). 3. Lampo, T. J. et al 2015. Biophysical Journal. 108. 4. Tampo, T. J. et al 2016. Biophysical Journal 110. 5. Cabeen, M. T. et al. 2010. Annual Reviews of Genetics 44. 6. Parry, B. R. et al. 2014. Cell 156 (1-2) 7. Koslover, E. F. 2013. Soft Matter. 9, 7016 8. Banchio, A. J. et al. 2003. The Journal of Chemical Physics. 118 (10323) 9. Aponte-Rivera, C. et al. 2016. Physical Review Fluids 1 (2). 10. Nieves-Rosado, L. et al 2016. Unpublished Work 11. Bülow, F. et al. 2016. Computer Physics Communitcation. 204.	0
actions allow them to control their external environment. Studiesoncausallearningshowthatasearlyas infancy, humans learn the relations between actions and their outcomes1 and are able to use this knowledgetoactassuccessfulagentsintheirenvironment.Mostexistingliteratureonagencyemphasizes comparing the outcome of one’s own actions with their internal predictions of thoseactions2-5.However, these processes have been limited to sensory perception and stimulus-response learning, precluding the ability to explore the effects of agency on episodic memory6 and how sequences of information which unfold due to causal actions drive brain regions to support memory. Background. Exercising agency over learning environments has been shown to improve memory7,8, even when the choices do not relate to the content of the to-be-learned items. Previous work from ourlabgaveparticipantsasimplechoicebetweentwo‘cards’whichwouldrevealanunrelateditem. Participants better remembered items that appeared as a result of their choice9,10. Further, this memory enhancement was driven by an interaction between anticipatory activation within the striatum, a region associated with causal actions and motivation, and hippocampal (HPC) engagement during encoding. While this shows how agency over a choice can positively affect memory fortheoutcomeofachoice,it does not shed much light onto memory for the overall decision sequence. To explore how agency over a series of events affectsassociativememoryforthecomponentsof the sequence, I developed a task where participant's agency was manipulated via a choice. In the “game show” task, participants assisted contestants in choosing one of three doors which reveal a hiddenprize. On each trial, participants saw a trial-unique contestant and either freely chose between the doors (“agency” trials) or selected a highlighted door (“forced-choice”trials).Unbeknownsttotheparticipants, the prize image presented was predetermined. After completion of the task, participants completed a surprise retrieval taskwhichtestedmemoryforthecontestantspresentedintheencodingtask,whichdoor they selected, and the outcome hidden behind the door in three separate, consecutive memory tasks. Across two studies (study 1 n = 28; study 2 n = 131), which serve as the foundation for this proposal, participants showed enhanced memory for the contestants (p<0.001), constant-prize pairs (p<0.05), contestant-door pairs (p<0.01), and prize-door pairs (p<0.01). These results show that by manipulating participant’s agency to select which door to open, we are able toenhancememoryforcues as well as associative memory between cues and outcomes. However, these results do not discriminate whether agency enhances memory separately for each individual pair, or whether agency facilitates the binding of all associations into one integrated sequence. Follow-up analysis explored whether agency facilitates memory integration by examining whether there was an inter-dependence upon memory measures such that memory for one pair was dependent onmemoryfortheotherpairs.Indeed,wefound memory for the contestant-prize pair was modulated by memory for recalling both the contestant-door and door-prize pairs (p<0.01). Further, this effect was significantly higher for pairs that occurred in agency trials vs the forced-choice trials. Intellectual Merit. While I have established a paradigm that modulates associative memory via agency, it isstillunclearhowmesolimbic-hippocampalengagementsupportsthislearning.Understanding the timescale of how these systems interact will inform us on how agency modulates encoding and connect human and animal research. Much of the existing human literature exploring mesolimbic contributions to memory focus onhowphasicdopaminedriveslearninginresponsetorewardfeedback11. However, rodent research has shown hippocampal engagement during exploration prompts sustained ventral tegmental area (VTA) engagement leading to greater response feedback to the hippocampus12. I propose exploring these temporal dynamics within the same paradigm to address the gaps in these two lines of research and contribute to the translation of rodent-to-human research. Using neuroimaging techniques, the current research seeks to: 1) examine sustained mesolimbic during encoding, 2)examine event-evoked VTA and hippocampal activationduringencodinganditseffectsonmemory,3)explorethe interactions between engagement at these different timescales. I hypothesize sustained mesolimbic activity during learning when an individual has agency increases cue and outcome based mesolimbic-hippocampal interactions. The relationship with the sustained and event-evoked activitywill bias encoding to promote associative learning across and within decision sequences. Methodology. Eighty healthy participants will berecruitedtoparticipateinastudyattheTemple University Brain Research and Imaging Center, which houses a Siemens Prisma 3T MRI scanner. Participants will complete a modified version of the game show task. On each trial, they will see a trial-unique contestant (2s), and then will seeandselectoneofthethreedoors(2-4s).Uponselection,the door will be highlighted, then removed to presentatrial-uniqueprizeimage(2s).Again,participantswill either get to freely choose one of the three doors (agency trials) or be forcedtoselectahighlighteddoor (forced-choice trials). Participants will complete three runs of each condition, each run containing 20 trials. Runs will be pseudo-randomized across participants so no more than 2 runs ofthesamecondition appear consecutively. Temporal jitters will be placed between cues and outcomes and between trials to improve both temporal and spatial resolution. Following encoding, participants will complete the three retrieval phases described earlier: contestant recognition, and contestant-prize, contestant-door, and door-prize associative memory. Analyses. Behavior: In brief, item memory will be calculated for contestants using corrected recognition, which accounts for false alarm rates. Associative memory metrics will be calculated as hit rates (percent correct in selecting the old item). I will compare memoryacrossagencyandforced-choice conditions using paired t-tests. In line with my previous findings, I expect memory for items and item pairs to be enhanced for those that appear in agency compared to forced-choice trials. Neuroimaging: 1) In order to examine sustained mesolimbic activation, Iwillcomparesustained baseline VTA engagementusingpairedt-testsonparameterestimatesfromanatomicalROIs.Ipredictthe sustained activation to be higher for agencycomparedtoforced-choicerunsandtrials.2)Toexplorehow the event-evoked VTA-HPC coupling may drive memory, I will use a 2x2 within-subjectsANOVAwith condition (agency, forced-choice) and memory (sequence intact, sequence disrupted) on parameter estimates from anatomical ROIs during cues and outcomes. I expect this couplingtopredictmemoryfor agency but not forced-choice trials. 3) To examine the interaction betweenengagementofthesedifferent time scales, I will relate measures of sustained VTA and VTA-HPC activations on the event-evoked memory signals acrossrunsforbothconditions,separately,usingmulti-levelGLMmodelswitharandom effect of participant. Post-hoc analysis willmakedirectcomparisonsacrossconditions.Iexpectsustained VTA activation and VTA-HPC coupling during agency runs to be associatedwithevent-evokedmemory effects in the VTA and the hippocampus on agency trials, across runs. Broader Impacts. Much of the literature exploring motivated learning comes from work that is particularly interested in response to reward feedback. However, motivated learning in the absence of reward may contribute toriskfordevelopmentofsubstanceabuse.Suchlearningmaypotentiatecuesand contextual factors that strengthen drug associations independent of the anticipation or experience of the reward. I will use agency as a model to elucidate the neural underpinnings of motivated learning in the absence of rewards. This model will allow for the isolation of the core mechanisms that underlie substance abuse, which may depend on VTA-HPC interactions. The proposed research will dissect how VTA-HPC interactions contribute to the complexity of behaviors associated with substance abuse by elucidating how both state-dependent and event-evoked interactions drive memory encoding. Additionally, the results of the proposed research may provide valuable contributions to improving pedagogical techniques. Understanding how agency might enhance learning at different timescales could support new teaching methods which incorporate agentic choices over both short and long term goals. Even inmyanecdotalexperienceemployingactivetechniquestoengagementees,Ihave found that giving individuals’ agency over minor choices, such as choosing what topic to read and discuss, and more substantial choices, such as choosing a research topic, leads to significantly more engagement and long-term retention. The proposed research will directly test how agency can affect motivation and learning at various timescales, which will contribute both to our understanding of howit can support learning and how we may utilize it to broadly enhance teaching methods. References. 1Kuhn, 2012 2Haggard et al., 2002 3Haggard, 2009 4Wolpert et al., 1995 5Chambon et al, 20146Hon, 20177Gureckis&Markant,20128Markantetal.,20169Murtyetal.,201510Murtyetal.,2019 11Shohamy & Adcock, 2010 12Lisman & Grace, 2005	0
free, first-principles theoretical treatment of core-to-core x-ray emission spectroscopy (ctc-XES). Success in this research program will have wide impact for refining analytical and fundamental study of the element- specific electronic structure in highly correlated materials, an extremely broad class with significant industrial, technical, and environmental relevance. Furthermore, by having fully addressed the forward problem, i.e., prediction with no adjustable parameters of ctc-XES spectra from local structure, we will be able to use unsupervised and supervised machine learning (ML) methods to understand the information content in core-to-core XES across systems already widely studied (e.g., 3d transition metals) and also systems that have seem comparatively little XES (e.g., materials with heavy d- and f-electron elements). This will likely lead to prediction of new diagnostic spectral signatures of magnetism in f-electron systems. Introduction and Background: X-ray emission spectroscopy (XES) is the very high-resolution study of fluorescence given off by the radiative decay of a core-shell excited atom, inherently probing the occupied electronic states. XES can carry information about the local chemical environment of the fluorescing atom, such as valence-level spin, oxidation state, ligand identity, local coordination geometry and bond covalency. While there are many truly first-principles theoretical tools for parameter-free calculation of most other advanced x-ray spectroscopies (e.g., XAFS, RIXS, and valence-to-core XES), the same is not true for ctc-XES, which is a deeply many-body problem where the treatment of highly correlated materials with partially filled d- or f- shells is especially challenging [1]. DFT approaches fail to describe the local many-body correlation effects while more accurate configuration interaction (CI) methods are computationally expensive and often difficult to implement beyond simple systems [1, 2]. Multiplet implementations are therefore the preferred theoretical framework for ctc-XES with many theoretical codes and models being developed over the last 40 years [3]. Those multiplet models show adequate agreement with experimental results after fits to screening and correlation parameters. Here, I will develop tools to calculate those parameters, moving from a descriptive treatment to a predictive treatment of ctc-XES. Research Plan: My research plan has three main components: (1) A theoretical component that builds off of my prior work (below) and the expertise available from my theory mentors, Profs. Rehr and Kas; (2) Validation via measurement of ctc-XES across a wide range of materials, this capability is firmly enabled by lab-based XES available in my Ph.D. advisor’s lab (Seidler group, UW); and (3) a ML component that will build on emergent methods in the XAS community, such as recent Seidler group work on unsupervised ML [6]. Hence, I am strongly supported by local expertise and needed facilities. Beginning with theory, Figure 1 shows the distinction between common practice, the result of my work over the past six months, and a large part of the proposed further improvements. First, the central green column highlights the key-components that go into standard Multiplet Ligand Field Theory (MLFT). Note the need for many local environmental components such as the crystal field and charge transfer leads to a large increase in free parameters, limiting predictive capability. Next, the leftmost column shows my progress over the past 6 months building on work by Haverkort et al. [4] by applying a DFT + MLFT approach to ctc-XES, using the full-potential local-orbital (FPLO) DFT code to determine many, but not all parameters needed by the multiplet engine (Quanty). Using ‘reasonable’ values for the remaining undetermined parameters, I find excellent agreement between my new calculations and experiment for the environmentally important speciation of Cr3+ with respect to the carcinogenic Cr6+, see Figure 2. Third, as shown in the right column of Figure 1, I will use the real-space Green’s function code FEFF to both replace FPLO and also implement new calculation of the thus far undetermined parameters for the MLFT treatment [5]. The result will be the first truly parameter-free, first-principles MLFT treatment of ctc-XES. Moving to experimental validation: past, ongoing, and recently funded work in the Seidler group includes ctc-XES measurements of numerous elements in battery materials, oxygen evolution reaction catalysis, cements used in long-term storage of toxic and radioactive wastes, and carcinogens occurring in consumer products or industrial wastes (e.g., Cr toxicity, such as probed by Figure 2). This provides a plethora of testing grounds across numerous problems with high societal relevance. In this work, I will be able to collaborate with other students in the group to design reference standard studies, validate against my theory calculations, and then apply the resulting information to draw best inferences about the systems of actual interest. Fig. 2. Agreement between current theory and experiment for the K XES of Cr(III) and Cr(VI). Finally, the validated theoretical approach will be distributed to the general x-ray spectroscopy community via the workflow management tool Fig. 1. Theoretical MLFT workflow schema: Corvus and will also be used as the basis for ML standard approach (center), current progress investigations of the information content of ctc- (left) and future plans (right). XES. This latter work will start with unsupervised ML, such as t-SNE, which the Seidler group recently introduced as an important way to determine the chemical information content in vtc-XES and XANES [6]. This will be the first ML study of this kind to be applied to ctc-XES as traditional theory methods are either too inaccurate or too computationally expensive compared to the novel DFT+MLFT approach. This work will help determine which general problems are, or are not, well-encoded into ctc-XES across the periodic table and different chemical classes. Intellectual Merit: The interleaved characterization of local atomic and electronic structure poses central challenges across numerous problems, including electrical energy storage, catalysis research, aging of construction materials, toxicity in consumer products, environmental consequences of industrial wastes, and low-diffusion matrices for long-term storage of toxic or radioactive wastes, all of which still have open questions that require an MLFT approach to accurately describe. These questions come at a time of rapid growth in access to experimental ctc-XES capabilities via the development of lab-based instrumentation for education and analytical study (a trend led by Seidler group), major upgrades of synchrotron facilities and specialized XES end-stations for applications in energy sciences, and the steadily increasing application of XES in ultrafast x-ray free element laser (XFEL) studies probing chemical and electronic dynamics. This project will have a uniquely outsized impact not only because of the importance of the social and scientific problems being addressed but because of the synergy with the emerging experimental access to core-to-core XES capabilities. Additionally, the open access model of the tools developed in this research program will facilitate broad adoption within the x-ray community, bridging the gap between accurate ab-initio theoretical methods and the experimental need for reliable first-principles theory. Broader Impacts: Much of my prior experience in outreach has centered around introducing people to a side of science which focuses on the curiosity and intrigue sparked by the natural world around us. I firmly believe that to accomplish this, access to intuitive introductory tools is a necessity. As addressed in my personal statement, I will continue to develop and refine x-ray specific educational material such as the XAS-RW, addressing the acute need for qualitative and intuition based introductory material in my subfield. I will compliment this with fun, science-based community engagement efforts through groups such as the UW Science Explorers and UW Stem Pals to bring hands-on physics directly into the classroom. This will provide the ideal environment to expand upon my Physics Field Day event, as I couple my organizational experience with new community collaborations to deliver a unique, immersive program. Works Cited: [1]doi:10.1002/qua.24905;[2]doi:10.1002/cphc.201800038;[3]doi:10.1016/j.elspec.2021 .147061;[4]doi:10.1103/PhysRevB.85.165113;[5]doi:10.1039/B926434E;[6]doi:10.1039/D1CP02903G	0
Title: Energy-information trade-off in vocal development Background: Vocal development is the result of interactions among several biomechanical and physiological processes, all of which are constrained by energy. Our attempts to understand how all these mechanistic pieces are coordinated over postnatal life is thus a formidable challenge. While the importance of energetic costs in development is obvious, we must also consider that these costs must be traded-off with information transmission: human infants and other animals use vocalizations (e.g., cries, contact calls) to solicit care from conspecifics. How these costs are traded-off during development has not been considered, yet it could provide a common, high- level framework across species within which low-level mechanistic findings may be interpreted. I propose to investigate whether a model that considers energy and information trade-offs best predicts the shape of vocal developmental trajectories in mammals; I will then test the model’s predictions with empirical data. Three different models could potentially explain the trajectory of vocal development, and each can be characterized mathematically using a cost function. The first model is through a constant and progressive change (Fig. 1a,d). The second model relies on a strong initial change followed by slow adaptation (Fig 1b,e). Both models can ac- count for associative learning. The second model can also account for bodily growth in at least 60 species3. The third model considers development Figure 1. Model schematics. (a) First model: linear as stepwise (Fig 1c,f). Using data from three dif- shift of the cost. (b) Second model: cost modeled ferent mammalian species—marmoset monkeys1,2, through recurrence equation4. (c) Third model: two bats5 and humans6,7, all of which exhibit vocal fixed costs balance each other. (d) Linear change. learning—I extracted four standard acoustic fea- (e) Gradual change. (f) Stepwise change. tures (call duration, dominant frequency, amplitude modulation frequency, and Wiener entropy) and calculated their principal component to generate one single acoustic measure. Using that measure, I found that the best model is the stepwise model. The adjusted R2 for the linear, gradu- al and stepwise model are shown in Fig. 2. The stepwise model is also the only one able to accu- rately predict the day of transition between immature and mature-sounding vocalizations. It will therefore serve as the model for investigating the constraints that shape vocal development. Hypothesis: The stepwise transition of vocal output is predicted to be shaped by physiological (energetic) and social (informational) con- straints. These can be manipulated inde- pendently to determine how each might influence development. Aim 1: Test whether energy changes the transition tim- ing in marmoset monkeys. One factor in- fluencing the energy required to vocalize is respiratory power, thus lighter air should reduce its energetic cost. I will fit the Figure 2. Model fitting. (a) Marmoset monkey. (b) Egyptian stepwise model with vocal data collected fruit bat. (c) Human. while infants were in a helium-oxygen (lighter air) during brief daily sessions over 2 months and compare it to data outside the helium-oxygen chamber (regular air). Aim 2: Test whether the in- formation transmission changes the transition timing in marmoset monkeys. Efficient infor- mation transmission is characterized by how well the infant’s vocalization can be used to predict the parent’s vocalizations. I will fit the stepwise model with infant vocal data recorded in ses- sions with high versus low levels of parental feedback over 2 months2. Methods: To achieve aim 1, I will study energetic manipulations using vocalizations recorded in a heliox chamber8. Heliox (20% O and 80% He) has a lighter mass than regular air, so the ener- 2 gy required to pump the air out of the lungs is lower. Marmoset infants are placed for 20 min in a chamber that holds 45 L of air, and their vocalizations are recorded. To achieve aim 2, I will use vocalizations from infants placed in a 20-min playback condition, in which infants receive audi- tory feedback from a closed-loop playback sys- tem11. The infants receive either the father’s or the mother’s mature calls. With the closed- loop system, we can control the amount of contingent playback they will receive, thus manipulating the rate at which infants can change the informational content of their vo- calizations. Figure 3. Schematics of energy cost manipulation. (a) Results Evaluation: I can use the stepwise Higher and (b) lower cost. (c) Simulation using costs model to predict the change in the transition from (a) and (b). day when either the energy or information cost is manipulated (Fig. 3). That change will be com- pared with the change obtained in the experiments. In aim 1, by decreasing the energy cost in the heliox chamber, the transition day should happen later. By decreasing the information cost in aim 2, the transition should happen sooner. If the predictions do not hold, then either the constraints of the model are not energy and information, or the way they constrain development is not the one proposed by the model. In the first case, I can investigate other possible constraints, for ex- ample, how differences in the environment affect development. In the second case, I can check if variations of the model (such as a different cost shape) can better explain the data. Intellectual Merit: The project has the potential to explain how vocal learning can happen in marmoset monkeys and likely other species, including humans and other vocal learners. It incor- porates different factors important for vocal development into one framework, highlighting the importance of the body and the environment into behavior. The stepwise model is also general- izable to other behavioral systems (e.g., locomotion), and thus potentially useful to describe the transitions we see when categorizing any behavior into different stages. Broader Impacts: Low socioeconomic status populations have a higher incidence of speech- language impairments, such as reduced vocabulary and phonological awareness9. A better under- standing of vocal development can meaningfully inform intervention programs. For example, it can be used to measure how diet (energy) versus social interaction (information) lead to healthy vocal development. The computational nature of my project allows me to involve undergraduate students early on in my research. This will be facilitated by the ReMatch program at Princeton, that encourages first and second-year undergraduates to do research. The subject is engaging due to the natural curiosity around how humans and other animals communicate. Moreover, I will disseminate my findings in scientific meetings as well as in forums accessible to the public (in- cluding video lectures). References: 1Takahashi et al., 2015. 2Takahashi et al., 2017. 3Renner-Martin et al., 2018. 4Fehér et al., 2009. 5Prat et al., 2017. 6Cruz-Ferreira, 2003. 7Brent et al., 2001. 8Zhang and Ghazanfar, 2018. 9Perkins et al., 2013.	0
Foreword: Because I am a BioMAT trainee with two years of NIH support, my adviser, Dr. Shuichi Takayama, allowed me to build my own project. Motivated by my interest in immunopathology, I independently created this project after becoming familiar with the work of our established airway disease collaborator, Dr. Rabindra Tirouvanziam at Emory University. My adviser plans to expand my idea into a full grant proposal that I will write with him. Motivation: Inflammatory airway diseases (IADs), including COPDa, CFb, and asthma, are the fifth leading cause of mortality globally.1 Chronic pulmonary inflammation can lead to fibrosis, recurrent infection, and loss of lung function that requires transplant. Neutrophils (PMNsc) are the perpetrators of this excessive inflammation, making them a target of therapeutics and motivating the study of mechanisms driving their pro-inflammatory conditioning. plug In vivo models are insufficient due to significant differences in the 1. generator function of inflammatory mediators and disease phenotype. They also do not offer the ability to systematically eliminate confounding variables, making in Apical vitro models attractive for mechanistic studies. In typical in vitro studies, channel investigators expose either blood PMNs or epithelial cell lines to a disease- Alvetex related stimulus (e.g. smoke, bacteria) in a static system and measure cells’ membrane responses. Despite these efforts, no therapeutics have yet halted the cycle of damage inflicted by PMNs, motivating more sophisticated mechanistic Basal channel studies to inform therapeutic strategies. In vitro models neglect fluid mechanical stress (FMS, caused by thickened lung mucus) and PMN transmigration into the lumen despite 1. plug 2. trans- evidence that severe, FMS-induced crackling sounds are one of the top three propagation migration 2 . clinical predictors of poor prognosis in CF and COPD,4,5 and PMNs that transmigrate into diseased lung fluids acquire a pro-inflammatory phenotype.2,3 These processes may be connected: thickened mucus causes apical ASL FMS-induced epithelial inflammation that recruits PMNs and makes them epithelium pro-inflammatory. Stressors are known to induce sterile inflammation in collagen many cell types.7 Bronchial epithelial cells produce exosomes, cytokines and Alvetex miRNA in response to compressive mechanical stress, and hypoxia-stressed endothelium epithelial cells release exosomes that activate proinflammatory signaling in basal ch. macrophages.8,9 I hypothesize that fluid mechanical stress induces sterile inflammation of the epithelium resulting in epithelial exosomes, miRNA and Fig. 1: Device design. Fig. 2: Cross-section. cytokines that contribute to the inflammatory phenotype of PMNs. The objective of my research is to establish a novel microfluidic model of pulmonary inflammation, incorporating FMS and PMN transmigration, to discover pro-inflammatory pathways that are inaccessible with current models. Aim 1: Design, develop and optimize the “lung inflammation-on-chip” microfluidic device. I am currently modifying the lab’s established lung device,2 which already includes FMS, to incorporate PMNs by adding a porous AlvetexTM membrane that our Emory collaborators use to model PMN transmigration.3 To generate confluent, primary epithelial and endothelial cells on the membrane, I will adapt established Transwell coculture methods: I will continuously flow media on both sides until confluence, which I will evaluate with trans-bilayer electrical resistance and staining for tight junctions.10 To differentiate the epithelial cells, I will remove media from the apical channel and flow 5% CO air for 14 days with media flowing in the basal channel. Liquid 2 plugs will be generated in the absence of neutrophil flow with PBS + 1.2 mg/mL of the surfactant Survanta; the target speed of the liquid plug is 2 mm/s at an air pressure of 1 kPa to model physiological conditions of sublethal stress.11 Liquid plug speed and pressure drop will be measured with established methods from our lab.2 Viscosity of the plug-generating fluid will be measured with rheometry. Computer-controlled electromechanical actuators will create the air flow switching that generates liquid plugs. To validate transmigration, neutrophils will flow on the basal side of undamaged epithelium and the apical side will be incubated with either a) RPMI control or, to induce transmigration, b) RPMI+100 nM LTB4 or c) patient airway surface liquid (ASL). PMN analysis described in [3], including flow cytometry and measurements of metabolism and bacterial killing, will validate that the model produces proinflammatory PMNs.3 Aim 2: Model stress-induced inflammation and infer novel inflammatory networks Epithelial cells will be exposed to 0, 6, 12 or 24 hours of liquid Liquid PMN plugging at a constant rate and pressure drop (5 plugs/min and 1 kPa), and plugging transmig. then PMNs will transmigrate through the stressed epithelium (see Figs. 1- collect apical & basal fluid 2 for diagrams and Fig. 3 for workflow). Exosomes will be isolated with an ExoQuick kit and lysed with 5% Triton X. 40 cytokines (including IL- PMNs : flow supern atant 8, CXCL1, IL-1β, IL-6, and IL-10) will be measured with Luminex assays, cyt, assays + exosomes from [3] and neutrophil elastase (NE) will be measured with ELISA for intra- and soluble cyto- extra-exosomal groups (extracellular NE activity is a predictor of lung e xosomes kines, m iRNA function in CF adults3). miRNA will be isolated from supernatant and e xosomal Lum inex, exosomal lysate with the miRNeasy Mini Kit. miRNA microarrays will cytokines, microarray, identify frequently occurring miRNAs in the supernatant and exosomal miRNA RT-PCR lysate. Quantitative RT-PCR will validate the microarray data.12 I will Fig. 3. Aim 2 workflow. compare the ASL and PMNs from models with stressed and unstressed epithelium, and I will include no-PMN devices as controls. I will also compare transmigrated and non-transmigrated PMNs from the same device. These comparisons will be made at all 4 timepoints so I can evaluate how the system evolves from healthy to diseased over extended stress exposure. To interpret my data, I initiated a collaboration with Dr. Kelly Arnold at Univ. of Michigan, whose lab specializes in analyzing COPD patient sputum and blood using data-driven computational approaches to infer cytokine networks driving cell behavior, tissue phenotype and disease progression. The Arnold lab will use systems biology computational methods to identify inflammatory PMN markers from my flow cytometry data and infer proinflammatory cytokine networks between epithelium and PMNs from the 40-plex Luminex assays. My hypothesis is correct if a) PMNs that transmigrate through stressed epithelium are pro-inflammatory (based on assays from [3] or cell surface marker expression) and b) proinflammatory cytokines or miRNA are produced by the epithelium in response to FMS that stimulate PMN inflammation. Broader Impacts: I am uniting leaders in microfluidics, immunology and systems-level data analysis to engineer a novel IAD pathology model and discover immunological mechanisms that will inform therapeutic design, ultimately reducing lung transplants and extending lifespans. This work is applicable to all IADs and the device can also be used to model immunopathology of pneumonia, lung cancer, or idiopathic pulmonary fibrosis. I will disseminate my results in publications, conferences and seminars with incarcerated people, and I will mentor undergraduates and minority high school students through ENGAGES. ● achronic obstructive pulmonary disease. bcystic fibrosis. cpolymorphonuclear leukocytes. Ref: 1“(COPD).” WHO, 2017. 2Tavana et. al (2011) Biomed. Microdev. 3Forrest et. al (2017) J Leukoc Bio. 1-11. 4Konstan et. al (2007) J Peadiatr 151:134-9. 5Jacome et. al (2017) Clin Resp J 612-620. 6Unpub., Tirouvanziam Lab 7Fleshner et. al (2017) Trends in Immuno 38(10):768-76. 8Park et. al (2012) Mech. of Allergy 130:1375-83. 9Moon et. al (2015) Cell Dth 6. 10Hermanns et. al (2004) Lab Invest. 84:736-52. 11Yalcin et. al (2007) J App Physio 103(5):1796:807. 12Ohshima et. al (2010) PloS ONE 5:e13247.	1
Sedge-Dominated Sites in Discontinuous Permafrost Peatlands Introduction: Rising temperatures in the subarctic are accelerating thaw of organic-rich permafrost peatlands, liberating organic carbon (C) from long term storage through microbial decomposition, and increasing methane (CH ) emissions1,2. Methanotrophic bacteria can oxidize 4 (consume) CH in thawing peatlands, producing carbon dioxide (CO )3,4. Recent biogeochemical 4 2 research has elucidated the controls of CH production and flux in thawing peatlands; however, 4 CH oxidation and its controls remain significantly less understood than CH flux. 4 4 Both CH flux and CH oxidation vary along the gradient of permafrost thaw. As thaw 4 4 occurs, hydrology, plant communities, and geochemical characteristics will all vary spatially and exert controls on the carbon dynamics of thaw5. MethaneandCO are both greenhouse gases, but 2 CH has a >30x larger warming potential than CO . Thus, it is critical to gain further insight into 4 2 the factors determining the ratio of CH flux and CH oxidation in thawing peatlands to produce 4 4 accurate emissions projections for climate change models. Preliminary data from July 2015 from Stordalen Mire in Abisko, Sweden (68°21'N, 18°49'E) provides evidence for permafrost thaw- induced methane oxidation at open-water sedge sites adjacent to collapsing permafrost palsas (Fig. 1). Sedges contain aerenchymous tissues that enable gas transport in and out of the water column4. Sedges act as a conduit for CH out of 4 the water column and transport oxygen (O ) into 2 the rooting zone (rhizosphere), enabling CH 4 oxidation below the water table4. One major gap in validation of biogeochemical models of wetland CH emissions 4 Fig. 1. Cores from above (surface) and below is the lack of Eh or redox potential measurements. (depth) the water table were extracted from 10 sites Field measured Eh of pore water in sedge rooting over a permafrost thaw gradient. Bars represent zones in thawing peatlands should show the extent mean oxidation rate across replicates ± standard error; n=4. By thaw stage: F = 19.75, p < 0.001, by of O diffusion into the water column and indicate 2 depth and thaw stage*depth: p > 0.05. whether aerobic (CH oxidation) or anaerobic (CH 4 4 production) processes are the dominant microbial metabolic pathways in thawing permafrost6,7. Little data is available on the redox state of peatland pore waters because reliable field Eh measurements have previously been difficult to attain and reproduce. Carefully calibrated field Eh electrodes will enable more accurate in situ redox potential measurements8. In addition to field Eh measurements, laboratory incubations of peat from these locations may link redox potential and areas of CH oxidation, particularly in the rhizosphere. 4 Measuring changes in redox potential through the water column of open-water sedge sites will enable more accurate modeling of carbon dynamics in thawing subarctic peatlands, as it will indicate the zones in which CH production or CH oxidation dominate. Current research at 4 4 Stordalen Mire uses the DeNitrification-DeComposition Model (DNDC) to test CH production 4 pathways and flux9. Further monitoring of CH oxidation and its relationship to redox potential 4 will enable DNDC to better integrate CH oxidation and validate wetland CH emissions 4 4 predictions. Understanding the connection between pore water redox potential and oxidation rates in open-water sites is critical to understanding how permafrost thaw will influence carbon dynamics and geochemical characteristics in transitional thaw stages in peatlands as thaw progression advances. Hypotheses: By pairing in situ Eh measurements in open-water sedge sites and laboratory incubations of biomass from sedge sites at each Eh measurement depth, I will test the following hypotheses. 1. A positive correlation exists between redox potential and potential oxidation rate. 2. Redox potential and potential oxidation rate will be highest in the rhizosphere relative to the rest of the water column in open-water sedge sites. 3. Incorporation of redox potential and CH 4 oxidation rates to DNDC will improve modeling of wetland CH emissions. 4 Methods: My research will focus on open-water sedge sites in Stordalen Mire, a thawing subarctic permafrost peatland complex in northernmost Sweden containing palsas, semi-wet Sphagnum sites, wet sedge-dominated sites, shallow lakes, and thaw ponds. As part of my REU experience, I collected preliminary oxidation data using incubations of peat from across a permafrost thaw gradient in July 2015, which revealed high potential oxidation rates in open- water sedge sites proximal to thawing palsas. I will measure redox conditions (Eh) with a platinum electrode8 through the water column in wet sedge areas in Stordalen Mire throughout the course of the snow-free season (June-September). I will also measure environmental correlates including CH flux, water table 4 depth, thaw depth, pH, and plant community composition to examine potential relationships between redox potential and other environmental variables. I will couple Eh measurements with aerobic incubations10 of sedge biomass to determine the relationship between redox potential and potential oxidation rates in sedge areas. Incubations will occur at in situ temperatures and CH 4 concentrations, as determined by field measurements from the preceding field season. Incubation protocol will be held constant across all replicates. After collection, redox potentials, oxidation rates, and environmental data can be incorporated into DNDC to test their effect on emissions scenarios from Stordalen Mire and other similar permafrost peatland complexes. I will work with my advisor’s (R. Varner) collaborators at UNH to integrate these data to DNDC. Intellectual Merit: This project will provide some of the first empirical data on in situ O 2 diffusion through the water column in thawing peatland complexes and its effects on carbon dynamics. As previous models rely on hypothesized values for Eh and O diffusion, these data 2 are essential to developing more accurate biogeochemical models of thawing peatlands to yield more reliable estimates of CH emissions from wetlands as climate changes. 4 Broader Impacts: I am committed to better understanding climate change and biogeochemical systems to both further scientific understanding and to guide mitigation of future carbon emissions and climate change effects. Elucidating less-understood aspects of carbon dynamics, like CH oxidation, and their effect on future emissions is key to creating effective mitigation. 4 I will participate in programs facilitated by the Joan and James Leitzel Center for Mathematics, Science, and Engineering Education (R. Varner, Director) to educate the wider community about global environmental change. Programming from the Leitzel Center aims to engage teachers, high school students, and undergraduates in STEM activities. I will implement educational outreach in regional schools and mentor undergraduate students on research projects, with particular interest in encouraging female and low-income students to pursue their interests in the STEM field. References: 1Turetsky, M.R., et al. (2014), Global Change Biol., 2183, 2Callaghan, T.V. et al. (2010), Geophys. Res. Letts., 3Kip, N., et al. (2010), Nature Geosci., 617, 4Ström, L. et al. (2005), Biogeochem., 65, 5Malhotra, A. and Roulet, N.T. (2015), Biogeosci., 3119, 6de Mars, H. and Wassen, M.J. (1999), Plant Ecol., 41, 7Popp, T.J., et al., (2000), Biogeochem., 259, 8Hagris, T.J. and Twilley, R.R. (1994), Res. Methods Papers, 684., 9Deng, J., et al., (2014), Biogeosci, 4753., 10Larmola, T., et al., (2013), Eco. Soc. of America, 2356.	0
Rationale: Coral reefs provide services totaling $10 trillion despite covering only ~0.3% of the ocean floor1. Their evolutionary success relies on the association between coral animals and symbiotic algae. Corals provide shelter and nutrients for symbionts which in turn supply sugars and O to their hosts2. Corals host symbionts within the symbiosome, an intracellular space 2 defined by the coral-derived symbiosome membrane. This membrane is thought to allow corals to regulate delivery of nutrients to the symbiont but the specific transport mechanisms are mostly unknown. Alarmingly, human-caused ocean warming, acidification, and eutrophication disrupt this symbiosis leading to the expulsion of symbionts (known as ‘bleaching’), decreased coral fitness, and death2. However, the lack of mechanistic knowledge of healthy symbiosis impairs our ability to understand why bleaching occurs, identify resilient and vulnerable species, and design conservation strategies. The mechanisms that deliver nitrogenous molecules (N ) to m symbionts are particularly important. Healthy corals must provide symbionts with enough N for m the repair of photosystem proteins and other basic functions; however, excess N could result in m symbiont overgrowth and bleaching2. Thus, corals must possess yet unidentified mechanisms to regulate N delivery to symbionts. m I propose to study the mechanisms controlling N delivery to symbionts and to m characterize responses to environmental stress in two coral species with differential susceptibility to eutrophication. In other animal models, NH moves across membranes via Rhesus channels 3 (Rh). When paired with an acidification pathway, NH gas combines with H+ to form NH + that 3 4 is trapped on the other side of a membrane3. Coral Rh is an ideal candidate for transporting NH 3 across the symbiosome membrane for N delivery for three reasons: (1) an “Rh-like” gene is m upregulated in anemones upon symbiont acquisition4, (2) corals acidify the symbiosome using V-H+-ATPases, which would favor NH + trapping in the symbiosome5, and (3) NH + is 4 4 symbionts’ preferred N source6. I hypothesize that (1) corals supply N to symbionts via Rh in m m the symbiosome membrane, (2) N supply is controlled via transcriptional and translational Rh m regulation and changes in Rh localization, and (3) future ocean conditions can bypass the Rh pathway resulting in bleaching. Preliminary Results: I cloned the first coral Rh from Acropora yongei (ayRh) (MH025799), developed anti-ayRh antibodies, and confirmed ayRh protein expression via Western Blots. I found that ayRh is more abundant in the symbiosome membrane during daytime compared to the night via immunofluorescence microscopy (IFM) (Fig 1). Aim 1: Establish ayRh Transport and Function. In vitro: I will express recombinant ayRh in Xenopus oocytes and measure its transport kinetics. Oocytes injected with ayRh cRNA or scrambled cRNA (controls) will be incubated with the radiolabeled NH /NH + analog [14C]methylammonium, 3 4 and uptake rates will be measured with a gamma counter. Since some vertebrate Rhs can also transport CO 7, I will 2 determine ayRh CO permeability by measuring CO -induced 2 2 changes in oocyte pH with the pH-sensitive dye SNARF1. I will run statistics in R and Prism™. I predict that ayRh is NH - and not CO -permeable, supporting my hypothesis of 3 2 Figure 1. Diel Rh localization to the Rh-mediated N m delivery. If ayRh transports both, I will SM (n = 50) (p = 0.0246*). adjust my hypothesis and explore the role of Rh in providing carbon and N for symbiont photosynthesis and metabolism. In vivo: I will explore the m correlation between Rh abundance and N transport rate in isolated coral cells hosting m symbionts5. I will measure Rh abundance by Western Blot and NH + uptake rates from seawater 4 using spectrophotometry. I predict a direct relationship between Rh abundance and capacity for N transport. All materials are already available in my collaborating and host labs. m Aim 2: Characterize Coral Rh Regulation. I will expand on my preliminary results (Fig.1) to identify mechanisms that regulate Rh abundance in the symbiosome membrane. In addition to A. yongei, I will work with Stylophora pistillata, which is more resilient to N eutrophication8. This m comparative approach may unveil species-specific mechanisms that confer resilience in polluted oceans. Transcriptional and translational Rh regulation will be tested using qPCR and Western blotting in coral samples taken during day and night timepoints. Rh’s subcellular localization and dynamics will be assessed in unprecedented detail via IFM on a super-resolution confocal microscope. Building on preliminary experiments, I will sample every three hours over a two- day period. Furthermore, I will use the highly specific photosynthesis inhibitor DCMU to determine if the presence of Rh in the symbiosome membrane depends on photosynthetic activity or simply on the presence of light2. I will automate IFM data collection and quantitative analysis with ZENTM software; I will use my coding experience to create custom workflows to achieve high throughput and bias reduction during analysis. I will run statistics in R and Prism™. I predict the Rh pathway is present in both coral species, that Rh trafficking to and away from the symbiosome membrane depends on photosynthetic activity, and that Rh mRNA and protein abundance will remain relatively constant reflecting basal turnover rates. Aim 3: Establish Rh Responses to Stress. To determine the effects of future ocean conditions on Rh, I will grow A. yongei and S. pistillata in three conditions: (1) control, (2) elevated N (10 m μM NH Cl), and (3) elevated N and CO (10 μM NH Cl, 1000 μatm CO ). I will collect 4 m 2 4 2 samples at 12:00 and 24:00 daily over a 70-day period (10 days of control, 30 days of treatment, and 30 days of recovery in control conditions) and rapidly analyze Rh expression and subcellular localization as described above; this method will also allow me to quantify symbiont density to estimate bleaching. Additionally, I will study symbionts’ photobiology using respirometry and PAM fluorometry and genotype symbionts to explore potential effects of symbiont strain. I will run statistics in R and Prism™. I predict the Rh pathway will be initially downregulated in both experimental treatments. I also predict that corals in elevated N and CO conditions will m 2 undergo the highest degree of bleaching due to larger loss of host control over symbiont growth; these effects will be more pronounced in eutrophication-sensitive A. yongei. Finally, I predict the Rh pathway will gradually return to normal during recovery and reestablishment of symbiosis. Intellectual Merit/Broader impacts: This study will characterize a novel N transport m mechanism in coral symbioses and develop much-needed biomarkers to evaluate species-specific vulnerability to environmental stress and early detection of bleaching. It also has the potential to reshape our understanding of coral symbioses by establishing a novel diel regulatory mechanism that traffics proteins to and from the symbiosome membrane. I am well qualified to conduct this research based on my experience with IFM, molecular biology, coral biology, and computer science. In my PhD, I will continue to mentor undergraduates through my tutoring program, many of whom are Latina females, and I will expand my program to low income high schools. Results from my project will be presented to the scientific community through peer-reviewed papers and conferences, and to the general public in youth activities, lectures, and exhibits through Sally Ride Science and the Birch Aquarium (which hosts 450k visitors annually). My career goal is to be an R1 professor and these activities will shape my future outreach and education programs. References: (1) Global Environ Change 2014, 26, 152-158. (2) Microbiol Mol Biol R, 2012, 76, 229-261. (3) Transfus Clin Biol 2006, 13, 85-94. (4) G3-Genes Genom Genet 2014, 4, 277-295. (5) PNAS 2015, 112, 607-612. (6) Mar Biol 1983, 167, 157-167. (7) Membranes 2017, 7, 61. (8) Mar Biol 2000, 19, 103-113.	0
Introduction: Understanding the atmospheric questions about the fate of ISOPO in differing 2 oxidation of organic compounds is important NO regimes. x for predicting the production of tropospheric To combat this, I aided in the development ozone and organic aerosols, both of which im- of a field-hardened high resolution time-of- pact our health and climate. Though the for- flight CF O- CIMS coupled with a low pressure 3 mation of these two constituents involves many gas chromatograph (GC-HR-ToF) that is capa- compounds, isoprene plays a dominant role due ble of observing isomer distributions of various to its large biogenic emissions and rich oxida- oxidation products in ambient air. An initial tive chemistry. However, it is the complexity of field test has provided promising preliminary its oxidation scheme that serves as a major chromatograms of several isoprene products source of uncertainty when making predictions, and my plans to continue the development of generating chemical models that fall short of this instrument will allow it to serve as an inval- replicating ambient observations especially near uable analytical tool, allowing for an increased isoprene-dominated environments1. understanding of ISOPO chemistry and its ef- 2 One reason for this uncertainty is due to its fects on global air quality. peroxy radical (ISOPO ), formed predomi- 2 nantly through the oxidation of isoprene with OH. ISOPO is known to undergo three individ- 2 ual reaction pathways, with the likelihood of each strongly dependent on the availability of NO . In areas where NO levels are decreasing x x (e.g., the United States), the isoprene chemistry is being shifted towards a system dominant in autoxidation and HO reactions, resulting in the 2 production of species such as isoprene epoxides Figure 1: Instrument schematic of GC-HR-ToF. Green boxes represent mass flow controllers; blue circles repre- (IEPOX), which contribute to the formation of sent valves; arrows indicate direction of analytical flow. secondary organic aerosols2. Analogously, as (A) GC column; (B) Heating/cooling unit; (C) Radioac- NO x emissions increase in more pristine areas tive ionizer; (D) Glass flowtube; (E) ToF mass analyzer. (e.g., the tropics) ISOPO 2 begins to favor its re- Instrument Description: The GC-HR-ToF action with NO, either increasing the ozone pro- uses a 1-meter column with its resulting effluent duction potential of the area or forming reser- sampled directly into the CIMS. During collec- voirs, like isoprene nitrates (ISOPN), that can tion, the sample is cryofocused near the en- transport NO x elsewhere with unknown effects. trance of the column via a custom-built heat- Observations of ISOPO 2 products in the at- ing/cooling unit and the temperature of the cold mosphere can shed light on the relative im- trap is controlled through alternating CO cool- 2 portance of the reaction pathways that produce ing and resistive heating. Furthermore, during them. The utilization of clustering ion chemistry the trapping phase, ambient air is also simulta- in conjunction with chemical ionization mass neously analyzed by the CIMS, allowing for di- spectrometry (e.g., CF 3O- CIMS) has allowed rect comparison between the GC sample and low fragmentation sampling of important multi- traditional measurements. The overall instru- functional isoprene products considered too ment schematic is shown in Figure 1, with a fo- fragile to be detected otherwise3. However, be- cus on the GC components I constructed. cause isoprene oxidation results in eight Preliminary Results: Our first field test oc- ISOPO 2 isomers, CIMS cannot distinguish be- curred during PROPHET 2016, a campaign held tween the isomeric products, causing ambiguity at the University of Michigan Biological Sta- in the data provided and leaving unanswered tion. Sitting on the PROPHET tower 30 meters NSF GRFP 2017 1 Krystal Vasquez Graduate Research Plan Statement 6 10 14 Retention Time [min] Figure 2: (A) Peak assignment of ISOPOOH/IEPOX (blue) and ISOPN (red, signal x3) for data collected on 23 July, 10:00 EDT; (B) Sample of peak identification of ISOPOOH/IEPOX (black) using known product ions (red & blue, signal x2) from data collected on 23 July, 14:00 EDT above ground, the GC-HR-ToF sampled above be able to obtain isomer-specific measurements the tree canopy obtaining flux measurements of of isoprene products in areas with a spectrum of various compounds. Even more, in situ isomer NO concentrations. This will be key in under- x distributions of isoprene products (e.g. isoprene standing both the favorability of ISOPO path- 2 hydroxy hydroperoxides (ISOPOOH), IEPOX ways and its subsequent effects on air quality, and ISOPN) were observed for the first time in particularly in areas where VOC/NO ratios can x ambient data. I have performed preliminary be effected by increasing emissions, the peak assignment using known product ions2 to transport of NO or air quality regulations. x identify the compounds (Figure 2). The isomer Conclusion & Broader Impacts: The data I distributions determined by this data set will will obtain using the GC-HR-ToF will provide provide information regarding the fate of a critical test of our current grasp of isoprene ISOPO in a Northern Michigan forest influ- chemistry, adding to the current kinetics used in 2 enced by both pristine air from the north and models to improve predictions. In addition, its high-NO pollution from nearby urban centers. use by several atmospheric groups at Caltech, as x Research Plan: Though overall successful in well as collaborators, will extend the use of this the field, I plan to further improve the GC cry- new technique beyond our initial focus, demon- otrapping system. Despite the fact that the col- strating both its versatility and benefit to the sci- umn separated the product isomers, the high hu- entific community. midity levels seen in Michigan served as a ma- Lastly, as this instrument aims to better un- jor obstacle during sample collection; trapped derstand some of the fundamental science be- water easily degrades the chromatography by hind isoprene chemistry, it provides an oppor- hydrolyzing isomers and overshadowing the tunity for me to enhance my science communi- visibility of early eluting peaks. Though I per- cation skills. By using my blogging platform, I formed constant on-site adjustment to minimize plan to discuss the various aspects of atmos- this effect in the field, improvement of the tem- pheric chemistry and atmosphere-biosphere in- perature control and automating its adjustment teractions in order to teach a demographic com- with ambient humidity will make the GC system posed of young students and future scientists more robust in the field. about a field they may have otherwise never Afterwards, I propose to commission the in- been introduced to. Additionally, social media strument to continuously sample ambient air on has been and will continue to be utilized when Caltech campus (an urban high NO environ- out in the field to give my readers a peak into x ment) while also preparing for a summer collab- the day to day workings of an atmospheric oration at Indiana University (a rural low NO x chemist. References: (1) Horowitz, L. W. et al. JGR: regime). During both field experiments, as well Atmos. 112, 13 (2007). (2) Bates, K. H. et al. JPCA 118, as with future campaigns, the GC-HR-ToF will 1237-1246 (2014). (3) Crounse, J. D. et al. Anal. Chem. 78, 6726-6732 (2006). NSF GRFP 2017 2 langiS yrartibrA 1 0.5 0	0
The evolution of similar traits in distantly related species is one of nature’s great surprises. Convergent evolution of traits has been widely observed throughout the animal kingdom; however, it is often unclear whether this phenotypic convergence results from convergent evolution of genes (Stern, Nat Rev Genet 2013). On a molecular level, convergent evolution occurs when the amino acids of a specific protein preferentially undergo mutations that produce similar or even identical amino acid sequences within distinct evolutionary lineages. A high level of adaptive convergent evolution – that is, convergence due to positive selection of beneficial mutations – would suggest that some genes have “optimal configurations,” which evolution uses and reuses across species. If such genes exist, then evolution is somewhat predictable, proceeding by one of a small number of possible paths (Stern & Orgogozo, Science 2008). In contrast, a complete absence of adaptive convergence would indicate that protein configurations beneficial to one species are seldom optimal within other species, and that evolution may proceed by a much wider set of paths. Thus, two fundamental questions in evolutionary molecular biology are 1) how often and in which genes convergent molecular evolution occurs and 2) whether molecular convergence is a primary driver of phenotypic convergence. Most analyses of convergent evolution have been limited to single genes (Li et al., Curr Biol 2010) or small taxa (Bazykin et al., Biol Direct 2007); to date, no genome-wide analysis involving a wide variety of species has been completed. Recently, the advent of whole- genome sequencing has opened new opportunities for exploring molecular convergence. Published genomes now exist for over 100 animal species, and 177 more are currently underway (Koepfli et al., Annu Rev Anim Biosci 2015). Thus, a genome-wide search for convergent mutations across multiple species is now possible, and might reveal new evidence of adaptive molecular convergence. I plan to develop and apply a novel computational framework to test for convergent evolution among 61 sequenced mammalian species. I hypothesize that convergent molecular evolution occurs at a higher rate than has been previously observed, and that this genetic convergence drives convergence of observable traits. This framework will extend the boundaries of biological knowledge by quantifying the frequency of convergent evolution, and will augment existing phylogenetic methods in a broadly applicable framework that can be extended to other genomes in the future. Aim 1: Develop a novel computational framework for identifying convergent evolution between genomes. My framework will be generalizable to any data set with the following inputs: 1) a phylogenetic tree for a set of species, and 2) the pairwise sequence alignments of all proteins in those species. I will limit my analysis to genes unambiguously alignable to a one-to- one human ortholog in at least 80% of mammalian genomes. I have verified that even after filtering on these criteria, >50% of all human genes are retained. My analysis will integrate these data as follows:  Step 1: Infer ancestral sequences. For every amino acid in every sequence, I will compute amino acid sequences at each ancestral node in the phylogeny using the linear-time maximum parsimony method implemented in PAML 4 (Yang, Mol Biol Evol 2007).  Step 2: Infer patterns of adaptive convergent evolution between species pairs. For every pair of species within our analysis, for each gene, I will identify the set of amino acids that converged in that pair of species with probability >0.9, as well as those that diverged with probability >0.9, based on the ancestral sequences inferred in Step 1. I will report a convergence score for the gene in this pair of species: the ratio of convergent mutations to divergent mutations. Thus, genes with high rates of both divergent and convergent mutations (such as rapidly evolving immune genes) will score lower than those with relatively higher rates of convergent mutations. After computing the full distribution of convergence scores for every gene in every species pair, I will denote the upper outliers as convergence events.  Step 3: Evaluate model performance on simulated data. I will simulate evolution of protein sequences in which a small fraction of the sequences evolve under a non-neutral convergence pattern and the rest evolve neutrally. I will measure my framework’s precision and recall in inferring which sequences evolved under the convergent model. If my framework is able to detect convergence events in the simulated data at a low false discovery rate, but observes no convergence events in the biological data, then these results will cast doubt on the hypothesis that adaptive convergence is a significant driving force in molecular evolution. Aim 2: Quantify the levels of convergent evolution within mammalian genomes, and identify functions enriched within genes evolving in convergence.  Step 1: Identify specific genes that have evolved in convergence across species. Using public alignment tools (Kent et al., PNAS 2003), my collaborators in the Bejerano Lab at Stanford have provided cross-species sequence alignments and a phylogenetic tree for 61 mammalian species. I will apply my framework from Aim 1 to all pairs of sufficiently diverged mammalian species and identify genes within each species-species pairing that exhibit non- neutral convergence. I hypothesize that I will find evidence of adaptive convergent evolution within many genes and within almost all species-species pairings.  Step 2: Identify convergent genotypes potentially responsible for known convergent phenotypes. If evolutionary parallelism is truly adaptive, this would suggest that genes converge when they undergo similar selective pressures within different species (Castoe et al., PNAS 2009). Therefore, I hypothesize that genes facing similar pressures within independent species will be more likely to evolve in convergence within those species. For example, in aquatic mammals such as dolphins and manatees, we might expect high convergence of skin-expressed genes involved in thermoregulation. Indeed, my preliminary analysis has identified 4 convergent mutations in the gene TGM1 in dolphins and manatees, a significantly greater number than expected by chance. Human mutations in this gene cause a skin disease called ichthyosis, characterized by fish-like, scaly skin (Laiho et al., AJHG 1997). In order to find similar cases, I have identified a set of 20 convergent phenotypes that arose independently in mammals, such as adaptation to high altitudes or dry environments. For each of these phenotypes, I will identify genetic convergence events that may be responsible for the phenotype in question. If my hypothesis is correct, this analysis will suggest functions for which similar selective pressures across species have necessitated similar courses of protein evolution across these species. The specific amino acid substitutions that I identify will then be prime targets for further examination in functional assays, as described by Liu et al. (Mol Biol Evol 2014). Broader Impacts: This project will help us to understand whether phenotypic convergence is a direct result of genotypic convergence. I will create a publicly available, user-friendly visualization for the UCSC Genome Browser that highlights hotspots of convergent evolution. This tool will allow biologists to visually explore instances of adaptive molecular convergence and to ask even deeper questions about the specific functional roles and phenotypic effects of these convergent mutations.	1
Computational Design and Structural Analysis of Novel Peptidine Oligomers Key words: Peptidomimetics, rotamer library, foldamers, rational drug desing. Background and Significance Peptides are essential endogenous molecules with intriguing Figure 1. structures that enable them to have innumerable biorelevant functions. Oligomer-based Although peptides have been designed as potential pharmaceutical agents, biopolymers and their poor bioavailabity and poor in vivo stability makes them bad drug mimetics. Depicted candidates. This fact led to the study of peptidomimetics. One result from are structures of these efforts is peptoids, which contain a non-canonical peptidic backbone. peptide, peptoid and Peptoids appear to be resilient to degradation, but their structures possess the proposed more degrees of freedom and thus pay a higher entropic penalty for target peptidine oligomers. binding when compared to peptides (1). Monomer Backbone Peptidines are a novel class of oligomers, structurally derived from differences are peptides and peptoids (see figure 1). These molecules consist of repeating shown in red. units of N- substituted amidines, a functional group found in drugs such as the histamine receptor antagonist cimetidine and ranitidine (Zantac) (2). The unique structure of peptidines enables the duplication of the amount of side chain; this structural feature could confer a unique secondary structure. Peptidine synthesis is a straightforward process that consists of the iterative addition of imidoyl chloride and primary amines in sequence. To the present fifteen trimers, three tetramers and one pentamer have been synthesized in the Spiegel lab at Yale University (an example of one of this successful synthesis is shown in figure 2). However, studies regarding peptidine’s structure are lacking. Gaining insight into peptidine structure will allow further investigation to evaluate these novel oligomers as key [ ] = monomer molecules for the development of peptidine based, more efficient n sequence therapeutics. I propose to study peptidines structure using an array of powerful techniques such as computational modeling software, X-ray Crystallography and Nuclear Magnetic Resonance (NMR). I hypothesize that a) peptidines will have characteristic secondary structures similar to those found in peptides, although their secondary structures will be more rigid than peptoids and peptides due to their higher functional density; furthermore b) peptidine structure will vary with changes in pH, temperature and solvent, but c) by varying computationally the degree of steric hindrance on each side chain I will be able to control peptidine folding patterns in different chemical environments and thus control their function and selectivity once synthesized. Figure 2. Former synthesized peptidine trimer. Peptidines show higher functional density compared to peptides; this characteristic will enable the formation of rigid secondary structures. J. Torres-Robles, 2 Aim 1: To determine experimentally former synthesized peptidine’s structures using X-ray crystallography and multidimensional NMR spectroscopy. I will use X-ray crystallography to determine the electron density of former crystalline peptidines at Yale X-ray crystallography facility. Electron density data will allow me to characterize chemical bonds, electronic properties, dihedral angles and finally the mean position of atoms in each oligomer. To supplement crystallographic studies, I will use H1 NMR NOESY for the characterization of peptide secondary structure (3), these studies will be held at Yale west campus NMR facility. Unlike other 2D NMR techniques (ex. COSY and TOCSY) NOESY detects spin polarization caused by through space dipolar interactions of atoms within 5 Å of distance. Thereby, using this data I can assign and analyze the sequence of interactions through a single oligomer which will lead me to its secondary structure. Using the same technique I will determine how these interactions change with variations in pH, temperature and solvent. Aim 2: To determine computationally peptidine’s energetically favored conformations Recent expansions to the ROSETTA algorithm software allow the study of non-canonical backbones (4). In order to do molecular simulations using this program it is necessary to create a rotamer (energetically favored rotational conformers) library (5). To construct the library, dihedral and torsional angles from side chains and backbone must be determined. Using computational software like Chimera, I can predict torsional angles for a set of side chains. This data will be incorporated to ROSETTA along with dihedral angles to do molecular modeling (Density functional theory (DFT) and molecular mechanics (MM) calculations will be used in case ROSETTA software do not recognizes peptidine primary structure). I will then be able to predict peptidine structural preferences with various side chains. Intramolecular interactions can be studied for different sets of side chains this will be useful to determine folding patterns. Also, intermolecular binding activity will be studied to determine the degree of peptidomimetics in this new type of oligomers by modeling with biological receptors. This data will shed light on peptidine’s function and its relation to their structure. Intellectual merit Studying the structures of peptidines will be a worldwide innovation in the field of peptidomimetic. It will contribute to the fundamental understanding on the relation between structure and function in oligomers that contain intriguing secondary structures that allow them to perform by efficient chemical mechanism in vivo. This infromation will allow the design of molecules with the desired secondary structures in order to improve and control their binding selectivity and function. Fundamentally the prediction of peptidine’s biological interactions will be useful to establish their potential use as better drug leads. Broader impact Because peptidines are expected to have a more rigid structure, in comparison with peptoids and peptides, they will pay a lower entropic penalty for target binding. These novel oligomers are thus potential candidates to develop more efficient drugs by rational drug design to treat a wide variety of diseases. Robust peptidine based libraries can aid in the development of novel therapeutics that will be expected to have better binding efficiency, higher selectivity and vastly improved bioavailability, compared to those of peptoids and peptides. References 1. Josephson,K.; Ricardo, A.; Szostak, J.W. (2014) Drug Discov. Today. 2014, 4, 388-399. 2. Silverman, R.B. and Hollday M.W. the organic chemistry of drug design and drug action. 3rd ed., 2014, pp. 151-155. 3. Stanger, H.E.; Gellman, S., et al. PNAS. 2011, 98, 12015-12020. 4. Drew, K., et al. Plos one. 2013, 8, e67051, 1-17 5. Butterfoss, G.L.,et al. JACS. 2009, 131, 16798-16807.	0
Motivation: Hydrogenation is an especially important industrial reaction that finds uses in pharmaceutical, agrochemical, fragrance, and fine chemical synthesis. 10-20% of chemical reactions at Roche (the world’s third largest Biotech company) are catalytic hydrogenations1, and hydrogenation of N to NH consumes an estimated 2% of the world’s energy supply. 2 3 Supramolecular chemistry utilizing adjustable hemilabile ligands that approximate enzymatic activity has been an extremely active area of research, and hydrogenation can be improved significantly with respect to its chemoselectivity by tuning the catalyst to minimize over- hydrogenation. Mirkin et al. have constructed elegant ‘molecular tweezers’ that take advantage of chloride binding and supramolecular interactions to create switchable on/off catalytic turnover2. Miller et al. have recently published a PCN (phosphorous-carbon-nitrogen donor atoms) pincer ligand that takes advantage of macrocycle-cation interactions to enable both switchable and tunable catalytic activity2; however, this ligand’s catalytic potential has not yet been fully realized. Cation interaction with the macrocycle can speed the catalyzed reaction rate of olefin isomerization by over three orders of magnitude3. In situ control of catalytic activity via hemi-lability of crown ether/cation interactions will revolutionize homogenous hydrogenation catalysis. The ability to completely quench catalysis via addition of an anion, resulting in precipitation of a simple salt and re-coordination of the unoccupied crown-ether to the catalytic metal center, will give a new degree of control to hydrogenation catalysis. This will eliminate over-hydrogenation, a well-documented issue with isophorone, an essential polycarbonate precursor4 as well as other feedstocks. Enzymes are notable for their ability to use first-row transition metals to catalyze a wide variety of molecular transformations. These metals are generally more abundant, less toxic, and easier to dispose of than their heavier isoelectric counterparts. Replacing precious metals with ‘greener’ options is of vital importance to sustainable chemistry. Hanson et al. have reported an air- and water-stable Co(II) complex with a pincer ligand that is capable of hydrogenating alkenes, aldehydes, ketones, and imines at low pressures of gaseous hydrogen (1-4 atm) and low temperatures (>60°C)5. A Re complex with PNP was recently reported to activate N , but does 2 not have switchable or tunable properties6. Research Plan: Pincer ligands are attractive due to their thermal and oxidative stability as well as ease of modification. I hypothesize that installing a hemi-labile macrocycle to a PNP ligand will allow significant switchable and tunable activity of an established homogenous hydrogenation catalyst. The synthesis of a novel ligand is shown in Scheme 1 using commercially available starting materials and known methodology. The phosphine, macrocycle, secondary amine, and metal ions can all be varied to generate a large library of Scheme 1: Synthetic route for L. X= Halide; M= complexes for catalytic analysis. The double Co (II), Ir (II), Re (II), Rh (II); M’=Na+, Li+, Ca2+; crown ether moieties are hypothesized to R=Ph, Me, Cy. increase the tunability of catalysis even further than the published example by Miller et. al. Tunable steric bulk arising from the cation complexed crown-ethers, is hypothesized to allow for substrate specificity in hydrogenation. Coordination of a vacant crown-ether to the catalytically active metal will block the association of H that must take place for catalysis to occur; however, when a Group 1 or 2 cation 2 Kevin Michael Wyss – Research Proposal binds to the crown ether, it will detach from the Co(II) and allow H to access and be activated by 2 the transition metal (Scheme 2). My first goal will be to optimize of the synthesis and purification of L. Characterization of L will include single crystal XRD, mass spectrometry, and NMR. Determination of the stability of L to air and water, and its solubility, will be essential to further analysis of the complex. My second goal is to assess the ability of L to catalyze the selective hydrogenation of furfural. Furfural is a popular industrial feedstock that is a bio-based renewable building block for a wide range of polymers and fertilizers. Selective hydrogenation of furfural to furfuryl alcohol is a vitally important part of Scheme 2: Showing the switchable activity of L. functionalization, and often requires the use of noble metals10. It is presumed that hydrogenation using Ir(II) will occur readily for a variety of alkenes, alkynes, imines, and carbonyls through redox pathways as there is significant precedent for this in literature. A mechanism showing the hypothesized hydrogenation is shown in Scheme Scheme 3: A proposed mechanism for the 3. Besides furfural, it is hypothesized that this hydrogenation of furfural to furfuryl alcohol. same mechanism should allow for selective hydrogenation of substituted benzaldehydes and cyclohexenes, two other classes of molecules that that are of high industrial importance. Screening multiple L complexes with an array of substrates, under varying reaction conditions will identify trends to investigate in further studies. Addition of anions to the reaction solution is hypothesized to result in a switchable activity. The identity of the cation used to dislodge the crown-ether from the active site, is also expected to result in tunable rates of reaction, with Group 2 likely exhibiting the fastest rate as the fit best within the crown ether. I will then determine whether earth abundant Co(II) can substitute Ir(II) in the catalysis. Hanson et al. assert that the mechanism of hydrogenation proceeds through a Co(II) hydride intermediate, and that the coordination of a secondary amine to the metal center is essential5. It is hypothesized that although 3d catalytic pathways often favor one-electron processes due to size, as L still contains the secondary amine, hydrogenation via reductive elimination of the hydride intermediate can still occur. Use of deuterium-labeled H and solvents to probe the catalytic 2 mechanism will result in crucial fundamental research in this budding class of catalysis. Mechanistic study of catalytic cycles is an important area of fundamental research, and it is necessary to improve selectivity and turnover, or to design complementary catalysts. Broader Impacts and Intellectual Merits: Earth abundant metals are cheaper, safer, and greener alternatives when used as catalysts, and this is an important aspect of establishing sustainable chemistry as well as increasing the long-term economic security of the U.S. by decreasing dependence on unstable sources of precious metals. The work will also contribute to our fundamental knowledge of switchable and tunable catalysis which is a relatively new and unexplored area. 1) Curr Opin Drug Discov Devel., 2001, 4(6), 745-755. 2) Science, 2010, 330, 66-69. 3) J. Am. Chem. Soc., 2014, 136, 14519-14529. 4) J. Am. Chem. Soc., 2015, 137, 12121-12130. 5) J. Am. Chem. Soc., 2013, 135, 8668-8681. 6) J. Am. Chem. Soc., 2018, 140, 7922-7935. 7) ACS Catal., 2017, 7, 1720-1727. 8) Russ. J. Gen. Chem., 1986, 56(8), 1777-1781. 9) J. Organomet. Chem., 2017, 845, 82-89. 10) Sci. Rep., 2016, 6, 28558.	0
[1], a major gap still exists between the skills of these graduates and the skills needed for industry success [2]. Two major components of this gap are programming skills (e.g. code style) and computational thinking (e.g. extending algorithms). My research will enable teachers to close these gaps by building a toolkit to integrate better-targeted problem types than those currently in use. Programming assignments in CS classrooms, primarily code tracing and code writing, lack the granularity to target students’ Zones of Proximal Development (ZPD, i.e. challenging but solvable problems). Code tracing questions involve reading and understanding code, but they may not trigger students’ mental models of concepts because they are tedious or too easy [3, 4]. Code writing questions are a large leap from code tracing, conflating many programming skills into a single solution (e.g. design, computational thinking, programming, code style) and supporting a wide space of disparate approaches and solutions. Extrapolating from earlier results [3], I propose the use of Parsons problems (e.g. [5]) to help students remain in their ZPD while learning computational thinking and programming through advanced CS concepts. Parsons problems involve unscrambling chunks of code, often individual lines, from a target program into a correct solution. Such problems provide similar learning gains to code writing problems – often 30% faster – for introductory assignments [3]. My research will enable teachers to integrate Parsons problems into existing curricula to better attain their teaching outcomes by providing a toolkit for these new problems. Preliminary Work Within my Ph.D., I have been studying how Parsons problems can be applied to improve CS pedagogy. In Spring 2018, I ran an exploratory between-subjects study which found that when students begin by solving a Parsons problem instead of writing code, they are more able to generate multiple alternative solutions, addressing a skill gap in the “ability to generate alternate solutions” [2]. In Fall 2018, I ran a within-subjects study, followed by a structured interview, teaching algorithms to students who had not taken an algorithms class. They were taught two algorithms by either writing code from a pseudocode specification (i.e. a language-independent solution) or by solving a Parsons problem. Students with a range of skills noted that Parsons problems let them focus on the logic of the algorithm, engaging them in computational thinking. Students found writing code from pseudocode to be either too complex, distracting their focus, or too trivial, not engaging with the algorithm. These results suggest that Parsons problems can support a variety of students in practicing their computational thinking. Approach In my thesis research, I will explore how we can leverage new problem types, e.g. Parsons problems, to supplement existing teaching tools for advanced CS concepts. I will run longitudinal studies on these new problem types by partnering with UC Berkeley professors who teach relevant classes with hundreds of students. I will then synthesize the results from these studies to develop a toolkit for teachers to help teachers easily adapt existing material into Parson problems and achieve their curricular desires. RQ1: How can Parsons problems support the development of computational thinking with algorithms? Based on my Fall 2018 study, I am now exploring how to help students further focus on computational thinking with new Parsons problems that flexibly blend pseudocode with code within or between problems. By using pseudocode, students are constrained to rely less on syntax and more on computational thinking. I will measure students’ learning gains of the taught algorithms as well as their performance on interview-style algorithm questions. RQ2: How can Parsons problems improve programming ability by teaching programming idioms? One major risk of supplementing existing assessments with Parsons problems is that it could hurt students’ programming skills by reducing their time spent practicing writing code. To overcome this pitfall, I will explore how Parsons problems can improve code writing by teaching programming idioms (i.e. common code and design patterns such as finding the five largest numbers in a list). There is a strong connection between students’ ability to write well-styled code and their ability to select and apply appropriate programming idioms [6]. However, complex idioms are often not explicitly taught. Results from my Spring 2018 study indicate that Parsons problems could support students exploring multiple solutions to a problem using different idioms, helping them compare when they are effective to use. They could also expose students to a range of problems where an idiom is applicable, helping students learn how it can be applied. I will run a formative study to better understand how students learn and select idioms. I will measure ABC scores of solutions, a well-established metric for code complexity, to evaluate students’ ability to efficiently apply idioms [6]. RQ3: How can instructors easily integrate Parsons problems into their teaching? Even if these new problem types are found an effective teaching tool, it must also be straightforward for instructors to generate and integrate them into the classroom. I will interview professors to explore the range of teaching methods used in classes and homework. These interviews will guide the design of a system to give teachers a powerful tool to target specific learning goals with more problem types. For example, a teacher could use think-pair-share in class to encourage students to share their problem-solving strategies by having students discuss which line of pseudocode should be placed next in a Parsons problem. Or, the large corpora of student solutions could automatically generate Parsons problems for teachers to modify and use. Resources UC Berkeley provides rare access to collaborate with teaching professors in large, innovative classrooms. Here, my research will change how thousands of students learn CS. Intellectual Merit My work will enable further research of teaching tools throughout the CS curriculum. My proposal explores how students learn and use computational thinking and programming idioms in complex problems through problem types and assessments. While there is a plethora of research on teaching introductory CS concepts, there is minimal research on how to teach advanced CS concepts, which are closer to real-world needs. The results of my work will empower teachers to create new resources to help students learn complex CS concepts. This research will be the first to explore the effectiveness of Parsons problems beyond introductory courses, creating new interactions and contexts for Parsons problems. This will inspire applying Parsons problems in new ways: incorporating them into more domains in CS curricula, using them for post-school learning such as API tutorials or system documentation, or new situations where engaging with multiple solutions is beneficial. Broader Impact My research is inspired by a desire to make CS concepts more accessible. Code writing problems are “one of the most significant reasons for giving up” by online learners in introductory classes [3]. These techniques will help improve learners’ self-confidence in these areas, with the aim of reducing impostor syndrome and attrition, as a step towards making CS programs more inclusive. The results of this research will be disseminated within top-tier publications in HCI and CS Education. My software engineering experience enables me to make the toolkits developed over the course of my research robust and publicly available. Together, these will help researchers and content creators make knowledge more accessible to a diversity of audiences. [1] https://nces.ed.gov/programs/digest/d17/tables/dt17_322.10.asp [2] Radermacher et al. “Gaps between industry expectations and the abilities of graduates,” SIGCSE ’13 [3] Ericson et al. “Solving parsons problems versus fixing and writing code,” Koli Calling ’17 [4] Denny et al. “Evaluating a new exam question,” ICER ’08 [5] https://js- parsons.github.io/ [6] Wiese et al. “Teaching Students to Recognize and Implement Good Coding Style,” L@S ’17	0
