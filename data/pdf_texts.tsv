title	text	Success
2	"Background:Children who are victims of interpersonalviolence have an elevated risk of engaging in aggressive behavior and perpetrating violence in adolescence and adulthood1. Youth currently in the foster care system are particularly vulnerable to this cycle2,and are considerably more likely than their counterparts to have contact with the criminal justice system. This has created substantial public costs for the United States3. However, while research has shownhigh rates of criminal involvement within foster populations4, not all victimized children engage inviolent behavior. In fact, a significant portion of this population demonstrates relatively uncompromised, or “resilient” functioning5. Factors such as presence of supportive adults, satisfaction in school, and participation in extracurricular activities were found to be protective factors in high-risk populations6. However,access to and enhancement of these resources due to foster placement and stability has been less well studied. Similarly, while risks associated with negative outcomes have been identified, such as previous physical abuse and delinquency in youths’ original families7, an examination of how these factors areamplified and affected by the foster care system has not taken place. So while adverse childhood experiences often predict aggressive behavior4, there is variability in delinquency rates within this population and little agreement about what is responsible for increased or decreased risk of violent behavior. The lack of understanding of contributions to this variability complicates the development of effective social interventions within the foster care system and adds greatly to economic burden in the United States3. Proposed Research:To better understand this variability,I aim to examine the relationship between history of violence and rates of delinquency, focusing on the factors that potentially moderate this association. The proposed study will focus on factors which mayplace children at increasedrisk, but alsofactors that may explain resiliencein this population.My analytic strategy will incorporateboth quantitative and qualitative techniques. Combiningthese approaches will provide multidimensional understanding of complex issues that cannot be obtained through one method alone8. Large scale quantitative research will provide precise foundational information used to conduct the study, analyze the data, identify factors that can be intervened on, and verify the findings. Smaller scale qualitative analyses will expand understanding by exploring subjective factors, identifying other factors not captured in the quantitative data, gaining insight into social processes, and giving voice to participants in the study. First, I will analyze the Midwest Evaluation of the Adult Functioning of Former Foster Youth at Chapin Hall9, self-reported survey data collectedfrom 732 study participants when they were 17 or 18 years old. Within this data set, I will examinehistoryof maltreatment, delinquency, foster care service factors, access to protective factors, and placement satisfaction.History of maltreatmentwill be assessed by The Lifetime Experiences Questionnaire.Delinquencywill be measured by surveys regarding history of arrest, conviction for committing a crime, and overnight stay in a correctional facility. This analysis will also include surveys collected regarding victimization in the past 12 months, as well as perpetrator status in the past 12 months.Foster servicefactorswill be measured by data regarding age at entry into foster care system, number of placements, type of placements, and total length of stay in foster care system.Access to protective factorswill beassessed by The MOS Social Support Survey, surveys regarding the impact of foster care on ability to attend school, and information regarding educational attainment, such as last grade level completed. Lastly,placement satisfactionwill be analyzed by a survey concerning attitudes and satisfaction with most recent placement situations, and a survey regarding the likelihood of turning to the child welfare system for support in the future. A series of linear regression models will explore the effects of maltreatment, foster care service factors, protective access, and satisfaction on delinquency.I aim to(1) comparedelinquency rates for maltreated and non-maltreated foster youth, (2) within maltreated foster youth, identify which factors are associated with increased or decreased delinquency,and(3) determine whether foster care service factors, access to protective factors, and placement satisfaction moderate the relationship between violence exposure and violent behavior within the foster care system. Second, I will conduct qualitative analysis using a smaller sample of foster youth, recruited at University of Illinois at Chicago. I will interview foster care youth (n=50), ages 16 to 18 with a history of maltreatment and delinquency history tounderstandself-reported protective and risk factors in the current operation of foster care.Example questionsare ""What do you think could be improved in your experience with foster care?"", and ""what do you feel has been useful to you during your foster care experience?"". After transcription of these interviews, a code book will then be developed for the identification and interpretation of patterns and themes in the textual data. This qualitative research will allow for better insight into social interactions, foster care delivery processes, and subjective factors that may not have been included in the surveys of the Midwest Evaluation. This qualitative research will generate explanatory models and theories, which will also be useful in the creation of interventions. Working collaboratively with the quantitative data, these interviews will provide valuable insight, allow foster youth to have a voice, and add a more comprehensive analysis to numeric methods. HypothesesI hypothesize thatHypothesis (1)Maltreatedyouth will experience higher rates of delinquency.Hypothesis (2)Within the populationof maltreated youth, higher violence exposure will be associated with lower rates of foster care service factors, access to protective factors, and placement satisfaction.Hypothesis (3)Differences in thesefactors will identify which victims of abuse are more likely to engage in delinquency, and thus strengthen or weaken the association between violence exposure and future violence.Hypothesis (4)The qualitativeinterviews will provide unique insight on complex social issues that will give voice to children in foster care and generate explanatory models that can serve to devise new kinds of interventions. Intellectual MeritThis study will address the relationshipbetween maltreatment and incarceration in a novel way that will advance understanding of variabilities in violent behavior among maltreated foster youth. By interpreting these factors that contribute to variabilities, I will add to research that will determine what is responsible for increased or decreased risk of delinquency and incarceration. As this topic has never been addressed from both qualitative and quantitative perspectives, this study will offer comprehensive and unprecedented data that will begin to address the enormity of cycles of violence among foster youth. FeasibilityThe Midwest Evaluation is a longitudinaldataset that has already been collected and is publicly available with data use agreements. My proposed qualitative interviews will add an unexplored construct to ongoing studies with minimal burden to participants. Preparation and analysis of this data will be conducted with support from Dr. Kathryn Grant, an expert in maltreatment, or with Dr. Elizabeth Raposa, who has worked extensively with foster youth and juvenile delinquents. This NSF award will permit me to pursue coursework that will be critical to the success of my project and allow for dedicated mentored training. The success of the research will be assessed via communication of these results by publications of peer-reviewed first-authored manuscripts, presentations at research conferences such as the International Association for Child and Adolescent Psychology and the International Society for Traumatic Stress Studies, and the development of interventions based on this research. Broader ImpactsThe estimated economic burden resultingfrom cases of child maltreatment in the United States is approximately $124 billion3. An additional $5.1 billion is used annually to incarcerate former foster youth in State and Federal prisons10.Not only are we ethically bound to serve and provide for these underrepresented children, but we are also bound to advance the progress of science and reduce economic burdens for our nation. This research will identify which factors strengthen or weaken cycles of violence, and outline how to specifically support children within the system who have been exposed to violence. This work will enhance and improve foster care procedures that specifically decrease rates of violent behavior and incarceration. My goal is to ultimately improve foster care service factors, access to protective factors, and placement satisfaction. This research will lead to better interventions, decreased rates of incarceration, and therefore a decreased economic burden for maltreatment in the United States. References1. Widom,Science1989 2. McMillen et al.Child Psychiatry2005 3. Fang et al.,Child Abuse Negl. 2012 4. Font et al.Crime Delinquency2021 5.Jones,Soc. Work J.2012 6. Luthar,Developmental psychopathology2006 7. Datta et al. 2019 8. Bartunek& Seo,Journal of Organizational Behavior2002 9. Courtney, Terao, Bost,Midwest Evaluation of theAdult Functioning of Former Foster Youth: Conditions of Youth Preparing to Leave State Care2011 10.Administration for Children and Families, Preliminary Fiscal Year 2009U.S. Department of Healthand Human Services 2010"	Winner!
3	PRIMARY RESEARCH OBJECTIVE: The primary objective of my PhD research is to formulate novel statistical methods for accurately quantifying uncertainty and variability in life-cycle assessments (LCA), the primary method used to assess embodied carbon (greenhouse gas emissions associated with the production, transport, construction,andend-of-lifeofbuildingmaterials).Theresultsofmyworkwill advance LCA research by enabling more robust decision-making,moreaccuratesensitivityanalyses,and enhanced comparability between whole-building LCAs. Academics, practitioners, and policymakerswill thus be empowered to more effectively understand, quantify, and ultimately reduce embodied carbon. Immediate embodied carbon reduction is necessary for curbing the catastrophic effects of the climate crisis, yet the current body of research is severely limited, and few policies or industry standards incorporate embodied carbon into emission reduction strategies. JUSTIFICATION: Although whole-building LCA methodology is standardized by an international coalition of technical standard-setting bodies (ISO 14040/14044),thereisconsiderablevariabilityindata quality, impact assumptions, and scope. This variability is particularly appreciable for biogenic carbon, the physical carbon that is stored in biological materials such as wood, hemp, and straw. Negative biogenic carbon emissions due to carbon storage are treated inconsistently across whole-building LCAs because these assumptions are not standardized. In a five-building case study series, embodied carbon normalized by floor arearangedfrom-936to207kgCO e/m2whenbiogeniccarbonstoragewasincluded 2 in theanalysisand132to557kgCO e/m2whenbiogeniccarbonstoragewasexcluded[1].Scientistshave 2 investigated several approaches to incorporate uncertainty in whole-building LCA, including but not limited to: probability density functions to model building element service-life [2], probability density functions to model manufacturing emissions [3], and more advanced, exhaustive methods that involve conducting aglobalsensitivityanalysisacrossparameterspacesdeterminedviaLatinhypercubesampling [4]. Unfortunately, these modeling practices are not yet standardized or integrated into whole-building LCA. From 2014-2019, 44% of published LCA studies did not mention uncertainty and 36%mentioned uncertainty but did not incorporate it in the analysis [5]. In summary, there is no consensus among available methods to incorporate uncertainty in whole-building LCA, and available methods are seldom incorporated in LCA research. I aim to build upon theseavailablemethodsbyintroducingaprobabilistic framework to standardize uncertainty characterization in biogenic carbon accounting, enabling better decision-making and comparability between whole-building LCAs. Objective 1: Literature Review. Intent: To assess sources of uncertainty and uncertainty characterization methods in scientific literature and industry case studies. Methods: I will conduct a literature review of academic LCA studies,whole-buildingLCAs,material-specificLCAs,andembodied carbon benchmarks. I will select the most appropriate sources and characterization methods for LCA uncertainty. I hypothesize that Monte Carlo simulation with probabilistic modeling will most accurately characterize emission uncertainty for a linear model like whole-building LCA. Objective 2: A Probabilistic Framework for Whole-Building LCA. Intent: To increase the potential for synthesis between data sets for whole-building LCA in academic research and in practice. Methods: Based on my findings from the literature review, I will develop a statistical framework for classifying, characterizing, andquantifyinguncertaintyinLCA.Ihypothesizethatrunningwhole-buildingLCAswith my proposed probabilistic approach will more accurately modelthisuncertainty,whichwillallowme(1) to standardize uncertainty quantification in LCA research and (2) to enable more accurate comparisons and better decision-making. Objective 3: StandardizingBiogenicCarbonAccounting.Intentandbackground:Currentapproaches to biogenic carbon accounting are deterministicandquantifyneitheruncertaintynorvariabilitythatcome from widely varying assumptions. The ‘0/0approach’assumesnetcarbonsequestrationfromtreegrowth balances with end-of-life carbon emissions, and thus, the biogenic carbon storage and its subsequent release are ignored [6]. The ‘-1/+1 approach’ considers biogenic carbon as a negative carbon emission during Life Cycle Stage A (the production stage) with carbon released during Life Cycle Stage C (the end-of-life stage) [6]. Dynamic LCA relies on the regrowth of biogenic carbon put into a building by accounting for forest rotation periods over a period leading up to or during the building life [7].Carbon discounting methods calculate a net present value of carbon emissions avoided with biogenic carbon storage [8]. Ton-year accounting converts the time-value of biogenic carbon storage into a carbon offset equivalent that yields anestimateofnegativeemissionsperton-yearofstorage[9].Insummary,thereisa vital need toformulateanaccurateandstandardizedmodelingmethodologythatproperlyaccountsforthe benefits of biogenic carbon storage in buildings while also quantifying variability and uncertainty in the calculation. Methods: I will formulate stochastic models for biogenic carbon that producemoreaccurate and consistent whole-building LCA results. I hypothesize that introducing these stochastic models to dynamic LCA will yield the most accurate and descriptive results but will becomputationallyexpensive and difficult to implement for regular use. Therefore, I posit that a stochastic model based on ton-year accounting will yield a viable, but easy-to-implement approach. My proposed method (likely to be stochastic ton-year accounting) will enable practitioners to circumvent end-of-life assumptions for biogenic carbon, which are often the source of significant uncertainty. INTELLECTUAL MERIT: Despite being a necessary component to solving the climate crisis, embodied carbon reduction is not widely studied in theUS.LCAistheprimarymethodusedtoresearch, understand, and reduce embodiedcarbon,butdisparatedatasetswithwidelyvariedassumptionspreclude comparison ofexistingstudiesandmoreadvancedanalysis.Withmyresearch,Iwillsteerresearchersand practitioners towards a more complete understanding of the most important data needed to describe the total embodied carbon footprint of a building. With support from the National Renewable Energy Laboratory (NREL) and the University of Colorado, Boulder’s Center for Research Data & Digital Scholarship, I will elevate the standards for embodied carbon and LCA research. BROADER IMPACTS: The two most impactful ways to facilitate widespread, effective embodied carbon reduction are (1) establishingembodiedcarbonbenchmarkingmethodologyand(2)disseminating statistically rigorous embodied carbon reduction tools. Benchmarks: Designers need science-based embodied benchmarks to inform effective target setting, though none currently exist. These benchmarks must describe expectedrangesofembodiedcarbonperusablefloorareaandaccountforvariablessuchas lateral design requirements, building geometry, building type, and soil quality. My research will be integrated with ongoing research in Dr. Srubar’s Living Materials Laboratory to establish embodied carbon benchmarking methodology. Our lab group has collaborators at NREL who are particularly interested in integrating this methodology with their analyses of operational carbon (greenhouse gas emissions associated with building energy use). Embodied carbon reduction tools: Industry-standard LCA software programs are proprietary and provide conflicting results with ill-constrained uncertainty. My research will culminate in publishing an open-source, easy-to-implement software packagetoensure broad implementation by researchers and practitioners. I will then distribute my findings through prominent academic and industry organizations (e.g., CLF, AIA, SEI, UNFCCC, USGBC, WGBC). CONCLUSION: I am uniquely suited to conduct this research because of (1) my first-hand experience with LCA, engineering design work, and computational research, (2) the exemplary leadership I have demonstrated in the embodied carbon space, and (3) my position in a leading embodied carbon research groupwithimportantrelationshipssuchasthatwithNREL.Myresearchwillelevatethestandardforhow academic and industry practitioners conductLCA,whichwillbeessentialformoreeffectivelycombating the climate crisis. I will ensure the findings of my research are disseminated to salient technical communities and so that they are applied ubiquitously in industry and academia. [1] TallWood Design Institute CLT Case Studies (2020); [2]K.Goulotiet.al.,BuildingandEnvironment (2020); [3] M.A. DeRousseau et. al., Journal of Cleaner Production (2020); [4] E. Igos et. al., The International Journal ofLifeCycleAssessment(2019);[5]N.Bamberet.al.,TheInternationalJournalof Life Cycle Assessment (2020); [6] Hoxha et. al., Buildings & Cities (2020); [7] A. Levasseur et. al., Environmental Science &Technology(2010);[8]L.Marshall,A.Kelly,WorldResourcesInstitute(2010); [9]P. Moura Costa, C. Wilson,Mitigation and AdaptationStrategies for Global Change(2000)	Winner!
4	The problem: While most plants rely on soil nitrogen (N), plants capable of symbiotic N fixation (SNF) can acquire N directly from the atmospheric N . Because N is inexhaustible, 2 2 SNF is convenient. However, SNF has a high cost2 due to the need to break the triple bond of N . 2 Following previous literature1,2, I define the C cost of SNF as the respiration (CO flux) needed 2 to drive SNF divided by SNF itself (N flux). Biochemical calculations estimate the C cost of 2 SNF to be slightly higher than using nitrate and much higher than using ammonium1. Measurements of these costs in nodules (the root structures that house symbiotic bacteria) have been close to the biochemical predictions2. However, these measurements have been carried out at constant temperatures. As explained below, the cost might vary widely across temperature. The cost of SNF helps determine its effectiveness, both within a plant (using SNF vs. soil N) and across species (competition between N-fixing and non-fixing plants). A lower cost makes SNF viable even when soil N is abundant, whereas a higher cost makes SNF untenable even when soil N is scarce. Therefore, variation in the cost of SNF across temperature would have far- reaching implications. For example, it could help explain why N-fixing trees are successful in warm areas3, and could also affect how SNF will change with climate. Despite its importance to fundamental biology, research on temperature responses of SNF has long been beset by technological constraints. Using a novel method that overcomes these constraints, I will ask one main question: What is the temperature response of the C costs of SNF? I will address this question using the tree Robinia pseudoacacia, which lives across a wide climatic range, accounts for 64% of tree-based SNF in the contiguous USA5, and is common across Eurasia3. Hypotheses: My hypotheses are based on previous measurements of the components of the cost: respiration and SNF itself. Previous work6 has observed that SNF plummets at low (near 0°C) and high (near 50°C) temperatures. There are few data on nodule respiration at different temperatures, but leaf respiration continues across 0-50°C7, suggesting nodule respiration might too. Respiration rates well above 0 divided by SNF rates near 0 mean that (H1) the C cost of SNF will be well above the biochemical predictions at low and high temperatures. My hypotheses about temperature optima stem from measurements from my lab, which recently developed a system for non-destructive, extremely sensitive, and continuous measurements of nitrogenase activity6. Preliminary research using this system has shown that the optimal temperature for SNF is much higher (29-36°C) than previously assumed (25°C)4, as shown in Fig. 1a. I do not know how nodule respiration will change with temperature, so I have competing hypotheses. (H2a) If respiration peaks near the same temperature as SNF (green curve, Fig. 1b), then the C cost will have a similar temperature optimum as SNF (green curve, Fig. 1c). (H2b) Alternatively, if respiration rises continually (blue curve, Fig. 1b), as leaf respiration does6, then the temperature optimum of the C cost will be lower than the optimum of SNF (blue curve, Fig. 1c). Hypotheses H1, H2a, and H2b are represented in Fig. 1c, which also shows the equation for the C cost of SNF that is used in many models8(black curve). This equation assumes that the change in C cost of SNF with temperature is inversely proportional to the SNF rate and is scaled to the biochemical C cost of SNF (7.5-12.5 g C g N−1). As explained above, I believe this model is flawed as it does not account for how nodule respiration changes with temperature. Methods: Using growth chambers at Columbia University, I will grow 30 Robinia pseudoacacia seedlings (from seed) under a temperature regime of 26°C during day and 20°C during night using a 14-hour light and 10-hour dark photoperiod with relative humidity and CO₂ concentrations of 70% and 400 ppm to emulate controlled climate conditions6. The seedlings will be inoculated with slurries of crushed nodules as well as bacteria cultured from these nodules to ensure the plants can establish symbiotic partnerships, and will be fertilized with limited levels of N (1.5 g N m−2 yr−1) but ample amounts of all other nutrients to promote SNF. The nodules will be measured for SNF and respiration continuously across 1-50°C over the course of 3 hours. The excised nodules will be placed in a sealed chamber with 2% acetylene (the concentration at which the system measures nitrogenase activity most precisely and accurately6). After accounting for leakage and other factors6, the rate at which acetylene is reduced to ethylene (measured with a Picarro G2106 laser) gives a measurement of nitrogenase activity9. Preliminary work in our lab has shown that Robinia nodules have stable nitrogenase activity at least 6 hours after excision, and that the ratio of 15N to acetylene reduction is stable 2 across the temperature range of our study (TA Bytnerowicz, pers. comm.). CO₂ flux in the chamber will be synchronously measured by a Licor LI-62626 to determine the temperature response of nodule respiration. I will process and analyze the data by modifying R scripts previously developed in the Menge lab (ref. 4 for processing, TA Bytnerowicz, pers. comm. for temperature responses of SNF). The analysis will yield temperature response curves for SNF, respiration, and the ratio of the two (the C cost of SNF). Intellectual Merit: This research will answer questions fundamental to the biology of the symbiotic relationship between legumes and N-fixing bacteria. At the level of plant ecophysiology, at what temperatures is it energetically favorable for Robinia pseudoacacia to fix N? At the level of community ecology, how does Robinia pseudoacacia compete against non- fixing plants if it relies on SNF? Broader Impacts: The paucity of knowledge on how SNF and its C cost respond to temperature has been a major constraint on global biogeochemistry and climate modeling. As described above, temperature response functions for SNF and for the C cost of SNF are already in use in terrestrial biosphere models, despite few data for the temperature response of SNF itself and zero data for the temperature response of the C cost of SNF. My work will lead to direct improvements in the representation of SNF in these models, and thus will directly influence our ability to predict global biogeochemistry and climate change. In addition to publishing in academic journals, I will present my work at academic conferences, such as SACNAS’ National Diversity in STEM Conference, and outreach programs, such as the Ecological Society of America’s SEEDS program and Women In Science at Columbia (WISC). As a former McNair Scholar, I am well aware of the disparity of resources within underrepresented populations. Because of this, I willalso contribute my mentorship to the Environmental Justice and Urban Ecology Summer Research Program, a funded program for high school students at the Washington Heights Expeditionary Learning School. 1Gutschick 1981, The American Naturalist. 2Tjepkema & Winship 1980, Science. 3Steidinger et al. 2019, Nature. 4Houlton et al. 2008, Nature. 5Staccone et al. 2020, Global Biogeochemical Cycles. 6Bytnerowicz et al. 2019, Methods in Ecology & Evolution. 7Heskel et al. 2016, PNAS. 8Fisher et al. 2010, Global Biogeochemical Cycles. 9Hardy et al. 1968, Plant Physiology.	Winner!
5	Key Terms: wildfire, water stress, land surface temperature Motivation: The primary wildfire monitoring system in the United States is the National Fire Danger Rating System (NFDRS)1,2. NFDRS maps are routinely used by land managers and regional governments to allocate fire mitigation resources and track fire risk. Although NFDRS is a standard risk assessment tool, it faces several disadvantages. Calculation of a NFDRS rating requires advance knowledge of site conditions and manual input of user parameters into closed source software. For non-experts, NFDRS is difficult to use. Despite the complexity of NFDRS, its primary fire danger metric is a coarse 5-level categorical scale (from “low” to “extreme”) that cannot be forecasted beyond 24 hours. This project will improve wildfire modeling with empirical techniques that enable proactive wildfire mitigation and long-term forecasting. Objectives: Climate change is expected to produce more intense wildfires more frequently in the American West3. Wildfires are intensified by high plant biomass (i.e. fuel load) and low fuel moisture. Both of these factors can be remotely sensed over large areas4,5. Given that vascular effects of water stress linger in plants weeks to months after a drought6, drought conditions early in spring may predispose water-stressed forests to wildfire the following summer. This project will produce remote sensing data streams of plant water stress and vegetation growth as inputs for an open source wildfire predictive model covering forested regions of the western United States at 1 km2 spatial resolution. I hypothesize that (1) water stress in early spring increases wildfire intensity the following summer, (2) fuel load can be estimated from a time series of a vegetation growth index and (3) by measuring these variables in early spring (Fig. 1a), wildfire-prone regions can be identified weeks before ignition (Fig. 1b). In short, water-stressed locations with high plant biomass will be identified as wildfire hotspots before the fire season begins. I will perform computationally-intensive spatial analysis using open source cloud infrastructure to ensure usability by non-experts. Aim 1: Calculate and validate water stress index. Water stress can be estimated quantitatively from canopy temperature (i.e. leaf temperature) and vapor pressure deficit using the crop water stress index (CWSI)5. CWSI is related to evapotranspiration and is applicable to all leaved plants. To calculate CWSI, I will use vapor pressure deficit at daily temporal resolution and 1 km2 spatial resolution from Daymet, a continuous, Figure 1. (a) Raster stacks of CWSI and GSI gridded meteorological product covering the become a time series for each grid cell. (b) Early contiguous United States7. To determine plant stress may indicate wildfire hotspots. canopy temperature, I will use land surface temperature (LST) calibrated with the normalized difference vegetation index (NDVI)8. My source of LST and NDVI data will be the Terra MODIS satellite, for which cloud-free, gap-filled LST data have recently been developed9. This method of determining canopy temperature requires only occasional NDVI values, so clouds will not prevent canopy temperature measurement8. Although MODIS is approaching retirement, its 20-year data archive is desirable. For recent fire years I will also work with the current-generation LST sensor ECOSTRESS. A field campaign will be performed in fire-prone western forests to calibrate CWSI, to validate canopy temperature measurements, and to determine the relationship between CWSI and plant water potential. Aim 2: Calculate and validate fuel load index. I will employ the growing season index (GSI) to estimate fuel load over time. The GSI quantifies how plant growth is limited or unconstrained by humidity, air temperature, or photoperiod4. GSI therefore remains measurable under all weather conditions using Daymet data, unlike spectral biomass indices such as the leaf area index. GSI will be validated against in situ measurements of fuel load as part of the field campaign in Aim 1. Aim 3: Model wildfire likelihood. I will use an existing dataset of wildfire occurrence from 2000- 2019 to produce annual wildfire presence maps aligned on the same grid as the CWSI and GSI calculations. For each year of data, I will calculate CWSI and GSI for each grid cell at daily temporal resolution from February 1 to May 31 to produce a stack of CWSI and GSI grids through time (Fig. 1a). Possibly, the end of the data collection period will be adjusted later or earlier in the year to optimize model performance and parsimony. Each cell of the raster stack will be inserted into a high-dimensional dataset where each row is a time series of CWSI and GSI values and a single column indicating whether the cell experienced fire that year. I will use the CWSI and GSI time series as predictors in a partial least squares regression (PLSR) model with a logarithm link function. PLSR reprojects the predictor variables in a typical linear regression to a lower- dimensional space that is maximally correlated with the response matrix. PLSR therefore resolves issues with correlated predictor variables and, via weights, identifies which CWSI and GSI measurements contribute most to the model prediction. Wildfire occurrence is a binary response variable, so a logarithm link function will enable calculation of wildfire likelihood for each cell in the modeling area. A variant of PLSR for binary classification tasks, partial least squares discriminant analysis, is also a candidate modeling approach. I will evaluate the proposed model with standard measures for a binary classifier with an imbalanced response variable. I will also compare the proposed model against NFDRS maps produced on May 31 the same year. Project success is defined as accurate prediction of 1 km2 pixels as fire-present or fire-absent, emphasizing a low false negative rate, and a model which land managers prefer over existing NFDRS maps for allocating management resources. Intellectual Merit: This study will improve wildfire prediction by employing empirical methods and longer forecasting times. Improved wildfire modeling will enable proactive wildfire mitigation and clarify factors that drive wildfire occurrence. In particular, this project will produce the most spatially extensive measurement of forest water stress to date and determine how wildfire is influenced by water stress early in the growing season. This study will also demonstrate how remote sensing datasets can be combined to model ecosystem processes. Broader Impacts: Wildfire intensity and frequency is expected to increase in the American West as climate change continues3. Wildland firefighters must effectively allocate tens of billions of dollars to mitigate this annual emergency. Advance warning of fire risk will enable proactive management that utilizes resources effectively. Such warnings also enable the public to avoid injury and property damage. In society at large, less wildfire smoke reduces respiratory illness and avoids air travel disruption. Knowledge of forest response to drought conditions will also improve timber production in the logging industry. All computer code and data products generated by this project will be open source. Model results will be distributed via a non-technical online dashboard. I will demonstrate the analysis workflow at workshops and reach out to potential users who would be interested in using the wildfire model produced by this work. References: [1] U.S. Forest Service p. INT-GTR-169. [2] W.M. Jolly, National NFDRS 2016 Rollout Workshop (2018). [3] A.P. Williams et al., Earths Future 7, 892 (2019). [4] W.M. Jolly et al., Glob. Change Biol. 11, 619 (2005). [5] S.B. Idso et al., Agric. Meteorol. 24, 45 (1981). [6] C.R. Brodersen et al., Annu. Rev. Plant Biol. 70, 407 (2019). [7] J.T. Abatzoglou, Int. J. Climatol. 33, 121 (2013). [8] M. Blum et al., Agric. For. Meteorol. 176, 90 (2013). [9] S. Shiff et al., Sci. Data 8, 74 (2021).	Winner!
6	Introduction Many metabolites are reactive and unstable, making them prone to undesired chemical modification outside of the intended pathways1. While metabolism as a whole is well-studied, the biochemical mechanisms of managing such reactive metabolites are not. Proteins which are closely metabolically involved with each other can frequently be found in protein-protein interactions which are essential for nearly all cellular function. It is currently believed that most, if not all, proteins participate in protein-protein interaction networks2. Such a network suggests the possibility of metabolic substrate channeling, in which a metabolite travels from one enzymatic active site to another without freely diffusing into the surrounding medium. Evidence for substrate channeling has been observed in enzymes of many major biochemical pathways and is being increasingly recognized as foundational to metabolic regulation3. Through substrate channeling, an intermediate metabolite may be retained for use in a specific pathway, protected from degradation, or prevented from causing damage to the cell4. The goal of my proposed research is to discover mechanisms by which unstable metabolites are managed in biochemical systems. This research will provide insight into cellular control over reactive metabolites, protein-protein interactions, and substrate channeling. To achieve this goal, I will uncover mechanisms of substrate channeling for the reactive metabolite Δ1-pyrroline-5-carboxylate (P5C). P5C is an unstable intermediate at the intersection of proline, glutamate, and ornithine metabolic pathways (Figure 1). P5C has been shown to react with other metabolites and inhibit enzymes and is linked with human disease including hyperprolinemia type II5. Under physiological conditions, P5C exists in dynamic equilibrium with glutamic semialdehyde (GSA) via spontaneous nonenzymatic hydrolysis and condensation reactions. In the proline biosynthetic pathway, P5C serves as an intermediate for production of proline from glutamate. Glutamate is reduced to GSA by P5C synthetase (P5CS), then P5C is reduced to proline by P5C reductase (P5CR). Similarly, the glutamate biosynthetic pathway uses P5C as an intermediate in the production of Figure 1. Involvement of P5C with proline, glutamate from proline. Proline is oxidized to P5C by glutamate, and ornithine metabolism. proline dehydrogenase (PRODH), then GSA is Adapted from Stránská et al6. oxidized to glutamate by P5C dehydrogenase (P5CDH). In an alternative pathway, P5C can be produced or consumed by ornithine aminotransferase (OAT), which uses ornithine as a reactant or product for P5C production or consumption. Under normal physiological conditions, OAT functions in the “forward” direction from ornithine to P5C, however, under extreme dysregulation, OAT may catalyze the opposite “reverse” reaction from P5C to ornithine6. Aim 1: Protein-protein interactions among enzymes of the proline/glutamate/ornithine pathways. Existing research already supports protein-protein interactions between PRODH and P5CDH7. However, the involvement of other enzymes in the proline, glutamate, and ornithine pathways is unknown. This aim will focus on potential protein-protein interactions between P5CS–P5CR, OAT–PRODH, and OAT–P5CDH. Because of the current discord surrounding the cellular locations of P5CS and P5CR (e.g., mitochondrial vs. cytosolic), OAT–P5CS and OAT–P5CR complexes will also be considered. In fact, the expression in some organisms of ornithine cyclodeaminase, which directly catalyzes ornithine to proline, supports potential protein-protein interactions between OAT and P5CR8. Enzymes used in these studies will be expressed as His-tagged proteins, as done previously with PRODH and P5CDH7. Stable protein-protein interactions will be examined through coimmunoprecipitation with anti-His-tag antibodies and supplemented with pull-down assays via Ni-NTA chromatography. In each experiment, the identification of specific proteins will be accomplished through Western blot. Transient protein-protein interactions will be observed through surface plasmon resonance (SPR) at the BIAcore 3000 instrument maintained by the Nanomaterials Characterization Core Facility at the University of Nebraska Medical Center. For SPR, an enzyme’s His-tag will be used to anchor the protein to a Ni-NTA sensor chip. SPR will allow the characterization of protein-protein interaction association and dissociation constants. For cellular characterization of protein-protein interactions, fluorescence resonance energy transfer and yeast two-hybrid system experiments will be carried out2,7. Aim 2: Kinetics of P5C channeling among enzymes of the proline/glutamate/ornithine pathways. Even in the case of weak, transient protein-protein interactions, substrate channeling may occur. Substrate channeling of P5C has been identified in the glutamate biosynthetic pathway but has yet to be characterized among other protein pairs of the proline, glutamate, and ornithine pathways7. Previous studies with PRODH and P5CDH have used a free diffusion two-enzyme model to simulate reaction progress curves. In these models, the presence of substrate channeling can be inferred by comparing theoretical non-channeling versus experimental differences in transient time7. In addition, P5C trapping studies will be carried out to support results from the simulated progress curves. In these experiments, ortho-aminobenzaldehyde (oAB) is conveniently used to “trap” P5C in a spectrophotometrically-monitored oAB–P5C complex. In the presence of P5C channeling, less P5C will freely diffuse to be complexed with oAB as compared to a negative non-channeling control. In addition, I will use stopped-flow kinetics to measure the transient time of P5C channeling between the enzymes. Intellectual Merit My proposed graduate research plan to examine unstable metabolite management is a natural continuation of research I have been involved with in the Becker lab at the University of Nebraska-Lincoln. Previously, I conducted research investigating P5C channeling in the glutamate biosynthetic pathway, so I am well-prepared to expand into protein-protein interactions and broader aspects of substrate channeling. I have experience with fundamental methods used for protein and metabolic research, including protein overexpression, purification, UV-visible spectroscopy, stopped-flow, and steady state enzyme kinetics. This research will be well-supported in the Becker lab at the University of Nebraska-Lincoln, home of the Center for Biological Chemistry and the Redox Biology Center. My projects will be pursued in collaboration with existing structural biology partners in the Tanner lab at the University of Missouri. Broader Impacts It is estimated that over 80% of proteins rely on protein-protein interactions2, and substrate channeling has been identified in many major metabolic pathways3. My research will provide specific knowledge in unstudied interactions and channeling between proteins of the proline, glutamate, and ornithine pathways, offering insight into the biochemical methods of unstable and reactive metabolite management. Additionally, this research will serve as a case study to lay the foundation for metabolite management experiments in other major pathways. Research surrounding P5C has implications for biological mechanisms of metabolism relevant to all life. Outside of strict academics, I will leverage this graduate research in a way that benefits the scientific community and the general public. I will present results at regional and national conferences and publish using accessible language in open-access journals. Through this research, I will take advantage of important mentorship opportunities. Mentoring undergraduate or beginning graduate students in the lab will allow me to help develop the next generation of scientists as I provide a rigorous yet supportive environment to foster academic, professional, and personal growth. References 1Lerma-Ortiz et al. Biochem. Soc. Trans. 2016. 2Berggård et al. Proteomics. 2007. 3Sweetlove et al. Nat. Commun. 2018. 4Liu et al. Arch. of Biochem. & Biophys. 2017. 5Farrant et al. J. Biol. Chem. 2001. 6Stránská et al. Plant Sig. & Behav. 2008. 7Sanyal et al. J. Biol. Chem. 2015. 8Goodman et al. Biochem. 2004.	Winner!
7	billion breeding birds11. Among the species that showed some of the steepest declines were migratory shorebirds, one of which is the Lesser Yellowlegs (LEYE, Tringa flavipes). LEYE, a once common bird that breeds in the boreal forest, has declined by 80% range-wide since 19661,3 and is estimated to lose an additional 50% of its global population within 11 years9. As a result, LEYE has been designated as federally threatened in Canada and a species of high conservation concern in the U.S.9 This species likely encounters multiple threats during its 8000-mile migration journey3, but agricultural practices in one of their most critical stopover regions, the Prairie Pothole Region (PPR) have the potential to impact much of the breeding population3. Reductions in survival due to exposure to agricultural insecticides in the PPR is one novel hypothesis that has been proposed to explain why many shorebirds in North America have declined, including LEYE. However, this hypothesis has not been thoroughly explored. Investigating population-level threats to rapidly declining species like LEYE is a critical conservation priority. The migratory period may be the most critical to annual survival due to the high energetic demands that if not fulfilled can lead to reduced survival and reproductive success10. However, to ensure a timely arrival at breeding and wintering sites, migratory birds must balance their time spent refueling at stopover sites with their migration speed10. The optimal bird migration theory predicts that migrants constrained by time should adjust their stopover duration to their refueling rate, and thus minimize time spent on migration to maximize their fitness2. Research has shown that individuals with low refueling rates depart later from their stopover sites relative to individuals with higher refueling rates, indicating that birds wait until they reach a threshold of fuel stores before departing10. This suggests that the quality of stopover habitat affects the decision of when to leave a stopover site, which is of critical importance for migration success. During migration, shorebirds are exposed to neonicotinoids6, the most widely used class of insecticides in the world, which pose significant risks to birds and other wildlife2,4. Neonicotinoids cause impaired immune function, rapid reduction in food consumption, and lower reproductive success, which can result in greater energetic demand, reduced fat stores, delayed migration and low survival1,4. Because migration delays can carry over to affect survival and reproduction1, neonicotinoids have the potential to impose population-level impacts. Although their adverse impacts have been established in songbirds1,4, we have little information regarding their effect on shorebirds, highlighting a critical information gap. In a recent study, GPS transmitters were deployed on over 100 LEYE in Alaska and Canada3. Of the birds that bred west of James Bay, Ontario, 90% stopped in the PPR to refuel during their migration to South America, with stopover duration times varying from a few days to over a month, indicating the importance of this region during migration. High presence of neonicotinoids has been reported in these prairie wetlands and agricultural fields5, which are important foraging habitats for migrating shorebirds. Proposed Research: Using the optimal bird migration theory as a framework, I will investigate the threat of neonicotinoids on the fitness and migration of fourteen shorebird species of high conservation concern9 that heavily rely on the PPR. This study will investigate a potential contributor to the observed population declines of shorebirds and will help guide on-the-ground management decisions for agricultural solutions. Hypothesis: Migrating shorebirds with high plasma concentrations of neonicotinoids will be physiologically and behaviorally impaired relative to birds with low concentrations. Similar to what has been observed in studies of captive birds1, I predict that wild shorebirds with high neonicotinoid concentrations will exhibit: A) lower plasma triglyceride and higher uric acid levels, indicating lower fueling rates and fat deposition, B) poorer body condition (measured by body mass and fat scores), C) reduced foraging behavior, D) prolonged migration stopovers, and E) later migration departure dates. Research Plan: To establish an environmental gradient in pesticide contamination, I will pre-screen wetlands by measuring neonicotinoid concentrations in water samples. At sites with low and high concentrations of this pesticide, I will capture LEYE and thirteen other shorebird species, collect blood samples, and measure body mass and fat over two fall and two spring migration seasons. I will measure the concentrations of neonicotinoids in bird plasma as well as key metabolites in blood using cutting- edge LC-MS/MS techniques8. Prediction A: I will measure plasma concentrations of triglycerides and uric acid and correlate them to plasma neonicotinoid concentrations7, thereby testing for a link between pesticides and fuel deposition rates. Prediction B: I will compare body mass and fat scores of birds with high, moderate, and low neonicotinoid concentrations to better understand how neonicotinoids affect body condition. Prediction C: I will conduct behavioral surveys on shorebirds at high and low contamination wetlands to determine if there is a relationship between neonicotinoid exposure and foraging behavior. After randomly selecting an individual, I will record the length of time spent in different behavior categories (foraging, resting, etc.) for a duration of 5 minutes. This will be repeated for 10 individuals per wetland. To account for time and weather, I will conduct surveys in the morning and will record temperature, wind, and cloud cover. Predictions D & E: To understand if neonicotinoids are impairing migratory ability and causing migration delays, I will deploy Lotek PinPoint GPS transmitters that will allow me to track the migration, departure dates, and stopover durations of birds with varying levels of neonicotinoid exposure. The results of this study will provide critical information on how environmental contaminants interfere with optimal migration. To minimize confounding factors, I will only capture adults, and will stratify results by sex, species, and migration season. Facilities & Mentorship: I have two mentors: Dr. Christy Morrissey at the University of Saskatchewan and Dr. Courtney Conway at the University of Idaho (UI) where I will matriculate. Dr. Morrissey is a global leader in avian ecotoxicology and has developed novel and extremely sensitive methods for neonicotinoid analysis8. Dr. Conway is a renowned expert in ecology and migration of birds. Intellectual Merit: Regional efforts to study neonicotinoids in songbirds1 and wetlands6 in the PPR are ongoing and our project expands on this by investigating the effects of neonicotinoids on shorebird health, a novel yet timely research topic. This project would build upon an existing and growing partnership among 8 state agencies, federal agencies, South American agencies, and universities in both the U.S. and Canada, as well as farmers and landowners in both countries. My findings will advance the fields of migration ecology and ecotoxicology and will be highly applicable to developing conservation strategies for shorebirds in the PPR because it will improve our understanding of the effects of agricultural insecticides. This project aligns with the 3-Billion Birds Campaign11 to reverse population declines and is part of an international effort to understand threats impacting LEYE throughout their annual cycle. This study fills a critical information gap by investigating a major threat during migration that may have carry over effects to survival and reproduction and will inform managers and farming communities about the effects of agricultural insecticides on birds. Broader Impacts: To increase participation of underrepresented minorities in STEM, I will develop an internship opportunity through the Doris Duke Conservation Scholars Program at UI that will engage students from diverse backgrounds to participate in my research and develop their own independent projects. To improve STEM education and outreach, I will create a program called Backyard Bird Banding for underrepresented students from rural schools and tribal communities to watch how we capture and band shorebirds and participate when deemed appropriate. I will enhance the experience with engaging kid-friendly games and shorebird ID cards for teachers and students to use while out in the field. This event will be recorded and made publicly available world-wide on social media. I plan to develop this program with the following rural ND schools: Glenburn, Kenmare, and Turtle Mountain Community High School. In addition to hands-on field activities, I will use real-time shorebird migration data to link schools through social media platforms in ND and Alaska, and through the Outreach International Environmental program in South America. Students will be able to track the migrations of birds tagged in or passing through their neighborhoods via Movebank, an animal tracking database. Our outreach goal is to engage with at least 200 students in our programs. To increase public engagement, I will develop high-impact outreach and educational materials about shorebird friendly agricultural practices and alternative biological pesticides in the PPR. I will work closely with the Lesser Yellowlegs working group, the Coalition for Conservation & Environmental Education, farmers, and landowners in the PPR to find practical, long-term solutions that will benefit bird populations and farming communities. 1Eng et al. 2019. Sci. 365:1177; 2Alerstam et al. 1990. Bird Mig. 331-351 ; 3McDuffie et al. 2021. Pro. 1- 134; 4Gibbons et al. 2015. Env. Sci. Poll. Res. 22:103; 5Main et al. 2014. PLOS 9:1; 6Malaj et al. 2020. Sci. Tot. Env. 1-10. 7Li et al. 2020. Nat. Sus. 8Bianchini et al. 2018. Env. Sci. & Tech. 52:13562; 9U.S. Shore. Cons. Plan. 2016. 10Zhao et al. 2017. Move. Eco. 5-23; 11Rosenberg et al. 2019. Sci. 120-124.	Winner!
8	Hypothesis: Ocean wave energy has vast potential as a renewable power source, but traditional sequential design methods perpetuate prohibitively high device costs. Applying systems optimization techniques to wave energy would unlock new architectures that are cost-competitive at utility scale. Introduction: Climate change is the most critical problem facing humanity. It threatens warming, flooding, erosion, and storms that damage infrastructure, agriculture, health, and biodiversity. The fatal potential for 4oC of warming can be limited to 1.4oC if complete decarbonization occurs by 2055 [1]. The electricity sector contributes 40% of global CO emissions, representing perhaps the largest challenge and 2 opportunity for decarbonization [2]. Existing renewables have limitations: wind and solar are intermittent and unpredictable; hydropower and geothermal have few suitable sites. To reach 100% clean electricity, we must supplement these sources by developing diverse renewables to technical and economic maturity. Ocean wave energy is uniquely attractive due to its predictability, geographic abundance, and continuous availability [3]. Despite an ability to fulfill 34% of US electricity demand [4], wave energy technology’s high cost, long design cycles, and risk-intensive investment have prevented full-scale deployment [3]. The wave energy converter (WEC) industry has so far followed a trajectory similar to aerospace, focusing on technical risk reduction with an expectation that costs will fall after the technology matures. However, recent analysis indicates that this path is infeasible, and success hinges on a reinvention of the typical design cycle to emphasize early cost and performance innovation before deploying expensive prototypes [5]. Multidisciplinary Design Optimization (MDO) and Control Co-Design (CCD) techniques can provide the design paradigm shift that is required for dramatic cost reduction. MDO and CCD are emerging techniques that depart from the standard sequential design process by considering subsystem interactions early on. MDO is an optimization framework for interdisciplinary design problems. MDO has been used successfully in the automotive, energy, and aerospace sectors but never attempted for wave energy due to novelty and computational costs [6]. CCD, the use of control principles to inspire device design, has never been applied to a utility-scale WEC for similar reasons. CCD offers significant benefits: in offshore wind turbines, CCD decreased structural loading by 99% [7], and one estimate predicts 30% cost reduction potential for wave energy [8]. Overall, MDO and CCD are promising methods to advance wave energy design towards full decarbonization of the electricity sector. Research Plan: This project will be completed over the course of my PhD studies at Cornell University in the Symbiotic Engineering Analysis (SEA) Lab, led by Professor Maha Haji. Objective 1: Develop a novel WEC design framework using principles of MDO and CCD. I will create a multidisciplinary model of WEC dynamics and performance. As shown in Fig. 1, the model will bridge previously disparate simulations in controls, structures, powertrain, and hydrodynamics, which are recognized as the four most impactful subsystems to drive down WEC costs [9]. I will prioritize appropriate model fidelity, simplifying wherever feasible while still capturing important subsystem interactions. One key tradeoff is the decision to model in the stochastic, frequency, or time domain. Degrees of freedom will be selected strategically, balancing computational tractability against flexibility to describe diverse architectures. Objective 1 is attained when a design optimization process can be clearly articulated and validation efforts show that the model can correctly predict performance of existing WECs. This framework forms the foundation for the second research stage. Objective 2: Utilize the design framework to obtain a novel WEC architecture that is cost- competitive at utility-scale. The MDO-CCD model and process developed in Objective 1 will be implemented and iterated upon, focusing on interactions between device controls and structures. Initially, optimization will identify the best combination of existing configurations; ultimately, model degrees of freedom will be extended, allowing optimization to yield new architectures altogether. Sensitivities will quantify impacts to inform further innovation. Objective 2 is complete when a WEC design with levelized energy cost below $0.30/kWh is found, representing a 75% cost reduction from current technology [10]. Objective 3: Validate key features of the optimal design through wave tank testing. The final stage of research involves the detail-design of a scaled prototype of the optimal solution, followed by the manufacturing, testing, and data analysis of the prototype. The testing investigates real-time control implementation, validates model robustness, and provides a practical industry-relevant realization of the new design process and design. Cornell University is equipped with the facilities to enable this testing through the DeFrees Hydraulics Lab wave tank, and collaborations with nearby institutions including the University of New Hampshire or the University of Maine could be leveraged for larger wave tank access. Objective 3 is complete when any major differences between model and test are identified and explained. Intellectual Merit: Wave energy is an under-utilized and under-researched renewable source with great potential. Multidisciplinary integration is expected to be a key enabler of future cost- competitive wave energy. The proposed work will be the first to apply MDO and CCD techniques to a utility-scale wave energy converter, potentially unlocking radical cost savings. My design process will be the first to unify disparate WEC domains; my optimization will provide new insights to advance the field toward design convergence; and my test data will confirm understanding and applicability of these novel contributions. My undergraduate background provides relevant technical depth in mechanical design, power electronics, hydrodynamics, and controls, and above all I have the systems mindset to effectively unify these fields. This experience, coupled with the SEA Lab’s expertise exploiting synergies in marine technologies, uniquely qualifies me to succeed at the proposed research. Broader Impacts: Wave energy has the potential to meet 34% of national electricity demand [4]. My research will enable cost-competitive wave energy, adding to the renewables mix to combat climate change. The integrated MDO-CCD design process applies widely to any system with multidisciplinary interactions and embedded dynamic controllers. Thus, the proposed research advances knowledge in both renewable energy alternatives and systems optimization methods, both of which broadly benefit society. To circulate my research to the academic community, I will publish in journals such as Ocean Engineering and Renewable Energy and present at diverse venues such as IEEE OCEANS and CESUN. Leveraging my advisor’s connections, I will initiate collaborations with the National Renewable Energy Lab and companies such as CalWave. These partnerships will amplify the impact of my research by ensuring its alignment with current industry goals and its rapid dissemination to others upon completion. Finally, I will engage in outreach to both the general public and students of all backgrounds. Societal acceptance is key for widespread adoption of wave energy technology, and education about the opportunities and challenges of renewable energy encourages sustainable habits as well as an interest in engineering. My contributions to STEM outreach and curriculum development through SWE, Splash, and GRASSHOPR are detailed in my personal statement. In sum, my work as an NSF fellow would promote STEM engagement, advance systems design techniques, and enable a carbon-free future. References: [1] “Climate change 2021,” IPCC, 2021. [2] “Net zero by 2050,” IEA, 2021. [3] F. Mwasilu and J. Jung, IET Renewable Power Gener, 2019. [4] L. Kilcher, et al., NREL, TP-5700-78773, 2021. [5] D. Bull et al., Sandia, SAND2017-4507, 2017. [6] J. Sobieszczanski-Sobieski and R. Haftka, Struct. Optim., 1997. [7] X. Du, et al., ASME 2020 Int. Mech. Eng. Congr. and Expo., 2021. [8] M. Garcia-Sanz, ARPA-E, 2018. [9] D. Bull, et al., Sandia, SAND2013-7204, 2013. [10] G. Chang, et al., Renewable Energy, 2018.	Winner!
9	of bacteria (~99%), followed by archaea, then eukaryota, collectively forming the human microbiome.1,2 Over the past decade, research has revealed impacts of colon microbiota on human physiology, including roles in digestion, metabolism, immune system regulation, hormone signaling, and development.3,4 Colon microbiota reside in a specialized niche—a layer of mucus secreted by the colon epithelium, heavily comprised of the gel-forming protein MUC2.5 Adenomatous Polyposis Coli (APC) is a scaffolding protein that has long been studied as a tumor suppressor antagonist of the Wnt/-catenin signaling pathway, with additional roles less defined. In Dr. Kristi Neufeld’s lab at the University of Kansas, we have accumulated evidence suggesting multiple novel functions of APC, including in the expression of MUC2. Given this evidence, I hypothesize that APC directly or indirectly promotes MUC2 expression, and therefore has a role in colonic mucus generation, microbiome homeostasis, and colon function. Uncovering roles of APC in MUC2 expression would fill a critical knowledge gap not only in our understanding of the human microbiome and its homeostasis, but also in the cellular function of highly conserved APC-like orthologs found nearly universally across invertebrates and vertebrates.6 In a recent study, using Human Colon Epithelial Cells (HCECs), we found that reduction of APC led to a 75% decrease in IL-1R, an integral membrane receptor protein that, when bound to a ligand, promotes MUC2 expression.7 We found a similar trend in the IL-1R ligand, IL-1. To further investigate the relationship between APC, IL-1 signaling, and MUC2 expression, we modified a human colon cancer cell line, DLD-1, using CRISPR/Cas9. We inserted wildtype (WT) APC under control of a doxycycline (Dox) responsive promoter, which allowed us to treat cells with Dox and induce expression of WT APC. Using unmodified (parental) DLD- 1s as a control, we found that IL-1 and WT APC individually increased MUC2 expression 2-fold, but when expressed simultaneously, MUC2 expression increased 15-fold (Fig. 1).7 Figure 1. RT-qPCR for MUC2 in This evidence suggests that IL-1 signaling and APC act Parental and APC-Inducible DLD-1 synergistically to promote MUC2. This evidence also suggests that cells. (Two-way ANOVA with IL-1R activation increases MUC2 expression, which may occur Tukey’s range test, **P <0.005, through signaling of PKC-, a protein previously correlated with IL- ***P<0.0005, ****P<0.0001) 1R activation and MUC2 expression.8 To investigate DNA- binding of APC that might be involved in MUC2 promotion, we used a dataset from a recent ChIP-seq study9 and found that APC binds to the promoter of the MUC2 gene. Combined with our APC gain and loss of function studies, I created a simple model to reflect hypothesized interactions between IL-1R, APC, PKC-, and MUC2 (Fig. 2). Aim 1: Elucidate the Molecular Roles of APC in MUC2 Expression. As evidenced by our research, APC expression Figure 2. Model of relationships that would leads to an increase in MUC2 mRNA levels and APC loss allow IL-1R and APC to increase MUC2 leads to reduction in IL-1R. However, whether APC induces expression individually and synergistically.* translation of MUC2 or IL-1R is yet to be determined. With the help of Dr. Yoshiaki Azuma, a CRISPR/Cas9 expert at the University of Kansas, I have designed and begun creation of two novel cell lines to uncover the molecular roles of APC. By modifying HCEC and DLD-1 cells, we will be able to perform quick, cheap, efficient, and consistent 1) APC KO using an Auxin- Inducible-Degron (AID), 2) basal-level APC expression with no treatment, and 3) APC overexpression using Tetracycline-Inducible-Expression (TIE). The AID system uses OsTIR1, an auxin-responsive ubiquitin ligase, to polyubiquitinate AID-tagged proteins for degradation in the proteosome.10 I am tagging endogenous WT APC with an AID for APC KO. The TIE system initiates transcription of a gene through a tetracycline (TET)-responsive promoter11 and will be paired with the endogenous AID-tagged APC. I will use MUC2 promoter/Luciferase Reporter Assays (LRAs) and our in-lab luminometer to confirm response to various levels of cellular APC. An increase in luciferase indicates that APC increases MUC2 promoter activity, a neutral result indicates no effect of APC, and a negative result indicates APC reduces MUC2 promoter activity. This design can be repeated with reporters for all my candidate gene promotors, including IL-1R, MUC2, synergism between APC and IL-1 for MUC, etc. Regardless of my findings, these results will direct my further studies on APC and the other proteins of interest in this system, including investigation on protein localization, protein modifications, and signaling pathways. I will use co- immunoprecipitation followed by mass spectrometry performed at KU’s Core Mass Spec Lab to identify signaling proteins, florescence microscopy using our in-lab microscope to understand protein localizations and treatment phenotypes, SDS-PAGE and western blots for exposing signaling pathway patterns, mobility shift assays for uncovering post-translational modifications and protein activation, and much more to understand my proteins of interest and their signaling pathways. Aim 2. Monitor Impacts of APC on Colon Microbiome Homeostasis. In another recent study, we created transgenic mice with an APC allele containing mutations in the protein’s Nuclear Localization Sequences (APC-mNLS), preventing APC from entering the nucleus, and thus, inhibiting potential direct APC-driven nuclear promotion of gene expression. In this study, we uncovered that APC-mNLS mice had a >90% decrease in MUC2 expression following colon epithelial injury compared to control mice.12 I hypothesize that APC-mNLS mice have lower levels of MUC2, reducing the amount of mucus habitable by microbiota in the colon and thereby decreasing microbiome diversity and microbiota count. Using this mouse strain, with our extensive mouse care facilities and IACUC approval (137-01), I will collect fecal and colonic mucus swab samples of both mutant mice and control littermates for Zymo Research’s Microbiome Analysis Service,13 which provides publication-ready data of a list of microbiomic data, including absolute microbiota counts, multi-kingdom accounts of microbiome diversity within and between samples, and more. In addition to these data, I will use immunohistochemistry to visualize MUC2 in the colon epithelium using anti-MUC2 antibodies, comparing mutant and control littermates. This will provide comprehensive evidence of any changes to the microbiome and the colonic mucus layer in response to loss of nuclear APC, providing insight into APC’s physiological function and connection to the microbiome in vivo. Broader Impacts: The proposed research will expand our understanding of non-Wnt functions of APC, the function of APC in the generation of the colon mucus layer, the impact of APC on the microbiome, and provide insight on the function of APC orthologs across the animal kingdom. These data have the potential to uncover numerous novel functions of APC and better our understanding of microbiome homeostasis in relation to physiological function, a field of study that is still in its infancy. The cell lines I will create and data I will collect will be powerful research tools for myself, the Neufeld lab, and other researchers studying APC and colon microbiomics. I will communicate the conclusions of my research through publications, as well as through posters and oral presentations at regional and national conferences. I will also continue to develop the Rural Scientist Initiative (RSI) outlined in my personal statement; the NSF GRF would allow me to dedicate more time to the RSI while attending graduate school. Development of the RSI includes presenting this research to rural students along with information on scientific careers to ~15-30 of the 70 high school students at Norwich High School per year, based on student interest. I will continue communicating my experience as a scientist from several underrepresented communities to these students, encouraging rural students to pursue careers in scientific research. References. 1. Sender et al (2016) PLOS Biol 14(8):e1002533; 2. Qin et al (2010) Nature 464:59-65; 3. Heintz-Buschart et al (2018) Trends Microbiol 26(7):563-74; 4. Schroeder et al (2016) Nat Med 22:1079- 89; 5. Johansson et al (2016) Nat Rev Immunol 16:639-49; 6. Bienz et al (2002) Nat Rev Mol Cell Biol 3:328-38; 7. Gomez et al (2020) Exp Physiol 105(12):2154-67; 8. Tiwari et al (2011) J Immunol 187(5):2632-45; 9. Hankey et al (2018) Oncotarget 9(58):31214-30; 10. Natsume et al (2016) Cell Reports 15, 210-18; 11. Das et al (2016) Curr Gene Ther 16(3):156-67; 12. Zeineldin et al (2014) Carcinogenesis 35(8):1881-90; 13. Zymobiomics (2020) 5:159-63; *Created with BioRender.com	Winner!
10	Introduction:In recent decades, reports of re-emergingand novel phytopathogens have increased dramatically in forests.1These pathogens threatenforest health and pose serious risks to plant biodiversity. Studies indicate climate change (e.g. warmer temperatures, wetter growing seasons) has accelerated forest decline within the United States by expanding plant pathogen ranges.2The effectsof climate change have heightened and extended the infection period for pathogens, making trees more vulnerable to outbreaks of less aggressive phytopathogens.3Plant pathogens in the family Nectriaceae, including undescribed species, have been indirectly linked to climate change.4In addition, these changes in temperature are known to increase sporulation and virulence of fungal pathogens, as cold periods would ordinarily reduce the populations of pathogens by arresting their growth. Trees at high elevations including red spruce, Fraser magnolia, yellow birch, striped maple and mountain ash are buffered from many pathogenic fungi due to persistent cold temperatures in their habitat; however, warmer winters have increased the risk for biological invasion of these species.5 Although significant progress has been made regarding the taxonomy of these nectriaceous fungi, additional data are needed to clarify species boundaries and their evolutionary relationships. Likewise, these fungi pose risks that must be fully assessed by more robust studies on host range and pathogenicity. Long studied forests are now experiencing epidemics of these emergent plant pathogens (EPPs).6While beech bark disease andFusarium-associated diseasesare highly-studied pathosystems, native, often less virulent, nectriaceous fungi are becoming more abundant. My objective is to protect Appalachian forests by 1) drawing connections between abiotic stressors and the prevalence of nectriaceous fungal pathogens, 2) identifying these fungal pathogens, and 3) assessing the effects of temperature on the aggressiveness of these pathogens.I propose to expound upon currentclimate change models and forest pest predictions, particularly for nectriaceous fungi on these high-elevation tree hosts. Aim 1:Assessing abiotic stressors contributing tothe emergence of fungal pathogens. Using the ‘Climate by Forest’ tool provided by the U.S. Forest Service, I will review changes in forest health and climate projections for forests throughout the Appalachian region. The ‘Climate by Forest’ tool is a novel interface in which users can select regions of national forests and look at various climate trends and variables.7From these projections, forests thatare predicted to have significantly warmer winters will be selected for sampling. High-elevation tree species will be selected based on their known range across high elevation zones throughout Appalachia. Symptomatic tissues and conspicuous fungal fruiting bodies from these species in our sample sites will be surveyed, collected and processed for culture- and DNA-based studies. I will also compile and analyze temperature data across the Appalachian region to quantify and evaluate the abiotic stress these forests have endured.Hypothesis: Fungal pathogens and abiotic stresses are synergizing declines in native tree species. Aim 2: Characterizing known and unknown nectriaceousfungal diversity in Appalachian forests. Hypothesis: Despite known diversity of Nectriaceous fungal pathogens across Appalachian forests, many remain undetected.Molecular tools must be usedin combination with existing morphological methods to capture the full diversity of phytopathogenic fungi. Sanger sequencing will be used for pure cultures of my suspected fungal pathogens recovered from trees sampled inAim 1. Targeted loci (LSU and EF1-α) are widely used for phylogenetic inference in Nectriaceae.8Illumina amplicon sequencing—a multiplexed PCR approach—will be used to identify asymptomatic fungi that may also be contributing to forest decline. Aim 3: Determining the interaction between individualnectriaceous fungi and targeted tree species in central Appalachia along a temperature gradient. Hypothesis:Nectriaceousfungi have contributed differentially to tree disease epidemics, which are driven in part by changes in temperature throughout our Appalachian forests. To simulate globalwarming and to assess the effects of temperature on fungal growth and pathogenicity, temperature-dependent pathogenicity assays will be conducted. In climate-controlled growth chambers, saplings of the aforementioned five species will be grown at varying temperature ranges (0°C, 10°C, 20°C, and 30°C). Trees will be inoculated with select nectriaceous fungi discovered inAim 2and tree healthwill be monitored at 6-MPI and 12-MPI. Any notable canker formation will be measured at the end of the inoculation period. To fulfill Koch’s postulates, trees will also be sampled to see if the original inoculum can be recovered. This experiment will quantify the aggressiveness of suspected nectriaceous pathogens at varying temperatures, allowing me to infer the impact novel phytopathogens will have on our forests as global warming worsens. Intellectual Merit:As a member of Dr. Kasson’s forestpathology lab since mid-2020, I have direct experience identifying and characterizing diverse fungal phytopathogens in West Virginia. During my efforts to delimit the species boundaries ofNeonectriamagnoliae,I have already identified numerous nectriaceous fungi on a wide range of hosts, including some novel species we are describing. Putative pathogens in the Nectriaceae are abundant and appear to be emerging as the result of the unique overlap of biotic and abiotic factors. I have and will continue to collaborate with forest pathologists throughout the Appalachian region to compare DNA sequences, host range, and pathogenicity of these fungi under supervision of Dr. Matt Kasson (WVU), my current advisor and forest pathology expert. With his support and the support of my forest pathology colleagues, I will not only unravel the contributions of these fungal phytopathogens to the decline of our forests, but provide novel information to our Appalachian communities, our foresters, and our scientists.WestVirginia is the “black box” of biodiversity: severely understudied with much to discover. There are an estimated 150 different tree species in WV: more than anywhere else in North America. My contributions to forest pathology will revolutionize how we look at and care for our trees in Appalachia. Broader Impacts:West Virginia (WV), my homestate, is suffering from educational neglect. It has the lowest number of Bachelor’s degrees (20.6%) per capita of any state, and we have the second lowest per capita graduate degrees.9In Appalachia—and WV specifically—we deal with low science literacy and a fear of science. Our region once powered the country with coal, but the profits of this mono-cultural economy were not reinvested in our communities. As coal has faded away and global temperatures rise, there is much skepticism and fear around climate action in West Virginia. West Virginians need scientists from our own communities trained to identify the challenges we face, develop solutions to these problems and share them with our own.Thesescientists, like myself, will have a broad and immensely positive impact on my community. As science outreach has been an integral part of my undergraduate career, I will curate my own environmental science outreach program to invest my project in our Appalachian youth. Through my Appalachian Children’s Environmental Research program(ACER), I will recruit fellow young scientists to bring presentations to K-12 students and also offer field trips to teach a variety of environmental concepts. In this novel program, I will provide resources (i.e., guides, lessons, and accessible information) on climate change, forest health, mycology, and other topics pertaining to preserving the integrity of our ecosystems.My lifetimeof learning has prepared me to fulfill this next stage of my career, one that will ensure West Virginians are not left behind. References:[1] Karunarathna et al. 2021.Front CellInfect Microbiol. [2] Kasson et al. 2009.Mycologia. [3] Dukes et al. 2008.Can J For Res. [4] Pavlov etal. 2020.Sci Rep. [5] Pauchard et al. 2015.Biol Invasions. [6] Corredor-Moreno et al. 2019.New Phytologist.[7] U.S. Forest Service. 2018. Climate by Forest. [8] Stauder et al. 2020.Fungal Ecol. [9]West Virginia QuickFacts. U.S. Census Bureau.	Winner!
11	actions allow them to control their external environment. Studiesoncausallearningshowthatasearlyas infancy, humans learn the relations between actions and their outcomes1 and are able to use this knowledgetoactassuccessfulagentsintheirenvironment.Mostexistingliteratureonagencyemphasizes comparing the outcome of one’s own actions with their internal predictions of thoseactions2-5.However, these processes have been limited to sensory perception and stimulus-response learning, precluding the ability to explore the effects of agency on episodic memory6 and how sequences of information which unfold due to causal actions drive brain regions to support memory. Background. Exercising agency over learning environments has been shown to improve memory7,8, even when the choices do not relate to the content of the to-be-learned items. Previous work from ourlabgaveparticipantsasimplechoicebetweentwo‘cards’whichwouldrevealanunrelateditem. Participants better remembered items that appeared as a result of their choice9,10. Further, this memory enhancement was driven by an interaction between anticipatory activation within the striatum, a region associated with causal actions and motivation, and hippocampal (HPC) engagement during encoding. While this shows how agency over a choice can positively affect memory fortheoutcomeofachoice,it does not shed much light onto memory for the overall decision sequence. To explore how agency over a series of events affectsassociativememoryforthecomponentsof the sequence, I developed a task where participant's agency was manipulated via a choice. In the “game show” task, participants assisted contestants in choosing one of three doors which reveal a hiddenprize. On each trial, participants saw a trial-unique contestant and either freely chose between the doors (“agency” trials) or selected a highlighted door (“forced-choice”trials).Unbeknownsttotheparticipants, the prize image presented was predetermined. After completion of the task, participants completed a surprise retrieval taskwhichtestedmemoryforthecontestantspresentedintheencodingtask,whichdoor they selected, and the outcome hidden behind the door in three separate, consecutive memory tasks. Across two studies (study 1 n = 28; study 2 n = 131), which serve as the foundation for this proposal, participants showed enhanced memory for the contestants (p<0.001), constant-prize pairs (p<0.05), contestant-door pairs (p<0.01), and prize-door pairs (p<0.01). These results show that by manipulating participant’s agency to select which door to open, we are able toenhancememoryforcues as well as associative memory between cues and outcomes. However, these results do not discriminate whether agency enhances memory separately for each individual pair, or whether agency facilitates the binding of all associations into one integrated sequence. Follow-up analysis explored whether agency facilitates memory integration by examining whether there was an inter-dependence upon memory measures such that memory for one pair was dependent onmemoryfortheotherpairs.Indeed,wefound memory for the contestant-prize pair was modulated by memory for recalling both the contestant-door and door-prize pairs (p<0.01). Further, this effect was significantly higher for pairs that occurred in agency trials vs the forced-choice trials. Intellectual Merit. While I have established a paradigm that modulates associative memory via agency, it isstillunclearhowmesolimbic-hippocampalengagementsupportsthislearning.Understanding the timescale of how these systems interact will inform us on how agency modulates encoding and connect human and animal research. Much of the existing human literature exploring mesolimbic contributions to memory focus onhowphasicdopaminedriveslearninginresponsetorewardfeedback11. However, rodent research has shown hippocampal engagement during exploration prompts sustained ventral tegmental area (VTA) engagement leading to greater response feedback to the hippocampus12. I propose exploring these temporal dynamics within the same paradigm to address the gaps in these two lines of research and contribute to the translation of rodent-to-human research. Using neuroimaging techniques, the current research seeks to: 1) examine sustained mesolimbic during encoding, 2)examine event-evoked VTA and hippocampal activationduringencodinganditseffectsonmemory,3)explorethe interactions between engagement at these different timescales. I hypothesize sustained mesolimbic activity during learning when an individual has agency increases cue and outcome based mesolimbic-hippocampal interactions. The relationship with the sustained and event-evoked activitywill bias encoding to promote associative learning across and within decision sequences. Methodology. Eighty healthy participants will berecruitedtoparticipateinastudyattheTemple University Brain Research and Imaging Center, which houses a Siemens Prisma 3T MRI scanner. Participants will complete a modified version of the game show task. On each trial, they will see a trial-unique contestant (2s), and then will seeandselectoneofthethreedoors(2-4s).Uponselection,the door will be highlighted, then removed to presentatrial-uniqueprizeimage(2s).Again,participantswill either get to freely choose one of the three doors (agency trials) or be forcedtoselectahighlighteddoor (forced-choice trials). Participants will complete three runs of each condition, each run containing 20 trials. Runs will be pseudo-randomized across participants so no more than 2 runs ofthesamecondition appear consecutively. Temporal jitters will be placed between cues and outcomes and between trials to improve both temporal and spatial resolution. Following encoding, participants will complete the three retrieval phases described earlier: contestant recognition, and contestant-prize, contestant-door, and door-prize associative memory. Analyses. Behavior: In brief, item memory will be calculated for contestants using corrected recognition, which accounts for false alarm rates. Associative memory metrics will be calculated as hit rates (percent correct in selecting the old item). I will compare memoryacrossagencyandforced-choice conditions using paired t-tests. In line with my previous findings, I expect memory for items and item pairs to be enhanced for those that appear in agency compared to forced-choice trials. Neuroimaging: 1) In order to examine sustained mesolimbic activation, Iwillcomparesustained baseline VTA engagementusingpairedt-testsonparameterestimatesfromanatomicalROIs.Ipredictthe sustained activation to be higher for agencycomparedtoforced-choicerunsandtrials.2)Toexplorehow the event-evoked VTA-HPC coupling may drive memory, I will use a 2x2 within-subjectsANOVAwith condition (agency, forced-choice) and memory (sequence intact, sequence disrupted) on parameter estimates from anatomical ROIs during cues and outcomes. I expect this couplingtopredictmemoryfor agency but not forced-choice trials. 3) To examine the interaction betweenengagementofthesedifferent time scales, I will relate measures of sustained VTA and VTA-HPC activations on the event-evoked memory signals acrossrunsforbothconditions,separately,usingmulti-levelGLMmodelswitharandom effect of participant. Post-hoc analysis willmakedirectcomparisonsacrossconditions.Iexpectsustained VTA activation and VTA-HPC coupling during agency runs to be associatedwithevent-evokedmemory effects in the VTA and the hippocampus on agency trials, across runs. Broader Impacts. Much of the literature exploring motivated learning comes from work that is particularly interested in response to reward feedback. However, motivated learning in the absence of reward may contribute toriskfordevelopmentofsubstanceabuse.Suchlearningmaypotentiatecuesand contextual factors that strengthen drug associations independent of the anticipation or experience of the reward. I will use agency as a model to elucidate the neural underpinnings of motivated learning in the absence of rewards. This model will allow for the isolation of the core mechanisms that underlie substance abuse, which may depend on VTA-HPC interactions. The proposed research will dissect how VTA-HPC interactions contribute to the complexity of behaviors associated with substance abuse by elucidating how both state-dependent and event-evoked interactions drive memory encoding. Additionally, the results of the proposed research may provide valuable contributions to improving pedagogical techniques. Understanding how agency might enhance learning at different timescales could support new teaching methods which incorporate agentic choices over both short and long term goals. Even inmyanecdotalexperienceemployingactivetechniquestoengagementees,Ihave found that giving individuals’ agency over minor choices, such as choosing what topic to read and discuss, and more substantial choices, such as choosing a research topic, leads to significantly more engagement and long-term retention. The proposed research will directly test how agency can affect motivation and learning at various timescales, which will contribute both to our understanding of howit can support learning and how we may utilize it to broadly enhance teaching methods. References. 1Kuhn, 2012 2Haggard et al., 2002 3Haggard, 2009 4Wolpert et al., 1995 5Chambon et al, 20146Hon, 20177Gureckis&Markant,20128Markantetal.,20169Murtyetal.,201510Murtyetal.,2019 11Shohamy & Adcock, 2010 12Lisman & Grace, 2005	Winner!
12	"language pairs and translation tasks. However, two of the most pressingopenproblems inNMT are domain adaptation and low-resource scenarios [1] (i.e., language pairs for which large parallel corpora do not exist). This project will address both issues via cross-lingual domain adaptation in an extremely low-resource setting. The work is motivated by an urgent humanitarian crisis: refugees seeking asylum in the US who speak only Central American indigenous languages like K’iche’, Mam, Kanjobal, and Mixtec [2]. The scarcity of interpreters in these languages makes itdifficult for speakers toaccess legalservices,andexistingnonprofits and NGOs providing legal aidto refugeesdonothaveadequate resourcestoprovideinterpreters. Additionally, existing methods for low-resource NMT do not scale down to the extremely low-resource situation for these Central American languages. Improving both extremely low-resource applicability anddomain adaptationfor NMT will increasethenumberof scenarios in which NMT is a viable solution. Additionally, domain adaptation techniques that workinthe challenging low-resource setting will also improve NMT domain adaptation in higher-resource settings. Thus, this project addressesachallenging andwidelyapplicable scientific problemwith immediate humanitarian impacts. Objectives and Hypothesis: The objective of my proposed research is atwo-prongedapproach to domain adaptation in low-resource NMT. I will address the domain issue by fine-tuning a pretrained NMT model on synthetic parallel data generated via backtranslation [3] of monolingual in-domain data in the target language. I will first validate my approach in a high-resource setting by fine-tuning a baseline Spanish-English model using backtranslated in-domain English data. Independent of the performance on low-resource languages, a Spanish-English translation system that is well-adapted for asylum testimonials will be of great use to organizations providing legal aid to refugees. Then, I will apply the fine-tuning via backtranslation approach to the low-resource case. In order to successfully backtranslate the English finetuning data, a decent English-LR model is needed. Thus, I plan to apply a combination of unsupervised NMT, transfer learning, multilingual translation models, and various data augmentation approaches totheproblem ofextremelylow-resource NMT.Although various methods to augment parallel corpora have been proposed [4-7], they typically augment corpus size from a few hundred thousand sentences to a few million . They do not adequately address a context in which only thousands or tens of thousands of sentences are available, as is the case for K’iche’ and other indigenous languages. This project will develop a linguistically-informed method to generate enough plausible, syntactically correctsyntheticdata that existing corpus augmentation techniques like backtranslation can be applied, bridging the chasm between researchers’ assumptionsabout theamount of availablemonolingual dataand the reality for many low-resource languages. Method and Research Plan: In this research, I will use K'iche as a case study language to develop methods, then I will apply those methods to Kanjobal, Mam, and Mixtec. BLEU score, the standard evaluation metric for machine translation [8], will be the primary metric for evaluation of this proposed research. Possible BLEU scores range from 0 to 100, with higher scores indicating a closer match to the reference translation. In my exploratory work on the Magdalena Peñasco Mixtec dialect,achievinga maximumBLEU scoreof7with amodel trained from scratch on 8000 Mixtec-English parallel sentences. After this preliminary work, I decided to use K’iche’ as the test language in further study for two reasons: first, monolingual K’iche’ speakers are more common among migrants to the US than monolingual Mixtec speakers; second, there are more than fiftydistinct dialectsof Mixtec,each withvaryingdegrees ofmutual intelligibility, which makes collection and cleaning of data intractably difficult. The first year of my work will focus on domain adaptation to asylum testimonialsin the high-resource Spanish-English setting. High-resource NMT models are primarily trained on parallel data extracted from news articles, while LR models are generally trained on whatever portions of the Bible are available for a givenlanguage.Because theultimate goal ofmyproject is to translate first-person testimonials ofasylees aboutthe violencefromwhich theyarefleeing, I needto adapttheNMT modelsto fluentlytranslatefirst-person narrativesandto understandthe specific vocabulary relevant to asylees. One way to accomplish this adaptation is to fine-tune a pretrained translation model on in-domain data. While parallel data is ideal,itis also possibleto use monolingual target-language data via backtranslation. I plan to use the transcripts from the MALACH dataset [9], compiled from the USC Shoah Foundation’s Video History Archive, which collects testimonials from Holocaust survivors. I hypothesize that these testimonials are similar enough to the stories of modern asylees to be useful as in-domain training data. In the subsequent years of the project, I will apply the domain adaptation technique developedin thefirstyear totheextremelylow-resource setting.Thefirst priority for Year2is to gather and clean a K’iche’-English parallel corpus (at least the New Testament and some Jehovah’s Witness literature is readily available). Next, I will establish a baseline machine translation result for K’iche’-Englishusing available parallelcorporawithnodata augmentation. Then, I will apply the same backtranslation for domain adaptation technique used for Spanish-English. However, I expect that the backtranslation results will be unsatisfactory because of the limitations of the English-K’iche’ model used for backtranslation. Thus,I planto develop a data augmentation scheme syntax-aware compositional data augmentation methods from [10], withsome simplifications necessaryduetodata scarcity. Sentencesproducedwiththis method are guaranteed to be grammatically correct, but may not make sense semantically; however, they are much easier to produce than semantically correct sentences, especially in a low-resource setting. This research will investigate whether NMT systems can learn useful information from synthetic data that is syntactically, but not semantically, correct. In additionto data augmentation, I plan to explore whether the K’iche’-English and English-K’iche’ models can be further improved by transfer learning and unsupervised NMT techniques. Intellectual Merit:Thiswork addressesacrucial gapincurrent methodsfor low-resource NMT: cases where there are fewer than 10,000 sentences of monolingual data available. My proposed data augmentation method would allow researchers to generate enough synthetic monolingual data to apply state-of-the-art methods in low-resource NMT to languages that would otherwise be intractable due to lack of data. If this data augmentation is successful,it willshowthat NMT systems can learn grammar and syntax from synthetic training data that is semantically very noisy, opening up future research on how exactly NMT systems learn grammar and how this learning differs from learning vocabulary. Broader Impacts: The human rights impact of my proposed research is immediate and transformative. Interpreters for K’iche’ and related languages are hard to find and often prohibitively expensive. Translation systems, even imperfect ones, would allow non-governmental organizations and pro bono immigration lawyers to help asylees they were previously unable to serve. Widespread access to legal assistance would speed up backlogged immigration courts and give thousands of asylum seekers per year a chance to enter the US legally. This work could save lives, because many of these asylees are children and teenagers fleeing from horrific gang violence. References [1] Koehn, Philipp, and Rebecca Knowles. ""Six Challenges for Neural Machine Translation."" Proceedings of the First Workshop on Neural Machine Translation. 2017. [2] Medina, Jennifer. “Anyone Speak K'iche' or Mam? Immigration Courts Overwhelmed by Indigenous Languages” New York Times (2019). [3] Sennrich, R., Haddow, B., & Birch, A. (2015). Improving neuralmachinetranslationmodels with monolingual data. arXiv preprint arXiv:1511.06709. [4] Zhang,Jinyi, andTadahiroMatsumoto. ""CorpusAugmentationbySentenceSegmentation for Low-Resource Neural Machine Translation."" arXiv preprint arXiv:1905.08945(2019). [5] Li, Hongheng, & Heyan Huang. “Evaluating Low-Resource Machine Translation between Chinese and Vietnamese with Back-Translation”. arXivpreprint arXiv:2003.02197(2020). [6] Currey, Anna, Antonio Valerio Miceli-Barone, and Kenneth Heafield. ""Copied Monolingual Data Improves Low-Resource Neural Machine Translation."" WMT 2017. [7] Fadaee, Marzieh et al. ""Data Augmentation for Low-ResourceNeuralMachine Translation."" ACL 2017. [8] Papineni, Kishore, et al. ""BLEU: a Method for Automatic Evaluation of Machine Translation."" ACL 2002. [9] Ramabhadran, Bhuvana, et al. USC-SFI MALACH Interviews and Transcripts English – Speech Recognition Edition LDC2019S11. [10] Andreas, Jacob. ""Good-enough compositional data augmentation."" arXiv preprint arXiv:1904.09545(2019)."	Winner!
13	"Introduction. The question of how a child learns her native language remains highly debated in linguistic research. Cross-linguistically, children learn much of the morphology (word structure) of their native language by age three, when their vocabulary is approximately 1000 words.1 Yet the frequencies of words in child-directed speech follow a skewed, roughly Zipfian distribution, with the frequency of a word being proportional to the inverse of its rank.2 This means that a few words may occur hundreds of times in the child's linguistic input, but most occur only a few times. Similarly, most words only appear in a few of their possible inflected forms (e.g. the child may hear fall and fell, but not falls or fallen).3 Such input contrasts with the larger, more saturated data used by most machine learning systems today. How, then, does a child learn the morphology of her native language from such a skewed, sparse input? During my PhD, I seek to answer this question via computational modeling of morphological acquisition. A plausible model of morphological acquisition should follow the developmental and behavioral patterns of children, which may be studied experimentally and through analysis of errors in children's speech. Productivity of a linguistic process is marked by its ability to generalize to novel contexts, and is a foundational component of language. In the famous wug study, for example, most English-learning children generalized -ed, -ing and -s to novel verbs (e.g. gling) by age three, demonstrating that they had learned the productivity of these suffixes.4 Further, most errors in children's speech are caused by over- application of productive processes: English verb acquisition is known to follow a ""U-shaped"" curve, where a sudden dip in performance is caused by the overapplication of -ed to irregular verbs (e.g. goed, feeled) when the child discovers its productivity.5 Several promising computational models that account for these facts utilize the Tolerance Principle (TP), a threshold of productivity which posits that a child generalizes a process when it is more computationally efficient to do so under a Zipfian distribution.2 Such distributional learning models have been the focus of my undergraduate research: working with Dr. Charles Yang, I developed a model that acquires meaning-form mappings (e.g. PAST = -ed) between suffixes and their corresponding semantic features, such as person (first, second or third), number (e.g. singular, plural), and tense (e.g. past, present, future).6 This model follows developmental patterns and correctly acquires morphological rules on small vocabularies of Spanish and English verbs. I also created a model that acquires such mappings for German plural nouns and English verbs, even displaying U- shaped regression in English, and contributed to a third model with comparable results.7 It is my goal to build on these models to create an integrated, incremental, and cognitively-plausible model of morphological acquisition that succeeds on a wide array of languages. Aim 1: To create a model of incremental morphological acquisition that succeeds on a typologically diverse set of languages. While the models outlined above provide promising results, no single model is able to account for all languages: the latter two succeed on concatenative, non-agglutinative languages (e.g. English, German), but fail to model non-concatenative (e.g. Hebrew, Arabic) and agglutinative (e.g. Spanish, Swahili, Japanese) languages. Segmenting a word into morphemes is more challenging in such languages: in Spanish, for example, a verb may take multiple suffixes (e.g. ama-ba-s = love-past-2nd+ singular = ""you loved""). These models are also not incremental learners, but extract morphological rules from fixed-size vocabularies; this contrasts with the incremental nature of language acquisition. In my first aim, I thus plan to build on the models above to design a novel algorithm that incrementally acquires morphological rules across agglutinative, non-agglutinative, and non-concatenative languages. While current models take in each item as a lemma, inflected form, and semantic feature set (e.g. walk, walked, {PAST}), the child may be able to group each of the inflected forms in which she has seen a word (e.g. get, gets, gotten). I hypothesize that doing so will allow for cleaner segmentation and identification of morphemes, helping the learner to succeed on a wider array of languages. To test this, I will create such a model and compare it with experimental findings on the aforementioned languages and others. I will work closely with linguists and cognitive scientists from differing subfields to ensure that this work benefits from both theoretical and experimental insights and provides a plausible account of acquisition. Aim 2: To extend this model by incorporating models of other portions of language acquisition. Morphological acquisition does not happen in isolation, and morphology is known to interact with other levels of linguistic representation, particularly phonology (e.g. -s is pronounced /s/ in cats but /z/ in dogs). The nature of the input to the models discussed above assumes that the child has already learned much of the phonology of their native language and extracted the relevant semantic features such as person, number and tense onto which they will map the segmented input. The grouping of forms as discussed in Aim 1 makes the additional assumption that the child is able to form these groups. To create a more holistic account of acquisition, I will thus integrate models of morphological learning like the above with models of acquisition of other levels of linguistic representation, particularly those on which Aim 1 relies. It is highly plausible that similar learning algorithms are used for each level of representation, so I will begin by testing the ability of the algorithm developed above to account for these other levels. I will also test integration of the model developed under Aim 1 with existing models in the literature. With the end goal of an algorithmic hypothesis about how children acquire their native language, I will collaborate with experts on each of the levels of representation I will consider. I will compare the integrated model's predictions with experimental findings on a typologically diverse set of languages to ensure that this algorithmic account of learning is a plausible and generalizable one. Intellectual Merit. The end goal of this work, an input-to-grammar model of morphological acquisition, will provide insight into linguistic theory and the learning mechanisms employed by children. The model developed under Aim 1 will provide an algorithmic hypothesis regarding how children learn from skewed, sparse data, and structural hypotheses regarding morphological knowledge in the mind as the end result of acquisition. Both will be valuable in answering questions of learnability and the structure of linguistic knowledge, and may also provide insight into atypical language development. The models developed under Aim 2 will yield hypotheses about the interactions between linguistic levels of representation, which may be compared with theoretical hypotheses to provide new insight into linguistic structure. Further, these models will give a bottom-up account of language acquisition, and thus yield testable hypotheses regarding the innate factors that may enable it, a highly-debated topic. This model will, to my knowledge, be the first to model morphological acquisition from phonological input to a structured grammar, and it will thus provide a basis for further integrated models of language acquisition. Broader Impacts. This work has applications to Natural Language Processing (NLP), which focuses on the creation of language technologies. Models used in NLP are typically trained on data several orders of magnitude larger than that to which the child is exposed. This can lead to biases and makes models inaccessible for “low-resource” languages for which large corpora do not exist, such as Indigenous languages and languages of Africa and Asia.7 Cognitively-motivated approaches already show promising results,8 and since the algorithms I will develop are designed to succeed on small, sparse input, they will be strong candidates for use with low-resource languages and for testing bias mitigation strategies. This, in turn, will allow for the creation of more accessible, equitable language technologies for all. References. [1] Brown, R. 1973. A first language: The early stages. Harvard University Press. [2] Yang, C. 2016. The price of linguistic productivity: How children learn to break the rules of language. MIT Press. [3] Chan, E. 2008. Structures and distributions in morphological learning. UPenn Dissertation. [4] Berko, J. 1958. “The child’s learning of English morphology.” Word. [5] Pinker, S. and Prince, A. 1988. “On language and connectionism: Analysis of a parallel distributed processing model of language acquisition.” Cognition. [6] Payne, S, et al. 2021. “Learning Morphological Productivity as Meaning- Form Mappings.” Proceedings of the Society for Computation in Linguistics. [7] Belth, C, Payne S, et al. 2021. “The Greedy and Recursive Search for Morphological Productivity.” Proceedings of the Cognitive Science Society. [8] Bender, E, et al. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” Proceedings of the ACM Conference on Fairness, Accountability, and Transparency. [9] Xu, Chao et al. 2020. “A Cognitively Motivated Approach to Spatial Information Extraction.” Proceedings of the Third International Workshop on Spatial Language Understanding."	Winner!
14	Progress in neuroscience is limited by the lack of proper tools available to biologists and neuroscientists to study neural circuits with high spatial resolution and cell type specificity. One area of neuroscience that is particularly affected by this absence is the study of somatosensory and motor control systems. Currently available tools used to study these systems and mimic their functions consist of electrode arrays, such as the polymer cuff electrode, attached to the peripheral nervous system1 or the Utah electrode array implanted directly into the motor cortex2. These electrode arrays, however, lack the ability to induce or record neural activation with cell specificity. Herein, I propose the development of a novel class of miniaturized, battery-free, wireless, soft, implantable neural machine interfaces (NMIs) utilizing optogenetics to study the somatosensory system in non-human primate (NHP) models via the peripheral nervous system (PNS). This proposal considers the recent advancements within the fields of optogenetics and photometry, advanced micro- and nano-fabrication methods, and the necessary collaborations to bring this project to fruition within a three-year period. Background Optogenetics is a growing neuroscience tool which utilizes viral injections to genetically modify neuron populations to express light-sensitive ion channels. The targeted neurons can be selectively stimulated among other tissues by selecting viral vectors and opsins with preferential tropisms. In NHPs, initial research in optogenetic stimulation of the peripheral nervous system shows channelrhodopsin-2 (ChR2) and Chronos delivered via adeno-associated virus and stimulating muscle injection to be successful3. Once the opsins are virally delivered, these neuron populations can be excited or silenced by targeting them with varying wavelengths and stimulation frequencies from light-emitting diodes. Recent papers have shown the success of optogenetics in stimulating the central nervous system via the brain and the spinal cord4,5. Similarly, genetically-encoded calcium indicators (GECI’s) and photometry can be used to visualize neural activation of defined cellular populations in-vivo6. These tools have significant advantages over electrical probes which lack the stimulation and recording specificity required for high resolution research into light touch information propagation through the low-threshold mechanoreceptor afferent neurons in the dorsal root ganglia (DRG). Neuron populations of particular interest for this study are the low-threshold mechanoreceptor afferent neurons within the DRG located in cord segments C6, C7, and C8, which are responsible for light touch information propagation from the lower forelimb and hand7. Aim 1: Optical Recording and Stimulation of Low-Threshold Mechanoreceptor Afferent Neurons The primary functions of the proposed device are to optically record the neural activity of low-threshold mechanoreceptor afferent neurons in a healthy NHP’s DRG, and to stimulate those neurons to replicate light touch information being transmitted through the neural circuit up A- and A- fibers. The cells will be targeted following the methods outlined by Williams et al. using ChR2 and Chronos, and with GECI’s. To enable both recording and stimulation, the device will employ a colloc𝛽𝛽ated micr𝛿𝛿oscale inorganic light- emitting diode (μ-LED) and photodetector (μ-IPD), both interfaced with a microcontroller for stimulation control and data processing respectively. Additional functional requirements to ensure device reliability are highly deformable mechanics and a usable lifetime of 10 years or longer. To ensure the device function for applications lateral the spinal column serpentine geometries, polymer substrate and encapsulations, and thin annealed metal traces will be employed to keep local strains under fatigue limits even under high bending and linear loads. To extend the usable lifetime of these devices, dielectric interlayers of thermally grown Silicon Dioxide and Hafnium Oxide will be used. These interlayers, employed in a total thickness up to 100µm, work to extend usable lifetime by retarding the ingress of ions and water vapor and disrupting pin-hole defects while remaining translucent8. Device encapsulation and fatigue mechanics will be tested using accelerated life testing (ALT) in an 87℃ phosphate buffered saline bath with complex mechanical loading conditions for 4 months, simulating an implanted lifetime of 10 years and 8 months. While ALT is being performed, the long-term reliability of the stimulating and recording capabilities will be assessed by measuring the irradiance of the μ-LED and the recorded signal of an external light source via the μ-IPD over time. Device electronics and wireless power harvesting will be assessed via continuous data logging. Aim 2: In-Vivo Testing in Non-Human Primates The final aim of the proposed research is to implant the proposed device into NHPs to conduct research on light touch propagation via low-threshold mechanoreceptor afferent neurons in the DRG. The anatomical similarities of mechanoreception between NHPs and humans allows for the study of light touch perception in the forelimbs that could not be studied in small animal models. This work will be done with collaborators who conduct NHP behavioral studies, external to the Yoon Lab at the University of Michigan where I propose to do my PhD. One study of interest includes training the NHPs to perform two-alternative forced choice tasks involving the differentiation of textures on their fingertips or palms and studying the neural activity for each of the presented textures. After the behavior is learned with accuracy of 95% or greater and the neural activity has been recorded and decoded, the NHP will perform the task again. However, this time the NHP will receive light touch information from the device via optogenetic stimulation of the mechanoreceptor afferent neurons in the C6 C7 and C8 DRG without any textures presented. Future Directions Upon completion of this work, the goal is to transition from the study of the peripheral nervous system’s role in light touch perception to implantation within a NHP amputee. Once implanted, this device will interface with an upper-limb prosthetic via near field communication and used to replace the lost light touch perception abilities of the amputated limb. If shown to be successful, the next step is to use this device in conjunction with functional magnetic resonance imaging to study the long-range neural circuits of the somatosensory system, a study never before possible. Intellectual Merit The development of this device will utilize recent advances in materials science, fabrication, and optogenetics to advance neuroscience tools. The design process and in-vivo testing will also require collaboration with the departments of Biology and Neuroscience as well as external collaborators to inform the selection of virus, opsin, μ-LED, and μ-IPD. Once developed, these devices would directly promote an advancement in the understanding of the peripheral nervous system’s role in somatosensory processing and propagation and introduce a platform of devices for the targeted study of the somatosensory system and other short- and long- range neural circuits in-vivo. Broader Impacts These devices would be applicable not only in the study of light touch information but any neuron type and thus have broad impacts in neuroscience, neurotherapies, and limb rehabilitation or replacement for paralyzed or amputated individuals. Outside of the medical field, the ability to transmit somatosensory from an external input to the peripheral nervous system could also be used to advance entertainment systems and virtual reality to include touch perception. References 1. Elyahoodayan, S., et al.. Acute in vivo testing of a polymer cuff electrode with integrated microfluidic channels for stimulation, recording, and drug delivery on rat sciatic nerve. J. Neurosci. Methods 336, 108634 (2020). 2. Maynard, E., et al.. The Utah Intracortical Electrode Array: A recording structure for potential brain-computer interfaces. Electroencephalogr. Clin. Neurophysiol. 102, 228–239 (1997). 3. Williams, J., et al.Viral-Mediated Optogenetic Stimulation of Peripheral Motor Nerves in Non- human Primates . Frontiers in Neuroscience vol. 13 759 (2019). 4. Ausra, J. et al. Wireless, battery-free, subdermally implantable platforms for transcranial and long- range optogenetics in freely moving animals. Proc. Natl. Acad. Sci. 118, e2025775118 (2021). 5. Kathe, C. et al. Wireless closed-loop optogenetics across the entire dorsoventral spinal cord in mice. Nat. Biotechnol. (2021) 6. Burton, A. et al. Wireless, battery-free subdermally implantable photometry systems for chronic recording of neural dynamics. Proc. Natl. Acad. Sci. 117, 2835 LP – 2845 (2020). 7. Vanderah, T. W. & Gould, D. J. Nolte’s the Human Brain: An Introduction to its Functional Anatomy. (Elsevier, 2021). 8. Jeong, J. et al. Conformal Hermetic Sealing of Wireless Microelectronic Implantable Chiplets by Multilayered Atomic Layer Deposition (ALD). Adv. Funct. Mater. 29, 1806440 (2019).	Winner!
15	Background: The vestibular system, located in the inner ear, provides sensory information regarding spatial positioning and balance, enabling coordination of movement and orientation1. Additionally, the vestibular system plays a key role in enacting compensatory eye movements in response to body movement through the vestibulo-ocular reflex (VOR)1. This system can become unilaterally impaired in the presence of vestibular schwannoma, a benign tumor that develops on the vestibulocochlear nerve connecting the sensory organs to the brain2. Due to the vestibular system’s active role in locomotion, impairment via vestibular schwannoma may lead to balance deficiency, vertigo, and oculomotor process changes2. Previous work has demonstrated that in patients with vestibular schwannoma, head movements during locomotion and gaze stability exercises are less rapid and the VOR is impaired2,3. Encouragingly, resection, or removal, of vestibular schwannoma has been demonstrated to result in improvement of kinematic parameters and other head movements, with major changes occurring within the first six weeks post-operation4. Generally, normal vestibular activity is reflected in the parieto-insular and temporo-parietal junctions; however, little is known about cortical changes that occur due to vestibular impairment, with most studies focusing on kinematic parameters and quantification of specific visual reflexes5. Therefore, there is a need to examine the impact of vestibular schwannoma on neural activity at large. Obtaining this information about the ways the human brain copes with vestibular system deficiency will elucidate both patterns of neural plasticity in response to specific sensory input alteration, and how best to approach treatment of those who are impacted by vestibular schwannoma. I propose to determine the patterns of neural activity and gaze focus in response to locomotor tasks in people with vestibular schwannoma, both pre- and post-resection. This work will be conducted across three Aims: first, a comparison of electroencephalography (EEG) and gaze-tracking patterns in healthy subjects and vestibular schwannoma patients; second, a longitudinal study of EEG and gaze patterns in vestibular schwannoma patients pre- and post-resection; finally, generation of a support vector machine (SVM), a machine-learning technique which will discriminate between gaze patterns of patients with vestibular schwannoma and healthy controls. Intellectual Merit: This project will contribute to the body of knowledge surrounding normal vestibular function, as well as provide insight into the specific pathological state inherent to vestibular schwannoma. The insight into the changes in gaze functionality will be especially important because the impact of vestibular schwannoma on overall gaze patterns is not well-understood beyond interruption to the VOR. Changes to further parameters such as saccade frequency, fixation time, and primary areas of focus during locomotion are unknown. This project will elucidate changes in those patterns. Research Plan: Aim 1: Collection of baseline patterns in vestibular schwannoma patients vs. healthy subjects Hypothesis: Vestibular schwannoma patients will display greater primary motor cortex activation than healthy control subjects during gait, representing a more effortful process due to balance impairment. In this phase, we will focus on establishing a baseline of functionality in healthy subjects and patients with vestibular schwannoma, recruited from Johns Hopkins Acoustic Neuroma Center, a specialty clinic focused on the treatment of vestibular schwannoma. Testing will consist of a modified functional gait assessment (FGA) battery. The ‘gait with eyes closed’ portion of the FGA will be excluded, due to the inability to record gaze location while eyes are closed. During the FGA, subjects will have gross brain activity recorded via a 58-channel EEG cap connected to a wearable Arduino Uno microcontroller to allow for continuous mobile data collection. Additionally, subjects will don wearable eye-tracking glasses to assess continuous gaze location. This phase will conclude with successful collection of gaze-tracking and EEG data for a matched number of healthy subjects and vestibular schwannoma patients executing the FGA tasks. Aim 2: Longitudinal study of vestibular schwannoma resection Hypothesis: Between six weeks and six months after vestibular schwannoma resection, patterns of neural activity in vestibular schwannoma patients will become more like that of healthy subjects during gait. This phase will enact a longitudinal study of patients with vestibular schwannoma pre-resection, approximately six weeks post-resection, and at least six months post-resection. This experimentation will consist of the same paradigm as Aim 1, with subjects performing the modified FGA. Additionally, during this phase, the EEG signal collected in Aim 1 and Aim 2 will be processed and analyzed. Preprocessing will consist of an independent component analysis, wherein the multivariate EEG signal for each subject will be decomposed into additive ‘components’, which combine at different weights to compose the overall EEG signal. These components will be assessed via visual inspection to remove extraneous signal, such as eye blinks and motion artifacts. Then, processed data will be examined for event-related potentials at several key points in the gait cycle. This phase will conclude with the accomplishment of two tasks: successful collection of gaze-tracking and EEG data for vestibular schwannoma patients pre- and post-resection, and analysis of differences in patterns of neural activation between healthy controls, pre-resection vestibular schwannoma patients, and post-resection vestibular schwannoma patients. Aim 3: Generation of Support Vector Machine to categorize gaze patterns Hypothesis: Individuals with vestibular schwannoma will display distinct gaze patterns, including greater gaze latency to area of interest, as compared to healthy control subjects. This phase will center around the generation and validation of a support vector machine (SVM) that will enable automatic machine classification of vestibular schwannoma patients versus healthy controls. An SVM is a supervised machine learning approach that uses a hyperplane to split groups of variables, or support vectors, into discrete classes (shown in two dimensions in Figure 1); once trained on the stereotypical values for each class, it compares new data to those clusters to classify the state of the new input6. An SVM has been used to accurately detect pathology based on gaze patterns; Figure 1: Design of an SVM 7 specifically, individuals with dyslexia versus healthy controls while reading text6. Measured parameters will include number of saccades, number of gaze fixations in key areas of interest (ground underfoot, ground ahead of stride, wall), length of gaze fixations, and latency of gaze arriving at areas of interest. Within this model, some erroneous classification is inevitable. Because the primary purpose of the model is quantifying impacts of vestibular schwannoma to the ocular system, it is preferable to bias the model towards detecting vestibular schwannoma in order to find any and all gaze-pattern disruptions. Accordingly, the SVM will be tuned to have higher sensitivity to prevent false negatives. This phase will conclude with the demonstration that the resultant SVM is able to discriminate between the gaze patterns of healthy individuals and those who suffer from vestibular schwannoma, with at least an 85% detection rate for positive cases. Alternative Approaches: An SVM can have limited efficacy if the training dataset is of insufficient size. If there is not be a significant dataset that will allow for training of the model and model accuracy is below the 85% threshold, an alternative approach may be the use of a convolutional neural network (CNN), which consists of a cluster of signal-transmitting kernels that loosely resemble neurons. A CNN has previously been used to classify gaze-tracking data based on what website a user was viewing; however, this methodology has not been extended to pathology detection8. If needed, this possibility could be explored. Facilities: This work will be conducted with Dr. Kathleen Cullen at the Cullen Laboratory at Johns Hopkins University. This laboratory has previously conducted studies of vestibular schwannoma patients using kinematic parameters and is equipped to continue this work with other methodologies. Broader Impact: While the primary purpose of this work is to learn about the unimpaired vestibular system through a study of vestibular schwannoma as a disease state there is potential for secondary application as a diagnostic measure. The diagnostic process for vestibular schwannoma is two-stage – first-round testing consists of a battery of hearing tests, and if those tests suggest the presence of a tumor, second-round testing consists of an MRI with contrast. With the rising costs of healthcare in the United States, finances can be a prohibiting factor to patients pursuing this diagnostic testing. The creation of a lower-cost intermediate diagnostic would prevent unnecessary clinic visits for final diagnostic testing for patients whose hearing loss may have a different cause. If the SVM can recognize patients with vestibular schwannoma, then an intermediate gaze-tracking diagnostic tool could be developed and used to screen patients with hearing loss to determine whether vestibular schwannoma is a likely culprit for their symptoms. References: 1. K. Cullen, Nat Rev Neurosci, 2019; 2. A. Batuecas-Caletrio et al. Laryngoscope, 2015; 3. L. Wang et al. Sci Rep, 2021; 4. O. Zobeiri et al. Sci Rep, 2020; 5. E. Nakul et al. Front Neurol, 2021; 6. L. Rello et al. W4A ’15, 2015; 7. “Support Vector Machine”, javaTpoint; 8. Y. Yin et al. ICMLA ’18, 2018.	Winner!
16	in coral reef environments and providing ecosystem services that are intrinsic to the longevity of society. The diverse microhabitats provided by the elaborate morphologies of corals function as predation refuge and are essential for supporting the low trophic level (LTL) fish community.1 Specialist fish species will live within one coral colony (or others of similar morphology) for much of their lives, whereas generalist fish can associate with a wider variety of microhabitats. Trophic cascading of the LTL fish community results in flourishing commercial fisheries, which are estimated to be globally valued at $5.7 billion USD annually.2 Yet, the existence of coral dominated tropical reefs is largely threatened by global scale, anthropogenic warming-induced coral bleaching events—which has in part contributed to a 50-75% decline in worldwide coral cover over the last ~35 years.3,4 The loss of microhabitat often leads to drastic declines in the reef fish community5 and can crash commercial fishery markets. To mitigate against the further decline of coral reefs and the fisheries they support, restoration strategists in-part rely on large- scale coral propagation and outplanting—involving the artificial fragmentation of reef-obtained donor colonies and returning the clonal population back to the reef.6 Often, studies attempting to describe coral reef environments solely focus on percent coral cover and fail to capture the complex nature of coral reef ecosystems.7 It remains unclear how reef fish community assemblages are directly affected by bleaching- induced changes in microhabitat availability. Understanding fish-microhabitat associations is essential for devising targeted, efficient fisheries restoration efforts. The proposed research aims to elucidate the unique fish-microhabitat associations to better inform outplanting-based fisheries restoration efforts. Revealing fish-microhabitat associations would lead to the development of a comprehensive Coral Outplanting for Fisheries Guide (COFG) to be leveraged by coral restoration and fisheries managers. This research also aims to capture changes in the population levels and spatial distributions of commercially valuable high trophic level (HTL) populations while under the presumed pressure of depleted LTL prey populations following a bleaching event. Hypotheses: I hypothesize that (1) bleaching events will induce the largest decreases in specialist, LTL fish populations relative to generalist fish populations (H1), (2) bleaching events drive HTL predator populations to relocate to less-affected regions of the reef where food sources are sufficient, or in more extreme cases, recruit to nearby reefs owing to the reductions of LTL fishes associated with H1 (H2), and (3) outplanting of fisheries-specific coral taxa will facilitate the recovery of the fishery stock (H3). Experimental Approach: Timeseries Density Maps: The framework of one entire reef would be imaged before and after a single bleaching event, which would be scheduled according to existing local degree heating week (DHW) data. DHW is a measure of accumulated thermal stress obtained by the 12-week time-integration of sea surface temperature data exceeding the local bleaching threshold and is a reliable bleaching predictor.8 The onset of bleaching is expected when DHW values reach 4 °C-weeks, whereas mass bleaching and mortality is expected at 8 °C-weeks.9 The framework would be characterized by generating high taxonomic resolution photomosaics10 of the benthic coral community coupled with ArcGIS-generated density maps of coral colony microhabitat volume approximated using structure-from- motion (SfM). SfM is a computationally intensive software that would allow me to digitally reconstruct the reef and extract microhabitat volume data from each colony. The movements of lower and higher trophic level fish populations would be continuously monitored utilizing size-specific acoustic telemetry transmitters and receivers to create 3D population density maps in ArcGIS. It is imperative to implant size-specific transmitters to minimize potential adverse health impacts to best isolate for tracking the natural movements of the fish.11 Fish populations would be estimated for all implanted fish taxa using well-established tag-recapture techniques and the appropriate stock assessment model according to the species-specific life history traits. Statistics: To determine specific fish-microhabitat associations and build the COFG, colony location and taxonomic classification will be tested against the time-based location density of the tagged LTL fish. To test for potential reef-level population reduction differences in LTL fish species (specialists vs. generalists) (H1), I would linearly model the tag-recapture-obtained population abundance data and evaluate whether species, time, and the interaction of species and time are significant predictors of mean abundance. To test for potential significant changes in the movements of HTL predator populations (H2), I would model the time-based location density of tagged LTL fish populations paired with the time-based location density of tagged HTL predators. I would encourage future studies to utilize the COFG produced by this research to answer H3. These studies would require quantifying the background recruitment rates of LTL fish populations and the new recruitment rates following a large-scale outplanting effort. I would also overlay the microhabitat volume density maps with the fish population density maps to allow for better visual interpretation of the data. Resources: I am applying to be advised by Dr. Sandin who is a leading expert in coral reef ecology at the Scripps Institution of Oceanography (SIO). Dr. Sandin’s team is comprised of many individuals with years of experience who would assist me in reliably imaging the reef and identifying coral colonies. I hope to also receive guidance from Dr. Brice Semmens (SIO), who often uses telemetry techniques in his research, to safely implant fish with acoustic transmitters and reliably track their movements. This study would rely on the resources available to SIO, especially the use of custom-framed, study-optimized cameras12 and SfM to digitally reconstruct the reef from imagery and perform the colony microhabitat volume calculations. The spatial monitoring of LTL populations would require surgically implanting fish with Juvenile Salmon Acoustic Telemetry System (JSATS)13 microacoustic transmitters and installing JSATS N201 receivers around the perimeter of the reef. The spatial monitoring of HTL populations would follow the same methods but require Vemco™ V16p-4H transmitters and VR2 receivers. Intellectual Merit: This research would provide valuable insight to ecologist’s holistic understanding of successional reef fish communities. Although previous work has evaluated the role of decreased reef framework on fish community composition using transect based methods,14 it remains unknown how shifts in specific coral species alter the population level and distribution of specific fish species. This research aims to reveal these mysteries with the increased quantized framework resolution from SfM and the paired monitoring of fish movements. This project would provide great predictive value to infer what reef-associated fish communities may look like in the future if the current frequency of bleaching events continues. Particularly, which fish species we might expect to decline at a given reef site if targeted conservation efforts, such as those that would be made possible by the COFG, are not enacted. Broader Impacts: The COFG developed from this research could guide the decisions of coral restoration and fisheries managers by detailing which coral species serve as primary microhabitat for a particular LTL fish population—enabling managers to easily identify which coral species to outplant to maximize the available microhabitat for LTL prey species for a specific fishery. In theory, increased microhabitat availability and trophic cascading would result in increased LTL prey population(s) and the HTL fishery stock. However, the world’s leading coral outplanting organization, the Coral Restoration Foundation (CRF), has only optimized the large-scale propagation and outplanting of 4 coral species. The fisheries- relevant coral species identified by this research that are not currently being outplanted would provide reason to increase funding for the development of new programs working to optimize the large-scale propagation of these species. The realization of this optimized restoration strategy would require a collaborative effort between SIO, CRF, and fisheries managers around the world to outplant the specific coral taxa that provide the most relevant microhabitat for prey of target fisheries. This innovative restoration optimization would be a valuable strategy to help work towards sustainable fisheries and maintaining their incredible economic value for future generations. I intend to disseminate the findings from this project via publications in peer-reviewed journals to drive similar studies that would build upon my findings and broaden the geographic relevancy of the COFG. I will present my findings to students at nearby institutions to inform aspiring ecologists of the problems our worlds reefs are facing, hopefully inspiring them to pursue related careers and research. References: [1] Bellwood et al (2004) Nature 429:827-833. [2] Cesar et al (2003) Cesar Environ Econ Consul, NLD. [3] Goreau & Hayes (1994) Ambio 23:176-180. [4] Bruno et al (2019) Ann Rev Mar Sci 11:307-334. [5] Jones et al (2004) PNAS 101:8251-8253. [6] Rinkevich (1995) Restor Ecol 3:241-251. [7] Brito-Millán et al (2019) Mar Ecol Prog Ser 630:55-68. [8] Liu et al (2014) Remote Sens 6:11579-11606. [9] Liu et al (2003) Eos 84:137-144. [10] Gracias et al (2003) IEEE J Ocean 28:609-624. [11] Lefrancois et al (2001) Mar Biol 139:13-17. [12] Kodera et al (2020) Coral Reefs 39:1091-1105. [13] McMichael et al (2010) Fish Res 35:9-22. [14] Richardson et al (2018) Glob Change Biol 24:3117-3129.	Winner!
17	Introduction: Development of personalized medical treatments and diagnostics is limited by our ability to engineer tools that can keep up with demand. Exosomes are a type of nano-sized extracellular vesicle (EV) naturally produced by cells and released via endosomal fusion with the plasma membrane. Although primarily utilized in the detection of diseases such as cancer, exosomes are increasingly being investigated as a means of drug delivery due to their potential for multi-functional targeting and inherent biocompatibility [1]. A longstanding setback in the study and application of exosomes for therapeutic purposes is the lack of standardized methods with which to isolate, concentrate, and characterize them [2]. Although scientists can functionalize exosomes with targeting molecules and drug payloads, scale-up is limited by the ability to quickly identify favorable processing conditions and thus good manufacturing practices. In this project, I propose creating a system that will reduce the burden of the screening process by developing a tool to rapidly assess exosome production and functionality. This study will help others engineer EVs as a tool for medical and non-medical applications. Traditional methods quantify exosomes via nanoparticle tracking analysis (NTA) which relies on light scattering of particles to judge size and concentration via analysis of Brownian motion. However, NTA also counts non-exosome particles such as protein aggregates, leading to large discrepancies in actual determination of exosome quantity in solution. Moreover, NTA does not account for functionality of engineered exosomes, which typically display targeting molecules on their surfaces. Current exosome purification methods are labor intensive, requiring multiple days of centrifugation and gradient separation to remove the crude EV material prior to quantification and analysis of the exosome product [2]. Therefore, developing a method to rapidly quantify and assess exosome functionality with targeting molecules would enable scientists to focus their attention on scaling up and purifying only the most promising therapeutic exosome processes. This development would bypass the current time-consuming roadblocks associated with engineered exosome production, advancing the field. Objective: I will establish a high-throughput method to screen processing conditions for engineered exosomes. Protein microarrays, which have previously been used in diagnostic exosome assays [3], have the potential to quantify functionalized exosomes in an efficient and accurate manner. Therefore, I hypothesize that protein microarrays can be utilized as a high-throughput method to screen processing conditions for engineered exosomes. I plan to (1) investigate optimal microarray conditions by engineering spot formulations and process steps and (2) evaluate ideal processing specifications for exosome production by running protein microarrays in tandem with cholesterol-based quantification standards. Aim 1: Investigate optimal microarray conditions to create a high-throughput screening tool for functionalized exosomes. Protein microarrays can be modified to include different antibodies in each spot. The ideal formulation would be one that binds only targeted exosomes. Exosomes express the surface ligands CD9, CD63, and CD81 as unique identifiers from other EVs [2,3]. Antibody cocktails for these ligands, as well as the expressed targeting molecules, would enable binding of only the desired exosomes— even in the presence of a crude EV sample—while washing away all other materials in solution. I will determine the concentration of each antibody for optimal exosome binding kinetics, which is a function of the desired targeting molecule antibody. The formulation must also be modified to include a solvent material that is suitable for microarray printing but that does not interfere with the antibody-exosome interactions. Once I find the ideal antibody cocktail ratios, I will test the formulation to ensure that it prints appropriately on the microarray slides. The solution’s fluid properties must enable it to flow easily for spotting on the slides, dry quickly, and spread evenly. Different additives noted in literature would be investigated to find those that work best with the solution. I expect that a 5% glycerol content will produce the desired results, as it was used in a previous publication illustrating a method for exosome phenotyping using protein microarrays [3]. Various factors affect microarray accuracy, including sample application times, wash and blocking buffer identities and concentrations, number and duration of washes, and drying times. Each of these factors will be explored systematically using a factorial design of experiments (Fig.1a). If the microarray cannot be optimized for EVs, a column-based affinity separation method could be used to purify and confirm functionalization of exosomes. Completion of this aim will optimize the microarray as a tool for engineered exosome analysis. Aim 2: Evaluate ideal process conditions for engineered exosome production. Having identified the most favorable microarray conditions, I will compare processing conditions for engineered exosomes to prove the rigor of the quantification method. With this high-throughput method, dozens of different processing specifications—such as days in culture, feed conditions, pH setpoints, and centrifugation steps—could be analyzed simultaneously with a minimal demand on sample volume. The first step in engineering a suitable high-throughput method is to create a reliable standard for comparison. While working at Codiak Biosciences, I pioneered a cholesterol assay that circumvented time-consuming NTA by quantifying exosomes based on their cholesterol content and presented a poster on this work at the International Society for Extracellular Vesicles (ISEV) 2020 Annual Meeting. The fluorescence plate-based Amplex Red Cholesterol Assay was quick and accurate within a prescribed range of 0-8 µg/mL. Employing it also enabled me to quantify exosomes without the need for purification, reducing cost and time spent. I will utilize this preliminary work to develop a standard method for exosome quantification for this project. Once the standard is established, exosome samples conjugated with fluorescent tags will be applied to the microarray spots. After washing, only the exosomes with the desired surface ligands will bind to the spot and be fluorometrically detected (Fig.1b). The relative fluorescence of each spot will be compared to a standard where only the exosome detecting antibodies, not the targeting molecule antibody, are present. This relative fluorescence describes the number of functionalized exosomes in the sample. These signals will be compared to the cholesterol-based assay to obtain a quantitative readout of the number of engineered exosomes and their relative protein loading. If fluorescent tags are not feasible, dyes, luminescent substrates, or other types of markers could be conjugated to the exosomes to create a measurable readout. The results of this aim will elucidate the ideal conditions for engineered exosome screening. Intellectual Merit: This work will generate a deeper understanding of protein microarrays as high- throughput screening tools and can be applied to development of new exosome or nanoparticle-based technologies, which could be used in applications ranging from drug delivery to water treatment. It will expand the foundational knowledge surrounding EVs and support future endeavors to optimize the production of different types of engineered exosomes, aiding in the discovery and development of EV therapies. As a member of the Leonard Lab, which has expertise in exosome production and engineering, I am well positioned to complete this project. Broader Impacts: This proposal integrates chemical engineering, bioengineering, and biochemistry principles to create an interdisciplinary project that advances dynamic drug delivery platforms through high-throughput screening. Successful completion of this project will enable accurate exosome quantification, reducing labor and time investments. Rapid identification of improved processing conditions will support efficient and sustainable production of therapeutic exosomes, increasing manufacturing feasibility, and thus increasing their eventual accessibility on a global scale. I will leverage my connections at Codiak Biosciences to establish a collaboration to facilitate and support the project. I will disseminate the knowledge gained from this research to the scientific community for feedback and further development through conferences and publications, such as the ISEV Annual Meeting. Providing students with opportunities to learn about STEM fields and to participate in projects directly will foster the next generation of research scientists. I plan to direct my outreach toward programs encouraging underrepresented students to consider graduate school, such as REUs and the Northwestern Morning Mentors and Mentorship Opportunities for Research Engagement (MORE) programs, where I will mentor students and encourage their participation in STEM research. References: [1] Wang, J. et al. (2017). ACS Applied Materials & Interfaces, 9(33), 27441-27452. [2] Chia, B. S. et al. (2017). TrAC Trends in Analytical Chemistry, 86, 93-106. [3] Jørgensen, M. et al. (2013). Journal of Extracellular Vesicles, 2(1), 20920.	Winner!
18	Intellectual Merit How do you study something you cannot see? Dark matter (DM) is responsible for shaping the large-scale structure of the universe we see today, comprising 80% of all matter.Decadesof direct and indirect searches for annihilation radiation have notyieldedanysignals. InordertostudyDM astrophysically, we must use the luminous parts of the universe — the galaxies and galaxy clusters that reside inside DM halos — as tracers. The DM halo relationship is cleanest near the outskirts of these structures, where non-gravitational physics, such as AGN feedback, have the least effect. Whiledifficult to observe due to their low density, new and upcoming advancements in instrumentation are detecting observational tracers in halo outskirts for the veryfirsttime.SubsequentmeasurementsofDMhalomass and dynamical quantities, such as accretion rate, will constrain not only large-scalestructurebutalsothe nature of the DM particle itself. My projectexplorestheoutskirtsofbothindividualgalaxiesandgalaxyclusters:twostructuresat different cosmological scales but subject to fundamentally similar dynamics. Starting on the scale of galaxies, the brightest cluster galaxies(orBCGs)haveextendedfaintstellarhalosthatarenowthoughtto have physically significant edges [1]. Moreover, in recent optical images from the Hyper Suprime-Cam Subaru Strategic Program (HSC survey), it has been shown that the stellar mass in the outskirts (10-100kpcradii)oflow-redshiftBCGsisanexcellentproxyforDMhalomass[2].Theoutskirtsofthese massive galaxies are dominated by stars accretedduringmergerswithprevioussatellitegalaxiesandthus provide an estimate of “historical richness” for their DM halos. While promising for its potential of measuring DM halo mass, this result leads to more questions: Why is the 10-100kpc region significant? Could an even tighter relationship to DM mass exist with stellar profiles past 100kpc? On a larger scale, the outskirts of galaxy clusters (>1Mpc) also provide a laboratory for the effects of DM. At these distances, dark matter particles turn around at their apocenter, called the splashback radius, the location of which depends on the halo’s mass accretion rate(MAR)[3,4].Recent simulation work suggests this radius coincides with an analogous “stellar splashback” radius of the cluster’s stellar distribution (or intracluster light, ICL), which would also vary with MAR and reveal intriguing DMhalodynamics[1].Thesmallsampleanduseofzoom-insimulationsinthisstudywarrants afollowupinvestigationwithcosmologicalboxsimulationsandalargersample.HowobservablethisICL edge will be with future instruments is also unclear and requires further modeling. There are many other unexplored phenomena at theseclusteroutskirts,notonlyinDMbutinthe hydrodynamics of gas. In particular, an accretion shock must beproducedwhencoldgasfallsintoahalo and experiences a drastic jump in temperature. According to the self-similar collapse model [5, 6], the radius at which this shock appears should be almost identical to the cluster’s splashback radius [7], making it another potential observational tracer of DM. However, the model has not held up in simulations, in which the shock radius has been found to be 20-100% larger than the splashback radius [8]. This disagreement is a fundamental theoretical gap that needs to be understood to interpret current high-resolution observations of the Sunyaev-Zeldovich effect in clusters (from the South Pole Telescope and Atacama Cosmology Telescope) as well as near-future radio observations of accretion shocks. Cosmological simulations offer the opportunity to understand the connections between baryonic physics and underlying DM halos inordertobothinterpretobservationsandmakepredictionsouttohalo radii we cannot yet observe. While many processes in simulations are implemented via subgrid models, the processes in halo outskirts are mostly-first principle physics producible even with these limited modeling conditions. I will use cosmological simulations of dark matter and galaxies includingthelarge volumecosmologicalboxsimulationsuiteIllustris-TNG(TNG)[9].IproposetouseTNGtoinvestigate multiple baryonic physics phenomena at theoutskirtsofgalaxiesandofgalaxyclustersaspotential tracers of dark matter halo properties.My projectconsists of 3 scientific goals: 1. Develop robust techniques to measure BCG light profiles out to large radii to find optimal estimators of DM halo mass 2. Test the connection between ICL edges and splashback radius 3. Analyze the relationship between the accretion shock radius and splashback radius 1. Galaxy Stellar Outskirts andDMHaloMassTheHSCsurveyisamajorstepinobservers’abilityto optically image BCG outskirts out to 100kpc given its high-quality seeing conditions and a depth 3-4 magnitudes deeper than the SDSS. In collaboration with the HSCteam,Iwillusethesedatatodevelopa new technique for extrapolating stellar mass profiles of high-mass BCGs beyond their observable radius by testing them against a sample of simulated, mock-observed TNG galaxies of similar mass. A crucial consideration in simulation-observation comparisons is how to recreate the conditions used by observers. To achieve this science, we need to overcome technicalhurdles.Forexample,thefile ordering of the TNG output data is by friends-of-friends groups, suchthatoverlappinggroupscouldlead to one group missing particles near its outskirts. Forgoingthisdataorganizationtoinsureallparticlesare accounted for makes extracting complete profiles out to large radii in large simulations is a non-trivial task. I recently implemented a function in a simulation-extraction software, Hydrotools, that overcomes this challenge and allows users to extract all particles within a given radius. This function lays the foundation for any project relying on large-scale radial profiles. 2. ICL and Mass Accretion Rate Satellites fall into a halo with various velocities, orientations, and internal energies, so it is not immediately clear that disrupted stars in the ICL are faithful tracers of the DM halo potential. I will use Hydrotools to extract complete stellar and DM profilesfromhalosinTNG to investigate this connection. The splashback radius is signified by a caustic in the DM density profile whereparticlespileupaftertheirfirstorbit,sothe“stellarsplashbackradius”canbefoundanalogouslyin thestellardensityprofile.Iwillcalculatethedifferencebetweentheseradiiandtheirdifferentcorrelations with MAR for a sample of various mass halos, predicting the use of the ICL as a DM halo tracer. 3. Shock and Splashback Radii While TNG does not outputshocksurfacesspecifically,wecaninstead detect a sharp drop in gas entropy profiles to signify the accretion shock radius. I will analyze the relationship between the shock and splashback radii in massive halos over different stages of cluster merger events, using complete gas and DM profiles. These results will provide context to interpret the rapidly growing amount of Sunyaev–Zeldovich, and soon radio, accretion shock observations. Future Directions The outskirts of galaxies and of galaxy clusters are a clear next place to look for potential tracers of DM halos. Upcoming instruments and surveys will revolutionize our constraints on DM features such as the splashback radius. Moreover, the low surface brightness of BCG and ICL outskirtsarealreadydetectablewiththeHSCsurveyandwillbeevenmoresowiththeupcomingVera.C. Rubin Observatory and the Nancy Grace Roman Space Telescope. In the radio regime, the Square Kilometer Array (SKA) will vastly increase the number and resolution of observed accretion shocks. However, to interpret any of these observations we need to improve our theoretical understanding. My project will build off of current observational work and provide the foundation for interpreting future data. Broader Impacts I will continue my work to limit barriers for underrepresented minorities (URM) in astronomy. Particularly, I will use the remainder of my term serving on the AAS SMGA (sexual orientation and gender minorities in astronomy) committee to develop a mentorship program for LGBTQ+ early career astronomers. This program will match graduate students and postdocs with more experienced mentors who share a LGBTQ+ identity. Mentors will guide mentees through navigating the field, especially challenges specific to LBGTQ+ individuals in the workplace. I will also continue to co-lead the BANG! (Better Astronomy for the Next Generation) seminar series in my departmentwhich covers alternate career paths and EDI issues in Astronomy. I will focus on planning seminar sessions covering previously under addressed topics, such as accessible teaching strategies and equitable workplace practices. Finally, I will continue to work with the GRADMAP (Graduate resources for advancing diversity with Marylandastronomyandphysics)teamtoprovideexternalresearchexperiences for students at minority serving institutions by teaching workshops in career skills and serving as a summer research mentor. [1] Deasonetal(2020)[2]Huangetal(2021)[3]Diemer&Kravtsov(2014)[4]Adhikarietal(2014)[5] Bertshinger et al (1985) [6] Shi etal(2016a)[7]Shietal(2016b)[8]Aungetal(2021)[9]Pillepichetal (2018)	Winner!
19	been political debated, although empirical work suggests that low-skill immigration has a small negative effect on average British wages. However, individual-level migration data suggests that Eastern European migrants tend to be highly educated and highly skilled [1], motivating my proposed investigation of the effects of the emigration of high-human capital individuals from Eastern Europe following the collapse of communism in the nineties. In particular, what are the macroeconomic impacts of a shock to the distribution of human capital as a result of immigration? Empirical work suggests that there are large differences in output per worker across countries [2], and that differences in total factor productivity (TFP) account for the bulk of these differences in output [3]. The mass emigration of high human capital workers decreases aggregate productivity directly through the decrease in labor productivity. Research on the emigration of academic Jewish scientists in Nazi Germany [4, 5] shows that there are persistent negative impacts on emigrants’ departments’ performances because the replacements are of lower intellectual caliber and positive effects on receiving departments’ patenting. This micro work validates macro-theoretical research on shocks to human capital. One aspect of Schumpeterian growth theory is that innovations are the result of firms’ investment decisions based on expectations of future profits. Previous work has made the connection between national knowledge stock and innovation at the scientific frontier. My work bridges the connection between negative shocks to population and aggregate human capital with innovation in a technology- follower setting. I incorporate the firm’s trade-off between investment in R&D and production as a function of the distributional shift in human capital, specifically firms’ decisions to invest in R&D as a result of Eastern European emigration in the 1990s. Methods: I will construct a general equilibrium overlapping generations (OLG) model of a small open economy populated by heterogeneous agents that vary in their stock of human capital and age, and then apply this framework to estimate the impact of the migration-induced productivity shock on Bulgaria’s economic growth. Using the structural model, I am able to analyze the welfare effects on heterogeneous individuals through equilibrium values of the model. Human capital accumulation is an endogenous process, meaning that agents choose their optimal level of human capital. The agents allocate their time between labor, leisure, and human capital accumulation in each period. Firms split labor and capital inputs into production and research, where the research production function is governed by a Poisson process. Intuitively, investment in research yields innovations or tangible improvements in technology at random and fairly rare points in time, and this understanding of knowledge production closely fits within a Poisson process. The parameter that governs this Poisson process is endogenously determined by the stock of human capital in the labor force available to the firm and the existing stock of knowledge. In other words, within a discrete period of the model, the discovery of one innovation does not have any bearing on the discovery of a second innovation (memoryless property of the Poisson distribution), but between periods the total innovations impact the rate of knowledge production. The firm faces a tradeoff between production for profit in a given period and investment in research for a potential payoff in a future period. This decision involves risk. Firms also consider distortionary taxes and expectations about economic conditions in their decision. One metric of innovation is patent applications. Applications are a reasonable proxy for immigration because patent applications are not dependent on a government agency’s determination of the worthiness of an innovation for being patented. In a revealed preference framework, applying for a patent indicates that the firm believes it has produced innovation worthy of patenting. This belief informs their decision to invest in research and development. The preparation of a patent application is not without effort; therefore, applying for a patent represents the firm beliefs I am interested in capturing. I will calibrate my structural OLG model to Bulgarian data to conduct a policy experiment on emigration. Using individual-level data, the Mincerian earnings function for the returns to schooling and experience can be calculated [6], which gives direct estimates of the parameters governing the human capital accumulation decision by the agent. The weight of consumption in utility is adjusted to capture aggregate hours worked in the data (available from OECD), and this parameter characterizes the labor- leisure decision of the agents. The parameters governing the relationship between the rate of knowledge production and human capital are estimated in the literature. These parameters combined with the first- order conditions of the structural model determine the firm’s behavior. Because I can fluently read and speak Bulgarian and my personal experiences, I am uniquely able to obtain the necessary innovation data to calibrate the rest of model. Otherwise, standard data and computational resources are sufficient. Because the migration decision depends on a number of unobservable characteristics, it is implausible to include this decision in a structural model. Information about the types of people who migrate, include their ages, educational levels, and work experience are observable in individual-level surveys conducted in Bulgaria. Accordingly, emigration is captured by changes in the relative sizes of human capital and age cohorts as well as level changes in population. Additionally, as a result of bottlenecks related to work visas, documentation, and language barriers, this mass migration does not happen immediately after the collapse of communism. This delay in the timing of the migration shock allows for a few years post-collapse to establish the baseline macroeconomic trends. Thus, there are two balanced growth paths of interest. One growth path is the case where no migration occurs, and the model is calibrated to match the known pre-shock periods. The other growth path includes the migration shock, and data on immigrants is used to adjust the measures of ages and human capital types. The comparison between the second balanced growth path and the data measures the performance and predictive capacity of the model, while the comparison between the second growth path and the first (the simulated counterfactual) represents the effect of the migration shock. A successful model will replicate targeted moments of the data. Broader Impacts: Understanding the effects of high-skilled emigration is crucial to reconciling why the economies of Eastern and Western Europe have not converged since the 1990s and designing policies that encourage talent to remain in Eastern Europe. The model I propose captures another dynamic of migration through shifts in the age distribution. Empirical work indicates that migrants tend to be younger, and a large migration event such as in Eastern Europe following 1991 may shift the age distribution upward. Previous work has analyzed the macroeconomic implications of an aging population as well as changes to human capital separately, but the interaction between the two remains an open question. My model and associated calibration would partially fill this gap in the literature. Furthermore, the mass migration of young people negatively shocks the population growth rate. Because the growth rate of the population is determined by the proportion of young people, the one- period shock to the population growth rate propagates through R generations, where R is the cutoff between young and old. Combining this with the level decline in population as a result of the shock, my model also captures the absolute population declines observed in some Eastern European countries. Because population declines independent of wars, pandemics, and the like are rarely observed, there is little empirical work on the macroeconomic implications of declining populations. My model and computational approach incorporates these effects and could isolate them via simulating an age-biased, human capital-neutral migration event, where migrants are young but equally skilled as the population. Several countries in Asia, Western Europe, and Latin America are expected to experience population decline in the near future, and the mass-migration events of the nineties started this process earlier in Eastern Europe than the rest of the world. Accordingly, my findings on the effects of population decline are of significant interest and would contribute to an open and deeply relevant question. Moreover, the relationship between high-skilled emigration and innovation is not limited to Eastern Europe. There are several US states, including my home state, Kansas, with net high school and college graduates leaving, resulting in a negative shock to human capital. During the Covid-19 pandemic, several Midwestern cities, including Topeka, Kansas, adopted policies that gave workers a lump-sum transfer of money in exchange for moving and living in the city for at least a year. A natural extension of my work is to evaluate the prevalence and demographics of uptakers of such policies, and then analyze my model with shocks to the human capital distribution as observed in the data. References: [1] IMF report “Emigration and its Economic Impact on Eastern Europe” [2] McGrattan and Schmitz, (1998) Federal Reserve Bank of Minneapolis [3] Hall and Jones (1999) QJE [4] Moesa et al (2014) American Economic Review [5] Waldinger (2016) Review of Economics and Statistics [6] Patrinos (2016) IZA World of Labor	Winner!
20	free, first-principles theoretical treatment of core-to-core x-ray emission spectroscopy (ctc-XES). Success in this research program will have wide impact for refining analytical and fundamental study of the element- specific electronic structure in highly correlated materials, an extremely broad class with significant industrial, technical, and environmental relevance. Furthermore, by having fully addressed the forward problem, i.e., prediction with no adjustable parameters of ctc-XES spectra from local structure, we will be able to use unsupervised and supervised machine learning (ML) methods to understand the information content in core-to-core XES across systems already widely studied (e.g., 3d transition metals) and also systems that have seem comparatively little XES (e.g., materials with heavy d- and f-electron elements). This will likely lead to prediction of new diagnostic spectral signatures of magnetism in f-electron systems. Introduction and Background: X-ray emission spectroscopy (XES) is the very high-resolution study of fluorescence given off by the radiative decay of a core-shell excited atom, inherently probing the occupied electronic states. XES can carry information about the local chemical environment of the fluorescing atom, such as valence-level spin, oxidation state, ligand identity, local coordination geometry and bond covalency. While there are many truly first-principles theoretical tools for parameter-free calculation of most other advanced x-ray spectroscopies (e.g., XAFS, RIXS, and valence-to-core XES), the same is not true for ctc-XES, which is a deeply many-body problem where the treatment of highly correlated materials with partially filled d- or f- shells is especially challenging [1]. DFT approaches fail to describe the local many-body correlation effects while more accurate configuration interaction (CI) methods are computationally expensive and often difficult to implement beyond simple systems [1, 2]. Multiplet implementations are therefore the preferred theoretical framework for ctc-XES with many theoretical codes and models being developed over the last 40 years [3]. Those multiplet models show adequate agreement with experimental results after fits to screening and correlation parameters. Here, I will develop tools to calculate those parameters, moving from a descriptive treatment to a predictive treatment of ctc-XES. Research Plan: My research plan has three main components: (1) A theoretical component that builds off of my prior work (below) and the expertise available from my theory mentors, Profs. Rehr and Kas; (2) Validation via measurement of ctc-XES across a wide range of materials, this capability is firmly enabled by lab-based XES available in my Ph.D. advisor’s lab (Seidler group, UW); and (3) a ML component that will build on emergent methods in the XAS community, such as recent Seidler group work on unsupervised ML [6]. Hence, I am strongly supported by local expertise and needed facilities. Beginning with theory, Figure 1 shows the distinction between common practice, the result of my work over the past six months, and a large part of the proposed further improvements. First, the central green column highlights the key-components that go into standard Multiplet Ligand Field Theory (MLFT). Note the need for many local environmental components such as the crystal field and charge transfer leads to a large increase in free parameters, limiting predictive capability. Next, the leftmost column shows my progress over the past 6 months building on work by Haverkort et al. [4] by applying a DFT + MLFT approach to ctc-XES, using the full-potential local-orbital (FPLO) DFT code to determine many, but not all parameters needed by the multiplet engine (Quanty). Using ‘reasonable’ values for the remaining undetermined parameters, I find excellent agreement between my new calculations and experiment for the environmentally important speciation of Cr3+ with respect to the carcinogenic Cr6+, see Figure 2. Third, as shown in the right column of Figure 1, I will use the real-space Green’s function code FEFF to both replace FPLO and also implement new calculation of the thus far undetermined parameters for the MLFT treatment [5]. The result will be the first truly parameter-free, first-principles MLFT treatment of ctc-XES. Moving to experimental validation: past, ongoing, and recently funded work in the Seidler group includes ctc-XES measurements of numerous elements in battery materials, oxygen evolution reaction catalysis, cements used in long-term storage of toxic and radioactive wastes, and carcinogens occurring in consumer products or industrial wastes (e.g., Cr toxicity, such as probed by Figure 2). This provides a plethora of testing grounds across numerous problems with high societal relevance. In this work, I will be able to collaborate with other students in the group to design reference standard studies, validate against my theory calculations, and then apply the resulting information to draw best inferences about the systems of actual interest. Fig. 2. Agreement between current theory and experiment for the K XES of Cr(III) and Cr(VI). Finally, the validated theoretical approach will be distributed to the general x-ray spectroscopy community via the workflow management tool Fig. 1. Theoretical MLFT workflow schema: Corvus and will also be used as the basis for ML standard approach (center), current progress investigations of the information content of ctc- (left) and future plans (right). XES. This latter work will start with unsupervised ML, such as t-SNE, which the Seidler group recently introduced as an important way to determine the chemical information content in vtc-XES and XANES [6]. This will be the first ML study of this kind to be applied to ctc-XES as traditional theory methods are either too inaccurate or too computationally expensive compared to the novel DFT+MLFT approach. This work will help determine which general problems are, or are not, well-encoded into ctc-XES across the periodic table and different chemical classes. Intellectual Merit: The interleaved characterization of local atomic and electronic structure poses central challenges across numerous problems, including electrical energy storage, catalysis research, aging of construction materials, toxicity in consumer products, environmental consequences of industrial wastes, and low-diffusion matrices for long-term storage of toxic or radioactive wastes, all of which still have open questions that require an MLFT approach to accurately describe. These questions come at a time of rapid growth in access to experimental ctc-XES capabilities via the development of lab-based instrumentation for education and analytical study (a trend led by Seidler group), major upgrades of synchrotron facilities and specialized XES end-stations for applications in energy sciences, and the steadily increasing application of XES in ultrafast x-ray free element laser (XFEL) studies probing chemical and electronic dynamics. This project will have a uniquely outsized impact not only because of the importance of the social and scientific problems being addressed but because of the synergy with the emerging experimental access to core-to-core XES capabilities. Additionally, the open access model of the tools developed in this research program will facilitate broad adoption within the x-ray community, bridging the gap between accurate ab-initio theoretical methods and the experimental need for reliable first-principles theory. Broader Impacts: Much of my prior experience in outreach has centered around introducing people to a side of science which focuses on the curiosity and intrigue sparked by the natural world around us. I firmly believe that to accomplish this, access to intuitive introductory tools is a necessity. As addressed in my personal statement, I will continue to develop and refine x-ray specific educational material such as the XAS-RW, addressing the acute need for qualitative and intuition based introductory material in my subfield. I will compliment this with fun, science-based community engagement efforts through groups such as the UW Science Explorers and UW Stem Pals to bring hands-on physics directly into the classroom. This will provide the ideal environment to expand upon my Physics Field Day event, as I couple my organizational experience with new community collaborations to deliver a unique, immersive program. Works Cited: [1]doi:10.1002/qua.24905;[2]doi:10.1002/cphc.201800038;[3]doi:10.1016/j.elspec.2021 .147061;[4]doi:10.1103/PhysRevB.85.165113;[5]doi:10.1039/B926434E;[6]doi:10.1039/D1CP02903G	Winner!
21	framework for a planet’s formation, evolution into its present state, and past and present geophysical properties such as magnetic fields and atmospheric conditions. In this era of prolific exoplanet discovery, the quest to investigate planetary interiors and surface conditions is more pressing than ever. With the growing number of exoplanets ranging in size from super-Earths to sub-Neptunes, and the omnipresent goal of “finding a new Earth”, it is becoming evident we need to concentrate our studies on such planets. Currently, radial velocity and transit methods used to detect exoplanets give mass and radius data for exoplanets but offer no compositional information. Comparing the mass and radii of exoplanets with mass- radius relationships of pure materials such as iron, silicates, and ice, and stoichiometric mixture thereof, offer a glimpse into their plausible bulk compositions [2-4]. However, attempts to infer bulk composition have resulted in degeneracy with many interior composition combinations fitting mass and radius values for a particular exoplanet. Many models assume planets are fully differentiated, yet previous works using density functional molecular dynamics (DFT-MD) simulations at high pressures and temperatures have predicted deviations from this model. For example, the miscibility of H O with H and He [5] and miscibility 2 of Fe and MgO [6]. Contrary to traditional models, we infer “fuzzy layering” if material is miscible at boundary conditions, which would result in the gradual mixing of heavier elements into the upper, less dense layers. DFT-MD simulations are a powerful tool in predicting the equation of state (EOS) of a wide variety of planetary materials and mixtures at conditions that are difficult to achieve empirically. Hypothesis. Some super-Earth and sub-Neptune exoplanets, termed waterworlds, contain a significant amount of water (H O) ice overlaying a magnesium silicate interior [7,8]. The stability of such a stratified 2 internal structure depends on whether these simplified two-layer models reflect realistic water world structures. Instead of relying on static two-layered models, I am motivated to explore the dynamics of material mixing at the magnesium silicate-ice boundary layers under P-T conditions relevant to the interior conditions of waterworlds. Understanding whether these two materials are miscible will help us better resolve the internal composition and stratification of water worlds. I propose to study the miscibility of a common high-pressure planetary magnesium silicate, enstatite (MgSiO ) [9], and high-pressure 3 water (H O) ice using DFT-MD. MgSiO will be referred to as rock and ice is assumed to be water ice for 2 3 the remainder of this proposal. Research Plan. (Research Goal 1: Building Rock-Ice Systems and running DFT Simulations) I will build the rock-ice systems and equilibrate each to a respective pressure in gigapascals (GPa) (Table 1). Table 1. Systems with MgSiO and H O crystal phases, space groups, and initial system pressure 3 2 System MgSiO phase Space group (MgSiO ) H O phase Space group (H O) P[GPa] x 3 3 2 2 1 ppv Cmcm [63] ice X Pn-3m [224] 120 0.29 2 pv Pnma [62] ice X Pn-3m [224] 60 0.29 3 pv Pnma [62] ice VIII I41/amd [141] 30 0.26 The crystal phases chosen for each material present at 0K were the most stable structures at each respective pressure (Table 1.). I will run simulations on each system using a canonical ensemble (constant number of particles N, constant volume V, and constant temperature T) increasing the temperature of each system from 500 K to 8000 K with the Nosé-Hoover thermostat [10]. I will perform simulations in 500 K increments in a “heat- until-mixes” approach, similar to the “heat-until-melts” method [11]. Although this approach is prone to overestimating melting temperatures, my goal is to calculate the upper bound P-T conditions for rock-ice mixing which will be accomplished with my MD. I describe each system by its ice to rock mass ratio [𝑚 /(𝑚 +𝑚 )], for example, System 1 has an ice to 𝐻2𝑜 𝐻2𝑜 𝑀𝑔𝑆𝑖𝑂3 rock mass ratio of 0.29. Then to investigate in which proportions rock and ice will mix I will simulate additional systems with ice mass ratios of 0.29 and 0.20. I will accomplish this by increasing the number of rock molecules while keeping the number of ice molecules constant. If the two materials spontaneously mix during the simulations, I will know rock and ice are miscible at this temperature. Preliminary results from my MD simulations of System 1, run at 8000 K (Fig 1.) show exciting, novel results of miscibility. (Research Goal 2: Determining Miscibility) An efficient way to detect mixing is to analyze how far atomic species move from their original positions by calculating their mean squared displacement (MSD). When the diffusion coefficient for all species is above zero, I will consider the system fluid. However, I will not know based on MSD alone whether the atoms crossed the rock-ice interface. The system could contain molten rock and water which remain immiscible instead of forming a homogeneous mixture. Therefore, I will also visualize each trajectory and verify that diffusion occurs across the boundary. My final method for confirming miscibility is to calculate the radial distribution functions (RDF) of magnesium (Mg) and silicon (Si) versus the oxygens (O) in MgSiO , termed rock oxygens, and oxygens associated 3 with ice, termed ice oxygens. My goal is to show that Mg and Si lose coordination with the rock oxygens and interact with the ice oxygens. For example, when I plot Mg−O and Mg−O , at the same temperature, rock ice if rock and ice are miscible, their radial distribution curves should overlap. Intellectual Merit. In addition to working with my Ph.D. advisor, Dr. Burkhard Militzer at U.C. Berkeley, I will collaborate with Dr. Sarah Stewart, a professor in the Earth and Planetary Science Department at U.C. Davis, to perform dynamic smoothed particle hydrodynamic (SPH) simulations of colliding planetary bodies. This will give insight into whether these large impact events produce the conditions necessary for material mixing. It is important to determine post impact conditions because giant impacts govern an important stage of planet formation, mold their interiors, and drive geophysical properties [12]. Multicomponent EOSs of material mixing will shift the planetary science community’s focus from static planetary models, where fully differentiated layers are modeled, to dynamic ones which include chemistry deep within the planet. If we neglect the presence of “fuzzy layers” within planets, we may miss key planetary properties such as its thermal evolution and magnetic field generation, which influence other properties such as tectonics, outgassing, dynamics, and volcanism. I plan to continue investigating rock-ice miscibility by considering other rocky material, such as Mg SiO or MgO with H O, and exploring lower 2 4 2 pressure regimes [8]. Moreover, provided that the necessary conditions are reached, I will further my research to elucidate whether a homogeneously mixed rock-ice layer could persist over long periods of time and even become stably stratified within water worlds. This will affect overall heat flow throughout the planet which will help us better understand the evolution of water worlds. Broader Impacts. My proposal has applications in a diverse range of disciplines such as condensed matter physics, high energy density physics, geochemistry, and geophysics. I will publish my work in journals (e.g. PNAS), present at conferences (e.g. AGU and APS), and most importantly continue outreach by presenting my research at local, public seminars (e.g. BASIS, SLAM, and Compass Lectures). I spent my first summer as a graduate student volunteering at two workshops recruiting students to pursue graduate school in planetary science. I also began tutoring environmental sciences at San Quentin Prison which helps keep me informed on challenges facing our most at risk communities and how to aid in their success as aspiring geoscientists. After only one year in graduate school, I have already helped form the first Unlearning Racism in the Geosciences (URGE) pod at Berkeley. We will present our pod’s work of integrating URGE deliverables into the Berkeley EPS department-level strategic plan for enhancing diversity at AGU 2021. The NSF fellowship will allow me to produce and share my findings with my peers as well as a general audience, increase equity in my field, and recruit the next generation of geoscientists. References. [1] B. J. Fulton et al. The Astronomical Journal 154, 109 (2017). [2] A. Vazan et al. arXiv preprint arXiv:2011.00602 (2020). [3] O. Shah et al. Astronomy & Astrophysics 646, A162 (2021). [4] S. Seager et al. The Astrophysical Journal 669, 1279 (2007). [5] F. Soubiran et al. The Astrophysical Journal 806, 228 (2015). [6] S.M. Wahl et al. Earth and Planetary Science Letters 410, 25 (2015). [7] M.S Marley et al. Journal of Geophysical Research 100,348 (1995). [8] T. Kim et al. Nature Astronomy, 5, 815-821 (2021). [9] T Duffy et al. Front. Earth Science 7, 23 (2019). [10] N. Shuichi Progress of Theoretical Physics Supplement 103, 1 (1991). [11] G. Robert et al. Physical Review B, 82, 104118 (2010). [12] P.J. Carter et al. Journal of Geophysical Research: Planets 125, 1 (2020).	Winner!
22	Introduction: Externally actuated micro/nanorobots have generated considerable excitement over the last decade due to their potential to carry out controllable microbiological tasks.7 Specifically, microrobotic swarms,containingtensofthousandstomillionsofindividualrobotsthesizeofbacteria,havethecapacity to perform diagnostics and directed drug transport within deep tissues and microvasculars hitherto inac­ cessiblebyconventionalmeans.7 Unlikeindividualmicrorobots, swarmsleveragethecoupledinteractions between constituents to form large­scale collective motions that far exceed the speed, strength, and func­ tionalityofa singlemicrorobot. Because theseswarms areexternallyactuated bymagneticor opticfields, actuation schemes can be programmed to control a swarm’s collective motions and morphology. Specific examplesincludeclustering,swirling, dispersion,orribbonformation, whichtogetherallowforhighenvi­ ronmentaladaptivity.4 Unfortunately,experimentationaloneisinsufficienttoproperlydesignmicrorobotic swarms for real­world applications; instead, efficient computational tools are needed to augment experi­ ments by enabling rapid investigation into the effect of design parameters. However, modeling swarming microrobotsisinherentlychallenging—owingtothetheoreticalandcomputationalcomplexityofresolving the many­body hydrodynamic effects and short­ranged collisions. No state­of­the­art method is currently capable of accurately simulating real­world microrobotic swarms. This issue is further exacerbated by the generallackoffundamentalunderstandingofhowtobestgenerateandcontrolaswarm’scollectivemotions for specific tasks, especially within confined microfluidic environments. I aim to overcome these chal­ lengesby(1)creatingthefirsthigh­fidelity,scalablecomputationalframeworkabletosimulatedense suspensions of complex­shaped, microrobots and (2) numerically investigating how key parameters, likerobotshape,actuationscheme,andgeometricconfinementaffectaswarm’scollectivemotions. IntellectualMerit: Accurate simulation of microrobotic swarms within real­world microfluidic environ­ ments is essential if these systems are to be designed for practical biological applications. Previous work toward modeling these systems has primarily focused on dilute suspensions, where the long­range hydro­ dynamic interactions dominate the system dynamics, allowing the effect of near­body dynamics to be ap­ proximatedbyrepresentingcomplex­shapedparticlesintermsofsimplegeometrieslikespheres,ellipsoids, or rods. However, as the number of particles per unit volume increases, near­body interactions become increasinglyimportantcausingtheseapproximationstobreakdown. Withindensesuspensions,evenseem­ ingly insignificant modifications to robot shape, like using spherical robots vs cubic robots, will result in drastically different close­to­contact dynamics, which directly impacts internal pattern formations. There­ fore,fordensemicroroboticswarms,itisimperativethatthenear­bodydynamicsbetweencomplex­shaped particlesbeaccuratelycapturedtocorrectlypredict/controltheircollectivemotions. Task1­Isogeometricanalysis: Toaddresstheseissues,Iproposetodevelopahighfidelitymodelcapable ofaccuratelyresolvingthehydrodynamicinteractionsbetweencomplex­shapedmicrorobots. Guidedbythis goal,IhavebeenworkingindirectcollaborationwithProf. B.Shanker(anelectromagnetsexpert)andProf. H.M.Aktulga(ahigh­performancecomputingexpert)todevelopanisogeometricboundaryintegralmodel, which I am implementing within Python. The fundamental principal of isogeometric analysis is to utilize smoothbasisfunctionstorepresentparticlegeometriesandthephysicsontheirsurfaces,therebyproviding higherorderdescriptionoffieldsandenablingaccurateresolutionofnear­bodydynamics. Towardsthisend, Ireformulatedanexistingboundaryintegralsolver3 basedontheassumptionthatmicrorobotsaretypically genus­zeroshapes,allowingmetopullbacksurfacequantitiestotheunitsphereandthendiscretizeinterms of spherical and vector spherical harmonic basis functions. I then solve the governing boundary integrals through Galerkin’s method. One of the challenges when solving hydrodynamic boundary integrals is the evaluation of the nearly­singular integrals that arise when solving particle to self and particle to nearby particleinteraction. Toaddressthisdifficulty,Iderivedasingularity­freemethodforevaluatingparticleself­ interaction through established techniques of singularity subtraction/isolation. My next step is to integrate adaptivequadraturetechniquestohandletheinteractionbetweenclose­to­contactparticles. Oncecomplete, Iwillresolvetheeffectofno­slipconfinementsbyaddedadditionalconstraintstomylinearsystembasedon well­establishedmethods.6 Myfirstmilestonewillbetobenchmarkthismodelagainstanalyticalsolutions fortheflowbetweensphericalparticlesbothwithandwithoutconfinement. Task 2 ­ High performance software development: High fidelity simulation of microrobot swarms is computationally intensive and requires fast, scalable numerical methods to make modeling real­world sys­ temsfeasible. Thekeybottleneckistherapidcomputationofpairwisehydrodynamicinteractionsbetween 𝑁 particles, which scales as 𝒪(𝑁2). To overcome this issue, I will integrate my hydrodynamic solver into theparallelcomputingframeworkdevelopedbyProf. Shanker. ThisFORTRAN­basedframeworkcenters aroundtheAcceleratedCartesianExpansions(ACE)algorithm,whichreducesthecomputationalcomplexity ofevaluatingthepairwiseinteractionsfrom𝒪(𝑁2)to𝒪(𝑁).1Toaccomplishthistask,Iwillfirstconvertmy currentPythonimplementationintoFORTRAN.Iwillthendevelopsuitabledatastructuresfortheefficient computationandcommunicationofsphericalharmoniccoefficientsandcreatecustomMPIcommunication schemes for the tonsorial kernels that arise in our calculations. These implementation details are vital for ensuring that our framework remains computationally tractable and will be validated based on scalability. TheNSFGRFPwillsupplementourcomputationalresourcesbyprovidingaccesstoXSEDE,enabling metofullyharnessthecapabilitiesofthishigh­performanceframework. Task 3 ­ Dense microrobotic swarms under rigid confinement: The simulation of dense microrobotic swarms requires simultaneously resolving the hydrodynamic interactions and short­ranged collisions be­ tween particles. Unfortunately, traditional collision resolution algorithms become numerically unstable when applied to dense assemblies. To overcome this limitation, my collaborator Dr. W. Yan (a compu­ tationalbiologist)developedacollisionresolutionalgorithmusinggeometricallyconstrainedoptimization.5 Throughthiscollaboration,IwillexpandDr. Yan’sexistingopen­sourcecode­basetoincludefastmethods forevaluatingthedistancefunctionsandsurface­normalsbetweencollidingnon­convexparticlesbasedon advances within the computer graphics community.2 I will then couple this code­base with the framework developed in Task 2. Once complete, I intend to leverage the speed and flexibility of this computational tool to analyze how key parameters like robot shape, robot actuation type, and confinement geometry af­ fectaswarm’scollectivemotionsandpatternformations. Bysystematicallyperformingsimulationswithin thisexpansiveparameter­space,Iintendtoprovideexperimentalistswithacomprehensivepictureofhowto bestdesigntheirmicroroboticswarms. Toquantifytheeffectstheseparametershaveonaswarm’scollective motions,Iwillutilizemyexistingpost­processingtoolkit,whichIhaveappliedtoactivemattersystemsfor extractingtheirlarge­scaletopologicalstructuresandensembleaveragedstatistics. Basedontheseresults,I williterativelydesignloadingschemesandrobotshapestostreamlinethetransportandcapturingof large­scalecargowithinsimulatedmicrovascular­likeenvironments. BroaderImpacts:Controllablemicroroboticswarmshavethepotentialtoprofoundlyimprovepublichealth by facilitating novel treatment methods like the transport of chemotherapy drugs directly to cancer sites.7 My work’s efficient computational framework will augment existing experimental techniques by enabling researchers to virtually prototype their swarm designs within realistic environments. To facilitate the use of this framework by others, I will open­source and thoroughly document all software I develop. In doing so, Ihopetohaveafar­reachingimpactonthefieldsofsoftcondensedmatter, robotics, andmicrofabrica­ tion, whichcouldbenefitsignificantlyfromamodelfordenseparticulatesuspensions. Furthermore, Iwill disseminatethisresearchtonon­engineersbyparticipatingintheAlliancesforGraduateEducationandthe Professoriate’sChalkTalks,whichseektodistillcomplexscientificworksforgeneralaudiences. References:[1]Baczewski,A.D.,etal. 2010,JournalofComputationalPhysics,229[2]Bender,J.,etal. 2014,Comput. Graph. Forum,33,246–270[3]Corona,E.,etal. 2017,JournalofComputationalPhysics, 332, 504 [4] Xie, H., et al. 2019, Science Robotics, 4, eaav8006 [5] Yan, W., et al. 2019, The Journal of ChemicalPhysics,150,064109[6]Zhao,H.,etal. 2010,JournalofComputationalPhysics,229,3726[7] Zhou,H.,etal. 2021,ChemicalReviews,121,4999	Winner!
23	Background: Consistent individual differences in behavior – or “personalities” – are ubiquitous in animals and have long captivated biologists1,2. Individual differences are a prerequisite for natural selection, and many evolutionary biologists explore how variation in behavior predicts survival and reproduction. Meanwhile, neuroendocrinologists experimentally alter an animal’s internal state to change behavior. Only recently have these disciplines been integrated to begin evaluating the mechanisms that maintain natural individual differences in adaptive behaviors in wild animals3. So far, this work has advanced our understanding of the neural mechanisms of social behavior, but remarkably, has found few patterns linking brain gene expression to individual behavioral differences4,5. This suggests that top-down processes are missing key determinants of individual variation in behavior. Certainly, behavior requires more than motivation; it also requires that both brain and body are properly fueled. Thus, I propose that peripheral metabolic processes may be the fundamental force driving consistent individual differences in behavior. The liver is the primary driver of metabolism and is under high demands to power the brain and muscle to execute energetically expensive behaviors (Figure). In times of endurance, the liver undergoes ketogenesis to secrete ketone bodies into the blood for organs to use as energy6. This metabolic pathway is associated with behavioral variation: Migrating birds use ketogenesis to maintain energy homeostasis7, and racehorses have higher ketone levels during long-distance compared to short-distance races8. Given supplemental ketones, bees behave more aggressively9, and human athletes improve exercise efficiency relative to controls10. These observations suggest that variation in ketogenesis is critical for performing energetically expensive behaviors, but this has never been assessed at the individual level. Consequently, there is uncertainty about how natural selection maintains animal personalities. I hypothesize that natural individual differences in behavior stem from variation in the ability to mobilize energy. I will test my hypothesis with two specific aims, focusing on social aggression in free-living female birds. First, I will assess how natural differences in aggression correlate with: (a) hepatic HMGCS2, the rate-limiting enzyme in ketogenesis, and (b) beta-hydroxybutyrate (BHB), the main ketone body produced6. Second, I will manipulate circulating BHB and test effects on individual aggressiveness in a repeated- measures design. Both aims build off preliminary data I generated in my first year as a PhD student. Study system: Tree swallows (Tachycineta bicolor; TRES) are obligate secondary cavity-nesters; they cannot excavate a nesting site and must fiercely compete for a pre-made cavity to reproduce. Females readily take to artificial cavities (i.e. nestboxes), and they are more aggressive than males. Social aggression requires endurance, as females engage in extended aerial chases and intense physical attacks during competition for nestboxes. High aggression individuals have better body condition11 and are more likely to breed than low aggression females, showing that aggression is adaptive12. Such strong natural selection should erode this trait variation, and yet substantial individual differences in aggression persist. Preliminary data: Last spring, I conducted 5-minute simulated territorial intrusions (STIs) on free-living female TRES. I measured aggression (e.g. time spent hovering, diving, pecking) towards a conspecific decoy placed at the nestbox. Individual aggression was repeatable in consecutive STIs (R=0.90; p<0.001). 2-7 days after the last STI, I collected 10 high and 10 low aggression females and conducted a genome- wide analysis of their brains (i.e. RNAseq in hypothalamus and amygdala). Despite a well-powered design, I found very few differentially expressed genes between high and low aggression birds, indicating that substantial behavioral variation cannot be explained by differences in baseline neural gene activity. These results further support my hypothesis that behavioral differences emerge beyond the brain. AIM 1: To what degree do individual differences reflect variation in ketogenesis? From these same high and low aggression birds, I will extract RNA from the liver, where I confirmed HMGCS2 is highly expressed based on TRES transcriptomic data13. Using established lab protocols, I will design HMGCS2 primers and perform qPCR to measure HMGCS2 gene expression, running samples in triplicate for HMGCS2 plus two endogenous control genes. I will also quantify BHB concentration in 10μl of blood from these individuals, using test strips read by a handheld ketone meter that is already validated in wild birds14. I will employ linear models to examine the degree to which natural variation in aggression is predicted by HMGCS2 expression, BHB concentration, or a combination of the two. AIM 2: How does experimentally manipulating BHB alter individual aggressiveness? I will expose incubating females to a commercially available BHB cream (BPI Keto Cream) applied to a fake egg in the nest for 12 hours. As the female incubates overnight, BHB will be absorbed via the brood patch, a featherless area of vascularized skin on the belly. Control females will receive a fake egg with a vehicle cream. Past work has used this noninvasive approach to manipulate hormones in TRES15. Here, it will allow manipulation of ketone levels independent of handling-induced stress. In a within-subjects design, I will use 30-minute (prolonged) STIs to measure intensity and duration of aggression the morning before and after BHB (or control) treatment, analyzing results with a repeated-measures ANOVA. Blood BHB will be quantified in both groups after the second STI using the ketone meter described in Aim 1. I will also separately validate that BHB treatment elevates BHB blood concentration in a subset of birds. Predictions, Alternatives, & Next Steps: I predict that individual variation in aggression will positively correlate with both HMGCS2 and BHB, with greater levels in high vs. low aggression individuals (Aim 1). Likewise, I predict that individuals will increase aggressiveness in response to supplemental BHB (Aim 2). Support for my hypothesis in Aim 1, but not Aim 2, would suggest that birds must engage in prolonged competition to promote ketogenesis, considering that females in Aim 1 were unprovoked at the time of collection. In this case, a future step would be to assess ketogenesis during sustained competition by manipulating nestbox availability, which is shown to increase aggression and metabolically challenge the brain16. I am also well-positioned to explore additional tissues from these same birds, such as the pectoral muscle where BHB is converted into usable fuel6. Nearly all TRES fighting occurs in flight, suggesting the pectoral muscle is a promising tissue to connect energetic constraints to social behavior (Figure). Intellectual Merit: Individual differences serve as the raw material for evolutionary change, leading to the diversity of behaviors seen in nature. However, we have limited insight into the origin of this variation. My work explores the potentially critical role of peripheral energetics in shaping natural individual differences in the wild. Recent work reveals other routes by which the periphery influences brain and behavior (e.g. gut-brain axis, microbiome), a view that my research extends. In the long-term, my proposal will not only build the foundation of my dissertation, but it will also serve as a springboard for applying energetic perspectives more broadly, to understand how metabolism accounts for diverse behavioral differences within and among species. Ultimately, my work will examine how both evolutionary and proximate mechanisms work together to build an aggressive female, an overlooked perspective in a field that, since Darwin, often assumes that females do not compete or that their aggression is just like that of males. Broader Impacts: As a first-generation, biracial graduate student, I strive to diversify STEM by helping historically underrepresented undergraduates overcome institutionalized barriers, thereby demystifying academia’s hidden curriculum. My efforts include the creation of a “how to” guide for applying to graduate school, which I disseminated to local and national groups. As a co-facilitator of an anti-racism group at Indiana University (IU), I developed action plans to hire and support diverse undergraduate researchers in my lab. These efforts set the foundation for my goals as a graduate student and future faculty member to improve recruitment and retention in STEM. Mentorship is central to this plan. I honed these skills with mentorship training while working with an undergraduate mentee in IU’s Center for the Integrative Study of Animal Behavior NSF REU summer program. Moving forward, I will work with the Jim Holland Summer Enrichment Program, which provides research experience for high-achieving minority high school students and helps them transition into an IU STEM major, extending my efforts to broaden inclusion. References: 1Koolhaas et al. Front Neuroendocrinol (2010). 2Sih et al. TREE (2004). 3Hofmann et al. TREE (2014). 4Bell et al. Behaviour (2016). 5Benowitz et al. Behav Ecol (2019). 6Grabacka et al. IJMS (2016). 7Frias-Soler et al. Biol Lett (2021). 8Volek et al. Metabolism (2016). 9Rittschof et al. J Exp Biol (2018). 10Dearlove et al. Med Sci Sports Exerc (2021). 11Rosvall. J Avian Bio (2011). 12Rosvall. An Behav (2008). 13Bentz et al. Sci Rep (2019). 14Sommers et al. J Field Ornithol (2017). 15Vitousek et al. Proc B (2018). 16Bentz et al. PNAS (2021).	Winner!
24	A understand how neural circuits, assembled through genetic programs, can give rise to complex behavior. Through evolution, species display a wide range of behaviors, some of which have been mapped to specific genetic variations1. Genes that mediate complex behavior must act via neural circuits, yet little is known about these intermediate changes. In this proposal, I will bridge this knowledge-gap by investigating neural circuit differences that determine B vocal communication behaviors in two closely-related rodent species. Background and Rationale: Using sounds to communicate is widespread in nature — from croaking frogs, duetting birds, to us, humans, engaged in conversation. Our lab has recently discovered a rodent species (Alston’s singing mice) FIG. 1: (A) Phenotypic variation in that engages in similar fast vocal interactions. Singing mice vocalizations of the lab mouse and Alston’s breed in captivity, can be maintained in a colony, and show singing mouse. (Spectrograms from the stereotyped vocal behaviors even in laboratory settings. Phelps lab, U.T. Austin) (B) Divergence and Additionally, we have already established the use of viral tools duplication model as observed in cerebellar for mapping, manipulating, and measuring neural circuits2. nuclei8. Singing mice (Scotinomys teguina) and lab mice (Mus musculus) are separated by a few million years of evolution (Steve Phelps lab, unpublished), are roughly the same size, and brain slices are largely indistinguishable between the species. Yet, there are key differences in their vocal repertoires; Lab mice produce only short, variable ultrasonic vocalizations (USVs), while S. teguina produce both USVs and human-audible ‘songs’ (Figure 1A). Crucially, unlike singing mice, lab mice do not participate in vocal turn-taking. Thus, we ask: What are the neural circuit differences underlying this behavioral distinction? Though traditionally thought to be unique to the primate lineage, our lab recently demonstrated robust motor cortical control of vocal behavior in the singing mice. Using four complementary lines of evidence (intracortical micro-stimulation, stimulation induced vocal arrest, focal cooling and pharmacological silencing), we defined a region of orofacial motor cortex (OMC) that mediates flexible vocal behaviors in the singing mice2. In contrast, lab mice born without the entire cortex (including OMC) can still produce USVs3. Therefore, we predict that differences in the motor cortical circuitry between the lab mice and singing mice underlie differences in their vocal behaviors. We hypothesize that motor cortical control over vocalization in the singing mice evolved from the ancestral orofacial control neural circuits via a duplication of OMC followed by cell-type divergence (Figure 1B). This duplication- divergence model predicts the existence of a dedicated group of song-specific neurons in the singing mouse OMC with specific projection patterns to downstream vocal pattern generators in the midbrain and the brainstem. Using novel spatial transcriptomics and barcoded projection mapping methods developed in Tony Zador’s (my co-advisor) lab, I will determine the diversity of cell-types in the motor cortex and their downstream projection patterns in both the singing mice and the lab mice. Aim 1: Do motor cortical cell types differ between lab mice and singing mice? The duplication and divergence model suggests that neural cell types in the OMC of singing mice evolved in a spatially segregated manner. First, to determine differences in cell types, I will perform single cell RNA sequencing (scRNAseq) in the OMC of lab and singing mice. Analysis of scRNAseq data requires aligning sequenced reads to a genome, publicly available for the lab mouse and recently generated by our collaborators for the singing mouse (unpublished, Steve Phelps). Cell types will be identified using known marker genes found in the literature. We will identify potentially novel cell types as those which have no assigned identities based on canonical marker genes. While scRNAseq will allow us to quantify differences in neural cell types through in-depth transcriptomics, we lose spatial information. To determine spatial location of neuronal cell types, we will use a spatial transcriptomic method, BARseq, developed in the Zador lab4. This technique uses hybridized probes and in situ sequencing to determine spatially resolved expression data for hundreds of genes in parallel4. I am confident that I can perform this experiment as the Zador lab has a dedicated pipeline to complete this experiment and regularly performs spatial transcriptomic experiments. Aim 2: Do projection patterns of motor cortical neurons differ between lab and singing mice? To determine OMC projection patterns, I will first perform viral tracing experiments. I will inject AAV vector that expresses GFP into the OMC of both species and image the brains using confocal FIG. 2: MAPseq protocol involves injecting barcodes into target microscopy. While viral tracing can detect area and sequencing barcodes expressed in neural projections in bulk anatomical differences, this method downstream areas5. lacks accurate quantification of projections and cannot distinguish changes that occur on the single cell level. To address these inadequacies, we will also be performing MAPseq, a method for single cell tracing developed by the Zador lab. MAPseq is a method that uses virus to infect neurons with a DNA barcode that is expressed in the cell body and axon of the neuron5,6. Through dissection and sequencing, we can recover the projection patterns of thousands of individual cells (Figure 2). I am confident that I can perform these experiments as I have already generated preliminary results for the lab mouse OMC. Furthermore, we can combine MAPseq with our spatial transcriptomic method to correlate projection patterns and cell types4,7. Anticipated results: If the duplication-divergence model of the singing mouse OMC holds true, I would expect to observe the following results: (1) novel cell types in the OMC of singing mouse (2) the spatial location of these novel cell types to be located in a spatially distinct area, and (3) novel projection patterns, perhaps to brainstem pattern generators, correlated with these novel cell types. In summary, the duplication-divergence model predicts correlated changes in cell transcriptomes and their projection patterns. Of course, another possibility is that cell type and projection pattern differences occur independently. Even so, I will be able to distinguish independent changes due to the resolution of the outlined experimental design. Thus, we have designed experiments that will produce results whether or not our expected model (duplication and divergence) is true. Intellectual Merit: I anticipate three major contributions to neurobiological methods, as well as our understanding of the evolution of neural circuits. First, this study can identify distinct neural populations based on projection patterns and/or genetic markers. Identifying neural populations in this manner allows scientists to target these neural population for further functional validation and experimentation. Second, our results could identify genes underlying neural circuits in vocal communication, findings which could contribute to the development of better molecular tools for manipulating vocal circuits. Lastly, this study would provide insight into the evolutionary underpinnings and biological basis of vocal communication. Broader Impact: I plan to make code and data available on open-source websites including GitHub. During my time at NIH, I created a RNAseq tutorial and shared resources on my GitHub page in addition to uploading code I wrote for analyzing RNAseq data9. I plan to maintain my GitHub page and upload code developed for analyzing data collected through this project for other scientists to consult and use. In addition, I plan to create an online tutorial geared toward high school and/or college students that have little coding experience or exposure to bioinformatics. I also plan to publish our results in open access journals including uploading early drafts of the manuscript to bioRxiv to facilitate timely advancement of scientific knowledge. References: [1] Metz et al. 2017. Current Biology. [2] Okobi, Banerjee et al. 2019. Science. [3] Hammerschmidt et al., 2015. Scientific Reports. [4] Chen et al., 2019. Cell. [5] Kebschull et al. 2016. Neuron. [6] Han, Kebschull, Campbell et al., 2018. Nature. [7] Sun, Chen et al., 2021. Nature Neuroscience. [8] Kebschull et al. 2020. Science. [9] https://github.com/eisko/RNAseq/	Winner!
25	Background: With the growing implementation of inquiry-based labs in physics, students are no longer following rote procedures and are expected to utilize complex experimental skills like measurement uncertainty, experimental modeling, and computation, which require students to engage in sensemaking [1]. We view sensemaking as “a dynamic process of building or revising an explanation in order to ‘figure something out’—to ascertain the mechanism underlying a phenomenon in order to resolve a gap or inconsistency in one’s understanding” [2]. Given that existing research on sensemaking has focused on textbook problem solving, research is needed to understand how sensemaking appears in inquiry-based labs, given their increasing prevalence [3]. Physics is often viewed by students as a confusing, unapproachable subject; understanding student sensemaking is an important step in developing labs that are more accessible to a range of students, beyond physics majors. Preliminary Results: In response to the call for inquiry-based physics labs, as well as labs that serve as better preparation for pre-medical and other life science students, the PER group at the University of Utah has implemented Introductory Physics for Life Sciences (IPLS) labs, in which students investigate physical mechanisms in the context of biological systems. In my preliminary work with the group, I explored student sensemaking in IPLS labs, and I found that the instances of sensemaking led to students having a deeper understanding about the relationship between their data and the relevant physical systems. Furthermore, this initial analysis was instrumental in determining an appropriate theoretical framework for the proposed research. First, I noticed that students didn’t often fully articulate their thinking and thus my data was limited. To ameliorate this problem, I aim to use a think-aloud protocol in which students are encouraged to share all their thoughts during the lab investigations, which will provide a more complete data set. However, a limitation of interviews is that they are not as authentic, so I find it is important to triangulate interview data with the lab observation data. Second, I observed that students were frequently comparing their model that was generated as a result of collecting and analyzing data to their existing mental model of the relevant system. Theoretical Framework and Research Plan: For my research plan, I draw on two complementary theoretical frameworks. First, I will use the modeling framework for experimental physics, which was first developed for upper-division physics labs and functions on a recursive interaction between a student’s physical system model and their measurement model [4]. Given that measurement models are less common in introductory physics, I focus on a data- based model, which captures the focus of these labs where students are predominately analyzing data; this is a similar adjustment to that which has been implemented elsewhere [5]. Second, I will utilize epistemic games, which are defined as the rules and strategies that guide inquiry; in PER, this framework has been used to study structured problem solving and knowledge development tasks [6-8]. Defining an epistemic game includes specifying the target epistemic form, constraints, entry conditions, moves within the game, and transfers to other games [9]. Based on my initial analysis, student sensemaking in labs has characteristics that parallel the form of an epistemic game, e.g., making moves toward an end goal of resolving inconsistencies. These two frameworks are complementary as the recursive elements of modeling translate to moves in a game. Each framework on its own has certain limits, but together they are more comprehensive and powerful; they will allow for a rigorous analysis of student sensemaking in inquiry-based labs. The steps of my research plan are as follows: Step 1: I will identify all instances of sensemaking in the existing lab observation data, focusing on the classroom environment factors that contribute to the sensemaking. Step 2: Based on identified classroom environment factors from Step 1, I will write a task-based interview protocol intended to prompt sensemaking, and I will conduct these interviews with a different population of undergraduate life-science students. Step 3: I will first identify instances of sensemaking in the interview data. Then, using instances of sensemaking from both the observational data and interview data, I will code my data using a coding scheme developed for the different parts of the modeling process, based on the modeling framework (e.g., revision of the data-based model, comparison between models). Step 4: With the modeling framework analysis done in Step 3, I will define sensemaking epistemic games that occur in the inquiry-based lab environment by coding the data with key features of epistemic games. Depending on the prior analysis, I will either define one epistemic game that describes general sensemaking or a number of games that each describe a different form of sensemaking. The merger of these two datasets, as well as the combination of the modeling framework and the epistemic games framework will allow for an in-depth understanding of student sensemaking in inquiry-based physics labs. Intellectual Merit: The inquiry-based physics labs are designed to better replicate genuine research environments, as well as encourage students’ agency and scientific inquiry; as such, they are increasingly prevalent in undergraduate curricula. But yet, there is limited research on sensemaking in these labs. To address this hole in the literature, I will take a novel approach of combining two disparate theoretical frameworks, epistemic games and modeling, and two complementary data sets, to gain a holistic understanding of sensemaking in inquiry-based labs. Results will be new knowledge about sensemaking in these increasingly prevalent labs that can serve as a foundation for future lab research, along with an example of how to effectively combine disparate theoretical frameworks that can serve as a model for other scholars in the field. Broader Impact: Understanding student sensemaking in inquiry-based labs through the epistemic game framework will have direct instructional implications. For instance, we can improve training so that TAs and instructors can recognize sensemaking epistemic games in labs and utilize them in instruction, asking questions that prompt sensemaking, rather than recollection of facts. Such instructional changes will improve undergraduate physics education, and specifically better prepare pre-medical students for medical school and other life science students for postgraduate studies and careers. Life science students often view physics as a foreign subject; supporting pre-medical students in productive sensemaking in these labs may encourage more students to continue in these labs and be successful in long term careers in the medical field [10]. While typically unintentional, physics can be a “weed-out” course for these students, so these changes intended to increase retention have the potential to improve equity and contribute toward diversifying the medical field. Beyond the effect on pre-medical students, encouraging sensemaking is a step toward teaching students that they have relevant experience that can be used in a physics setting, making physics a more accessible subject for all. References: [1] Holmes, N. G. et al. Proc. Natl. Acad. Sci. (2015). [2] Odden, T. O. B. and Russ, R. S. Sci. Ed. (2018). [3] Odden, T. O. B. and Russ, R. S. Phys. Rev. Phys. Educ. Res. (2019). [4] Zwickl, B. M. et al. Phys. Rev. Phys. Educ. Res. (2015). [5] Vonk, M. et al. Phys. Rev. Phys. Educ. Res. (2017). [6] Tuminaro, J. and Redish, E. F. Phys. Rev. Phys. Educ. Res. (2007). [7] Hu, D. et al. Phys. Rev. Phys. Educ. Res. (2019). [8] Chen, Y. et al. Phys. Rev. Phys. Educ. Res. (2013). [9] Collins, A. and Ferguson, W. Edu. Psych. (1993). [10] Moore, K. et al. Am. J. Phys. (2014).	Winner!
26	Introduction: Applications that demand large cooling capacities, such as high-power electronics, rely on the elevated thermal energy density that is associated with the latent heat of vaporization. Flow boiling of a liquid in a channel is one of the most effective approaches for ensuring high heat transfer efficiencies. For example, single-phase convection can remove heat with a rate up to 20 kW/m²K, whereas flow boiling easily reaches 100 kW/m²K.1 Flow boiling within microchannels increases the heat transfer potential even further, due to the enhanced surface-to-volume ratio. Many electronics and microelectromechanical systems (MEMS) with high power densities thus rely on microchannel flow boiling as their primary cooling strategy. Consequently, an efficient and reliable operation and control of this chaotic multi-phase flow process is hence indispensable. In both macro- and microchannels, the two-phase mixture can be characterized by different flow regimes, which are defined by the relative amounts and configuration of the liquid and vapor phases, and ranges from bubbly flow (few vapor bubbles in the center) to slug flow, annular flow, and eventually mist flow (few liquid droplets in the center). Despite the great promise that two-phase flow boiling poses, it presents many challenges. For example, dry-out, or the critical heat flux (CHF), occurs at the onset of mist flow and leads to a drastic rise in wall superheat that can induce material thermal fatigue and ultimately failure. Unfortunately, its occurrence and location there-of is difficult to predict, leading to the implementation of large safety margins during the initial design of the thermal management system. A different flow instability arises from the rapid expansion of bubbles, which causes pressure oscillations that can ultimately jeopardize the structural integrity of the channel and even initiate flow reversal.2 These instabilities have presented major challenges toward flow boiling applications and serve as a focal point for recent studies. To overcome the existing limitations, I propose to use machine learning (ML) to detect and ultimately control the onset and nature of these flow instabilities. Autonomous detection and self-stabilization of flow boiling instabilities would significantly enhance the reliability of two-phase cooling systems, while reducing costs and greenhouse emissions. Intellectual Merit: The use of ML in thermal management is a novel method for predicting heat transfer properties. A recent publication by Ravichandran et al.3 has shown that neural network models can predict the margin to the boiling crisis in a pool setting. I propose that in flow boiling, the onset detection and solution to mitigating flow instabilities can similarly be accomplished through the implementation of a deep learning algorithm. Aim 1: Collect images and videos of various flow instabilities for different flow conditions. ML requires the collection of large amounts of data to train an algorithm. I will construct an apparatus for recording various instabilities. Korniliou et al.4 describes a method for microchannel fabrication onto a polydimethylsiloxane (PDMS) substrate with an infrared transparent tin indium oxide back wall. I will implement IR imaging to record wall surface temperatures while simultaneously recording high speed optical imagery at identical frame rates. The setup will include a pressure transducer mounted at both ends of the channel to monitor differential oscillations. A micropump will serve to circulate the fluid after it passes through a vacuum degasser. The ratio of heat flux to mass flux has been used to quantify microscale instabilities by defining their oscillation periods.5 I plan to induce various instabilities within the microchannel by adjusting flow rate and heat generation while recording the resultant effects on temperature and pressure. Each instability will be categorized into an appropriate set of resultant conditions. In addition to recording my own footage, I will also use data available in open literature and contact authors of recent publications to share their data. Aim 2: Train a machine learning algorithm to predict each type of instability based on pre- defined conditions. I propose to train an artificial neural network (ANN) model by inputting measurements of pressure oscillations, nucleation site density, surface temperature changes, and bubble movement into each instability category. A feature ranking algorithm will be implemented to define the key measurement parameters. I will incorporate Google’s machine learning library TensorFlow to accomplish these tasks. The data will be split appropriately among sets for training, validating, and testing the model. Model evaluation will be conducted through 10-fold cross validation to ensure proper fitting. I will utilize the model’s mean absolute percentage error as the primary evaluation metric. The results of the model will be used to determine the corresponding flow, thermal, and differential pressure conditions associated with specific instability types. Broader Impacts: This project serves as a vital proof-of-concept for validating the use of machine learning to detect flow instabilities. Time permitting, I plan to program the model into a closed-loop control algorithm. Upon the detection of a specific instability, the algorithm would contain and implement a pre-programmed solution, such as the adjustment of the flow rate or pulsation of ultrasonic waves. This solution will serve as a pioneering step toward the implementation of machine learning in thermal management and demonstrates a potential method for revolutionizing the peak performance and safety of electronics cooling applications. The development of a working control algorithm would allow for system optimization and self-stabilization, which would answer direct needs of the thermal fluids community. During our STEM club sessions for EduMate NYC, as introduced in my personal statement, we received strong positive feedback from the students during the computer science presentation. I therefore intend to use this project to initiate an Introduction to Artificial Intelligence virtual workshop for high school and middle school students in the St. Louis area. The St. Louis metro region is one of the most segregated cities in the United States, and its long history of racial disparity contributes to educational inequity to this day. I will discuss the fundamentals of artificial intelligence, including machine learning, natural language processing, and computer vision in weekly sessions throughout 5 weeks in the summer months. An additional 5 weeks will be spent discussing the applications of artificial intelligence, such as robotics, transportation, and my proposed project. The goal behind these events is to create a foundational understanding of artificial intelligence while inspiring students to pursue these topics in their future studies and careers. Allowing young innovators to develop skills in one of the most important technologies today is valuable for contributing to their future success. I will collaborate with the St. Louis Academy of Science to host these events and provide outreach assistance. References: [1] Bergman, et al. (2011) Fundamentals of Heat and Mass Transfer [2] O’Neill, et al. (2020) International Journal of Heat and Mass Transfer [3] Ravichandran, et al. (2021) Appl. Phys. Lett. [4] Korniliou, et al. (2017) Applied Thermal Engineering [5] Prajapati, et al. (2017) Experimental Thermal and Fluid Science	Winner!
27	Hypothesis: The compositional evolution of crustal material (both felsic and mafic) following impact events can be simulated by heating and partially vaporizing such materials at high temperatures in containerless experiments. Varying the temperature (T), time (t), and oxygen fugacity (fO ) in the 2 experiments enables the creation of synthetic impact glass analogues. Comparing the textures and compositions of these analogues to those of natural glasses will inform how these materials evolve during the tektite formation process. Incorporating the results of partial vaporization experiments into mixing models of tektite formation will result in more realistic predictions of tektite parent materials and potentially explain why tektites do not form from mafic protoliths on Earth. Background: There are currently 190 confirmed craters on Earth’s surface1, only four of which are in basaltic targets. Lonar crater, which formed in Deccan Trap basalt, is the only well-known and fully accessible of these. During impact events, shocked and melted material may be ejected from the site of impact. Material that is thrown more than 2.5 crater diameters away from the impact location is known as distal ejecta2. Glassy, chemically homogenous, and aerodynamically shaped distal ejecta are known as tektites. Only four of the 190 terrestrial craters have resulted in tektite distribution over wide geographic regions known as strewn fields, and all known tektites originate from target rock that is felsic, approximating rhyolitic compositions. This research will investigate why tektites have never formed from mafic (basaltic) target rock during a terrestrial impact event, and to seek to better understand the nature and evolution of the starting materials that formed tektites from the four major strewn fields. Existing mixing models3–6 assume compositions of tektites represent idealized end-member mixtures of the melted target material, and most attempts to identify the parent materials involve general comparison among chemical components of tektites and predicted parent materials5. These models are overly simplistic as they do not account for the loss of chemical constituents to volatilization at high temperatures in the impact plume. Research Plan: (Research Goal 1: Tektite experiments) I will synthesize glasses that replicate the geochemistry and textures of tektites and Lonar crater impact glass in an aerodynamic levitation laser furnace7 (ALLF). Melting/vaporization experiments will vary T, t, and fO to identify conditions that form 2 Lonar glass analogues (Table 1). Table 1. Experimental conditions based on previous experiments estimating tektite thermal histories7 Series 1 2 3 4 5 6 7 8 9 T(°C) 1800 1800 1800 2000 2000 2000 2200 2200 2200 t (s) 10-120 10-120 10-120 10-120 10-120 10-120 10-120 10-120 10-120 fO Ar O CO+CO Ar O CO+CO Ar O CO+CO 2 2 2 2 2 2 2 Starting materials are Deccan Trap basalt, USGS basalt (BCR-2), and USGS rhyolite (RGM-2). Experimental methods will follow previous ALLF tektite experiments7. Approximately 10 mg of the starting material is heated with a CO laser at low power to fuse the sample and form spheres suitable for 2 levitation. The fused spheres are then levitated on a flow of gas (Ar, O , or CO+CO ) while being heated 2 2 with the laser for seconds to minutes. After heating to the desired T-t, laser power is cut, and the melt spheres cool quickly to form glass. The chemical, mineralogical, and textural characterization of synthesized and natural glasses will be completed via scanning electron microscopy with energy dispersive x-ray spectrometry (SEM/EDS) at Indiana University–Purdue University Indianapolis (IUPUI) and electron probe microanalysis (EPMA) at Washington University in St. Louis (WUSTL). These analyses will be compared to electron microprobe analyses of Lonar glass8, and previous work on felsic material7. Exploratory experiments using the starting materials and methods described above resulted in dark colored, glassy spheres with little to no vesicles or mineral growth. SEM/EDS analyses of these preliminary experiments show preferential loss of Na and K from all melt compositions relative to non-alkali components. Further, heating basalts in an oxygen-rich environment results in more rapid and complete loss of K O (undetectable after 10 s at 2000 °C). 2 (Research Goal 2: Mixing model calculations) Tektites are not produced from a single, homogeneous source rock – thus, any explanation of tektite generation must involve a multi-component mixing model involving likely upper crustal target rocks (sediments) as the major components9. Previous studies have estimated the contributions of possible target lithologies to final tektite geochemistry using mixing models. However, the assumptions and limitations of existing models may hinder their ability to realistically represent tektite formation. Previous mixing models assume that end-member compositions of likely protoliths are unchanged during tektite formation, i.e., they do not take into account the effects of evaporative fractionation3 on the target materials as they are heated in the impact plume. I will address this by executing computationally intensive mixing models that properly account for fractionation due to vaporization and assess the extent of agreement between assumed target lithologies and tektite geochemistry. I will incorporate my experimentally derived volatilization rates into dynamic multi- component mixing calculations by modifying the GeoChemical Data toolkit (GCDkit) software to create a more realistic representation of the processes attending tektite formation, and to determine the relative contributions of each protolith to different tektite compositions. I will also compare my experimentally derived volatilization rates, and the results of my mixing models with predictions of compositional evolution of tektite melts from MAGMA code. My ongoing research uses the R programming language to perform multivariate analyses and implement machine learning algorithms to investigate tektite compositional trends. I am creating an open- source web-based application to classify unknown tektites into their strewn fields and subgroups within strewn fields. This NSF Graduate Research Fellowship will afford me the opportunity to expand my programming and modeling portfolio to more computationally intensive applications. Intellectual Merit: The modification of planetary surfaces through impact cratering is the most important surface modifying process on most rocky bodies in the Solar System10, yet it remains underrepresented as a field of study. My research will improve both the understanding of the evolution of Earth’s crust and the modeling of impact events. My mixing model calculations will be the first to properly account for the chemical modifications that occur in the impact plume. These calculations will produce a useful tool for the scientific community to analyze (or reanalyze) terrestrial impact products and target lithologies. My experimental data will also provide valuable insight into the volatilization behavior of felsic and mafic melts at conditions relevant to impact plumes. The results may shed light on the absence of mafic tektites. Broader Impacts: The results of my research will have applications in a diverse set of fields such as high- temperature geochemistry, computational modeling of geochemical processes, and impact processes. I will disseminate the results in journals (e.g., Computers & Geosciences), at conferences (e.g., AGU, GSA), and to GK-12 students and the general public at outreach events (e.g., Pacers STEM Fest, Celebrate Science Indiana). As an NSF Graduate Fellow, I will continue working towards a more equitable and inclusive culture. Already as a new graduate student I became a founding member of IUPUI’s Geology COmmunity for Racial Equity (GeoCORE). I also came up with the idea for the Mineral Eponym Crowdsourcing Initiative (MECI), an effort to facilitate an examination, analysis, and synthesis of the origins of mineral names, and the messages this historic naming system has created. This initiative is currently being led by members of the Mineralogical Society of America (MSA) Diversity Task Force (my advisor is a member) to seek funding for its execution. Most recently, I have been invited to participate in a project to create new curricular materials for the IUPUI Earth Sciences Department focusing on issues at the intersection of Earth sciences, ethics, and equity. My role in this project will be to analyze, quantitatively and qualitatively, the assessment results of teaching material effectiveness. The analytical and computational skills I will learn as an NSF Fellow will allow me to perform analyses that will directly impact the inclusivity, diversity, engagement, and retention of underrepresented groups within the geosciences. References: [1] Earth Impact Database, PASSC [2] Glass, B. P. et al. Elements 8, 43–48 (2012) [3] Ackerman, L. et al. Geochim et Cosmochim Ac 276, 135–150 (2020) [4] Ferrière, L. et al. Chem Geol 275, 254–261 (2010) [5] Love, K. M. et al. 52, 2085–2090 (1988) [6] Meisel, T. et al. Meteorit Planet Sci 32, 493–502 (1997) [7] Macris, C. A. et al. Geochim et Cosmochim Ac 241, 69–94 (2018) [8] Ray, D. et al. Earth Moon Planets 114, 59–86 (2014) [9] Koeberl, C. Tectonophysics 171, 405–422 (1990) [10] Koeberl, C. in Treatise on Geochemistry 73–118 (Elsevier, 2014)	Winner!
28	Stress in the city: investigating the effect of urbanization on coyote oxidative stress and diet Background: As the world urbanizes, animals are adapting to novel environmental disturbances in human- dominated landscapes. The type and magnitude of these stressors vary considerably with human activity, culture, and socioeconomic status (SES), and each can influence the amount of biodiversity within a city1,2. Wealthier neighborhoods (higher SES areas) generally exhibit higher biodiversity and greater food availability, collectively known as the luxury effect1,2. Paradoxically, some low SES areas exhibit high biodiversity by providing greater refugia (e.g., abandoned buildings) for prey2. Overall, the luxury effect has repeatedly been shown to affect ecological dynamics at the community level, principally shaping species assemblages and interactions, which ultimately affect population and organismal ecology1,2. Few studies, however, have investigated how the luxury effect and urban stressors (e.g., light pollution) interact to affect the ecology and physiology of urban wildlife. Wildlife in urban and low SES environments face distinct environmental pressures relative to their conspecifics in rural and high SES areas1. However, how within-city differences in disturbance (e.g., noise pollution) affect stress levels and fitness outcomes is largely unexplored. Given that stressors in low SES neighborhoods are magnified1, organisms in these areas may experience greater oxidative stress, an imbalance between free radicals and antioxidants in the body4,5. Biological responses to oxidative stress can vary with environment, genotype, and activity levels5. Further, antioxidants gained through food can provide a defense against oxidative stress6. Anthropogenic food, however, is often protein-poor and low in antioxidants5. Hence, wildlife that exploits such resources are likely to develop health risks7 (e.g., hyperglycemia, which is correlated with low immunity8) and are at higher risk of the deleterious effects of free radicals4, including apoptosis5. Because these effects can reduce fitness4, it is imperative to uncover how human-driven impacts on habitat and diet shape an individual’s ability to cope metabolically with anthropogenic stressors (Fig. 1). My research will investigate the effects of within-city and among- city variation on the physiology of coyotes (Canis latrans). Specifically, I will test how oxidative stress and diet vary along an urban-socioeconomic gradient. Urban coyotes are well-suited model organisms to address the phenotypic consequences of variation in human disturbances within urban systems. Coyotes are ubiquitous across North America and have assumed the apex predator role in urban areas following the local extirpation of tertiary carnivores (e.g., wolves, Canis lupus)3. Moreover, coyotes often consume anthropogenic food9, but exhibit variation in diet9,10. As apex predators, stressors that affect coyote behavior or physiology will have top-down effects in urban ecosystems3. I predict low SES areas will represent poor habitat and diet quality via a reduction in prey diversity1. Therefore, coyotes will supplement their diets with a greater proportion of anthropogenic food subsidies relative to conspecifics in rural and high SES areas. I will trap coyotes (n=60) at 15 predetermined locations along an urban-socioeconomic gradient based on land cover, household density, and median household income11 across the Seattle-Tacoma, WA metropolitan region over three years during winter and summer. Coyotes show more restricted home ranges in urbanized areas and are unlikely to forage in non-adjacent territories12. I will collect blood and hair samples from captured animals and scat samples from within and around trapping sites. I will deploy GPS collars and use spatial data to determine the mean habitat type and SES area used by each individual, and relate these habitat measures to physiological data. Aim 1: Quantify oxidative stress variation in coyotes along an urban-socioeconomic gradient. H1: Coyotes in low SES areas are exposed to more stressors, leading to greater oxidative damage and hyperglycemia relative to rural and high SES coyotes. Alternatively, coyotes may cope with urban stress via access to higher prey diversity in low SES areas, where refugia for prey is more common. To quantify oxidative damage, I will analyze lipid erythrocyte proteins from blood samples for peroxidation4. Additionally, I will test for hyperglycemia by examining glycated serum protein levels8. 1 Cesar O. Estien Research Statement Aim 2: Determine the effect of urbanization on coyote diet composition and how diet influences their ability to mitigate oxidative stress. H2.1: Urban coyotes consume less natural food and more anthropogenic food than rural coyotes. Urban coyote fecal and hair samples will have lower nitrogen (δ15N) signatures, demonstrating a protein-poor diet, and higher carbon (δ13C) signatures, reflecting anthropogenic food consumption13. H2.2: Urban coyotes increase their antioxidant capacity by up- regulating antioxidant enzymes to cope with urban stress. To evaluate how coyotes mitigate stress, I will evaluate their (a) total antioxidant capacity, (b) activity of antioxidant enzymes4, and (c) diet composition. To evaluate (a), I will use the ferric reducing ability of plasma assay to describe the global antioxidant balance4; (b), I will measure glutathione peroxidase and superoxide dismutase activity4; (c), I will perform stable isotope analysis (using δ13C and δ15N)11 on hair and scat samples to determine diet composition (i.e., anthropogenic vs. non-anthropogenic food sources). Using a linear mixed-effects model framework with AIC model selection, I will analyze the effect of site characteristics (rural/low SES/high SES), sex, and reproductive status on oxidative stress and the effects of urbanization on diet and oxidative stress. Feasibility: My research project will bring a new avenue of research to an established system, the Grit City Carnivore Project (Dr. Christopher Schell, University of Washington). I will collaborate with the Urban Wildlife Information Network to effectively identify coyotes near trapping locations through existing camera trap data. Field sites and permits have already been approved. My field experience (trapping/sampling) and computational skills (database management/R), along with access to cutting-edge equipment and collaborators, will lead to the success of this project. Intellectual Merits: Understanding the consequences of urban systems on wildlife physiology and stress is essential to developing effective wildlife conservation plans. My proposed research will fill knowledge gaps by exploring the luxury effect in coyotes and identifying links between urbanization, SES, and oxidative stress. These results will bring a novel perspective to an emerging field investigating the influence of urbanization on the life-history of urban wildlife. This study will form a foundation for future studies on fitness outcomes and adaptation to oxidative damage and could establish coyotes as bioindicators that reflect the health of urban environments (e.g., high oxidative stress may reflect exposure to pollutants14). Further, this project will advance our knowledge of the biological processes within cities. Broader Impacts: (1) Community Engagement and Education: In addition to disseminating my results throughout the scientific community, I will also present these findings locally (e.g., Tacoma News Tribune, high schools). I will also engage directly with residents and students near urban trapping sites to observe coyotes closely and showcase the vibrancy of the urban biome. I will partner with Environmentalists of Color and The Nature Conservancy to create community engagement opportunities and accessible education materials in urban ecology, focusing on Black and Brown communities in the Seattle- Tacoma metropolitan area. I will work with Treehouse, an organization aiming to close the education gap between underrepresented foster youth and their peers, to develop engaging interdisciplinary assignments with real data that links math, science, and urban history. (2) Management Implications: I will leverage existing connections to work with Point Defiance Zoo and Aquarium and Woodland Park Zoo to develop workshops about urban wildlife natural history and conservation. My research will reveal how wildlife are modifying their behavior in urban areas and how wealth disparities in humans influence wildlife stress, which will help managers develop natural areas for urban wildlife. Through my collaborations, I will be able to interact directly with Seattle and Tacoma city officials to help develop environmental justice and urban conservation policies. References: [1] Schell et al. (2020) Science. [2] Kuras et al. (2020) Landscape Urban Plan. [3] Prugh et al. (2009) BioSci. [4] Herrera-Dueñas et al. (2017) Front. Ecol. & Evol. [5] Isaksson (2015) Funct. Ecol. [6] Arnold et al. (2010) Biol. J. Linn. Soc. [7] Strandin et al. (2018) Phil. Trans. R. Soc. B Biol. Sci. [8] Schulte et al. (2018) Cons. Physio. [9] Morey et al. (2007) Am. Midl. Nat. [10] Newsome et al. (2015) Oecologica. [11] Magle et al. (2015) Anim. Conserv. [12] Gehrt (2007) Proc. 12th Wildl. Damage Mgmt. Conf. [13] Windberg et al. (1991) J. Wildl. Dis. [15] Pérez-Coyotl et al. (2019) Env. Poll. 2	Winner!
29	Background: Martian ice likely holds the key to interpreting Mars’ past climate, but much is still unknown regarding the distribution and properties of Mars’ ice deposits. It is well known that Mars has extensive polar ice caps the size of Greenland. Included in these large polar caps are the north and south polar layered deposits (NPLD and SPLD, respectively), that are comprised of kilometers-thick deposits of water ice. In addition, surveys by Conway et al. (2012) and Sori et al. (2019) have identified craters in the surrounding terrains which contain “outlying” deposits of ice (Figure 1), which may or may not have formed at the same time as the deposition of the polar caps. These, as well as other efforts to identify and characterize ice, have used radargrams from the SHARAD (SHAllow RADar) sounding radar onboard NASA’s Mars Reconnaissance Orbiter (MRO) to analyze the subsurface in these icy areas, as radar images can essentially act as large-scale ice cores. However, additional data such as roughness and Figure 1. From Sori et al. (2019) dielectric constant of the shallow (<5 m) subsurface can be Locations of icy crater deposits near extracted from these radargrams by analyzing the reflectivity of the southern polar cap. All three the surface echo (Campbell et al., 2013; Castaldo et al., 2017; colors of points indicate outlying Grima et al., 2012). crater deposits that will be considered Research Objectives and Motivation: I propose to in this work. leverage this technique to analyze the physical properties of northern and southern outlying polar ice deposits. From my analyses, I will be able to draw conclusions about the purity, composition, and surface roughness of these ice deposits. Comparing similarities and differences between the ice deposits in the two hemispheres, as well as how the outlier deposits compare to their corresponding nearby polar ice caps, will allow me to assess how localized or global the climate processes were which led to the formation of polar ice deposits. In addition, I will analyze the subsurface radar echoes of the southern icy outliers and search for the existence of any buried CO deposits that may 2 have been sequestered from the atmosphere in past climate events (Figure 2). The main objective of this project is to identify differences in physical properties between southern and northern icy outlier deposits. Conway et al. (2012) found evidence that supports similarities in composition between the NPLD and outlying ice deposits located in northern craters. While the NPLD has been found to be made up of pure water ice (Grima et al., 2009), large deposits of CO ice have been found 2 sequestered in the SPLD (Phillips et al., 2011). Given the similarity in composition between the NPLD and northern outlying ice deposits, it stands to reason that such a similarity may exist between the SPLD and corresponding southern outlying ice deposits. This motivates my search for sequestered deposits of CO ice 2 within the southern outlying ice deposits. If such CO deposits are found, it would highlight a major 2 difference between the northern and southern outlier ice deposits, and would also place new constraints on the thickness of the atmosphere in Mars’ past. CO sequestered in the ice would have once been in the 2 atmosphere, which would have a significant impact on the Martian atmosphere and climate system. For example, the amount of CO that has been found in the SPLD alone is enough to have doubled the 2 atmospheric pressure on Mars pre-sequestration (Phillips et al., 2011). Even if sequestered CO deposits 2 are not found in these southern outliers, other differences in composition, purity, and/or surface roughness could provide exciting insights on the icy processes at work in each hemisphere. Methods: To determine if a difference in physical properties of ice in the northern and southern outlying crater deposits exits, I will examine the ice at two different spatial and temporal scales. First, I will use the reflectivity of the primary surface echo to infer the roughness and dielectric constant of the younger surface ice. Similar techniques have been used to make global maps of SHARAD parameters (Campbell et al., 2013; Castaldo et al., 2017; Grima et al., 2012), but have not been used to study these outlying icy crater deposits. This will allow me to compare the present-day conditions of ice in the northern and southern hemispheres of Mars. I will also leverage the subsurface capabilities of SHARAD to analyze older layers of ice below the surface in these outliers, with the additional goal of determining the existence of sequestered CO similar to that which Phillips et al. (2011) found within the SPLD. By analyzing the 2 properties of both subsurface and surface reflectors, I will be able to make comparisons of physical properties of northern and southern icy outliers as well as the climate conditions that could have formed them. Figure 2. Example SHARAD radargram from Phillips et al. (2011). Reflection- free subsurface zones (“RFZ”) were found to be pure CO2 deposits. Intellectual Merit: Such a study of the outlying ice deposits has not yet been done. The Martian ice acts as a record of the climate in which it formed, so understanding the ice in both hemispheres is necessary in order to be able to answer the questions of what conditions were present in Mars’ past. I expect that this study will provide new insights on the similarities and differences in composition, purity, and surface conditions of ice in the northern and southern hemispheres. Analyzing these properties can tell us how the ice was deposited; for example, very pure ice would be indicative of a deposition via snowfall, while low ice contents generally imply formation due to condensation of atmospheric water vapor within pore spaces of the regolith. Finding layers of ice with different properties would also provide strong evidence for changes in the climate, which could be linked to orbital/rotational variations analogous to Milankovitch cycles on Earth. This work may also confirm the presence or absence of sequestered CO 2 deposits in the southern outlying ice deposits, which if found would place new constraints on the thickness of Mars’ atmosphere pre-sequestration of the CO . 2 Purdue University is an ideal institution at which to carry out this research. Here, I am advised by Prof. Ali Bramson, who has extensive experience working with SHARAD observations of Martian ice (e.g., Bramson et al., 2015) and is a Co-I on the NASA-funded Subsurface Water Ice Mapping (SWIM) project, which uses multiple types of observations (including radar) to map subsurface ice through the mid-latitudes of Mars. I will also be working with Prof. Mike Sori (also at Purdue University), using his database of southern outlying ice deposits (Sori et al., 2019) as the set of southern deposits I will be characterizing. Broader Impacts: Indiana is home to the newest of our national parks: Indiana Dunes National Park (INDU). Using my experience as an Astronomy Ranger at Bryce Canyon National Park, I will work with the chief rangers of interpretation and education and INDU, Bruce Rowe and Kim Swift, to develop a night sky educational program that connects what we see in the sky to geology here on Earth. Though INDU is near the light pollution of Chicago, major planetary bodies are still visible. This night sky program will be held outdoors on the park’s West Beach weekly during the main summer season (April–November) and focus on water and sand dunes throughout the solar system that are also present in the park. This park is uniquely located near major metropolitan areas, and as such these programs will serve communities that are traditionally underserved by science outreach. More details on this planned project can be found in my Personal Statement. References: Bramson, A. M., Byrne, S., Putzig, N. E., et al. (2015). Geophys. Res. Lett., 42(16), 6566–6574. Campbell, B. A., Putzig, N. E., Carter, L. M., et al. (2013). JGR: Planets, 118, 436–450. Castaldo, L., Mège, D., Gurgurewicz, J., Orosei, R., & Alberti, G. (2017). EPSL, 462, 55–65. Conway, S. J., Hovius, N., Barnie, T., et al. (2012). Icarus, 220(1), 174–193. Grima, C., Kofman, W., Mouginot, J., et al. (2009). Geophys. Res. Lett., 36(3), 2–5. Grima, C., Kofman, W., Herique, A., et al. (2012). Icarus, 220(1), 84–99. Phillips, R. J., Davis, B. J., Tanaka, K. L., et al. (2011). Science, 332, 838–841. Sori, M. M., Bapst, J., Becerra, P., & Byrne, S. (2019). JGR: Planets, 1–21.	HM
30	The sparse and heterogeneous distribution of water in savanna landscapes largely determines herbivore distributions. Thousands of animals gather at watering holes and riverbanks, which become foci for both competition and predation. Species with relatively low water needs can take advantage of large, naturally occurring gaps (“refugia”) between persistent water sources to avoid competition and predation; in fact, these refugia can be critical to maintaining their populations1. Kruger National Park (“Kruger”), South Africa, offers a unique long-term experiment of the effects of surface water augmentation on herbivore distributions. Kruger was fenced in the early 1960s, obstructing historic migrations of grazers to dry- season water sources. In response, management installed hundreds of watering holes (“boreholes”), but these fragmented the dry refugia, allowing the ranges of drought-intolerant species (i.e. Equus quagga, Connochaetes taurinus) to expand into those of drought-tolerant, locally rare antelope1,2 (RA) (i.e. Taurotragus oryx, Hippotragus equinus, H. niger). This attracted predator attention to refugia and created more competition for forage during the severe droughts of 1981, 1985, and 19923. Surface water also attracts elephants, which can drastically alter the surrounding ecosystem and impact forage biodiversity4. This increased predation, competition, and ecosystem engineering all contributed to the alarming population declines in RA in the 1980s3. Realizing this, in 1997 Kruger’s management scheme was re- examined, and two-thirds of boreholes were closed4. The current conditions for Kruger’s RA are still fraught, however. Borehole closures have not returned refugia to their original extents4; many remain open for their touristic value (e.g. guaranteed presence of diverse species). In addition, climate models project that southern Africa will experience more frequent and intense droughts5, especially threatening Kruger’s drier northern areas, where most of the park’s RA refugia are located6. Motivation: Water access drives complex savanna dynamics. Lingering hydro-homogeneity in Kruger, along with projections indicating more frequent and intense droughts, make it imperative to understand how this community is structured, how species interactions shape responses to climate change, and how these might change in the future. I will determine (1) how surface water sources and resulting interspecific interactions affect how large herbivores respond to climate variables, and (2) how this changing community structure impacts resilience to extreme environmental events. I will also create a tool for Kruger park managers to infuse sharper knowledge of ecosystem structure into management aims. Methods: I will analyze herbivores’ interactions and responses using GJAMtime, a Bayesian time- series ‘generalized joint attribute model’ developed by Dr. Jim Clark7. GJAMtime models species distributions through environmental and interspecies interactions, improving upon traditional static species distribution models that fail to address interactions between species and temporal dependence on previous conditions. GJAMtime builds species migration, density-independent (DI) growth, and density- dependent (DD) interaction terms into a familiar Lotka-Volterra model and generates a species-interaction coefficient matrix (“a-matrix”), enabling dynamic community structure analysis8. In preparation, I have divided a map of the park into 710 30km2 grid squares, to which I either aggregated (borehole locations, fires, geology, climate variables) or interpolated (rainfall, grass abundance) annual environmental covariates. I determined Bayesian priors for species interactions and responses to covariates through review of savanna literature and personal communication with established savanna scientists. Aerial annual census data are rich; from 1977-98, park rangers counted all animals in parkwide airplane census; from 1998 onwards, 800-m wide transects replaced the expensive full-park census (providing 15-28% coverage instead)9. For transects, data on individual animals’ distance-from-aircraft were also collected. With knowledge of each species’ detection rate (a factor of coloring, preferred habitat, and size), I will derive species detection functions using the R package ‘DSim’ to estimate true herbivore abundances. Research Plan: This project extends my exploratory analyses, detailed in my personal statement, to include borehole and fire data, larger temporal extents, narrower hypotheses, and novel statistical tools. (1) How does surface water affect community structure, and how can this in turn limit RA population density? Hypothesis: If greater hydro-homogenization permits species invasion of refugia, then negative impacts on RA will be seen through (1) decreased populations and (2) negative a-matrix species interaction coefficients. Equilibrium population sizes are a function of migration, DI, and DD growth; I will compare equilibrium abundances and a-matrix coefficients from greater and lesser borehole concentration years to determine whether these boreholes negatively affect RA populations. I will compare pre-1997 data (high borehole density) to post-1997 data, the latter proxying a time with few boreholes. I will extract how much of each species’ climate responses are coming from migration, DI, and DD growth using GJAMtime estimates. If results do not support my hypothesis (negative effect of boreholes on RA population density), then this indicates the influence of other sources on RA population decline, including an increase in elephant disturbance after culling was ended in 1992; anthrax outbreaks in roan antelope in the early 1980s; or changes in fire management, extent, and intensity over time. (2) How does changing community structure affect the savanna ecosystem’s resilience to extreme events? Hypothesis: If these changing community structures reduce ecosystem resilience, then increasing hydro-homogeneity will (1) increase drought recovery time, (2) increase fire recovery time, and (3) increased community instability. Major droughts occurred in Kruger in 1981-82, 1985-86, 1991-93, 2014- 16, and 2018-20. These droughts straddle the borehole closures of the 1990s. Natural and prescribed fires, with varying intensities and times between burns, also occur regularly across the savanna. For intense local fires and parkwide drought events, I will compare each species’ population the year before the event to its population in subsequent years to determine population recovery time. I will compare these trends to a stability analysis of the a-matrix (with negative eigenvalues indicating stability) for these periods to see if community structure and species interactions are impairing ecosystem recovery after extreme events. If we do not see evidence for this, then the influence of management decisions (e.g. removal of fencing, end of elephant culling) or climatic interactions (e.g. fires during drought years) played a larger role than species interactions, setting the groundwork for future studies in community stability and resilience. Resources: The Kruger GIS Lab provided all data, and South Africa National Parks registered my project this fall. In March 2021, I will share my research at Kruger’s Savanna Science Network Meeting, discussing my assumptions and findings with other savanna scientists. I will also travel to Kruger in 2021 and 2022 to participate in dry season censuses and attend courses on savanna ecosystems. My math degree and two years’ coding experience at IBM prepared me well for the analysis of the complex models proposed in my study. I am advised by statistical and climate-change ecology expert Dr. Jim Clark and Kruger grassland ecologists Dr. Steve Higgins (University of Bayreuth) and Dr. Carla Staver (Yale). Intellectual Merit: This analysis is crucial to maintaining one of the world’s last Pleistocene megafauna savanna ecosystems. My novel statistical analysis will illuminate the relationship between species interactions and community environmental responses, a connection understudied in modern ecological literature. I also propose to determine how a changing community structure affects an ecosystem’s resilience to climate change, a pressing issue to today’s ecologists. Finally, my analysis of Kruger’s extensive, long-term datasets with GJAMtime’s Bayesian Gibbs-sampling techniques certainly meet the 2020 GRFP solicitation goal of supporting “computationally intensive research”. Broader Impacts: Adaptive park management requires use of near real-time animal census and climate data. I will provide park managers with a data workflow to feed each year’s new census data into my GJAMtime model. I will (1) convert GJAMtime’s outputs to be readable by non-statisticians (2) write change detection functions to highlight how herbivore responses vary from previous years, (3) convert the code from steps 1 and 2 into an open-source user interface in RShiny to allow park managers to run code without prior programming skills, (4) solicit regular feedback from park management, and (5) provide iterative, ongoing support and tool improvement through GitHub. These five steps provide a link between the environment-species interactions model and a more user-friendly interface for management decision- making. Additionally, as described in my personal statement, I will use my African savanna conservation research to develop open-source RShiny modules, based on Kruger herbivore and environmental data, to engage high schoolers in Durham Public Schools to boost their skills and confidence in math and coding. [1] CC Grant et al. 2002. Koedoe 45. [2] IPJ Smit 2011. Ecog. 34. [3] MP Veldhuis et al 2019. Ecol. Lett. 22. [4] IPJ Smit 2013. Pachyderm 15. [5] Working Group II, Fifth Assessment Report of the Inter- governmental Panel on Climate Change, 2014. In: Impacts, Adaptation, and Vulnerability, Part B: Regional Aspects. [6] JO Ogutu & N Owen-Smith 2003. Ecol. Lett. 6. [7] JS Clark et al. 2017. Ecol. Monogr. 87. [8] JS Clark, et al. 2020. PNAS, 117. [9] JM Kruger et al 2008. Wildl. Res. 35.	Winner!
31	I propose to develop and implement a new approach to quantum-light spectroscopy. I will benchmark the new approach against established classical-light techniques, namely nonlinear coherent spectroscopy, and use classical- and quantum-light spectroscopies to investigate the mechanism of circularly polarized photoluminescence from chiral perovskite thin films. Introduction: Quantum light has been the subject of research in many physics subdisciplines, but until recently has not been considered as a tool for molecular spectroscopy by physical chemists. Much of the current research on spectroscopy using quantum light has focused on using entangled photon pairs (EPPs) for nonlinear spectroscopies involving two-photon absorption1, effectively using quantum light as an analog for classical light in nonlinear applications. I propose to use an established technique in the field of quantum photonics – quantum-state tomography – in a spectroscopic application that treats both the quantum light and the light-matter interaction fully quantum mechanically. The advantage of the proposed quantum-light spectroscopy over known nonlinear ultrafast spectroscopy techniques is the ability to directly investigate the quantum mechanical nature of a material system and its dynamics. By measuring changes in the state of quantum light due to light-matter interactions, we can learn about phenomena such as entanglement of correlated species in matter and spin- orbit coupling. Chiral perovskite thin films are an exciting and important material system to investigate with this spectroscopy. Recent studies have shown that chiral perovskites act as sources for circularly polarized photoluminescence (CPL), a highly sought-after feature applicable to bioresponse imaging, 3D- LED displays, quantum computing, spintronics, and more.2 An investigation of chiral perovskite thin films by Di Nuzzo, et al. has shown that the unknown mechanism of CPL in Ruddlesden-Popper perovskite thin films does not agree with the model of Rashba spin-orbit coupling,3 the primary model used to describe spin-orbit coupling in 2D-semiconductors. I propose to use quantum-light spectroscopy, as well as two- dimensional photoluminescence (2DPL) and photo-induced absorption (2DPIA) spectroscopies, to explore the claim that the CPL of chiral perovskite thin films is due to the charge of photoexcited Wannier excitons and study the chiral symmetry transfer from optically inactive cations to excitons. Background: Quantum-state tomography is a technique used to completely characterize a quantum state by measuring its density matrix. Photonic-state tomography involves the measurement of an ensemble of polarization-entangled photon pairs to determine the full two-photon density matrix4. I propose to use the spectroscopic setup in Figure 1, in which EPPs are generated in a polarization basis via spontaneous parametric down conversion (SPDC) using a pair of BiBO crystals. The two photons, the signal and the idler, are spatially separated along the edges of the SPDC emission cone and sent down distinct optical paths. The idler photon is sent directly to a Figure 1. Proposed quantum-light spectroscopy setup polarimeter composed of a quarter wave plate (QWP), a half wave plate (HWP), and a polarizing beam splitter. The signal photon interacts with a sample before being sent to an identical polarimeter. The polarimeters project both photons onto a polarization basis defined by horizontal (H), vertical (V), diagonal (D), and right (R) polarizations. Finally, the photons are detected by single photon detectors that drive a coincidence counter. By determining the change in the full density matrix of the biphoton state, we can identify the transformation matrix associated with the light-matter interaction in the polarization basis. Previous Work: The experimental set-up in Figure 1 has been realized in the Silva lab and validated by taking tomographic measurements of several prepared biphoton states of the form and HH VV c |HH〉+c |VV〉 with a quarter waveplate (QWP) in the place of a sample. In the defined polarization basis, a QWP performs a unitary transformation defined by a 90° rotation of the Poincaré sphere about an axis dependent on the waveplate angle. Measurements taken with our experimental setup of a QWP at 0°, 22.5°, and 45° indicate such a transformation, corresponding to a rotation without loss of purity in the biphoton state. The proposed experimental set up is also easily combined with other spectroscopic techniques familiar to the Silva lab, such as pump-probe spectroscopy. Preliminary data taken with a similar experimental set up in which the sample is excited by a pump laser has shown that the quantum state of the probe biphoton pair is altered by scattering off of a triplet-triplet intermediate state involved in singlet fission. Through experimental measurements of bis(triisopropylsilylethynyl)tetracene (TIPS-tetracene) samples obtained through collaboration with Prof. John Anthony at the University of Kentucky and theoretical development done in collaboration with Prof. Eric Bittner at the University of Houston, we have studied the nature of the long-lived correlated triplet pair of TIPS-tetracene, which has been assumed to be entangled in the spin basis without experimental evidence.5 The experimental data, along with simulations of the TIPS-tetracene system and the many-body scattering theory of the biexciton probe have all contributed to a manuscript to be submitted to the Journal of Physical Chemistry C. Aim I – Theoretical comparison of quantum-light and 2D spectroscopies: The implementation of quantum-light spectroscopy is nontrivial; working in the photon counting regime and with ensemble measurements requires precise environmental control and careful error analysis. It is necessary to show the proposed quantum-light spectroscopy will yield information that is inaccessible with known spectroscopic techniques and worth the challenge. I intend to do this theoretically using Lindbladian models to predict the data that can (and cannot) be obtained for chiral perovskite systems comparatively with 2D and quantum-light spectroscopies as shown in our paper published on ArXiv6 for TIPS-tetracene. Aim II – Experimental investigation of CPL of chiral perovskite thin films: In concert with the proposed theoretical development, I intend to interrogate the mechanism by which chiral Ruddlesden- Popper perovskite thin films impart circular polarization on unpolarized incident light using 2DPL, 2DPIA, as well as the proposed quantum-light spectroscopy. I plan to synthesize the thin films with the help of Esteban Rojas-Gatjens, a groupmate co-advised by Prof. Seth Marder. To my knowledge, these thin films have not been characterized with any 2D-spectroscopy which could probe their exciton dynamics. Intellectual Merit: I am ideally positioned to do this research. As part of the Silva group, I have access to a full toolbox of 2D- and ultrafast spectroscopic techniques which are regularly used by our group to investigate perovskites. Moreover, the project will benefit from continued collaboration with Prof. Eric Bittner and future collaboration with Andrei Piryatinski at the Center for Nonlinear Studies at Los Alamos National Laboratory. The proposed research has potential to establish the experimental and theoretical basis for a new form of quantum-light spectroscopy while also learning about the fundamental mechanism of CPL from chiral perovskites, which could improve understanding of spin-orbit coupling in chiral materials. Broader Impacts: Understanding the chiroptical properties of chiral perovskite thin films would contribute to many possible applications, as cited above. However, chiral perovskite thin films are just one of many interesting material systems that the proposed quantum-light spectroscopy can be used to study. Development of a new quantum-light spectroscopy would expand the capacity of spectroscopists to study fundamental quantum mechanical phenomena. It could be used to characterize gates used in quantum computation, singlet fission materials, spin-based memory devices, and so on. [1] S. Mukamel, et al. J. Phys. B., 53, 072002 (2020). [2] G. Long, et al. Nat. Rev. Mat., 5, 423-439 (2020). [3] D. Di Nuzzo, et al. ACS Nano, 14, 7610-7616, (2020). [4] J. Altepeter, et al. Adv. Atom. Mol. Opt. Phys, 52, 105-159 (2005). [5] C. Yong, et al. Nat. Comm., 8, 15953 (2017). [6] arXiv:1909.12869	Winner!
32	conserved protein belonging to the ribonuclear binding protein family.[1]It is involved in diverse, essential cellular functions including RNA transport and alternative splicing. TDP-43 is also the primary component of cytoplasmic aggregates that are the hallmark of amyotrophic lateral sclerosis (ALS), a neurodegenerative disease with very limited treatments and an average survival-after-diagnosis of 2-3 years.[2]Some of these aggregates are amyloid—aggregatescharacterized by a fibrillar morphology and β-sheet-rich structure as well as a prion-like ability to propagate the amyloid structure.[3]Amyloid proteins are closely linked to human disease, including many neurodegenerative diseases like Alzheimer’s and Parkinson’s. One role TDP-43 performs in the cytoplasm is protecting RNA during cellular stress by forming membraneless organelles known as stress granules (SGs).[4]SGsare concentrated droplets of RNA and RNA-binding proteins that are formedvialiquid-liquid phase-separation(LLPS), the spontaneous de mixing of a solution into a metastable concentrated droplet phase and a dilute phase driven by transient, multivalent interactions. Phase-separated membraneless organelles have recently been discovered to play critical roles throughout the cell, but the complete composition and internal protein conformations of these droplets are not fully understood.[5]Long-lived SGs (the resultof prolonged cellular stress) have also been shown to lose fluidity and become proteinaceous aggregates, suggesting a connection between protein aggregation and phase-separation.[4]A link between LLPS and amyloid aggregation is likely, as many amyloidogenic proteins can phase-separate without the need of other proteins or RNA.[6]In fact, it has been shown that LLPS droplets made from the C-terminal domain (CTD) of TDP-43 act as an intermediate for amyloid aggregation in some conditions (Figure 1).[7] The study of protein conformation inside any LLPS droplet has been very limited. Better understanding of the basic composition of droplets (i.e. water content) as well as the secondary structure of protein in LLPS droplets will help Figure characterize how it may serve as an aggregation 1. intermediate. Investigating whether either of these TDP-43factors change with droplet age will add further to states. the story. For example, is droplet aging and TDP-43 in (a) soluble (b) phase-separated, and accompanying loss of droplet fluidity due to CTD (c) aggregated states as viewed by brightfield changes in protein conformation, water confocal microscopy. Scale bars are 10 µm. exclusion,oranotherfactor?Doessecondarystructureofconstituentproteinschangeasdropletsage,ordo proteins remain disordered throughout droplet lifetime? Do aggregates formed via a phase-separation intermediate differ substantially from those formed in non-phase-separating conditions? Being able to address these questions will help 1) better understand LLPS and how itcarriesoutitsmanycellularroles, and 2) identify possible drug targets if TDP-43 LLPS proves to be linked to pathological amyloid aggregation. This work aims to understand secondarystructureofTDP-43inphase-separateddropletsand track changes that may occur in droplet hydration and protein conformation during the transition from LLPS to aggregate. Methods and Experimental Design. I propose to utilize Ramanmicro-spectroscopy—asensitivetypeof vibrational spectroscopy which utilizes Raman scattering—coupled to a microscope, which allows for spatial resolution (Figure 2a).[8] Water scatters strongly in the 3000–3600 cm−1 range, allowing for an approximation of hydration in droplets based on the intensity of the water bands versus a standard. The Raman fingerprint region (1000–2000 cm−1) informsonthesecondarystructureoftheprotein(Figure2b). Specifically, the formation of β-sheet-rich structures upon amyloid aggregation results in a characteristic peak at ~1665 cm−1.[9]Sidechain packing can be analyzedviaC-H deformation modes at 1300-1500 cm−1.[9]One of the strengths of Raman spectroscopyis that a single droplet can be observed over time with Raman spectra taken at multiple time points. Raman data from Murthyet al.of another phase-separated amyloid protein similar to TDP-43 suggests that protein within the droplets resembles soluble protein at early time points, but it is unknown if this structure is sustainable or if it naturally proceeds to a more amyloid-like secondary structure.[10]By followingthe lifetime of TDP-43 droplets, I can determine changes in hydration and protein structure as the droplets age and lose fluidity, filling the gaps left by Murthyet al.and expanding the work to a proteinmore relevant to ALS pathology. Aim One: Collect Raman spectra inside of LLPS can act as a mechanistic intermediate during TDP-43 LLPS droplets TDP-43 amyloid formation. TDP-43 will be used to prototype the experiment Aim Two: Characterize Differences between CTD because it reliably forms β-sheet-rich amyloid, and I Aggregates have previously characterized its aggregation Figure 2. Raman spectroscopy kinetics in a variety of conditions. TDP 43 can CTD phase-separate on its own and has been reported to drive aggregation of the full-length protein.[11]After proof-of-concept, full-length TDP-43 will be expressed and purified. The protein will be placed in phase-separating solution conditions (high salt, neutral pH), and phase-separation will be confirmed viabrightfield microscopy (Figure 1b). I will create a catalog of Raman data from each time point giving a detailed view of the structural changes during droplet solidification. I hypothesize that: 1) the conformation of TDP-43 will differ between its soluble, LLPS, and aggregated forms, 2) water content in droplets will (a) Diagram decrease as droplets age and water becomes excluded illustrating the set-up of a Raman microscope, from the stacked β-sheets formed by proteins in the adapted from [12] (b) Representative Raman amyloid conformation, and 3) as the phase-separated spectra of TDP-43 fibrils with evident C-H droplet ages, the secondary structure band around CTD deformations an amide I band reporting on β-sheet 1600-1700 cm-1will narrow and sharpen, indicatinga content. transition to β-sheet that mimics amyloid aggregates. Taken together, this data would demonstrate that TDP-43 can aggregate even when phase-separation is not present. I will use Raman spectroscopy to analyze the secondary structure of aggregates formed with and without the phase-separation. This will reveal any polymorphism in structure as Raman spectra are sensitive to not just secondary structure, but also side chain packing as seen in the C-H deformation bands. I will also use Raman to examine samples propagated from the brains of ALS patients (patient samples are used to ‘seed’ recombinant protein and, due to the prion-like nature of TDP-43, structure is preserved). By comparing the Raman spectrum of the in vitrophase-separated and non-phase-separated aggregatesto the spectra of the patient-propagated samples, insight into whether disease-related aggregation stems from a phase-separated intermediate will be gained.Resources and Suitability.I have significantexperience using Raman micro-spectroscopy in Dr. Jennifer Lee’s lab at the NIH. Paired with my prior work with TDP-43 and strong record of independence and publication, this makes me uniquely well-suited to pursue this project. The work will be conducted utilizing the Raman micro-spectrometer at the University of Wisconsin Centers for Nanoscale Technology. Intellectual Merit.By characterizing the generalhydration and composition of LLPS droplets, this project will provide foundational information on LLPS, a process of interest to fields ranging from polymer chemistry to cell biology. Additionally, if LLPS are shown to be intermediates in amyloid aggregation, this opens new doorways for drug development targeting the proteins in droplets. Broader Impacts. This project illustrates the value of an interdisciplinary approach in studying human health and disease processes. Specifically, it demonstrates how physical chemistry methods—the most micro scale—can offer foundational, useful information on emergent processes in complex biological systems. I hope that demonstrating the utility of physical chemistry, which can often feel hopelessly far removed and theoretical for students, will pique the interest of the next generation of chemists as I work with them as a teaching assistant and research mentor. References. [1] Y. Sun, Biochemistry 2017. [2] M. Neumann, Science 2006. [3] J. L. Robinson, Acta Neuropathol 2013. [4] C. M. Dewey, Brain Res 2012. [5] S. Boeynaems, Trends Cell Biol 2018. [6] S. Elbaum-Garfinkle, J BiolChem2019.[7]W.M.Babinchak,JBiolChem2019.[8]R.R.Jones,Nanoscale Res Lett2019. [9] Z. Movasaghi,Appl Spectrosc Rev2007. [10] A. C. Murthy, Nat Struct Mol Biol2019. [11] A. E. Conicella, Structure2016. [12] S. Lohumi,Appl Sc.2018.	Winner!
33	(sRNA-seq) technologies have fueled the discovery of many new classes of biologically relevant non-coding sRNAs. Accumulating evidence suggests that sRNAs are critical contributors to the pathogenesis of various diseases and play an essential role in regulating gene expression levels1. RNA-seq analysis has revealed diverse classes of sRNAs circulating on various lipid and protein carriers, including high-density lipoproteins (HDL) 1. The most well characterized sRNAs are microRNAs (miRNA) and the sRNA-seq analysis tools currently available are designed to focus mostly on miRNA quantification. Due to the limitations of previous analysis tools, our lab developed a novel sRNA analysis pipeline (i.e. TIGER) which profiles many classes of host (e.g. mouse, human) and non-host (e.g. bacteria, archaea, fungal) sRNAs present on lipoproteins2. Using our new TIGER pipeline, we discovered that the overwhelming majority of circulating sRNAs on HDL are classified as host and non-host ribosomal RNA-derived fragments (rDF)2. Motivation: Although the functional role(s) of rDFs are poorly understood, mounting evidence suggests that rDFs are not products of random degradation, but regulated by specific endonucleolytic cleavage processes, similar to that of transfer RNA-derived fragments (tDFs)3. Indeed, various stressors were shown to induce transfer RNA cleavage events, producing stable tDRs3. Angiogenin is an RNase A-family enzyme that is thought to be primarily responsible for stress-induced tRNA fragmentation within mammalian cells4. Angiogenin has also been shown to induce rRNA fragmentation, although to a lesser extent4 Interestingly, the cleavage of tRNAs . and rRNAs are inherently linked to their chemical modifications (i.e. m1A and m5C)5. Both tRNAs and rRNAs represent the most abundant sRNAs, and the two most heavily modified RNAs in the eukaryotic genome. Although the identification of chemical modifications on tDFs has been actively pursued, few studies address the modifications found on rDFs. However, preliminary data I have generated using 2D-thin layer chromatography (2D-TLC) identified abundant base modifications (e.g. m5C, m6A) on sRNAs isolated from human HDL samples. A major limitation when exploring the sRNA world is that many base modifications can disrupt Watson/Crick base pairing and impede first-strand synthesis by reverse transcriptase (RT) 4. These chemical modifications therefore affect the detection and quantification of sRNAs, limiting the power of discovery. Although recent improvements to our TIGER pipeline have greatly enhanced our ability to assess sRNA content on HDL, base modifications would significantly impair efficient detection of these modified sRNAs. To circumvent these issues, a relatively new method was developed called AlkB-facilitated RNA methylation sequencing (ARM-seq) which exploits the RT roadblocks created by chemical base modifications (i.e. m1A, m3C and m1G) in tRNAs6. The E. coli AlkB homologs (ALKBH1 and ALKBH2) act as “eraser” proteins, catalyzing the demethylation of specific chemical base modifications6. This method represents a large step forward in the quantification of tRNAs, however a very limited number of studies have used ARM-seq for rDFs. Similar to tDFs and miRNAs, which were once readily discarded from RNA-seq datasets, rDFs may play important roles in the regulation of gene expression. As such, accurately quantifying rDFs and their modification status on HDL is key to gaining a more complete understanding of the biological functions of the epitranscriptome. Based on our previous studies and preliminary results I hypothesize that: (1) Improved sRNA- seq methods will increase the inclusion and identification of rDRs in HDL-sRNA datasets. (2) Stress factors induce parent rRNA fragmentation leading to an increase in circulating rDRs. I will address these hypotheses through two central aims. Aim 1: Enhance HDL-sRNA identification by characterizing the landscape of chemical base modifications found on sRNAs. To achieve this goal, we must capture and identify all host and non-host rDRs. This will include a.) Expanding bioinformatic analyses for rDRs, b.) Improving the identification of modified rDRs, and c.) Removing modifications on sRNAs for enhanced rDF inclusion in sequencing analyses. To address this aim I will first collect blood from healthy individuals and isolate their sRNAs found circulating on HDL using fast protein liquid chromatography. I will then pretreat HDL-sRNAs with the purified AlkB enzymes prior to cDNA synthesis (RT step) and library preparation. By comparing AlkB-treated and untreated samples, I will reveal the positional modification profile of HDL-sRNAs, including rDFs. The TIGER pipeline will be used to identify the diverse classes of sRNAs on HDL particles. The power of ARM-seq will be maximized by taking advantage of RNA modification databases, such as Modomics and RMBase. I expect ARM-seq to efficiently reveal chemical base modifications in the sRNA samples and increase the repertoire of rDFs. Aim 2: Characterize changes in parent rRNA fragmentation and cellular rDF export to HDL in response to environmental stress. Overwhelming evidence supports the role for specific environmental stressors to induce tRNA cleavage; however, very few studies have looked at rRNA fragmentation during environmental stress7. To determine whether oxidative stress, heat and cold stress, or γ-irradiation promote rRNA cleavage events, and the export of rDFs to HDL, I will treat human hepatic and non-hepatic cell lines with various environmental stressors (hydrogen peroxide, cold or heat shock, or irradiation with UV). Afterwards, the cells will be fractionated into nuclear and cytoplasmic extracts, and HDL will be isolated using a FPLC. To examine stress-induced rRNA fragments within these cellular fractions, I will use improved sRNA-seq approaches and confirm candidate rDFs using northern blot techniques. Moreover, we will quantify the export of hepatic rDFs to HDL in response to stresses to using HDL-sRNA export assays. I fully expect that exposure of specific environmental stressors will induce distinct parent rRNA fragmentation patterns and alter hepatic rDF export to HDL Broader Impact: Circulating sRNAs have been shown to be differentially altered in several diseases and hold great potential for the discovery of novel biomarkers and highly promising therapeutics. Given the value of potential biomarkers, the field of sRNA has led to cutting edge research. However, there are still gaps in our understanding of sRNA diversity on circulating HDL. My proposal helps to address this gap and may lead to the identification of yet unknown RNAs. With novel classes or sRNAs being discovered, and the validation of modified sRNAs, it is paramount that RNA-modification and sRNA databases are updated. I will disseminate my findings to web portals and servers dedicated to compiling databases for RNA modifications. Intellectual Merit: It was not very long ago that many sRNAs were considered “junk” and often removed from RNA-sequencing data analysis. However, we now know that sRNAs can regulate several aspects of gene expression. The novel pipeline generated by our bioinformatics team allows us to discern several classes of small RNAs found in both eukaryotes and prokaryotes. This interdisciplinary proposal applies techniques from bioinformatics, transcriptomics, microbiology, and biochemistry, and represents the first study aimed at identifying modified small RNAs on HDL. Successful completion of this proposal will not only expand the repertoire of sRNAs and rDRs but will also show how rDRs are important biological molecules. References: [1] Vickers et al. 2011. Nature Cell Biology. [2] Allen et al. 2018. Journal of Extracellular Vesicles. [3] Lambert et al. 2018. Non-coding RNA Investigation. [4] Su et. Al. 2019. J Biol Chem. [5] Rashad et al. 2020. Neural Regeneration Research. [6] Cozen et al. 2015. Nature Methods. [7] Thompson et al. 2009. Cell.	HM
34	Title:Investigating Maine’s Indigenous Fire Prehistoryto Inform Forest Management Under Global Change With climate change amplifying underlying environmental issues, modern wildfires have become enormous devastating forces, costing lives, our natural resources, and billions of dollars. Early US Forest Service practices focused on fire suppression as a management tool, which increased the presence of underbrush, snags, and flammable material in forests1.Though these practices have changed, those initial management plans coupled with drought, eco-tourism, and rising temperatures have led to the large-scale, uncontrollable, high-intensity fires in the West that, as of October 1, 2020, have burned nearly 7.7 million acres this year2. In contrast, indigenous fire management has played an important role in the ecology of many North American landscapes for thousands of years. Native peoples used fire to clear the land for cultivation, promote healthy and diverse food-rich forests, and facilitate diverse wildlife habitat3. These practices have largely been excluded in modern forest management plans. By the 1970’s the Forest Service began utilizing indigenous knowledge to set controlled burns in the West, Southwest, and Southeast, but not in the mixed hardwood forests of the Northeast1, wherefire is not widely considered to be an important process4. However, there have been large-scale destructivefires in the last century, like the Great Fire of 1947 in Maine. This drought-intensified fire consumed 17,188 acres, destroyed 240 buildings, and cost over $23 million in property damages5. As climate change is causing warmer temperatures and droughts in the Northeast6, there remains a critical need tounderstand the long-term history of fire (both natural and anthropogenic) in this region. Most of our academic knowledge about indigenous fire use in New England is largely based on journals and other written accounts by European settlers. However, those records lack the perspectives of indigenous people, and only explain fire use post-contact7.Historical observations and Wabanaki oral knowledge indicate that, within the last 500 years, Native peoples used fire to clear land for agriculture, and to improve hunting grounds south of the Kennebec River in Maine. Penobscot place names describe areas that experienced regular burning. For example, Schoodic (skudek) Peninsula, a part of Acadia National Park, means “burnt-land”8. Long-term fire records from lake sediment cores and tree rings have provided another valuable source of information about the relationships between fire, climate, vegetation, and people in the American West, Midwest, and Southeast, but are still lacking for the Northeast, including New England. A recent study synthesizing charcoal patterns across New England found no evidence of pre-European anthropogenic fire use, but this reflected a regional fire record that would not highlight the more localized scale at which indigenous peoples would have been burning4. Charcoalrecords in the Northeast have primarily been taken from large bodies of water9, which are biasedtowards large regional fires, instead of local, low intensity fires, which would have been the types of fires Native peoples used for land management4. Therefore, while previous paleoecological studies have been important for understanding the large-scale fire prehistory of New England and its relationship to climate, they are poorly suited to the study of anthropogenic fires. And, by failing to partner with Native scholars and incorporating oral knowledge of past land use, such studies mask indigenous peoples’ expertise and contributions to the health of the landscape10. Intellectual Merit My research goal is to reconstruct localized fire records in Maine to better understand fire as a prehistoric land management tool in New England. I will take a multi-pronged approach to this work: 1.) I will conduct an actualistic study to identify the signals of localized understory burns and small patch clearings in the charcoal and pollen records of forest hollows and small ponds. 2.) In collaboration with members of the Penobscot Tribe, I will collect sediment cores from small ponds and forest hollows near settlements and prehistoric hunting grounds to examine whether small-hollow cores can identify local, small-scale burning. 3.) I will then synthesize these findings with existing geoarchaeological, climate, and pollen records to assess the relationships between population and cultural shifts, climate, vegetation, and fire histories across scales. All of the necessary equipment and facilities to carry out this project are available at the University of Maine Climate Change Institute, and Dr. Gill is building tribal partnerships via collaborations with Penobscot faculty at UMaine: Dr. Darren Ranco, director of the WaYS program, and Dr. Bonnie Newsom, archaeologist. Partnership opportunities are also available at Acadia National Park through the National Park Service. Many fire-use studies focus on written accounts, the charcoal record, and tree scaring to reconstruct past fire regimes, but researchers have historically excluded indigenous communities when studying past human land use. My project will contribute to a more accurate historical record of prehistoric land use by better matching the tools to the questions to characterize anthropogenic fire histories and impacts. This project will also add to our understanding of charcoal records taken from small hollows. In contrast with the pollen record, hollow-based fire records are lacking, which limits our ability to interpret stand-scale fire impacts11. Broader Impacts Though fire is not considered to be an important process in the Northeast, with climate change exacerbating existing environmental issues, it is becoming an increasingly dangerous threat. This past summer, drought conditions and increased eco-tourism due to COVID-19 resulted in a summer of over 900 high-intensity, destructive fires in Maine6. Maine’seconomy depends on logging and tourism12and drought-induced fires put both of those industries at great risk. This study seeks to understand low intensity fires and will inform conservation and management practices. Such fires clear underbrush and snags, reducing the fuel load for uncontrolled fires. This would make Maine’s forests safer while also reducing tick populations by burning shrub species that foster these disease vectors13. Cleared underbrush would improve forest health by reducing canopy competition and eliminating weaker diseased trees. All of these benefits could increase timber quality and forest health, boosting two of the state’s major industries during a time of economic uncertainty. The Wabanaki Confederacy is a collection of Eastern Algonquin tribes including the Penobscot, Passamaquoddy, Mi'kmaq, and Maliseet people. I plan to use my research to contribute to the Native American Graves Protection and Repatriation Act (NAGPRA) by providing supporting evidence of long term tribal habitation. This project will contribute to a long-term collaborative relationship with local tribes and will provide critically needed information in support indigenous sovereignty claims. I also intend to collaborate with the NSF-funded Wabanaki Youth in Science (WaYS) program at UMaine. WaYS trains Wabanaki youth in both tribal knowledge and scientific approaches through summer camps and internships. I intend to include Wabanaki students in my project by bringing groups of students out into the field and mentoring students in the lab to learn sediment coring and paleoecological techniques. References. [1]Forest History Society.US Forest Service FireSuppression. [2]Congressional Research Service. 2020. 43. [3]Ryan K.C. 2013.Frontiers in Ecology and the Environment.[4]Oswald, W.W. 2020.Nature 3,241–246. [5]National Park Service, Acadia. 2020. [6]The Maine Monitor. 2020.Bangor Daily News. [7]Ruffner, C. M. 2005.USDA: Proceedings 16th CentralHardwood Forest Conference.[8]Francis, J.E. 2008.Farms, Forest, and Fire44(1): 4-18.[9]Patterson.1988.Holocene Human Ecology in Northeastern North America.[10]Kimmerer, R.W. and Lake, F. 2001.Journal of Forestry99(11):36-41. [11]Higuera P.E. 2005.The Holocene15(2): 238-251.[12]US Newsand World Report. 2020.Best States: Maine. [13]Gleim E.R. 2019.Nature.	Winner!
35	Introduction and Preliminary Results: Discovered at Drexel University in 2011, MXenes are a novel class of 2D materials that comprise metal carbides and nitrides. Due to their excellent electronic, optical, thermal, and mechanical properties, MXenes have great promise for applications in several technologies including additives in solar cells and electronic contacts for semiconductors [1]. Compared to other 2D materials, MXenes offer an optimal combination of high electronic conductivity, low cost, and facile synthesis methods. Furthermore, their tunable optoelectronic properties, such as work function and optical absorption, enable MXenes to improve emerging photovoltaic materials such as perovskites and inorganic semiconductors. To realize the full potential of MXene photodetectors, an improved fundamental understanding of their electronic and optical properties is needed. During my master’s thesis, I refined methods for photodetection analysis of MXene thin films. While it laid the groundwork for the optoelectronic study of MXenes, the underlying factors that drive MXene response to light (photoresponse), such as the impacts of the electrode and substrate type, are not well- understood. While MXenes have proven successful as additives and electrodes, I aimed to bring them to the mainstream as active materials. My work focused on the photoactive capabilities of Ti C , which has already 3 2 succeeded as a transparent photodetector electrode [2]. Although Ti C is the most commonly studied MXene, 3 2 its response to chopped illumination with visible light Figure 1: Ti C films exhibit consistent negative had not yet been reported. 3 2 photoconductivity when deposited on patterned Figure 1 compares the average change in fluorine-doped tin oxide (FTO) substrates without resistance (R) upon illumination for films with the use of silver paste contacts (black). However, different initial resistance values (corresponding to thin films with high R switch from negative to thickness) and with different contact methods. Here, 0 positive photoconductivity upon illumination with the application of silver paste as an electrical contact the application of metal contacts (red) and causes Ti C to deviate from innate behavior upon 3 2 experience suppressed photoresponse when illumination, while a change to a thinner substrate deposited on glass slides (green). Schematic (glass slides) suppresses the magnitude of the of MXenes shown in inset [2]. photoresponse. Ti C is a well-known metallic and 3 2 photothermal material with innate negative photoconductivity, leading to an expected increase in R upon illumination or heating; these observations that contradict expected behavior call into question the role of silver-MXene interactions, carrier dynamics, and heat transfer in determining the material’s photoresponse. I hypothesize that the photovoltaic (PVE) and the photothermoelectric (PTE) effect each play into the photocurrent generated by MXenes, giving them the power to serve multiple applications, from thermal imaging to photovoltaic electrodes. Research Plan: I aim to both experimentally and computationally study heat and charge transport in MXene photodetectors to guide their design in imaging and energy generation applications. I will begin my examination with Mo-based MXenes, a lesser-studied subset of MXenes with potential as a photoactive material. Previous empirical studies question computational results showing Mo TiC has semiconductor- 2 2 like properties [3]. This work will seek to confirm these analyses by isolating contributions to light-matter interactions for Mo-based MXenes. Furthermore, over 30 different MXenes have been reported to date [1]. This proposal outlines just the beginning of our exploration into MXene photodetection capabilities in response to visible light, as the methods listed can be applied to other photoactive MXenes as well. Objective 1 – Understand the impacts of device architecture: In varying the deposited film thickness, contact geometry, and substrate type, I will evaluate their individual influences on photodetector properties (responsivity, noise, stability) in response to chopped illumination with visible light. Using thermally conductive substrates, such as sapphire, the impact of thermal effects can be mitigated. I expect strongly absorbing films, non-metal electrical contacts, and thin, thermally conductive substrates to produce the strongest photoresponse for Mo-based MXenes. Upon gaining this phenomenological data, I will then study charge carrier dynamics to understand the light-matter interactions for each MXene device. Under the guidance and expertise of Prof. R.J. Holmes, I will probe exciton diffusion at the interface of the photoabsorbing MXene and the electrical contact via an external quantum efficiency measurement method curated in his group [4]. This broadly applicable method provides additional understanding of carrier dynamics upon photoexcitation and will guide selection of device architecture for improved performance. Objective 2 – Model optical and thermal transport kinetics: Through computational efforts to model contributions from the PTE and the PVE, I will confirm the dominant effect that determines the photoresponse. Should combined contributions dictate the photoresponse, I aim to create a secondary model system specific to MXenes. By compiling a model from literature for both heat and carrier transport, I will determine optimal film thicknesses, contact geometry, and substrate types for devices that rely on either the PVE or the PTE, creating two reliable device architectures with improved responsivity. Moreover, simulating the photodetector architectures created in Objective 1 using COMSOL will push them to their thermal and electronic limits, granting insight into widespread implementation of MXene photodetectors. Objective 3 – Create devices and optimize performance: Equipped with the knowledge of optimal device architecture and film deposition, I will build Mo-based photodetectors and investigate industrially relevant issues, such as stability, lifetime, and performance of larger area devices. Given the inevitable obstacles in scaling up a device, I must tailor the device parameters found in Objective 1 to suit applications that would benefit from either the PVE or the PTE, such as energy generation or thermal imaging, respectively. I envision my contributions will spur the development of MXene-based photodetectors that suit multiple purposes simply by changing the device architecture. Intellectual Merit: My well-rounded background in chemical engineering and materials science and engineering allows me to understand not only why MXenes behave the way they do, but also how we can implement these materials in devices. I will utilize the wealth of knowledge from multiple energy transport experts, including Prof. Holmes, as well as state-of-the-art facilities for nanotechnology research at UMN to ensure the success of this project. The proposed research will provide an improved fundamental understanding of the factors that influence MXene photodetection, an emerging field of interest with limited literature available. Upon gaining this understanding, we can continue to use MXenes in optoelectronic applications, reducing the cost to produce photodetectors and allowing for widespread implementation of more conductive, easily synthesized materials. Broader Impacts: My work aims to inspire other researchers to consider implementing MXenes in their devices, bringing the field closer to a reliable, reproducible method for renewable energy generation. Through my research on nanomaterials in energy applications, I aspire to make clean energy commonplace, expanding on my dreams of a sustainable future arising from wanting to develop accessible biodegradable plastics in high school. I also aim to continue my impactful record of mentorship and community outreach. Leaning on my extensive outreach experience described in the accompanying personal statement, I plan to create an interactive lesson on current and novel photodetectors and sensors through Science for All, a student-run group created to support and promote STEM fields to local, underserved middle schools in the urban Twin Cities. Prof. Holmes also has the laboratory facilities to package photodetectors, allowing me to bring samples to the classroom. Lastly, through the Undergraduate Research Mentorship Program (UROP), I will seek and recruit undergraduate students from underrepresented groups for this project, serving as a research and personal mentor to guide them through their technical careers and encourage them to continue their STEM education. References: [1] L. Zhao, et al. Tungsten, 2 (2020): 176 – 193 [2] K. Montazeri, et al. Adv. Mater., 31.43 (2019): 1 – 9. [3] G. Li, et al. Proc. SPIE 11279, 112791U (2020): 66 – 84. [4] T. Zhang, et al. Nat. Commun., 10.1 (2019): 3489 – 3495.	Winner!
36	"wetlands (UPOWs), are a practical, cost-effective, and highly scalable approach to managing environmental water quality.1 UPOWs utilize microbial growth in the benthic region within a photosynthetic biomat, hosting a stratified population in aerobic, anaerobic, and anoxic zones. Biomat microbe ecosystems have demonstrated treatment of influent water for nutrients, trace organic compounds, and other contaminants at rates that match—and in many cases exceed—those of traditional vegetated or subsurface wetlands.1 These microbial populations develop independently over multiple months, using algae and other detritus as carbon and electron sources. The low implementation and maintenance costs of these systems have drawn attention to their use for increasing water availability and decreasing risk in otherwise water-poor or unprotected communities, such as the Arequipa region in Peru. These communities utilize surface water from local rivers such as the Tambo, which contains concentrations of arsenic (As) exceeding Autoridad Nacional de Agua regulations for both domestic and agricultural use.2 Elevated concentrations are likely a result of high background levels of As in local geology and introduction from mining operations in the area. The surrounding community can no longer safely consume or export important commercial products such as rice or river shrimp because of this threat. The long-term effects of these economic limitations and human health impacts cannot be understated and will continue to persist so long as the region suffers from degraded surface water quality. Dr. Josh Sharp, professor of Civil and Environmental Engineering at the Colorado School of Mines, is currently working with the Universidad de San Augustín de Arequipa (UNSA) in Peru to study the potential for UPOWs to treat surface waters for metal and metalloid contaminants. UPOWs with the capability to remove these hazards would provide communities with a first step toward improving environmental water quality. Challenges of degraded water quality do not adhere to national or state boundaries. Communities all over the world, including here in the United States, contend with a lack of access to safe water sources. My goal is to apply research on UPOWs to ensure no one has to suffer from contaminated water. I will build upon Dr. Sharp’s research on UPOWs by determining their capacity for arsenic removal. Hypothesis: UPOWs are able to remove trace organics and nutrients from surface waters and are potentially capable of immobilizing metal and metalloid contaminants.1 Arsenic (V) is the most prevalent form of As in surface water, and poses a hazard to human and environmental health.3 I hypothesize that UPOWs incorporating sulfate-reducing bacteria have the capability to remove As (V) from surface water via precipitation with sulfide in a variety of geographic settings. Research Plan: Objective 1: Analyze the potential for As (V) precipitation as As S in the presence of 2 3 sulfate-reducing bacteria in UPOWs. Arsenic (V) removal by precipitation relies on the transformation of As (V) to As (III), as biological systems have previously been observed to utilize sulfate-reducing bacteria to precipitate As (III) with sulfide.3 H AsO , the naturally occurring form of As (III), reacts with sulfide to produce As S (Equation 3 3 2 3 1) which is insoluble in typical surface water conditions.4 2𝐻 𝐴𝑠𝑂 +3𝐻 𝑆 ⇌ 𝐴𝑆 𝑆 +6𝐻 𝑂 Equation 1 ! ! "" "" ! "" Some sulfate-reducing bacteria hold the potential to reduce As (V) to As (III).5 The presence of these bacteria could allow for the precipitation of As (V), proceeding as in Equation 1. Sulfate-reducing bacteria colonize and function well in biomat ecosystems,4 but they have yet to be tested for specific removal of As. Objective 2: Evaluate As removal efficiency of UPOWs across largely differing geographical areas. The formation and function of biomats in UPOWs rely on both biotic and abiotic constituents of surface water. Successful arsenic removal depends on the colonization and function of sulfate reducing bacteria, algae, and other diatoms, all of which could differ by region. I will observe whether biomat ecosystems will support microbe composition necessary to perform key contaminant removal functions regardless of UPOW location. Differing water chemistry and abiotic constituents could alter the As removal pathway or block As precipitation altogether. In some cases, As may complex with other constituents to produce undesired byproducts, such as Thioarsenic.6 A particularly interesting factor in the success of As removal may also be sulfate concentration, though determining direct influence is outside the scope of this project. Approach/Methods: Testing will occur at both the bench and field scales. After the effectiveness of the mesocosm-sized UPOW biomat has been determined, the biomat composition and design will be tested at the field scale at the Prado Wetlands in California and then at the UNSA in Peru. Objective 1: Bench scale biomats will be harvested from existing UPOWs in the Prado Wetlands and deployed in the lab at the Colorado School of Mines to be tested with spiked influent water. Test water will consist of local Colorado water samples spiked with known levels of arsenic. Sulfate will be held constant in a similar concentration to that of the Tambo. Removal efficiency will be calculated using a mass balance on As, with a known influent concentration, precipitation concentration determined using a modified Tessier extraction of the biomat,7 and effluent concentration determined using ICP-MS chromatography. Selective inhibitors for sulfate reducing bacteria, such as molybdenum8, will be used to evaluate arsenic precipitation in response to sulfate reducing capacity. Testing will move to the field scale as a validation step in large-scale performance after bench- scale testing has been completed. Field testing will first take place at the Prado Wetlands, where a colonized UPOW will be allowed to run for an extended period of time. As concentrations will be measured using the same methods as at the bench scale. The continual rise of precipitated arsenic in the biomat could lead to concern after long periods of time in field operating conditions. To mitigate potential release of highly concentrated arsenic from a biomat, I recommend an operating system in which sections of biomat could be harvested prior to the accumulation of dangerous levels of As or other harmful constituents. These sections could go on to be used as fertilizer to utilize their elevated levels of nitrogen, resulting from abundant nitrifying bacteria. Objective 2: Successful function of a UPOW system in Peru will first be estimated at the bench-scale using replicated Tambo River water at the Colorado School of Mines. Biomat colonization and As removal will be monitored over the course of several months using these waters, with an emphasis on the perception of sulfate reducing bacteria presence. Field-scale testing will occur at the UNSA and will mirror procedures in the United States, now using unmodified water from the Tambo River. Biomats will colonize and undergo testing in mesocosm and field-scale UPOWs and arsenic concentrations will be determined using a mass balance approach. Intellectual Merit: Metal and metalloid removal has not yet been tested in UPOW systems, but the successful performance of sulfate-reducing bacteria in previous UPOWs could lead to an innovation in As removal methods. This study would quantify the effectiveness of this mechanism for arsenic removal from surface water via UPOWs. UPOWs have been tested in the United States but have not been observed abroad. This study will demonstrate the effects of differing surface water characteristics of different regions on arsenic precipitation. Additional application of UPOWs, including biomat harvesting, could have basis for further investigation to increase utility of these already compelling systems. Broader Impacts: The development of this system, and other natural treatment systems, would make progress on the priorities set forth by the UN Sustainable Development Goals and the Grand Challenges of the National Academy of Engineering. My work would directly contribute to these efforts aimed at increasing global sustainability and standards of living. More immediately, an UPOW system has the potential to mitigate the threat of arsenic for the Arequipa community and others like it. Depending on sequestered metal concentrations, the periodic harvesting of UPOW biomats, for fertilizer or otherwise, could also prove UPOW potential to serve a purpose beyond water treatment. Further research on natural treatment systems will continue to drive down cost, increase understanding, and foster education in communities using natural water treatment. References: 1Jasper, et al. (2013). Environ. Eng. Sci. 30(8), 421-436. 2Autoridad Nacional de Agua. (2020) Ministerio de Agricultura y Riego. 3Lizama, et al. (2011). Chemosphere. 84(8), 1032-1043. 4Jones, et al. (2017). Appl. Environ. Microb. 83:e00782-17. 5Macy, et al. (2000). Arch. Microbiol. 179, 49-57. 6Stucker, et al. (2014) Environ. Sci. Technol. 48(22), 13367-13375. 7Tessier, et al. (1979). Anal. Chem. 51(7), 844- 851. 8Blum, et al. (1998). Arch. Microbiol. 171, 19-30."	Winner!
37	"Introduction: Glass-Ceramics (GC’s) are critically relevant materials for industry and scientific research, primarily due to their outstanding mechanical, thermal, and optical properties. High-grade GC’s such as lithium- aluminosilicate (LAS) are commonly used as insulation materials for high-performance aircrafts and missiles, optical bodies of precision optics, and biomaterials for medical equipment[1]. Although the material properties of GC’s are very attractive in many engineering fields, the cost of manufacturing complex geometries can be prohibitive, primarily due to the high cost of tooling and limited machining capabilities of present manufacturing methods. Therefore, it is the goal of the proposed work to implement a novel method of manufacturing GC’s with predictable, tailorable, and optimized material properties. GC’s are classified as two-phase materials; one being the glass matrix, the other being small volume fractions of nanocrystal inclusions. Typically, GC’s are manufactured through casting or forming methods based on glass-making techniques. In these methods, the glass matrix is heated to high temperatures using a two-step process. In the first step, known as nucleation, the GC is heated just past its devitrification temperature to create a high density of nuclei throughout the interior of the glass. The second step involves re-heating the GC to a highly controllable temperature which directly impacts the growth rate, crystal size, and region of crystallization[2]. Vat Photopolymerization (VP) is an Additive Manufacturing (AM) process which utilizes UV light to selectively cure a polymer-based resin in a layer-by-layer fashion. VP can offer unparalleled resolution, complex internal and external features, and high-solid loadings of GC’s to further enhance their applications. Digital Light Processing (DLP) is a sub-category of VP which uses a UV projector instead of a laser to expose cross-sections of the design geometry onto a resin vat. Therefore, the curing characteristics of the polymer resin are controlled by the light intensity and the effective pixel size from the projector. Additionally, by carefully tailoring the monomer, oligomer, and photo-initiator concentrations in the resin, high solid-loading of GC’s within the resin could be achieved[3]. The polymer matrix is finally burnt off through a debinding step before the sintering process, resulting in a highly pure and fully dense GC part. To further improve the mechanical, thermal, and optical properties of the GC’s, an Ion-Exchange (IX) process will be implemented after the sintering step. During this process, cations of small atomic size from within the glass matrix surface are replaced by larger cations from the molten salt bath through a diffusion mechanism driven by temperature difference, resulting in a permanent compressive force in the surface of the part, which suppresses the growth of surface flaws and reduces crack propagation within the glass[4]. Proposed Research Activities: Objective 1: Understanding primary and secondary parameter influence on material properties of Glass-Ceramics. From my previous and ongoing research, it has been noted that the concentrations of monomer, cross-linker, and photo-initiator in respect to the solid-loading of the matrix material greatly impact the printability and final properties of the GC. Therefore, it would be highly beneficial to understand the primary and secondary parameters during the printing, debinding, and sintering processes and their impact on the final material properties. Primary parameters from the printing step could include UV exposure times, light intensity, and layer thickness; secondary printing parameters could include layer waiting time, lifting speed, and vat temperature. Degree of Conversion (DoC) is a common measurement tool that utilizes Fourier-Transform Infrared (FTIR) characterization data to rapidly quantify the progress of the photopolymerization reaction. Primary and secondary printing parameters will therefore be directly quantified and compared through DoC by use of FTIR. For debinding, primary parameters could include ramping rates and holding temperatures; secondary parameters could include crucible materials and furnace atmosphere. Initially, Thermogravimetric analysis (TGA) will be performed on printed samples to determine ideal holding The University of Texas at El Paso 1 Sebastian Vargas NSF GRFP Research Statement temperatures in efforts to ensure complete removal of organic compounds. Additionally, Energy Dispersive Spectroscopy (EDS) will be used as an elemental analysis tool to compare anticipated composition of the GC’s to actual results. In terms of sintering, primary parameters could include final sintering temperatures and furnace atmosphere; secondary parameters could include ramping rates and cooling rates. X-Ray Diffraction (XRD) analysis will be performed on sintered samples to broadly determine the degree of crystallization by comparing results to literature and standards. Finally, by identifying the relationships (linear or non-linear) between parameters and material properties, a novel, reliable, and well-understood manufacturing method for GC’s based on the DLP process could be achieved. Objective 2: Inclusion of alkali modifiers for chemical strengthening through Ion Exchange (IX). Based on the recent work of Gy, R.[4] and Macrelli, et al.[5], lithium, potassium, and sodium ion modifiers will be added to the GC resin formulation in preparation for chemical strengthening through the IX process, which will be carried out directly after Objective 1. The depth of penetration of the cationic exchange layer, also known as case depth, is a direct metric of the effectiveness of the IX process and will be evaluated at various depths using a refractometer. As mentioned before, the strengthening process intrinsically develops a residual compressive stress at the surface of the GC. Therefore, the effect of cation modifier concentration on the final mechanical properties will be assessed by compressive, flexural, and hardness testing based on ASTM standards and will be performed with equipment available at the Keck Center and SMP lab. Extended Objective: Supporting the development of a machine-learning-based model to predict material properties of glasses from compositional data. Based on the recent work by Ward. et al[6], a framework capable of extracting predictive models from existing materials data is being developed by the Kansas City National Security Campus (KCNSC). My research will serve to provide the model with characterization and testing data obtained from Objectives 1 and 2 in order to effectively populate the model. More detailed information may not be suitable for public release at this time. Intellectual Merit: The proposed work represents a novel method for manufacturing two of the most relevant materials to society: ceramics and glasses. My research would directly advance the limited understanding of the intricate relationships between input parameters and material properties at different steps of the DLP manufacturing process. This critical understanding could potentially overcome a common barrier towards further development and implementation of DLP-based AM as a prevalent manufacturing method. Broader Impact: The development of DLP-based AM could enable previously unachievable part geometry of GC’s and therefore, become a highly tailored process to impact many industries including aerospace, defense, and medical. Additionally, this work could drive further research in STEM, including fields such as Additive Manufacturing, Machine Learning, and materials science. Finally, the proposed work would directly support ongoing research efforts at the KCSNC, and thereby, the National Nuclear Security Administration. References: [1] Elan Industries. https://www.elantechnology.com/glass/glass-materials/las-glass-ceramics/ [2] Rawlings, R. D., J. P. Wu, and A. R. Boccaccini. ""Glass-ceramics: their production from wastes—a review."" Journal of Materials Science 41.3 (2006): 733-761. [3] Kotz, F, et al. ""Three-dimensional printing of transparent fused silica glass."" Nature 544.7650 (2017): 337-339. [4] Gy, René. ""Ion exchange for glass strengthening."" Materials Science and Engineering: B 149.2 (2008): 159-165. [5] Macrelli, Guglielmo, Arun K. Varshneya, and John C. Mauro. ""Ion Exchange in Silicate Glasses: Physics of Ion Concentration, Residual Stress, and Refractive Index Profiles."" arXiv preprint arXiv:2002.08016 (2020). [6] Ward, Logan, et al. ""A general-purpose machine learning framework for predicting properties of inorganic materials."" Nature: npj Computational Materials 2.1 (2016): 1-7. The University of Texas at El Paso 2"	Winner!
38	Center reported a total of 19,105 new cases of spinal cord injuries with some amount of paralysis in 2019. Brain machine interfaces (BMIs) are a burgeoning technological solution to restore quality of life to these people through connecting brain signals to prosthetics. Current brain machine technology collects and transmits neural signals from implantable electrode arrays to be decoded using algorithms which are implemented on cumbersome and power inefficient computers. These systems also require daily calibrations by scientists and clinicians to maintain Figure 1: Proposed Brain Machine Interface their usability. Herein lies an approach to address these Architecture. problems through implementing specially designed algorithms that are memory and computationally efficient onto a low power application specific integrated circuit (ASIC) to decode patient intent using a fully implantable device. The goal is to engineer hardware to implement intelligent decoding algorithms that will increase the reliability of neural decoding systems and decrease the physical size and power requirements by orders of magnitude through removing the need for transmission of unprocessed neural data. I am working with professor Azita Emami at the Caltech Mixed-mode Integrated Circuits and Systems (MICS) laboratory to begin my work as an ASIC and algorithmic designer implementing these systems. Previous Work: The promise of this project to produce reliable power efficient BMIs relies on the development of power efficient algorithms. The MICS lab has developed efficient algorithms that utilize deep multistate dynamic recurrent neural networks [1], as well as done an assay on decoding algorithms [2]. Furthermore, if energy efficient algorithms do not provide the power performance required, in memory processing architectures could be utilized to reduce power lost from transporting weights between memory banks and processing units [3]. Intellectual Merit: Current BMI technology largely utilizes hardware implementations which transmit digitized neural signals back to a compute station. There are few BMI ASICs that decode patient intent directly without off-the-shelf hardware implementations. Custom machine learning ASICs provide an opportunity to optimize for power and area efficient systems. BMI systems have unique signal processing constraints due to relevant information being mixed across time, frequency, and space in highly dimensional, redundant datasets. These constraints often require complex computational algorithms with high internal state complexity, that generally decreases power efficiency [2]. This poses a particularly difficult engineering dilemma which requires a uniquely multidisciplinary approach to leverage knowledge of neuroscience with engineering expertise in ASIC design. This dilemma is to fit the required computationally complex task of decoding patient intent from neural signals into a power and area efficient package. Hypothesis: BMI ASICs implementing computationally, and memory efficient algorithms will be able to efficaciously ascertain patient intent while maintaining robust performance whilst still meeting power and area constraints requisite of fully implantable systems. Research Plan: Several crucial prerequisite steps for this proposal are already underway in Azita Emami’s lab including the evaluation of neural features measured from patient data for stability over time. Aforementioned specialized algorithms have been developed and evaluated for performance. While these prerequisite steps are crucial to the outcome of this project, the scope of this proposal is constrained to the implementation of these algorithms in hardware, optimizing for low power. Therefore, this proposal will only discuss the development, implementation, and testing of the algorithms into CMOS fabric. Stage I- Algorithm CAD Design and Testing: Digital and analog hardware will be described and laid out into Virtuoso Computer Assisted Design (CAD) program. The circuits will be fabricated with general power optimization techniques in mind such as clock and power gating portions of the circuit when they are not in use. Other techniques include using intentional specificity of transistor thresholds throughout the design so that leakage current is minimized, as well as a minimization of supply voltage levels. Furthermore, specific tradeoffs will be made between the bit precision of the algorithms and the resources required to run at those precisions. Significant energy is also lost by moving the algorithm’s weights from memory to the processing hardware, so a layout will be designed such that the algorithm weights are physically closer to where computations are done. While designing the circuits, each component will be characterized at a unit level so that the performance of the entire circuit can be ensured. Stage II-CAD Simulation and FPGA Testing: After designing the circuit in Virtuoso, the entirety of the circuit will be tested with patient neural data collected over several weeks. The circuit simulation will give significant insight into the performance of the decoder system once fabricated. The simulation will also allow for design bugs to be caught and corrected before resources are spent fabrication of the design into silicon. The digital components of the circuit will also be implemented on a commercially available Field Programable Gate Array (FPGA) which will not only give physical proof that the logic and algorithms designed will work, but will give an upper bound on the power and area usage for the ASIC implementation. Stage III-Hardware Fabrication: Several test chips will be fabricated to investigate the performance of the decoding algorithms. This allows for verification and debugging of the major components of the algorithm, with the final system chip produced after all components are aptly constructed and verified. The circuits will be implemented into standard cell libraries for 65 nm CMOS technology using Design Vision. Stage IIII-Hardware Testing: The fabricated chip will be designed for testability using techniques such as built in scan chains to give access to the internal states of the circuit. Custom test systems will be built to feed neural signals to the device under test and validate the performance of the hardware. Stage IV-Going Forward: Once designed, fabricated, and verified, the integrated circuits could be tested in vivo using the same experimental therapy program from which the neural data was harvested. Pitfalls and Alternatives: Due to variation in fabrication processes, the fabricated chips may exhibit characteristics and behavior that were not accounted for during simulation. If experienced, the test hardware designed into the chip will be utilized to identify and debug the flaw. Timeline and Collaboration: The duration of the proposed project is three years and will be conducted under the supervision of Azita Emami. Azita Emami’s MICS lab works in direct collaboration with Richard Anderson’s neuroscience lab. This is a collaboration between two leading scientists in their respective fields. The MICS and Anderson lab collaboration has significant skill and expertise to confidently produce the anticipated results of this proposal. Emami’s lab has adequate facilities and resources to fund the significant capital required to design and develop custom ASIC hardware. This project also has direct access to neural recordings of patients with implanted UTAH neural arrays which provides essential data for training the decoder algorithms. Anticipated Results: Using specially designed decoding algorithms, significant reductions in power and area will be observed in the resultant neuro decoding system. It is important to note this project aims to maintain decoding accuracy and stability, despite using orders of magnitude less power and area. Broader Impacts: Fully implantable neural intent decoders will not only greatly improve the quality of life of patients with paralysis, but also provide the basis for fully implantable ASIC chips designed for direct study of neural activity without the need to be linked to heavy, memory and power inefficient recording systems. The miniaturization of hardware and computational effort can further be generalized to many IOT or wearable systems which requires robust signal processing algorithms with limited power and area requirements. This will enable a variety of wearable devices to decode bio signals without the need to upload the signal data to an off-chip server, greatly improving security, power, and speed performance. References: (1) B. Haghi, S. Kellis, M. Ashok, S. Shah, L. Bashford, D. Kramer, B. Lee, C. Liu, R. Andersen, A. Emami, “ Deep multi-state dynamic recurrent neural networks for robust brain- machine interfaces”, Program No. 406.04. 2019 Neuroscience Meeting Planner. Chicago, IL: Society for Neuroscience, 2019. Online. (2) Mahsa Shoaran, Benyamin A. Haghi, Milad Taghavi, Masoud Farivar, Azita Emami, “Energy-Efficient Classification for Resource-Constrained Biomedical Applications” IEEE Journal on Emerging and Selected Topics in Circuits and Systems (JETCAS), 2018. (3) T.-J. Yang, V. Sze, “Design Considerations for Efficient Deep Neural Networks on Processing-in- Memory Accelerators,” IEEE International Electron Devices Meeting (IEDM), Invited Paper, December 2019.	Winner!
39	extraordinarily challenging by the extreme starlight suppression required to image faint planets around brilliant stars. The noise-limited performance of current high-contrast imaging instruments can resolve planets up to 10 million times dimmer than their host star. In order to access Earth-like planets – the highest-priority planetary targets named by the Astronomy and Astrophysics Decadal Survey [1] – sensitivity must be expanded to planets 10 billion times fainter than their star. The primary limitation on increasing contrast is speckle noise, which is scattering of the stellar point spread Fig. 1: Raw image of the function (PSF) that can mimic or obscure planet signals [2]. As observation planetary system HR8799. time increases, the total noise contributions of read noise and photon noise Four Jupiter-size planets attenuate, but speckle noise does not, establishing a high noise floor that are obscured by quasi- cannot be reduced without removing the speckles themselves. I propose to static speckles. Image: Dr. develop a computational method of speckle subtraction for data taken C. Marois with the Gemini Planet Imager (GPI), which will improve the precision of existing data and will enable future higher-contrast observations of exoplanets. In addition to improving the sensitivity of legacy data, the proposed work will provide timely support for the funded GPI upgrade beginning in 2020 and returning to science operations with commissioning of GPI 2.0 in 2023. Study Design: Speckle noise is created as starlight passes through non-uniform atmosphere and optics, producing a stellar PSF that varies with time. Atmospheric speckles can only be corrected by improved adaptive optics hardware. This leaves “quasi-static” speckles caused by non-common path aberrations within the instrument optics. Quasi-static speckles change slowly over timescales of minutes to hours [3], resulting in an effect that varies both chromatically and temporally. This project aims to model, and subsequently remove, these quasi-static speckles using the non-parametric technique of principal component analysis (PCA). PCA identifies “principal components” as linear combinations of input parameters, producing a dimensionally reduced result which identifies the strongest predictors of the features of that data. These results can be used to subtract the quasi-static speckles directly. Previous applications of PCA to high-contrast imaging (e.g. [4, 5]) have focused on subtracting the entire stellar PSF, both atmospheric and quasi-static speckles, which is useful for recovering target signal but results in improvement for only the dataset it’s applied to. The technique of applying PCA to isolated quasi-static speckle noise has never been successfully applied to exoplanet high-contrast imaging, but will result in a more flexible, broad correction for this type of noise. By characterizing quasi-static speckle behavior and evolution over given epochs and wavelengths, corresponding corrections can be applied not only to the training data, but any data of matching instrument, epoch, and wavelength. This method may also reveal stable speckle behavior which is present at all times and wavelengths, and can be universally subtracted. Once applied, the precision of legacy data is expected to improve by one order of magnitude, and this speckle subtraction will allow future GPI observations to achieve greater contrast by approximately two orders of magnitude [6], making important steps towards accessing Earth-like planets. The phases of this project are outlined below. Phase I: First, I will build a training dataset of GPI science images containing isolated speckle noise, which can be achieved by subtracting a noiseless model of the stellar PSF from all training data. This will leave only unexplained noise behind, the primary component of which is quasi-static speckle noise. Phase II: Grouping the training dataset over discrete time increments and wavelength intervals, I will apply PCA to the training data. The results of this analysis can be used to subtract an estimate of the stable components of quasi-static speckle noise from the data. Phases I and II will take place during years 1 and 2 of graduate school, which is well-timed to inform the concurrent GPI upgrade. Phase III: Then, I will quantify the effectiveness of this correction procedure by measuring the improvement of the intrinsic noise present in each subtracted image, as well as by performing injection- recovery tests with simulated planet signals. Phase IV: Finally, once these results have been verified, I will develop and release an open-source codebase for quasi-static speckle subtraction, intended for use by scientists working with GPI data. Phases III and IV will take place during years 3 and 4 of graduate school, which will align with the commissioning of GPI2.0 allow for my results to be folded into its data reduction pipeline. One anticipated challenge associated with Phases I and II is that speckle noise is difficult to isolate in legacy GPI data, either because accurate reference PSFs cannot be generated, or because the data contains systematics which may confuse the subsequent analysis. In this case, since GPI will be present at Notre Dame for upgrades, I will be able to collect data directly from the instrument using the telescope simulator operated by the Chilcote group. By allowing more control over observing conditions and precise knowledge of the input PSF, data taken using the telescope simulator will enable a cleaner first-step analysis, allowing more robust treatment of the legacy data when it is later re-introduced. Another anticipated problem is that GPI2.0 goes on-sky before Phase IV concludes. However, modifications to the processing pipeline can still be made after science operations commence since post- processing can be retroactively applied. Additionally, even if completion of Phase IV lags, the robustness and impact of this technique will be well understood from Phase III, and observations can be planned in anticipation of the correction tools of Phase IV being completed in the future. Intellectual Merit: The importance of increased contrast for high-contrast imaging campaigns is crucial even beyond exoplanets, with implications for the direct imaging of all astrophysical objects at small angular separations from a comparatively bright source — including circumstellar disks, stellar winds, or jets emitted from neutron stars, pulsars, or black holes. This project is a high-impact, far-reaching, and low-cost avenue to increasing the science yield of existing direct imaging instruments, increasing the sensitivity of extant and future data without the need for the expensive and prolonged development of new instrumentation. Additionally, while the proposed solution will be built specifically for GPI, it can be adapted to interface with other ground-based high-contrast imaging instruments, including SPHERE on VLT and CHARIS on the Subaru Telescope. A software-based speckle subtraction method also has strong implications for the development of space-based direct imaging missions, such as the Nancy Grace Roman Space Telescope (formerly WFIRST), as speckle noise is similarly dominant in space-based direct imaging [7]. The open-source release of my work will facilitate broad advancement and collaboration across astrophysics subdisciplines and different instrument teams. Years of previous research experience, including publishing papers, presenting at conferences, giving talks, and directing analysis, have prepared me to effectively execute the proposed work. I have years of experience with data analysis and visualization with Python, and as part of my work with CERN and GPI have worked with large datasets and distributed computing. I currently work on GPI with Professor Jeffrey Chilcote at the University of Notre Dame, so I am familiar with the instrument and am prepared to hit the ground running. I will also benefit from the technical expertise and support of the international GPI collaboration as I develop this project. Given the large volume of legacy data which will be analyzed during this project, as well as the computationally demanding nature of PCA, I will require access to high-performance computing facilities such as the Notre Dame Center for Research Computing. Broader impact: The NSF GRFP will support me to pursue high-impact research alongside community engagement. Alongside this speckle suppression project, I will establish a peer mentorship program at my graduate institution, as well as develop curricula and workshops to engage public elementary students in space science, both of which I detail in my personal statement. I will integrate these engagement efforts with the GPI Outreach team to create science communication materials conveying the excitement of squinting through stellar glare to find new worlds orbiting underneath. [1] National Research Council 2010, New Worlds, New Horizons in Astronomy and Astrophysics [2] Marois, C., Doyon, R., Nadeau, D. et al. 2003, EAS Publications Series, 8, 233-243 [3] Hinkley, S., Oppenheimer, B., Soummer, R. et al. 2007, ApJ, 654, 1, 633-640 [4] Wang, J., Ruffio, J.-B., De Rosa, R., et al. 2015, Astrophysics Source Code Library, ascl:1506.001 [5] Soummer, R., Pueyo, L., Larkin, J. 2012, ApJL, 755, 2 [6] Soummer, R., Ferrari, A., Aime, C. et al 2007, ApJ, 669, 1, 642-656 [7] Brown R., Burrows C. 1990, Icarus, 87, 2, 484-497	Winner!
40	"Motivation: Since the mid-1970s, global natural gas production has steadily risen to its current all-time high, and is projected to continue until at least 2040.1 Although natural gas is a cleaner burning fuel than gasoline, its implementation in the transportation sector has been stymied by its significantly lower volumetric energy density.2 Densification strategies, including liquefaction or high-pressure storage, have inherent safety and cost issues that are widely viewed as prohibitive for passenger vehicles.3 Introduction: Adsorbed natural gas systems that employ porous materials, such as metal-organic frameworks (MOFs) and covalent organic frameworks (COFs), have received considerable attention as potential alternatives to higher pressure and/or cryogenic storage methods.4,5 In these systems, favorable interactions between the gas and the porous solid increase the amount of gas that can be stored at a given temperature and pressure as compared to an empty tank.4 However, the non-molecular nature of these extended structures makes solubility non-existent, reducing compatibility with post-synthetic functionalization and modification that can be leveraged to increase gas adsorption capacity and bulk material properties, such as density and thermal conductivity. Hybrid metal-organic or all-organic molecular analogs of these porous materials, coined porous coordination cages (PCCs) and porous organic cages (POCs), respectively, directly address this issue while retaining the highly sought after permanent porosity and tunability of their expanded, 3-D counterparts.6,7 PCCs often contain open metal sites that provide favorable interactions for increased gas storage as compared to all-organic structures.6 However, these molecules tend to display surface areas that pale in comparison to MOFs and POCs. On the other hand, POCs display surface areas that are on par with many MOFs, but lack the tunable metal cation- gas molecule interactions seen in hybrid metal-organic systems.8 To address these issues, I propose the design, synthesis, and application of novel porous organic cages with integrated metal sites toward the selective adsorption and/or storage of small molecules. Preliminary Results: Although porous organic cages have been heavily investigated over the past decade, the study of the high-pressure storage of gases in these materials is still well in its infancy.7 As a result, relatively little is known about their utility as gas storage materials. Similarly, post- synthetic metalation of these systems to introduce sites with catalytic Figure 1: Known POC activity or selective gas adsorption has been unexplored. The standard targeted for preliminary work metric for porous materials, gravimetric surface area, is a simplistic representation of a material’s ability to store a gas. While the high surface areas that POCs have displayed provide a basis for using such materials as gas storage media, investigations into these materials for the specific task of gas storage is surprisingly limited. Incorporation of metal chelating sites within molecular cages will allow for the precise insertion of a specific metal post-synthetically. Metal cations can play an important role in tuning metal-gas interactions, which is necessary for creating a material for selective gas adsorption or high-pressure storage. I targeted a known POC based on triformylphenol and 1,2- diaminocyclohexane, where a half-salen unit is formed when the cage is constructed (Figure 1).9 Confirmed via SEM-EDX and XPS, initial results show coordination of a Ni2+ center into the cage’s structure, while retaining permanent porosity (Figure 2). Further gas adsorption studies to better understand the selectivity of the metal-integrated POC are currently underway. Aim 1: Create a library of ligands and cage topologies that are conducive to metal integration. Inspiration for this approach can be drawn from recent literature on the reported topologies of cages, where the most straightforward methods involve imine or boronic ester formation to create the covalently linked cage. Although specific angles must be considered within the ligands in order to access these desired topologies, functional groups and sizes of the ligands are typically tunable. Understanding this, cages will be functionalized and built around traditional multi-dentate ligands, such as salen, catechol, and 2,2’-bipyridine to form metal complexes after cage isolation. Aim 2: Isolate porous organic cages and introduce metals to their chelating sites. Typically, solution-based syntheses produce cages. The purity of isolated cages can be confirmed through several techniques due to their molecular nature, most readily being high-resolution mass spectrometry, NMR, and IR spectroscopy. For more complex cages, such as cages containing chiral centers, additional efforts will be put forth to obtain diffraction quality single crystals, utilizing techniques such as vapor diffusion and liquid/liquid diffusion, to further confirm cage Figure 2: Model representing formation. After successful isolation, the porous organic cages will then the integration of Ni2+ be introduced to metal salts to obtain their metalated counterparts via solvothermal methods and solid-state metalations. A plethora of techniques are available at the University of Delaware for the characterization of the metalated POCs, including SEM-EDX, XPS, EPR, and the previously mentioned techniques. Aim 3: Perform gas adsorption studies and identify the roles that both cages and metal-sites play in selective gas uptake and storage. Gas adsorption studies will be performed on both the base cage and the metalated cage to decipher the interplay of porosity and selective gas uptake. Surface area analyses will be performed in the Bloch Lab, using both CO and N as probe molecules, along with systematic high- 2 2 pressure gas storage studies (hydrocarbons, H , etc.) and enthalpy of adsorption calculations. 2 Intellectual Merit: While small molecule storage has been heavily studied in extended frameworks like MOFs and COFs, much less is known for porous molecular materials. These materials retain the sought after permanent porosity of expanded frameworks, as well as impart solubility that can lead to advantageous post-synthetic modification. This project will elucidate how POCs can be utilized as gas storage media and develop the novel field of metalated POCs, including their design, synthesis, and utilization as adsorbed natural gas materials. Broader Impacts: My proposed project has opportunities for collaborations that I will pursue heavily to better understand these systems. Collaboration with computational groups will help predict gas storage capacities and suggest more favorable interactions based on metals and ligands, and work with catalysis- focused groups can utilize my metalated cages as a homogenous setting for catalytic reactions. Just as importantly, I will continue to mentor undergraduate researchers and first year graduate students to teach them essential and advanced laboratory skills. I will share my findings at local, national, and international meetings when they are deemed safe, and until then, I will present my work virtually and continue to publish results. References 1. EIA, ""World Energy Outlook 2019"", 2019, https://www.iea.org/reports/world-energy-outlook- 2019/gas 2. EIA, “How much carbon dioxide is produced when different fuels are burned?”, 2019, https://www.eia.gov/tools/faqs/faq.php?id=73&t=11 3. DOE, “Natural Gas Fuel Safety”, https://afdc.energy.gov/vehicles/natural_gas_safety.html 4. Mason, J. A.; Veenstra, M.; Long, J. R. Chem. Sci. 2014, 5, 32-51. 5. Das, S.; Heasman, P.; Ben, T.; Qiu, S. Chem. Rev. 2017, 117, 1515–1563. 6. Gosselin, A. J.; Rowland, C. A.; Bloch, E. D. Chem. Rev. 2020, 120, 8987–9014. 7. Hasell, T.; Cooper, A. I. Nat. Rev. Mater. 2016, 1, 16053. 8. Zhang, G.; Presly, O.; White, F.; Oppel, I. M.; Mastalerz, M. Angew. Chem. Int. Ed. 2014, 53, 1516- 1520. 9. Petryk, M.; Szymkowiak, J.; Gierczyk, B.; Spólnik, G.; Popenda, Ł. Janiak, A.; Kwit, M. Org. Biomol. Chem. 2016, 14, 7495-7499."	Winner!
41	Introduction: Over the past 130 million years, flowering plants have evolved a variety of visual and chemical cues that mediate species’ interactions. The diversity of color phenotypes in flowers has been the subject of many ecological studies and the biosynthesis and regulation of the main compounds responsible for pigmentation is well understood1,2. These compounds are well-known for their role in pollinator attraction and additionally have many important biological functions that have been described (e.g., allelopathy, lignification, protection from UV radiation)3. However, despite the significance of pigmentation in plant growth, development, and reproduction, the role of pollen color remains unclear. About 75% of flowering plant species have yellow or white colored pollen, though pollen may also be pigmented red, blue, purple, or black4,5. The composition of specialized metabolites that occur in pollen across several taxa (e.g., phenolic compounds, alkaloids, terpenoids) have also been described6. Several of these compounds are known to be important for pollen development, pollen germination, pollen tube growth, and protection from abiotic stress (i.e., temperature, UV)7. A Recent work from Dr. Shu-Mei Chang’s lab at the University of Georgia, Athens (UGA), has discovered pollen color polymorphism in wild geranium, Geranium maculatum. Field observations along the Appalachian Mountain region show that B C D purple and yellow pollen color morphs persist in different ratios along an elevational cline8. Though pollen color polymorphism Confocal LC-MS/MS has been observed in other plant species, the ecological function Figure 1. Experimental Design. (A) Purple & and adaptive value of this trait is still unknown9-12. Therefore, I yellow pollen color morphs. (B) iNaturalist propose to examine the geographic distribution patterns of pollen distribution data by community scientists. (C) Confocal microscopy & LC-MS/MS color morphs, characterize their phenotypes, and evaluate the characterization of pollen phenotypes. (D) functional role of this trait in an ecological context (Fig. 1). Reproductive trait evaluation under abiotic stress. Aim 1: Determine the distribution of pollen color by integrating field surveys and community science In native G. maculatum populations, dark pollen color morphs have been observed at higher elevations8; a trend that has not been observed in other plant species9-12. iNaturalist is an online platform and smartphone application that allows anyone to record observations in nature. To date, there are over 13,000 records of G. maculatum and over 1,600 individuals that have made observations across the US. To determine if there is a correlation between elevation and pollen color, I will use this data to analyze the occurrence of purple and yellow pollen color morphs in a logistic generalized linear model with latitude and elevation predictors, as described by Austen et al12. I expect to see a positive correlation between elevation and pollen color intensity along an elevation gradient based on previous field observations. Furthermore, I will geo-reference 20 populations of G. maculatum along an elevation gradient to serve as collection sites for my study. To supplement my own collection material, I will advertise this study on the Ecological Society of America listserv and on social media pages of native plant societies to recruit community scientists. I will then create collection kits with an overview of the project and a guide to propagule collection to mail to each individual, who will harvest from their respective site and mail their completed kits to UGA. Clear instructions will be provided to each collector on how to image the population and gather one representative specimen to allow confirmation. Aim 2: Characterize reproductive traits and pollen phenotypes of G. maculatum populations Pollen features that vary among pollen color morphs can have a significant impact on male fitness of a plant13. Thus, I will examine the morphological and biochemical characteristics associated with pollen color. I will propagate G. maculatum from rhizomes from Aim 1 in the greenhouse at UGA and generate an F2 population that segregates in pollen color. Upon flowering, reproductive traits (e.g., flower number, flower size, flower color, pollen color, seed set) for each plant will be described. I will collect and image pollen by confocal microscopy at the UGA Biomedical Microscopy Core (BMC) for characterization of pollen features (e.g., size, surface ornamentation). Then, I will utilize the Proteomics and Mass Spectrometry facility at UGA to quantify specialized metabolite accumulation in anther tissue and pollen by liquid chromatography tandem mass spectrometry (LC-MS/MS). Specialized metabolites have previously been characterized for G. maculatum pollen and it was found that the metabolite profile differed significantly among four collection sites6. I will further characterize the correlation between pollen color intensity and metabolite profile to identify compounds that are important for pollen performance in Aim 3. Aim 3: Evaluate trait-correlated tolerance to abiotic stress To evaluate the pollen performance of the F2 individuals, I will measure pollen viability, germination, and siring success under different conditions. I will expose different colored pollen to a gradient of temperature and light intensity treatments (mimicking field conditions), and assay for pollen viability and germination in vitro, where pollen germination rates and pollen tube length will be recorded. Additionally, I will observe the siring success of each pollen color morph in vivo. I will then conduct common garden experiments to evaluate fitness in field conditions. Replicate F2 populations that contain identical genotypes (by splitting rhizomes) will be planted in common garden plots at the Highlands Biological Station in North Carolina (high elevation) and the UGA State Botanical Garden (low elevation). Floral/reproductive traits (as described in Aim 2) and transplant survival will be recorded for each population over the span of 2 years. I hypothesize that dark pollen individuals will have greater reproductive success in high elevation due to specialized metabolites that confer abiotic stress tolerance. I expect that light pollen individuals will have decreased transplant survival but compensate to some extent by producing more flowers with higher seed set; a maternal reproductive strategy described by Koski et al. in Campanula americana13. I will also collect the same subset of individuals from each garden for metabolite analysis (as described in Aim 2) to determine whether there is a genetic-by-environment (GxE) effect on their profiles. Intellectual Merit: My previous research experience in floral development, pollen biology, biochemistry, and molecular biology makes me uniquely positioned to lead this interdisciplinary and community-science supported endeavor. Under the guidance of Dr. Chang, an expert in plant ecology and plant mating systems, I will expand our limited understanding of the role of pollen color polymorphism as a strategy for reproductive success. Additionally, previous graduate students in the Chang group have led community- science research endeavors using iNaturalist, making this an ideal environment to build upon this infrastructure. In taking advantage of the biological research stations available to graduate students at UGA, this study will be the first of its kind to evaluate pollen color morph-dependent fitness in an ecological context. Moreover, coupling biochemistry and ecology approaches, I will generate a comprehensive understanding of the role that specialized metabolites play in pollen germination and reproductive success – information that has implications in both agricultural production and native plant conservation. Broader impacts: I foresee my graduate research as a vehicle for mobilizing community scientists, providing educational opportunities to underserved communities, and improving diversity in academia. I will work closely with K-12 instructors to develop plant biology lessons with field work and family engagement components to create community awareness of these relevant topics. Students and their families will be given demos on how to use the iNaturalist platform to encourage outdoor activity and participation in community science. In collaboration with Max Barnhart, a graduate student in the Integrated Plant Sciences program who is the lead PI on an American Society of Plant Biology (ASPB) science communication grant, I will create and distribute a zine to provide an overview of my work to the general public and the community scientists involved. I will also share my experience with the scientific community by developing a workshop entitled “Incorporating Community Science Into Your Research Program” for the annual ASPB conference. During this workshop, I will discuss the various platforms available for building community science projects, demonstrate how to navigate and utilize these platforms, and lead an exercise on brainstorming ways to engage community scientists in your research. References: 1Rausher MD. Int. J. Plant Sci. 2008. 2Grotewold E. Annu. Rev. Plant Biol. 2006. 3Jiang et al. Plants. 2016. 4Lunau K. Plant Syst. Evol. 1995. 5 Miller et al. Optics & Laser Tech. 2011. 6Palmer-Young et al. Ecol. Monogr. 2018. 7Muhlemann et al. PNAS. 2018. 8Udell-Perez R. Field Obs. 9Jorgensen & Andersson. New Phytol. 2005. 10 Koski & Galloway. New Phytol. 2018. 11Wang et al. Evolution. 2018. 12Austen et al. Ecology. 2019. 13Koski et al. J Evol. Biol. 2020.	Winner!
42	Novice programmers who are writing code encounter frequent challenges, which they often attempt to address by searching online to learn new concepts or debug their code [3]. However, novice programmers rarely receive explicit instruction on how to effectively resolve programming challenges with online search. Seeking help through online search is a critical self-regulatory skill for students, both in class and after graduation, but professional programmers agree that it can be difficult to learn [8]. Some prior work has investigated the challenges that professional developers face when searching and how to support them. However, there is little research about how programmers learn to use online search, how to teach search effectively, or how adaptive tools can support this process. Further, prior work has been limited to collecting user search queries and developer surveys, without using the programmer's code and errors to understand their context and the reason for their search. My research goal is to 1) understand the strategies that undergraduate novice programmers use to search for help online when writing and debugging code, and 2) explore how to design adaptive learning environments that support students in learning effective online search strategies. Building on Prior Work: Collecting data from novices’ programming environments has become popular in Computer Science (CS) Education due to their ability to provide granular views into the programming process via incremental code snapshots. These code snapshots have been leveraged to extract features such as compiler error encounter rate and error resolution time, which are used to understand novice debugging behaviors. In addition, code snapshots can be used in the design of interventions that dynamically provide feedback during the programming process based on current and past code contexts. It has been shown that intelligent tutoring systems (such as augmented programming environments) that provide timely automatic feedback can positively affect learning [5]. Work has been done on augmenting learning environments with tools such as web browsers that filter search results to be more understandable by novices [2,7]. However, many of these tools focus on improving search results: no tools have been developed with a focus on improving code search behavior and imparting generalizable long-term search skills. RQ1: What barriers do students face and what strategies do they employ when using web-based search systems to write and debug code? In Year 1, I will conduct exploratory lab and classroom studies to identify common barriers that novices encounter and strategies that they use for online search during programming problems. My goal is to discover what unique problems novices encounter when using online search, inform pedagogy on how online search should be taught, and learn what features to consider when designing a tool to support better search behavior. In small scale lab studies during the first semester, novice programming students will be asked to solve programming problems appropriate to their skill level while having access to online search. I will collect think-aloud protocols as students work, pre- and post-surveys, logs of students' search behavior, and fine-grained code snapshot logs. Classroom studies of NCSU’s introductory computing (CS1) course during the second semester utilize augmented programming environments and provide a large volume of search and code snapshot data, which will allow us to test if the findings from the lab studies are generalizable to the classroom. Using this data, we can evaluate how search behavior from beginner programmers may differ from or align with prior work on professional developer behavior, such as in query style, query reformulation rate, and search session length. Building on prior work on using code snapshots to track compiler error resolution [1], we will explore how to track students’ success in using online search to resolve errors. This will allow us to identify which students are struggling, where they are struggling, and what search strategies are effective. RQ2: How can we give students accurate and timely feedback on how to improve the effectiveness of their search queries? In Year 2, I will perform design-based research of a learning environment that will provide automated support to students to improve their search skills. While the ultimate design of the system will be shaped by student and teacher needs identified in Year 1, the system will support developing students’ skills in the key phases of the search process. The key phases of search are derived from theories of help-seeking (e.g. [4]): recognizing the need for help, gathering relevant information, formulating an effective query, identifying an effective source of help, and integrating the help into code. For example, consider a student who is stuck on an error and whose previous search attempts were not successful. The system could then suggest an effective help-seeking strategy that was identified in RQ1, such as query reformulation. The system will offer adaptive help, responding to a student’s current code and error message. I will achieve this by building on foundational work by the HINTS lab at NCSU on using program code to create adaptive feedback systems [6]. My current membership in the HINTS lab puts me in a unique position to design this system. RQ3: What effect does timely automated feedback on search queries have on novice programming behaviors and their ability to use web-based search to resolve future problems? After multiple rounds of development in Year 2, in Year 3, I will deploy the system and perform classroom studies to measure the impact this tool has on novices’ search and programming behavior. These classroom studies will initially take place over the course of a semester in NCSU’s CS1 course. The effectiveness of the intervention will be measured by the change in student programming performance and retention of help- seeking behaviors over time (as evidenced by features of successful help-seeking identified in RQ1, such as query styles and error resolution rate). In addition to quantitative metrics, qualitative feedback by students at the end of the semester will also inform the system’s overall impact. Intellectual Merit: This project will provide novel insight on the strategies that undergraduate novice programmers use to search for help online. By extending prior work on programmer search behaviors through the novel methods of observing program and error state, we can gain granular information about the contexts in which novice programmers seek help. In exploring effective search strategies (RQ1), we can inform the design of better pedagogy for teaching online search. The feedback methods designed in RQ2 and evaluated in RQ3 will inform future designs of intelligent tutors for help-seeking. This project will provide a base for future work on understanding programming help-seeking behavior, and the methods discovered could be extended to learner populations beyond novices. Broader Impact: In many CS programs, searching for code help online is a skill that is not explicitly taught, yet is expected of students in upper-division courses and after graduation. Explicit pedagogy around web search will normalize help-seeking behavior and reduce impostor syndrome. Automated feedback systems allow for the refinement of these skills even if an instructor is unavailable. This project will inform future online search pedagogy and the design of learning environments that reinforce these skills. By addressing these two issues, this project will help make CS more accessible. [1] Jadud. “Methods and Tools for Exploring Novice Compilation Behaviour” ICER‘06 [2] Lu et al. “ ” Information Retrieval Journal ‘17 [3] Muller, et al. Exploring Novice Programmers' Homework Practices: Initial Observations of Information Seeking Behaviors SIGCSE ‘20 [4] Nelson-LeGall. “Help- Seeking: An Understudied Problem-Solving Skill in Children” Developmental Review ‘81 [5] Nesbit, et al. “Intelligent Tutoring Systems and Learning Outcomes: A Meta-Analysis” Journal of Educational Psychology ‘14 [6] Price, et al. “iSnap: Towards Intelligent Tutoring in Novice Programming Environments” SIGCSE ’17 [7] Venigalla et al. “StackDoc - A Stack Overflow Plug-in for Novice Programmers that Integrates Q&A with API Examples” ICALT’19 [8] Xia, et al “What do developers search for on the web?” Empir Software Eng ‘17	Winner!
43	Introduction: Traditional stair construction, in which stair flights are rigidly connected to the structure at both ends (“fixed-fixed” connections), has been shown to cause damage to stairs and surrounding structural members during earthquakes due to stairs being stretched and compressed due to the relative displacement between a building’s floors1. (See Figure 1.) In response to this, alternative designs (“fixed-free” connections) have been developed2 that permit stairs to accommodate the relative deformation between stories by detaching the stair at one of the floors. Recent tests2 have confirmed that fixed-free connections have the potential to eliminate damage due to seismic forces being distributed to stairs, but further testing is needed to understand these novel designs. Scissor stairs, a common configuration in which the stair turns back on itself at a mid-story landing, have not yet been tested in conjunction with fixed-free connections. Stairs have been shown to affect a structure’s seismic response3, so it is essential to investigate scissor stairs with fixed-free connections not only in isolation but also interacting dynamically with a building. One concern is that releasing degrees of freedom for fixed-free connections may cause a whiplash effect due to the stair’s mass being less constrained, damaging nearby building components. Additionally, this whiplash effect may cause undesirable torsion in the building. Therefore, a key challenge in designing fixed-free stairs is to remove enough restraints to permit some movement while preventing completely free oscillation. I propose to observe and quantify the dynamic characteristics fixed-free scissor stairs and their effect on the lateral response of buildings. Three variations of fixed-free connections will be considered: 1) removing all connections from the lower end of the stairs so that it may slide freely on its landing, 2) use slotted connections to allow some movement while restraining most movement, and 3) using a sliding hanger connection to allow translation in all three dimensions without leaving the stair completely unattached. Hypothesis: Fixed-free stairs will prevent damage that traditionally constructed stairs would otherwise suffer by allowing relative movement between stairs and floors and dissipating energy through friction. Research Goal 1: Develop Models for Stairs with Fixed and Free Connections I will model four scissor stair configurations: three with fixed-free connections and a control case with traditional, rigid connections. Because building prototypes and performing dynamic testing is typically cost-prohibitive, I will develop finite element models of the stairs and their connections using the finite element program LS-DYNA. An essential feature of LS-DYNA is its ability to model friction and the interaction between components that come into contact with one another4. The models will be subjected to cyclic and dynamic loading protocols to identify the force-deformation behavior of the stair system under earthquake loading. Research Goal 2: Model Scissor Stairs in Structures Using the methodology described by Wang et al. (2015)5, I will represent fixed-free stairs in structures by developing a system of nonlinear springs using the force-deformation relationships found in Goal 1, which will be then integrated into full-building structural models to observe the effect of fixed-free stairs on the seismic response of buildings. For this task, I will use the structural finite element framework OpenSeesPy, which is more appropriate for modelling an entire building. In order to evaluate the effects of fixed-free stair connections, I will subject the models to dynamic loading using a variety of ground motions, then compare member forces and nodal displacements between models without stairs, with traditional stair connections, and with fixed-free stair connections. Research Goal 3: Improve Full-Building Models by Comparing to Shake Table Test Working under Professor Keri Ryan at the University of Nevada, Reno, I will participate in the shake table testing of a full-size, 10-story timber building at the NSF’s National Hazards Engineering Research Infrastructure (NHERI) facility at UC San Diego in 20216. Because this structure will include scissor stairs with various types of fixed-free connections, this will be an unprecedented opportunity to gather physical data showing the effects of stair-structure interaction. Using this data, I will develop a model of the building including the stairs using OpenSeesPy, which I will validate and calibrate with data from the shake table test. Discoveries from this test will allow me to improve the stair-structure models developed for Goal 2. Intellectual Merit: Although prior research efforts have investigated the performance of fixed-free connections2 and the interaction between scissor stairs and structural systems5, no study has yet addressed coupling the two. Previous tests have demonstrated the potential of fixed-free connections to mitigate damage in stairs2 sufficiently to warrant further investigation. Accurately characterizing the nonlinear force-deformation relationship of scissor stairs with fixed-free connections will facilitate future research into the seismic response of buildings. Furthermore, developing a nonlinear spring model in Goal 2 will be an important step in helping practicing engineers integrate the results of this research into design practice. This research will also advance the quality of computational analysis in structural engineering. To my knowledge, I will be the first student at the UNR to extensively use OpenSeesPy, an adaptation of the finite element framework OpenSees for Python. Python has many data science libraries that will improve analysis of data from shake table tests and computational models. Linking Python and OpenSees will improve the quality of structural research by facilitating pre- and post-processing of data from tests and simulations. Broader Impacts: Failure to account for differential movement between floors has led to stairs collapsing in the recent Wenchuan and Christchurch earthquakes2. Fixed-free connections can prevent similar collapses in the future, but first their effects on building response need to be considered before this life-saving technology can be fully implemented. Since stairs are the primary means of egress from a building during a catastrophic event, protecting stairs from collapse during seismic events is an essential task for preventing loss of life. Furthermore, better modelling of stair-structure interaction will ensure safer design and reduce damage, which will in turn facilitate rapid recovery after disasters by reducing building downtime and preventing economic losses. In order to promote the use of safer stair systems, I will disseminate my findings through publications in structural engineering journals, the NHERI TallWood outreach webpage, and through seminars hosted by UNR’s Earthquake Engineering Research Institute chapter. The NHERI TallWood project will give me opportunities to work closely with industry collaborators to further develop and promote fixed-free connections. I will also present a simplified version of my research to K-12 classrooms to foster youth interest in earthquake research. References: [1] D. Bull, (2011). Canterbury Earthquakes Royal Commission. [2] C. Black, et al., (2020). 17th World Conference on Earthquake Engineering. [3] J. Zhu, et al., (2011). Applied Mechanics and Materials. [4] https://www.lstc.com/products/ls-dyna [5] X. Wang, et al., (2015). Earthquake Engineering & Structural Dynamics. [6] K. Ryan, et al., (2020). Colorado School of Mines.	Winner!
44	which humans can run experiments and the enormous size of the search space.1 Recently, the materials- design loop has been accelerated through several different mechanisms, including robotic high-throughput experimentation (HTE), atomistic simulations, and machine learning (ML).1 While each of these techniques has independently shown promise for accelerating discovery, an approach that applies all three harmoniously would revolutionize chemical research with wide-ranging implications, enabling faster and cheaper discovery of new pharmaceuticals, catalysts, and photovoltaic devices. I aim to advance this effort in my own research by coordinating the interplay between these three methods for the discovery and design of new dye molecules. Dyes are a suitable class of molecules for testing an autonomous, integrated design platform because they have several readily measurable properties that must be optimized simultaneously for use cases ranging from solar cells to medical imaging. Specifically, I am focusing on the following objectives: (1) developing ML models to predict UV-Vis absorption and emission spectra accurately given a dye molecule and solvent pair, (2) creating a generalizable, automated active machine learning framework to improve the prediction models, and (3) utilizing this framework to design a novel near-infrared (NIR) dye for biomedical sensing and diagnostics. Objective 1 - Model Development for UV-Vis Spectra Predictions: Accurate prediction of UV-Vis optical properties is essential to dye design for any application. Previous work toward predicting UV-Vis spectra with ML has mostly consisted of the simpler task of predicting two scalars, the wavelengths of the peaks of maximum absorption and emission (𝜆 and 𝜆 )2, and has been limited by data sparsity. Since abs em starting my work with Prof. Rafael Gómez-Bombarelli in January, I have addressed this limitation by collecting all openly accessible UV-Vis data from seven online repositories (29,811 measurements in total) and standardizing it into a consistent format. I then used a combination of a directed message-passing neural network (DMPNN)3 and a feed-forward neural network to predict a value for 𝜆 given an input molecule- abs solvent pair and an analog of 𝜆 computed with time-dependent density functional theory (TD-DFT). abs Using this method, my model has achieved a test-set mean absolute error (MAE) of 8.68 nm (a 17% reduction in error over the previous best model) on the largest dataset for which ML predictions have been published.2 My first step toward extending this method to predict full spectra will be to train my model to predict the peak widths and intensities for each 𝜆 using the data I assembled. I foresee the limited abs quantity of available data presenting a challenge for predicting full spectra since the majority of openly accessible data contains only 𝜆 values for each molecule-solvent pair. I will address this issue with a abs pretraining strategy in which I train my model with lower-fidelity data and use the resulting neural network weights as the initial weights when training my final model (as opposed to a random initialization). I will then estimate the epistemic and aleatoric uncertainty in my model’s predictions using a deep ensembling approach.4 Finally, I will replicate the previous steps for predicting emission spectra. My accurate models for predicting absorption and emission spectra will aid experimentalists in choosing which molecules to test, even before I further automate this process in the following objective. Objective 2 - Active Learning: My models’ abilities to make predictions with corresponding uncertainties will fulfill an important prerequisite for implementing active learning (AL), which improves models by focusing the sampling of new data on molecules with high uncertainties in their predictions. The additional components needed for active learning are (1) a set of new molecules from which to sample, and (2) a method of measurement for each sample. My experimental collaborators in Prof. Klavs Jensen’s group have created (1) by extracting a list of 7 million purchasable compounds from chemical vendor websites. Further, they have created a method for (2) by building an HTE apparatus for measuring 96 UV-Vis spectra simultaneously. Since TD-DFT calculations are faster and cheaper than experiments, I propose using these to augment the strategy for (2) by reducing the number of necessary experiments. I will design a computational framework to automatically deploy calculations for molecules with a high epistemic uncertainty and retrain my models using this new data. From molecules that still have high uncertainty after the calculations, my system will use the uncertainty values along with molecular similarity to choose 96 molecules to recommend for measurement in the HTE apparatus. My models will automatically receive data from the experiments and repeat the previous steps iteratively until their predictive performances reach asymptotes of aleatoric uncertainty. The proposed AL framework integrates TD-DFT and HTE in an automatic fashion, which will enable significant time and cost savings and will be readily generalizable to many areas of chemistry and materials science research. Objective 3 - Design of a Novel Dye for Biomedical Imaging: Once I am able to demonstrate that my ML models are sufficiently accurate over a large region of chemical space, I will adapt my AL framework to design novel molecules with optimized properties for biomedical imaging. Specifically, it is favorable for dyes to have absorption and emission peaks in the NIR-II range (1000-1700 nm) because this range has deeper tissue penetration compared to visible or shorter-wavelength NIR-I light.5 High Stokes shift (𝜆 - em 𝜆 ) and high quantum yield are also desirable.5 Additionally, I will leverage the ongoing work of my abs collaborators in Prof. Bill Green’s group who are predicting solubility, toxicity, and photodegradation, as these are also important properties for this application.5 I will employ the generative models of Jin et al.6 to create new molecules out of substructures that are likely responsible for desired properties of interest in known molecules. Next, I will make predictions on these new molecules with my ML models. I will then modify my AL framework to explore the new chemistries proposed by the generative models; it will deploy TD-DFT calculations as necessary for molecules with uncertain predictions and ultimately recommend novel molecules with predicted properties in the target ranges to my experimental collaborators in the Jensen group. They will use automatic retrosynthesis methods7 to synthesize the novel compounds and will then use their HTE apparatus to test which proposed molecules indeed have the desired properties. Finally, I will propose the best-performing molecules to Prof. Angela Belcher’s group for further study and in vivo testing. If successful, this strategy could serve as a blueprint for combining experiments, theory, and ML for multi-objective molecular design across the field of chemistry. Intellectual Merit: Design problems in chemistry and materials science often suffer from a combinatorial explosion of configurations to explore, which makes solution of these problems intractable by brute force, or with HTE, physics-based calculations, or ML alone. By using all three methods simultaneously and automating the interactions between them, my work will be an advancement toward a “closed-loop” system that can explore massive chemical spaces with minimal need for human intervention beyond the specification of design objectives. Conducting my work at MIT gives me the opportunity to collaborate with experts who have proven records of integrating chemistry and computer science methods, and it gives me access to computing resources to run atomistic calculations and train ML models. An NSF fellowship would supplement my current computing resources with access to XSEDE and would ensure the necessary funding for my completion of this project. Broader Impacts: I will design a novel dye that could be applied to guide surgery or to detect cancer at earlier stages. My work’s flexible multi-objective optimization will also be able to design new dyes for additional applications such as dye-sensitized solar cells. Furthermore, the AL framework I develop could be widely adopted to design other types of molecules and materials, such as those in batteries and catalysts. I plan to make all code and datasets I develop openly available online with detailed documentation, which will enable other researchers to replicate and build upon my work more easily. Additionally, I will host a workshop to demonstrate my framework, with the goal that even experimentalists with little computational experience would learn to utilize the AL component of my framework to accelerate their progress in molecular or materials design. My work ultimately aims to encourage greater collaboration between experimental, theoretical, and computational researchers by automating the connections between their work in pursuit of design challenges that would otherwise be intractable. References: [1] Angew. Chemie Int. Ed., 2019, doi:10.1002/anie.201909987. [2] ChemRxiv, 2020, doi:10.26434/chemrxiv.12111060.v1. [3] J. Chem. Inf. Model., 2019, 59 (8), 3370–3388. [4] J. Chem. Inf. Model., 2020, 60 (6), 2697–2717. [5] J. Mater. Sci., 2020, 55 (23), 9918–9947. [6] ICLR, 2020, arXiv: 2002.03244. [7] Science, 2019, 365 (6453), eaax1566.	Winner!
45	Introduction/Intellectual Merit: Advanced metabolic engineering allows scientists to use genetic engineering techniques in lower organisms, such as Escherichia coli and Saccharomyces cerevisiae, to produce molecules of interest through recombinant pathways. Metabolic pathways are groups of genes that encode enzymes that work together to produce the molecule of interest. Scientists have identified the most efficient genetic modification methodologies to create optimal production strains, including promoter libraries. A promoter library consists of a variation in the DNA sequence which varies the transcription initiation rate of the associated gene. This variation can come from promoters in the native organism or in non-native organisms, which are identified using RNAseq data. These libraries have been extensively developed for model organisms such as E. coli and S. cerevisiae. However, these tools are currently limited for non-conventional organisms. Leaders in the field of biochemical engineering have identified the development of genetic tools for use with non-conventional organisms as a foremost goal because model organisms lack the complexity that non-conventional ones can provide1. Leveraging unique properties of non-conventional organisms allows scientists to build upon promising results that push the boundaries of the pharmaceutical, environmental, and cosmetic industries. Using the knowledge that I’ve gained from working in two metabolic engineering labs over the last three years, I intend to further explore the non-conventional oleaginous yeast, Yarrowia lipolytica. This yeast is particularly interesting because of its ability to naturally prevent bacterial contamination3, its utilization of various hydrophobic and hydrophilic carbon sources4, and its ability to efficiently produce large amounts of lipid-based products5. Y. lipolytica was originally identified in environments containing hydrophobic substrates and studied for its ability to biosynthetically produce enzymes and citric acids2. Recently, metabolic engineers have become more interested in this yeast as they explore non-conventional organisms for production of high-value compounds. Research Plan: Although genetic engineers have made significant strides in the toolkits available for Y. lipolytica, they currently lack the diversity necessary to fully exploit its potential. One of the newest toolkits was developed by a group in France and is called the Golden Gate toolkit for Y. lipolytica6. The Golden Gate toolkit allows researchers to efficiently transform and integrate heterologous pathways in the organism. This toolkit includes a validated promoter library and has been successfully used to express a functional xylose utilization pathway. A major goal of my research plan is utilizing this toolkit and others to integrate several metabolic pathways into the organism. Additionally, scientists have discovered more promoters and have begun to look into computational models for the organism. These promoters include TATA box promoters7 and one repressible and one bidirectional promoter2. Repressible promoters provide negative feedback to down regulate transcription pathways, which is useful for products that are inhibitory to growth. A bidirectional promoter can be initiated in both directions for transcription; this becomes important for efficient gene co-expression. However, in order to further enable the metabolic engineering goals for this non-conventional yeast, more native and non-native promoters need to be identified so that researchers can better optimize the genetic conditions for efficient pathway expression. One way I plan to find new native promoters is through the use of RNAseq, described in Figure 1. RNAseq is a technique that can identify transcriptionally active regions of the genome and provide data on the expression levels of genes under various growth conditions or metabolic states. This leads to the identification of relevant promoter regions and data that can then be compared between specified growth conditions and normal growth conditions in which the promoter region should not be active. This technique will also be applied to other organisms in order to create non-native promoters, allowing for the identification of exciting new Figure 1. Simplified steps of RNAseq promoters that behave differently than native ones in the host process for identification of promoters. organism. The identified promoter sequences will then be obtained for both sets of promoters and will be expressed recombinantly and characterized in the host organism. The identification of both native and non-native promoters will allow for a more robust toolkit than what is currently available. These new promoters will then be applied to pathways of high interest in the scientific community. One such pathway is the pathway for production of plant oils. Jojoba oil was identified as a leading ingredient for anti-aging formulas for skincare in the 2000s and is still widely used today with increasing demand8. Harnessing the ability of Y. lipolytica’s high metabolic flux towards fatty acids enables a new route of production for the long chain fatty acids that comprise Jojoba oil. The enzymes in this pathway have been identified9 and can be recombinantly expressed to produce Jojoba oil. This pathway will be a testing ground and motivator for identifying and characterizing novel promoters. After successful integration of the recombinant pathway, scale-up production studies can begin. The experiments for genetic optimization will take place in small volumes within 48-well plates (2mL). After creating multiple genetic libraries through use of promoters discovered and discussed previously, fermentation growth conditions can be studied in shake flasks (250mL). Studying the organism in shake flasks informs decisions about growth conditions in large scale studies, such as those done in a bioreactor. After determining high quality growth characteristics, studies will be moved into a bioreactor (typically 1.5L+) to study the industrial feasibility of the process. Few studies have been done using Y. lipolytica in bioreactors, so this process will require permutations of multiple parameters to determine the best operating conditions for growth and oil production. This objective will be assessed by the ability of the bioreactor process to be scaled and replicated and for Y. lipolytica to produce high oil titers at scale under the optimal conditions. Throughout the completion of genetic cloning and scale-up studies, enhancement of current computational models will be occurring in parallel. Genome-scale-metabolic models (GEMs) have been created for Y. lipolytica. GEMs provide a kinetic model of cellular metabolism, meaning that a GEM can predict the metabolic activity of an organism based on user-defined parameters. The Y. lipolytica models need to be improved so that a wider range of researchers can use them. The best models achieve around 80% accuracy to experimental findings, however for people outside the field of metabolic engineering, the models are hard to understand and use7. In order to enhance the model, experimental data will be collected on new promoters, new growth conditions, and scale-up studies. Confirmation of already existing gene editing tools will also be collected. Utilizing GEMs is radically different than metabolic engineering’s randomized approach for identification of optimal conditions. The GEMs allow researchers actively working with Y. lipolytica to first test their hypotheses in silico so that they do not spend excess time and resources attempting to screen every genetic and/or fermentation parameter. Broader Impacts: Currently, Jojoba oil is produced via extraction of the oil from the seeds of the Jojoba plant. This time-intensive and costly process could be made easier through the creation of a heterologous production host. An efficient recombinant host organism allows for scientists to produce the same quality oil with a significant decrease in the environmental stresses associated with harvesting from the natural Jojoba plant. Metabolic engineering enables Y. lipolytica to sustainably convert widely available carbon sources into high-value natural products. However, until further promoters are developed, and scale-up studies are done on the organism, its full potential cannot be harnessed. Furthermore, the in silico model of Y. lipolytica will allow a large community of scientists to work together to better understand the growth and production capabilities of the organism. Taken together, the work proposed above leads to a continued advancement of metabolic engineering technologies to benefit society through development of advanced methodologies for sustainable chemical production. References: 1. Whitehead, T. et al. Biotechnol. Bioeng. (2020). Journal (2014). 2. Hussain, M. TigerPrints (2017). 6. Larroude, M. et al. Microb. Biotechnol. (2019). 3. Michely, S. et al. PLoS One (2013). 7. Ma, J., et al. J. Ind. Microbiol. Biotechnol. (2020) 4. Ledesma, R. et al. Trends Biotechnol. (2016). 8. Ahmad, A. et al. Biomed. Dermatology (2020). 5. Gonçalves, F. et al. The Scientific World 9. Miklaszewska, M. et al. Plant Sci. (2016).	Winner!
46	Introduction: Understanding exactly how femalesmake matechoices,andhow thesemaponto male fitness and quality, has important ramifications for understanding trait evolution, and for conservation in species where female choice plays a large role in male success. Leks, where many males display at once hoping to attract a mate make an ideal model because female decision-making pathways should depend only on male display or other on-lek factors, and not on extrinsic factors such as territory quality or parental care. I propose to apply eye-tracking technology to female Greater Sage-Grouse (Centrocercus urophasianus) to determine how they acquire information from visual displays produced by males when selecting mates. Eye movements can be used to understand cognitive processes as animals must focusonaparticular subject to process the information given by it. Limitations on sensory processing determine the amount of information a female can take in about a male and thus limits what she considers when choosing a mate.1 I hypothesize that female choice is drivenby integrationof multiple signal elements. Aim 1: I will track female eye movements to determine which parts of the display females consider as they focus on one element at a time before moving on to another. Females cannot take in the whole of the display at once and should reduce their focus to only what they perceive to be the most important signal elements. Aim 2:Once females’preferredtraits areknown, Iwill assessthe differencesbetween successful and unsuccessful males to determine if there is likely selection pressure onthose traits. Finding those differences will demonstrate that females are intaking sensory information, processing it, and using it to make decisions about mating. Methods: I will travel to lek sites across Montana to conduct this research. Using cameras and GPS units2 placed on males, I will track general positions of males on the lek and estimate territory sizes for each male. I will also track mating success for each male by counting the number of matings attempted and presumably successful matings. For assessing females’ preferred traits, I will use eye-tracking technology3 to determine how females are acquiring information about important traits. Analysis of eye movements will show the specific features females target and use as criteria for mate choice. Using data from eye-tracking, I will measure the male ornaments females focused on, comparing between males with many matings, males that received some female attention butfewmatings, andmalesthat didnotreceive anymatings. I will analyze the differences between each group for each trait to assess whether the trait is under significant selection pressure driven by female choice. 1Dukas R. 2002. Behavioural and ecological consequences of limited attention. Philos. Trans. R. Soc. B Biol. Sci. 357, 1539-1547. 2Wann, GT, Coates, PS, Prochazka, BG, Severson, JP,Monroe, AP, Aldridge, CL. Assessing lek attendance of male greater sage‐grouse using fine‐resolution GPS data: Implications for population monitoring of lek mating grouse. Popul. Ecol. 2019; 61: 183– 197. 3Yorzinski JL, Patricelli GL, Babcock JS, Pearson JM, and Platt ML. 2013. Through their eyes: selective attention in peahens during courtship.J. Exp. Biol 216:3035-3046 Intellectual Merit: Eye-tracking is currently an under-utilized4 but insightful method in understanding visual cognition, especially in sexual selection. My research contributes to expanding use of this technology to understand how sight can affect trait evolution in an organism where visual traits are dramatic and emphasized. It provides other scientists with the understanding that these types of studies are feasible. It can be translated to see how cognition varies across the animal kingdom and increaseknowledgeabout visualcognition’sroleinsexual selection. Further, my research addresses the NSF Big Idea “Understanding the Rules of Life” because the process of decision making in sexual selection and female choice is still notclearly defined. Not only will I address the intraspecific interactions between males and females in a population, but I will assess cognition on an organismal level, to figure out how females intake information and perform a highly complex behavior, choosing a mate, in response. My experience working with autonomous recording units and GPS mapping with my undergraduate advisor and in my honors thesis has prepared me to work with various types of technology in the field. I am comfortable handling and managing equipment and data collected from the units. I am able to translate these experiences to this project to create an efficient workflow tohandle massamounts ofdata. I alsohave experienceanalyzinghighvolumesof data from audio recordings, which is easily transferable to processing video. Broader Impacts: With my proposed research, I will create opportunities for undergraduate research and citizen science. I will have undergraduate research assistants help me with capture, measurements, and placement of GPS receivers. These students will gain valuable training in fieldwork techniques andexperimental design,preparing them toalso goontoexpand scientific knowledgein ecology, animalbehavior, andorganismal biology.Asadisabled andfirst generation student, I will reach out to increase research participation for underrepresented students alongside my plans to expand access to research opportunities for underserved students. I will also recruit citizens from the surrounding areatohelp collectdata about thelek, thereby allowing more citizens to appreciate the habitat and gain an appreciation of science. Special effort will be made to extend this opportunity to local high school students who may not otherwise have the opportunity to conduct research with scientists. Leks areparticularly charismatic and tend to engage a wideaudienceof citizensof all agesinterested inbirds,which gives me the opportunity to teach them about behavior andhow tomake scientificobservations. At the same time as citizen outreach, I will also educate thepublic about thesagebrush habitat, Sage Grouse, and emphasize the importance of conservation science inmaintaining ahealthy environment. I also plan to collaborate with the U.S. Fish and Wildlife Service, local conservation organizations and Native American tribes to preserve habitat and reduce the amount of frackingin thearea,leadingtoimproved environmentalconditions, preservationof culturally important land, and more greenspaces, improvingmentaland physicalwellbeing for those living around sagebrush. Increased contact between the groups will also facilitate better working relationships and improve society for people who use these lands. 4Billington J, Webster RJ, Sherratt TN, Wilkie RM, and Hassall C. 2020. The (Under)Use of Eye-Tracking in Evolutionary Ecology. Trends in Ecology & Evolution.Trends Ecol. Evol.	Winner!
47	mate. The majority of research regarding the signal function of visual traits has concerned color or patches of color. This focus on charismatic coloration has left unexplored the potential signaling function of achromatic patterns. Some patterns, such as the white check patch of great tits (Parus major) or the black facial spots of female paper wasps (Polistes dominula), have been suggested to function as assessment signals1. Yet, there is little empirical evidence that females consider patterns when selecting a mate. In the songbird family Estrildidae, patterns or certain features of patterns may be dimorphic, which suggests that these may be sexually selected traits. This idea is only beginning to be explored, with some studies linking estrildid patterns to individual quality2,3. Little is known, however, about how females perceive these signals, including the extent to which features that stand out to human observers draw the attention of the birds themselves. The zebra finch (Taeniopygia guttata) is an ideal species in which to study the role of achromatic patterns in estrildids. This species exhibits four notable sexually dichromatic traits, two being related to color and two being related to pattern. These include red beak color, which is more pronounced in males than females, and an orange cheek patch, barred bib, and white flank spots that are only exhibited by males. Although the function of male beak color has been well studied, little attention has been paid to the other three dimorphisms, aside from one study showing that female zebra finches prefer males with more symmetrical barred bibs5. There also is little known about how these dichromatic patterns have evolved, and to what extent these patterns vary in wild populations. Furthermore, zebra finches come from the subfamily Poephilinae6, which exhibit a wide range of chromatic and achromatic patterns, allowing for a tractable phylogenetic comparison of perceptual abilities. I hypothesize that dimorphic achromatic patterns function as signals of quality independent of color, and that species with these patterns are able to perceive and assess variation in this signal better than those without. Aim 1a: Quantify the range of natural variation of zebra finch patterns. Using museum specimens from the University of Michigan’s ornithology collection, I propose to measure the degree of variation of plumage patterns within zebra finches. After photographing the ventral and lateral sides of specimens, I will use ImageJ software to determine the degree of variation in male bar and spot pattern. This is an important analysis because in order for sexual selection to operate, there must be existing variation for it to act upon. After determining the range of this variation, I will categorize the types of plumage variation that may exist among males. Likely, these will include differences in regularity, contrast level, and density of patterns, though other features may also differ. Finally, I will test if certain aspects of these patterns covary with body size, beak color, or other indicators of individual quality. Aim 1b: Determine the extent to which females can discriminate natural pattern variation. Using methods developed to test for categorical color perception in this species (i.e. training birds to flip discs of different colors for a food reward)7-9, I propose to ask how female zebra finches perceive variation in patterns of bars and spots. To do this, I will make discs using paper printed with images of male chest or flank plumage as determined in Aim 1a. I will place these discs on wells in which a food reward is found and train birds to select the pattern that varies the most from the others. Using the range of variation determined in Aim 1a, I will test for the extent to which two varied patterns are indistinguishable to zebra finches. Given that zebra finches court at close range, this approach matches how close females would be to these patterns when assessing males. Additionally, I will test for sex differences in contrast perception to ask whether dichromatic bar/spot patterns could function in female choice, male competition, or both. If dichromatic patterns serve as signals of quality, I predict that female zebra finches will have the visual acuity necessary to discern variation in this signal. Aim 2: Test female preference for pattern quality. I will test for female preference of male pattern variants using digitally manipulated images of male conspecifics. Using a television screen separated by a partition (hereafter “Bird TV”), I will observe how long females spend near one of two videos of male zebra finches as evidence of their mate preference. I will first record videos of male zebra finches and then digitally alter certain aspects of their achromatic patterning that have been positively correlated with body condition and dominance hierarchy in other estrildids: regularity of barred plumage2 and number of white spots10, respectively. Bird TV, a novel method for mate choice experiments, is currently being developed by my advisor, Dr. Steve Nowicki, to test female preference for male beak color, which is a well-supported preference. After this approach is validated, Bird TV will be used in my proposed experiment. I predict that females will prefer the aforementioned pattern variants that have been previously suggested to serve as signals of quality (regularity of barred plumage and large spot size), but have not been explicitly tested. Aim 3: Compare the perceptual abilities of closely related estrildids: The zebra finch is a unique estrildid in that it simultaneously exhibits dimorphic carotenoid coloration, barred plumage, and spotted plumage. As a result, it is possible that zebra finches are adept at both color and contrast perception. Other closely related species, such as the long-tailed finch (Poephila acuticauda) or double-barred finch (Taeniopygia bichenovii), exhibit fewer dimorphic traits. Therefore, I will compare the visual abilities of zebra finches to its closest relatives to see if there are trade-offs associated with color and contrast perception. For example, the double-barred finch does not exhibit carotenoid coloration, but both males and females have bars and spots, and so may have been selected to specialize in perceiving patterns. In contrast, the long-tailed finch exhibits carotenoid coloration in both male and female beaks, but lacks bars or spots, suggesting that it may have been selected to specialize in color perception. I hypothesize that there is a trade-off between color and contrast perception—specifically, that exclusively colorful birds are better at color perception, and that exclusively patterned birds are better at contrast perception. I propose to test the color and contrast perception of the long-tailed finch and double-barred finch using the methods from Aim 1, then test female choice using methods from Aim 2, and compare these results to those of the zebra finch. An inconspicuously colored and more distant relative of the zebra finch, the Bengalese finch (Lonchura striata domestica), was recently shown to discriminate colors differently than zebra finches11. My work will follow up on this investigation to test the effect of phylogenetic relatedness on color and pattern perceptive abilities in Estrildids. Intellectual Merit: In studying the potential signaling function of certain visual traits, it is critically important to first understand how these signals are perceived. If such signals are not being perceived or differentiated at all, then we need to re-evaluate what other function they could possibly serve apart from intraspecific communication. While patterned plumage has been previously proposed to function in intraspecific signaling, no study has actually examined how adept animals are at perceiving variation in these patterns. Therefore, my work will be a necessary first step in determining if patterned plumage should continue to be explored as a signal of quality. Broader Impacts: As an NSF fellow, I will ensure that my research, at every stage, contributes to my personal goal of retaining as many students as possible in the sciences. At Duke, I will invite undergraduates to not only assist in collecting data, but also in contributing intellectually so that they may share authorship on future publications. During the summer, I will train undergraduates through Duke’s paid Biological Sciences Undergraduate Research Fellowship. This program not only introduces students to biological research, but also provides professional development opportunities and a campus-wide showcase to present their research. I benefited greatly from summer research programs like this, particularly the NSF REU, so I know firsthand the impact they can have. I want to help Duke undergraduates feel comfortable in a research environment through close mentorship and reassurance that questions and mistakes are part of the learning process. Beyond Duke, I plan to share my research with scientists and science enthusiasts in the broader “Triangle” (Durham. Raleigh, and Cary, NC) area. The North Carolina Museum of Natural Sciences in nearby Raleigh hosts opportunities for science communication and outreach that I plan to fully engage in. For one, they host the Scientific Research and Education Network (SciREN) Networking Event, which gives an opportunity for scientists to adapt their research into K-12 lesson plans. They also host a Darwin Day event, in which people of all ages are invited to learn about Darwin and his legacy. I will share my findings at these events, and adapt them so that they are appropriate and accessible to all ages and backgrounds. Finally, I will participate in Skype-a-Scientist to connect with students globally. 1Pérez-Rodríguez et al. 2017. Proc. Royal Soc. B. 2Marques et al. 2016. Roy. Soc. Open Sci. 3Soma & Garamszegi 2018. Behav. Ecol. 5Swaddle & Cuthill 1994. Proc. R. Soc. Lond. B. 6Olsson & Alström 2020. Mol. Phylogenet. Evol. 7Caves et al. 2018. Nature 8Zipple et al. 2019. Proc. Royal Soc. B. 9Caves et al. 2020. Behav. Ecol. Sociobiol. 10Crowhurst et al. 2012. Ethology 11Caves et al. (in press) Am. Nat.	Winner!
48	Introduction: Recent advancements in technology have enabled new ways of constructing metamaterials that possess desirable mechanical properties. Materials that have high elastic stiffness and low density are considered some of the strongest, stiffest, lightest materials available today [1]. By controlling their microstructures, we can tune mechanical properties of metamaterials that endure extreme conditions. A trait of a metamaterial microstructure that is much ignored to date is randomness (aperiodicity), owing to the limitation of current design approaches based on unit cells. Aperiodic structures are likely to result from natural self-assembly and self-organization processes and may be more robust against uncertainty. Examples of robust microstructures can be seen in natural formations such as wood and nacre, or in parts of the human body such as bone [2]. Materials that are robust against uncertainty perform well under various forces and stresses they may encounter. Currently, state-of-the-art approaches for designing metamaterial microstructures for desirable properties can be categorized either as parameterized design or topology optimization methods. Both approaches build their foundation on the assumption that material microstructure consists of periodically repeated motifs (or unit cells). Parametrized design is a simpler design method to design structures like lattices; it allows for a few design parameters to map directly to specific properties, but it needs an ad-hoc design to start with. Since the design space is quite narrow, there is a narrow range of achievable material property ranges. Meanwhile, topology optimization is a design method that allows for freeform design of a structure with almost any geometry; it is mathematically well defined, but computationally expensive, leading to unpredictable geometries that are hard to manufacture [3]. Both approaches are difficult to use as tools to efficiently and effectively explore the vast design space of material microstructures. By combining optimal features of each method, this research aims to expand the microstructure design space to maintain local parametric behavior while enabling global freeform design. Through numerical approaches, one can program aperiodic material microstructures towards desirable properties using a “growth”-like process that is encoded by “DNA”-like pairwise combination rules. With this “growth” process, a method for physical self-assembly is desired as it allows for rapid production of programmable metamaterials. The mechanical self-assembly process has been explored in several applications across different scales [4, 5], but none have been able to achieve mechanical self-assembly of an aperiodic microstructure. This design approach allows us to efficiently generate new random yet ordered microstructures. It enables effective exploration of the material design space and pushes the boundaries of applying new stronger materials to applications such as shock absorption and acoustics. Research Plan: I aim to develop self-assembly methods to investigate how the shape and design of cellular automata base cells affect mechanical properties of programmable metamaterials. I will then develop specific tunable metamaterials for applications that desire the particular properties. Numerical, experimental (Fig. 1), and application phases can be achieved with FEA software, 3D printing, and mechanical testing equipment commonly used in any mechanical engineering lab. My previous research experience doing this with square-shaped tiles demonstrates my technical capability. Specific Aim I. Develop Baseline Samples: I will first develop a wave function collapse algorithm [6] for various polygons, such as pentagons and hexagons. The versatile algorithms will be developed in Python for both 2D and 3D self-assembly with flexibility for varying polygons. To do this, I will use a so-called wave function collapse algorithm, which performs a “growth” process similar to cellular automata. We define fundamental building blocks and connectivity rules over a cellular space. Once the first cell is set, connectivity rules are enforced to determine surrounding cell states; this process then repeats in propagating cycles. This will generate 2D and 3D samples for multiple shapes to be transformed into 3D objects in Rhino with Grasshopper C# that will be tested using FEA software. This forms a baseline mapping of the programmable microstructure design space as a success assessment. Specific Aim II. Perform Physical Experiments: From my numerical analysis of these structures, I will 3D print specific samples generated by the algorithms and physically perform the same mechanical tests as in the numerical phase using mechanical testing equipment, such as an Instron machine. The results of these tests can be compared to the numerical results to evaluate their degree of error. Physical 2D and 3D self-assembly methods for multiple shapes will be developed to gain further insight into this self-assembly method. Inspired by the mechanics of DNA self-assembly processes [7], I will design tiles for each shape with their respective channel types dictated by the polygon’s interior angle. This tile design and experimental setup must ensure some randomness and adhere to the algorithm- determined connectivity rules. The self-assembled 3D printed tiles will be used as a carrier casting mold in which to pour a plastic material to generate the metamaterials to be tested. Using the same mechanical testing methods as the previous samples, I will then compile and analyze data for information about the design space and microstructure properties with a focus on how base cell shape and corresponding interior angles affect mechanical properties of the metamaterial microstructures. At this point, a new physical method for developing aperiodic programmable metamaterials will have been created. The design space can be studied by constructing a material database. If the physical self-assembly method is not experimentally reliable, the initial algorithm-based numerical study and mechanical tests still provide a wealth of data to be used in the applications phase. Specific Aim III. Develop Metamaterials for Applications: Based on my findings from the studies performed in the previous phases, I will then study special properties of certain metamaterials that this self-assembly method yielded, such as shock absorbency and acoustic capacity. My experimental results will yield a desired mechanical property by tuning the specific base cell shape and quantity of channel type. With these settings, the physical self-assembly method can be used to create an array of samples used for additional testing for specific properties, such as impact testing in the shock absorbency case. These studies will demonstrate how the metamaterials yielded from the developed self-assembly method can be applied as lighter, stronger, and more flexible alternatives to materials currently used in aerospace and medicine. Intellectual Merit: This project will develop a mechanical self-assembly method of aperiodic programmable metamaterials, contributing new 2D and 3D self-assembly methods to metamaterials and mechanics research; this will improve the fundamental understanding of material microstructures and their properties. This project will also enhance understanding of the mechanics of self-assembly such as attractive forces and interlocking as seen in DNA self-assembly [7]. Working on this project at a research institution with a strong mechanical engineering program will provide the proper resources to research and publicize my findings related to metamaterials, bio-inspired processes, and their applications to medical and aerospace fields. Broader Impacts: This project will contribute new 2D and 3D self-assembly methods to metamaterials and mechanics research while developing new aperiodic programmable metamaterials with large degrees of tunability. Scale-independent metamaterials will be created that can be applied to aerospace materials, soft materials, and medical devices for its capabilities of enhanced shock absorbency, acoustic properties, elasticity, and strength. Additionally, through its biologically linked process of self-assembly, the microstructures developed have the potential to be applied to sustainable, environmentally friendly structures and devices. This project also has a parallel educational impact to introduce high school and undergraduate students to numerical and experimental methods in mechanical engineering and STEM. I plan to mentor students in researching metamaterials. [1] J. B. Berger et al., “Mechanical metamaterials,” pp. 533–537. [2] H. Wagner et al., “Bone microstructure,” pp. 1311–1320. [3] O. Sigmund et. al, “Topology optimization,” pp. 1031–1055. [4] E. Klavins, “Programmable Self-Assembly,” pp. 43–56. [5] G. M. Whitesides, “Self-Assembly,” pp. 2418– 2421. [6] Heaton, R. (2018). Wavefunction Collapse Algorithm. [7] S.-S. Jester et al., “DNA nanostructures,” pp. 1700–1709.	Winner!
49	Americans spend approximately 90% of their time indoors1, and by 2050 over two-thirds of the global population will live in urban environments2. Studies have shown that human exposure to pollutants indoors is orders of magnitude higher than the exposure experienced outdoors3. Focusing air quality research on the places occupied by the most people and where pollutant exposure is highest is important in preventing negative health outcomes. It is paramount to understand the chemical, physical, and societal processes of the interface between urban and indoor air quality. Volatile organic compounds (VOCs) are emitted into the atmosphere from both anthropogenic and biogenic sources, including industrial and transportation activity, biomass burning, and vegetation. Indoor VOCs accumulate both from outdoor sources permeating into indoor spaces and from indoor activities such as cooking, cleaning, and off-gassing of furniture. Besides these gaseous VOCs, other semi-volatile organic compounds (SVOCs) exist both in the gaseous and condensed phase due to their lower vapor pressure and higher boiling points. When VOCs and SVOCs react with sunlight and oxidants such as ozone and nitrogen oxides (NO ), secondary organic aerosols (SOA) are produced, particles with x known hazards to human health. While there have been mobile, real-time measurements of air pollutants such as PM 4 (particles 2.5 less than 2.5 microns in diameter), there have been limitations to similar VOC measurement campaigns: the measurement’s location was stationary5, the hour-long sampling time made it difficult to locate pollution “hot spots”6, or the instruments used could not measure compounds made possible by current technology7,8. Prior to recent advances in VOC mass spectrometry, it had been difficult to measure VOCs and SVOCs at (a) sensitivities that allow identification of compounds and measurement of accurate concentrations and (b) mobile, real-time temporal scales to link concentrations to specific sources. Recent technological advances have made it possible to overcome such limitations. The newest generation proton transfer reaction time-of-flight mass spectrometer, the Vocus 2R PTR-ToF-MS from Aerodyne, Inc., has world-leading real-time mass resolution and sensitivity. The new mass spectrometer measures concentrations of more than 1600 chemical compounds every second, including many high- molecular-weight molecules considered semi-volatile. It can detect concentrations at part per trillion levels with 1-second measurements and even part per quadrillion scales at 1-minute averaging for certain compounds, allowing us to explore the frontier of trace – and very toxic – air pollutants. Using the most advanced mass spectrometer for this project will lead to the reporting of both SVOC molecules and trace VOCs that past mass spectrometers did not have the sensitivity to detect. The new Vocus Inlet for Aerosol (VIA, Aerodyne, Inc.) gives us the novel ability to measure the time-resolved chemical composition of SOA and other particles found indoors and outdoors, which is important for quantifying human exposures. Research Objective As the first study to investigate the interface in VOC and SOA concentration between urban outdoor and indoor environments using world-leading measurement capabilities, this project aims to: (1) Quantify concentrations and human exposure to air pollutants that have been previously unidentifiable due to technological limitations. (2) Identify and apportion the major sources of VOCs and SOA found indoors and in urban areas. Task 1: In controlled experiments, we will characterize indoor concentrations of VOCs and SVOCs such as benzenoids, siloxanes, and hydrocarbons. In UT-Austin’s environmental chambers and UTest House – a full-scale house on UT-Austin’s research campus outfitted with an array of sensors – experiments will simulate common household events such as cooking and cleaning; VOC, SVOC, and particle concentrations will be measured. To supplement our work, we will also use past datasets of the HOMEChem measurement campaign, which used the UTest House to characterize typical household activities9. Due to the increased sensitivity and precision of our novel mass spectrometer, we hypothesize that we will make novel characterizations of SVOCs that previous instruments were insensitive to and that past research campaigns did not report. Task 2: Using citizen science pathways already established from past research campaigns10, we will recruit 20 volunteer homes of diverse economic and geographical locations around Austin, Texas to take 2 air samples of ambient air twice per day for 2 weeks, one inside their home and one directly outside of it. The air samples will be taken off-site for evaluation by our Vocus mass spectrometer to find VOC and SVOC concentrations across both temporal and spatial scales in urban and suburban environments. We expect that VOC concentrations will vary widely from home to home due to individual differences in ventilation, personal care product usage, and cooking routines; however, we hypothesize that homes within a neighborhood will have similarities due to proximity to vehicle and industrial emissions. Task 3: Concurrently with the citizen science measurement, we will drive the Vocus mass spectrometer around Austin to measure VOC concentrations found on Austin’s roads at 1 Hz time scales. Pairing the VOC data with GPS data will be vital in finding point sources and pollution “hotspots”. Every other day we will measure with and without the VIA aerosol inlet, collecting data on both the gaseous concentrations and chemical composition of particles. We hypothesize that many pollution “hotspots” will be from sources usually overlooked as key polluters. Due to the complex chemical and physical processes that impact air quality, it is important to measure other air pollutants, not just VOCs and SVOCs. We will also measure mobile concentrations of ozone, NO , and PM – all interdependent on x 2.5 the presence of VOCs – as well as meteorological data such as wind speed, temperature, and humidity. To add to our data set, we will use the Texas Commission on Environmental Quality’s (TCEQ) 6 Austin-area monitoring stations as supporting data for wind speed, temperature, PM , ozone, NO , and sulfur oxides. 2.5 x Task 4: Combining the mobile air pollutant data with citizen science air samples and environmental parameters will produce a rich dataset across temporal and spatial scales. Using source apportionment and multivariate analysis methods – for example, positive matrix factorization (PMF) and principal component analysis (PCA) – we will identify major sources of urban and indoor VOCs. We hypothesize that our analysis will reveal unknown, as well as verify known emissions from indoor sources – such as cleaning or cooking – but will also quantify indoor exposure to pollutants penetrated from outdoors. Broader Impact Due to the breadth of samples we will collect during Tasks 2 and 3, I will recruit at least 2 undergraduate students from both UT-Austin and Huston-Tillotson University (an H.B.C.U. located in East Austin) to help with the collection of the citizen air samples. To introduce them to air quality research, I will train them in off-line VOC analysis and data processing. By conducting this campaign in actual homes and roads in addition to a laboratory setting, our findings can be used quickly for recommendations in homes and residential developments in Austin and other cities. By utilizing citizen science, this research will increase public awareness of the pollutants that residents are inhaling. The papers published from this campaign will be used as recommendations for urban planners, environmental regulators, and, perhaps most importantly, the general public. By measuring around Austin, we will also report on the accuracy of the TCEQ stations and how each station’s neighborhood-scale measurements differ from precise ground-level measurements. Intellectual Merit While several studies have measured air quality across urban areas, this will be the first study to do so by measuring a wide range of VOCs (with atomic mass units from 30 to 500) in real-time using new mass spectrometry technology. It will also be the first campaign to use such a wide-ranging dataset – the controlled studies in the UTest House, the citizen science household air samples, TCEQ station measurements, and the mobile dataset – to better understand the urban and indoor air quality system. By focusing on air pollutants that are not well-understood due to past technological limitations, we will also report on exposures that have been overlooked. Works Cited [1] Klepeis et al. (2001), J Expo Anal Environ Epidem., 11(3), 231-252; [2] UN World Urban. Proj.: The 2018 Revision (2018), pg. 10; [3] Nazaroff (2008), Build. and Env., 43(3), 269-277; [4] Apte et al. (2017) Environ. Sci. Tech., 51(12), 6999–7008; [5] Deng et al. (2018), Aero. Air Qual. Research, 18, 3025-3034; [6] Zheng et al. (2020), Sci. Total Environ, 703, #135505; [7] Maji, Beig and Yadav (2020), Environ. Pollution, 258, #113651; [8] Crippa et al. (2013), Atmos. Chem. Phys., 13, 8411–8426; [9] Farmer et al. (2019) Environ. Sci: Proc. Imp., 21, 1280-1300; [10] Bi et al. (2018), Environ. Inter., 121(1), 916-930	Winner!
51	Tour Orbiter to the Ice Giant Planets Introduction and Background The ice giant planets, Uranus and Neptune, have only been observed directly during flybys of the Voyager 2 probe in 1986 and 1989. As it stands, the ice giants are two of the most under explored objects in our solar system, and raise many of the most important questions about planetary and solar system evolution. In order to complete a thorough survey of the outer planets, the Ice Giants Pre-Decadal Survey (IGPDS) decided on two areas of interest to pursue in the first flagship missions: the atmospheric composition of the ice giants, including the tropospheric 3-D flow, heat balance, and meteorology, and the composition, structure, and evolution of the ice giant satellites1. To accomplish both objectives, a flagship missionto either system would require a complex trajectory that takes into account both an atmospheric entry probe and a satellite tour of the system. Due to the lengthy flight time involved in such a mission, it would be beneficial to consider a pair of twin atmospheric entry probes in the interest of ensuring that the scientific objectives are met. Two atmospheric probes would allow for a greater spatial resolution as well as a second sampling of the atmosphere, which decreases the chances of the atmospheric probe landing in an unrepresentative region, as the Jovian Galileo atmospheric entry probe experienced in 19952. A flagship mission with twin atmospheric entry probes and a satellite tour brings forth a complex trajectory design problem when factoring in the vehicle weights, atmospheric entry locations, and launch windows of each planet. Proposal To further investigate the feasibility of a flagship mission to either Uranus or Neptune including two atmospheric entry probes and a satellite tour, I propose using two NASA trajectory design and optimization softwares, the Copernicus Trajectory Design and Optimization System and the Program to Optimize Simulated Trajectories II (POST2), in order to systematically test combinations of spacecraft weights, atmospheric probe weights, and separate atmospheric probe latitudinal/longitudinal entrances alongside traditional ballistic (chemical) trajectories and solar electric propulsion (SEP) trajectories. Methods In order to design and evaluate the various mission combinations with end-to-end optimization, I will utilize POST2 alongside the Copernicus software. Copernicus serves as the primary trajectory optimization tool for mission design at NASA, and as such, has many degrees of customizability in terms of low and high thrust trajectories.3This will allowfor the testing of various combinations of SEP in the inner solar system flight with a later transition to chemical propulsion. POST2 gives the ability to introduce multiple vehicles at any point in the simulation- these “child” vehicles inherit the state of their “parent” vehicle, and will be vital in further analyzing the trajectories of the two atmospheric entry probes once they begin separation from the parent spacecraft.4I plan to utilize the Copernicus API in order to rapidly test various configurations for the launch stage up to the entry probe separation stage of the mission, including the transition from SEP to chemical propulsion. I will then push this output into the POST2 software, which will complete the trajectory design by simulating the separation of the atmospheric entry probes, the atmospheric entrances, and the satellite tour of the remaining orbiter. I plan 1Ice Giants Pre-Decadal Survey Mission Study Report.,2017. (JPLD-100520) https://www.lpi.usra.edu/icegiants/mission_study/ 2Irwin, P. G. J.,2009. Giant Planets of Our Solar System. Giant Planets of Our Solar System: Atmospheres, Composition, and Structure, Springer Praxis Books. ISBN 978-3-540-85157-8. Springer Berlin Heidelberg, 2009. 3Williams, J. et al. “Overview and Software Architecture of the Copernicus Trajectory Design and Optimization System.” (2010). 4NASA Langley Research Center, “Overview of the Program to Optimize Simulated Trajectories II (POST2)”, https://post2.larc.nasa.gov/overview/ to write the data interchange software in either Python or Julia, a new programming software used for trajectory design, depending on the requirements of the two trajectory design softwares. The outputs of these combinations can then be compared in terms of the fastest route, the most cost-efficient route, and the lightest route. The combinations explored will be constrained by the following scientific considerations outlined in the IGPDS report5. Uranus The flight length and severe axial tilt of the Uranus brings forth many constraints to the mission: A combination of SEP and chemical trajectory could deliver a basic probe and orbiter combination to Uranus in ~11 years of interplanetary time, when considering an average vehicle, such as the Atlas V 551. Due to the axial tilt, the planet’s seasons last ~21 years. In order to observe a different season than that of Voyager 2’s observations, we must launch before the end of the 2030 decadal window so that the mission is completed before the 2049 equinox. Due to the alignment of the planets during this window, a gas giant flyby and/or gravity assist can only be considered with Jupiter. As such, the best gravity-assist sequence for this window is Venus-Earth-Earth-Jupiter (VEEJ). Neptune The flight length of a Neptune flagship mission would require covering a much larger inter-planetary distance than that of a Uranus flagship mission, and would in turn require a stronger launch vehicle. The Delta-IV Heavy or the SLS Block 1-B would lend itself better to a Neptunian system mission than the Atlas V 551, and would allow the spacecraft to complete only an Earth-Jupiter (EJ) gravity assist in order to reach Neptune. Neptune’s largest satellite, Triton, raises many questions about satellite evolution due to its retrograde orbit, and thus it may also be beneficial to explore multiple flybys of this moon during the satellite tour. Intellectual Merit Though the Uranian and Neptunian systems were brieflyanalyzed in the Voyager 2 flybys, there have been no further missions to these systems. Furthermore, the only scientific data regarding these systems are from remote telescopic observations and the limited Voyager 2 observations.6As the IGPDS report stressed the importance of a flagship mission with an optimal launch window in the 2030 decade, we must prepare a mission as soon as possible. Due to the extreme inter-planetary distances, we must optimize the mission to ensure that all scientific goals are met in both a timely and efficient manner. The inclusion of twin atmospheric entry probes would ensure that the in-situ atmospheric readings are precise and representative of the planet, while the satellite tour would allow for further exploration of the system and its evolutionary path. This proposed study will explore the trajectory design and optimization of a mission with twin atmospheric entry probes and a satellite tour, ensuring that the decades of preparation and execution involved in an ice giants mission will be as fruitful as possible. Broader Impacts A flagship mission to the ice giants would constitute a new age in space exploration, particularly if the mission included a set of twin atmospheric entry probes. As a twin probe setup has never been executed, this would revolutionize the future of in-situ atmospheric measurements, while also providing precise results for a relatively unknown part of our solar system. The trajectory considerations of a satellite tour could also result in the discovery of new satellites, as well as previously unknown chemical signatures and geographical landscapes. This study would allow for the exploration of a brand-new spacecraft and probe configuration as well as the potential for a flagship mission to one of the most under-explored areas of our solar system, allowing for technological and scientific research for decades to come. 5Ice Giants Pre-Decadal Survey Mission Study Report.,2017. (JPLD-100520)https://www.lpi.usra.edu/icegiants/mission_study/ 6Irwin, P. G. J.,2009. Giant Planets of Our SolarSystem. Giant Planets of Our Solar System: Atmospheres, Composition, and Structure, Springer Praxis Books. ISBN 978-3-540-85157-8. Springer Berlin Heidelberg, 2009.	Winner!
52	BACKGROUND RNA modifications, also known as epitranscriptomics, are emerging as a novel layer of dynamic gene regulation [1]. RNA modifications alter existing RNAs’ structure and function to influence various cellular pathways via RNA processes such as transcription and translation [2, 3]. Pseudouridine (Ψ) was the first RNA modification discovered [1]. Despite being the most abundant and widespread RNA modification in living organisms, little is known about its function. In this proposal, I will establish an approach to systematically investigate the biological roles of pseudouridine by focusing on the unique activities that this modification imparts to RNA. The isomerization of uridine to pseudouridine by pseudouridine synthases (PUS enzymes) structurally stabilizes RNAs via the formation of an extra hydrogen bond donor [3]. Ψ was previously thought to primarily stabilize tRNA and rRNA; however, new developments in modern-sequencing techniques reveal the more interesting downstream effects of pseudouridylation. For example, there is evidence that H/ACA RNPs, RNA-dependent PUS enzymes convert stop codons into sense codons in yeast [2]. Interestingly, PUS7-mediated pseudouridylation has been found to regulate stem cell growth and fate determination partly through tRNA modification, which activates tRNA-derived fragments to inhibit protein synthesis [3]. This provides evidence that Ψ modifications may be critical in cell lineage commitment. Specifically, mutations associated with several pseudouridylating enzymes, namely PUS1, PUS3, and PUS7, are associated with neuronal disorders and intellectual disability [4]. PUS1 mutations are associated with cognitive impairment [5]. Additionally, PUS1 acts on the steroid RNA activator, a co- activator of the nuclear estrogen receptor α regulating neuronal survival [5]. Truncated PUS3 and reduced levels of ψ U39 in tRNA were detected in patients with intellectual disability [5]. Lastly, mutations in PUS7 can cause intellectual disability and microcephaly in humans [4]. These studies suggest that Ψ can substantially impact neuronal differentiation and function; however, the mechanisms regarding pseudouridylation in neurogenesis are poorly understood. My results will contribute to our understanding of the importance of pseudouridylation in neurogenesis and neuron function. PROPOSED RESEARCH My overarching goal is to elucidate the molecular and functional roles of Ψ in neuronal cells. Based on the reported links to human brain function, I hypothesize that Ψ is a critical modification for neuronal differentiation and function. I further propose that the Ψ landscape between stem cells and neurons is unique and specific. I will test the above hypotheses with the following aims: Aim 1—To identify pseudouridylating enzymes for investigation. There are 13 known PUS enzymes in human cells; however, only a few predicted human PUS enzymes have been studied to date. In addition to PUS1, PUS3, and PUS7, I will predict other PUS enzymes associated with neuron function by first performing weighted gene co-expression network analysis (WGCNA). This analysis will reveal sets, or modules, of highly correlated genes. To perform WGCNA, I will use multiple human tissue RNA sequencing datasets from the GTEx project. I expect PUS1, PUS3, and PUS7 to appear in one module because these genes share connections to neuronal function. Other PUS enzymes that are expected to correlate with neuronal genes will fall in this module. I will then conduct gene ontology enrichment analysis to verify that the PUS candidates in that module are associated with neuron processes. These experiments will predict key pseudouridylating enzymes in addition to PUS1, PUS3, and PUS7 that will be investigated in neurons. Aim 2—To investigate the effects of PUS enzymes on neurogenesis. To define the impact of PUS enzymes on early neurogenesis, I will use CRISPR/Cas9 to knock out each candidate gene identified in aim 1 in iNGN cells. iNGN cells are a human induced pluripotent stem cell line which has been engineered to be readily induced into neurons within four days by doxycycline [6]. I will design a gRNA that targets the N-terminal coding exon of each gene to induce nonsense-mediated mRNA decay and validate that the enzyme is no longer expressed via Western Blot. I will explore two different methods of inducing edited-iNGN cells into neurons to gather more comprehensive results. For the first method, I will supplement cell media with doxycycline to induce the formation of neurons. Since this procedure only takes four days, it will allow for efficient generation of easily reproducible data. However, the rapid induction of robust neuronal morphology by doxycycline may limit the resolution of detectable phenotypic changes in these cells. Thus, the second, slower method enables me to mark differences at each stage during differentiation. I will differentiate successfully edited colonies of iNGN cells using a slow differentiation method following a previously published protocol [7]. I will use an inducible Cas9 system to knock out the PUS enzyme at different time points to determine the most critical points of pseudouridylation during neurogenesis [8]. To detect potential morphological alterations, I will use a neurite outgrowth assay and monitor the expression of neuronal markers via immunofluorescence. Rescue of the phenotypes by ectopic expression of the wild- type protein will control for the specificity of the effects. I will select the cell lines with the strongest phenotypes for additional study in aim 3. These studies will reveal how PUS enzymes impact neuronal maturation. Aim 3—To determine neuron-specific RNA targeting by PUS enzymes. To determine the positions of RNA binding by PUS enzymes at single-nucleotide resolution, I will perform UV cross-linking and immunoprecipitation, followed by high-throughput sequencing (iCLIP-seq) in iNGN-derived neurons expressing Flag-tagged PUS enzymes. I will corroborate these results by using specific antibodies to pull down endogenous PUS complexes. Mock-infected cells will be used as a control to exclude non-specific RNA binding. I will complement these results with Ψ-seq to confirm that the PUS-bound sites are catalyzed. A comparison of the sites bound in stem cells and neurons will reveal neuron-specific Ψ sites. Because PUS enzymes may have multiple substrates, it may be unclear how to assign mutant phenotypes to loss of modification in specific RNA species. I will control for this variable via rescue experiments by transfecting specific synthetic pseudouridylated RNA substrates identified by Ψ-seq experiments. These experiments will reveal the Ψ landscape in neurons and identify RNA targets for future study. Summary: Successful completion of these aims will shed new light on the poorly understood but critical roles that pseudouridylation plays in neuronal development. A potential follow-up study to investigate the function of Ψ in the identified RNA targets is an RNA pull-down assay. To determine the role of Ψ on the complex composition of RNA species, Ψ and non-Ψ RNA probes tagged with biotin can be pulled down, and the interactome can be analyzed by mass spectrometry. Future research into the biological roles of RNA modifications will reveal novel causes of neurological disorders. Intellectual Merit: In Dr. Murn’s lab, I’ve gained the necessary training in molecular biology techniques and RNA biochemistry to carry out this project. I am currently optimizing pseudouridine-seq and will use my experience in this technique for this proposal. I will receive the bioinformatic training necessary to carry out my proposal through future mentoring from Dr. Chaolin Zhang. Dr. Zhang’s lab at Columbia University is an ideal fit because of his focus on RNA-protein interactions, RNA regulatory networks in neural development, and expertise in high-throughput transcriptomic data analysis. Broader Impacts: As a graduate student and later as a professor, I will mentor undergraduate women of color and encourage them to pursue research careers. I will continue to empower young high school women by establishing additional chapters of Queens of STEAM. The support of the NSF GRFP will enable me to carry out my research while continuing STEM outreach. [1] I. A. Roundtree, M. E. Evans, T. Pan, and C. He, “Dynamic RNA Modifications in Gene Expression Regulation,” Cell. 2017. [2] J. Karijolich and Y. T. Yu, “Converting nonsense codons into sense codons by targeted pseudouridylation,” Nature, 2011. [3] N. Guzzi et al., “Pseudouridylation of tRNA-Derived Fragments Steers Translational Control in Stem Cells,” Cell, 2018. [4] H. Darvish et al., “A novel PUS7 mutation causes intellectual disability with autistic and aggressive behaviors,” Neurology: Genetics. 2019. [5] M. T. Angelova et al., “The emerging field of epitranscriptomics in neurodevelopmental and neuronal disorders,” Frontiers in Bioengineering and Biotechnology. 201. [6] V. Busskamp et al., “Rapid neurogenesis through transcriptional activation in human stem cells.,” Mol. Syst. Biol., Nov. 2014. [7] Y. Shi, P. Kirwan, and F. J. Livesey, “Directed differentiation of human pluripotent stem cells to cerebral cortex neurons and neural networks,” Nat. Protoc., Oct. 2012. [8] K. I. Liu et al., “A chemical-inducible CRISPR-Cas9 system for rapid control of genome editing,” Nat. Chem. Biol., 2016.	Winner!
53	topologically relevant materials. Successful execution of this approach will lead to new materials discovery and generate new methods for electronic band structure engineering. Introduction: Linking desired physical properties to structural motifs is a fundamental goal in solid-state chemistry. As we will see, topological semimetals are exemplars to this goal: ultrahigh mobility electrons arising from linear band crossings can be derived from specific structural motifs, further predicted from basic electron counting rules1. Previous work regarding topological materials has largely focused on systematically scanning the thermodynamic stability of compositions in particular space groups, followed by selected synthesis of the most promising candidates. More recently, scientists have explored how slight perturbations of topological systems can induce charge density waves through structural modulations rationalized via electronic considerations.2 That is, seemingly simple chemical substitutions used as n- or p-type dopants, aimed to adjust the Fermi energy to lie in a precise electron state. Changing the composition of a structure may induce a structural change, such as a Peierls distortion, which would open a up bandgap at the Fermi energy. Further, exactly how a given structural transition navigates its potential energy surface to form such specific distortions is largely unexplored in the literature. Finally, even if such chemical substitution does not significantly alter a structure, development of predictive strategies for diversifying the possibilities of elemental substitution should be developed. Exploring synthetic control over such distortions in topological materials can provide one such prescription to these issues. Thus, understanding how such distortions lead to thermodynamic favorability, paired with how such distortions alter physical properties, may then open a playbook for “band engineering” where finely tuning composition can gap out unwanted bands from the Fermi surface. Background: Topological materials can be well understood through the Zintl-Klemm concept: an electron counting method where transfer of electrons is assumed from the most electropositive element to the most electronegative element in a crystal structure. The remaining atoms then form covalent networks to reduce the total thermodynamic energy of the system. In Figure 1a, a Walsh diagram shows the thermodynamic stability of a Te chain as a function of bond angle. The linear geometry is found to be stabilized at 22 3 electrons, the third level, due to less overlap of anti-bonding interactions. In extended solids, it would then be predicted that linear chains form when there are 7 electrons per chain atom. Such is the case in UTe , 2 seen in Figure 1b, a promising candidate host for the sought-after Majorana quasi-particle.1 Each uranium is found to be in the 3+ oxidation state and transfers two electrons to the nearest tellurium sites, forming what can be thought of as [UTe]+ slabs and a linear Te- chain. As a result, the geometry enforces a linear crossing at the Fermi energy, shown in Figure 1c. Doubling of the unit cell, when considering 2 atoms per unit cell, folds the band structure back on itself creating a linear band crossing located at the Fermi energy. Figure 1. a) A Walsh diagram of a Te molecular unit. b) The unit cell of UTe , a topological 3 2 superconductor containing linear chains. c) The resulting band structure for an isolated linear chain of Te- ions. A linear crossing at the Fermi energy occurs halfway between the Γ and Ζ points. d) A folded band structure, arrived at by doubling the unit cell, forming a Dirac node at the Fermi energy. Proposal: Derivation from ideal electron counts in such covalent networks can lead to a structural distortion. In the classical example of a linear chain of atomic orbitals, a half-filled band will favor dimerization, leading to differing bond lengths, opening a band gap at the Fermi energy. However, recent investigations have found, counterintuitively, that such distortions can lead to improved topological band structures or high-mobility electrons. Despite a structural modulation in the square-net layer, which can be viewed as a two-dimensional linear chain, NdTe exhibits ultra-high mobility electrons and anomalous 3 quantum oscillation behavior1. Moreover, the distorted square nets in GdSb Te was recently shown to 0.46 1.48 gap out trivial band crossings, “cleaning” the band structure by retaining the screw axis associated with its structural distortion.2 For my analysis of similar systems, I will utilize recent adaptations of Density Functional Theory (DFT) outputs that establish direct links between local features in solid state compounds and their contribution to electronic and steric favorability. Specifically, the reversed applied Molecular Orbital (raMO) analysis aims to fit tight-binding parameters from DFT band structure calculations. As a result, DFT-calibrated molecular orbital diagrams are visualized to explain formation of closed-shell configurations (see Figures 1a) and c)). Parallel with this, DFT-Chemical Pressure (DFT-CP) resolves local packing frustrations that can arise within dense atomic packings, from which the role of atomic size can be assessed. Utilization of both these computational analyses can explain why a structure may obey or derivate from predictive bonding schemes such as the Zintl–Klemm concept.3 Both raMO and DFT-CP software packages are freely available via the GNU Public License and will be used in this work. I will also synthesize and structurally as well as electronically characterize the targeted compounds. Therefore, I will employ an iterative approach of experiment and theory to implement a frame of understanding structural distortions in topological motifs, targeting the following research objectives: 1. Determination of topological systems in which unexpected electron counts or steric packing frustrations may favor charge density wave formation. 2. Synthesis of candidates, as well as detailed structural and physical characterization of materials. 3. Demonstration of band engineering through the tuning of new superstructures guided by theoretical analysis. Intellectual Merit: Princeton University offers many unique multidisciplinary approaches needed to study topological materials. In the Schoop laboratory, I have access to core instruments needed for this investigation, including furnaces for solid-state synthesis, a single-crystal X-ray diffractometer for materials characterization, a magnetic properties measurement system, and a physical property measurement system with a dilution fridge. Through NSF supported opportunities, such as the Princeton Center for Complex Materials (PCCM), frequent collaborations will be drawn through an interdisciplinary research group focusing on topological quantum matter. Specifically, the Department of Physics offers many avenues for collaborative work in physical properties characterization with Dr. Nai Phuan Ong, Dr. Ali Yazdani, and Dr. Sanfeng Wu. There are also many opportunities through the NSF-funded Imaging and Analysis Center which provides access to instruments such as the scanning electron microscope. Frequent collaborations outside of Princeton University will also be utilized. So far, single-crystal diffraction data obtained for a linear chain system indicate missed satellite peaks with a modulation vector q = 0.06a*, indicating a massive 50/3 supercell periodicity. Despite prediction of such a distortion ruining the metallicity of the structure, electrical transport data suggests that the system remains metallic. To this end, a proposal has been written and sent to Dr. Yusheng Chen at Argonne National Laboratory, hopefully to be accepted for next cycle in March of 2022. Additionally, the Schoop laboratory frequently collaborates with scientists at Helmholtz-Zentrum Berlin such as Dr. Andrei Varykhalov at the BESSY II beamline for angle-resolved photoemission spectroscopy. Looking ahead, probing the band structure of a crystal related to this project may significantly improve the impact of my investigations. Broader Impacts: The predictive framework I develop will open paths to tailoring band structure through composition, allowing for more control of physical properties used in technological applications. In the future I will expand to other structural motifs such as the square- and Kagome-nets. Finally, visualization tools, like raMO, have chemical education implications for use in NSF-funded operations, such as PCCM’s Princeton University Materials Academy. References: 1. J.F. Khoury and L.M. Schoop. 10.1016/j.trechm.2021.04.011 Trends in Chemistry, (2021). 2. Lei S.; Theicher, S.M.L.; Topp, A. et al. 10.1002/adma.202101591 Adv. Mater. (2021). 3. Warden H.E.M.; Lee S.B.; Fredrickson D.C. 10.1021/acs.inorgchem.0c01347 Inorg. Chem. (2020).	Winner!
55	fluorescence from satellites Across the globe, the terrestrial biosphere is responding to growing climate extremes, including more frequent heatwaves, droughts, and high-impact weather events1. For North American forests, this means increasing physiological stress on one of the continent’s most important carbon sinks, along with the vast quantity of biodiversity that these ecosystems support. A fundamental way to understand and potentially mitigate the ecological impacts of extreme heat and drought events is by tracking carbon uptake through photosynthesis (i.e., gross primary productivity, GPP), across seasons at the forest-scale2. However, large- scale monitoring of GPP is challenging when considering the highly dynamic and remote nature of mountain biomes, which make up a substantial portion of North American biomass3. Montane landscapes exhibit vast gaps in spatial coverage of surface-level GPP measurements, along with complex topography that makes land surface models and atmospheric tracer approaches prone to significant uncertainty4. Thus, remotely-sensed data from space present a promising tool to fill in these spatial gaps to better understand forests’ response to stress and the implications for the terrestrial carbon cycle. Traditionally, forest-level GPP has been derived from satellites using reflectance-based indices that quantify the “greenness” of a land surface. However, the temperate and boreal forests that comprise many North American mountain biomes consist mainly of evergreen conifer trees which retain their needles, and therefore, greenness, even in photosynthetically dormant seasons (e.g., drought or winter). Thus, studies that use reflectance-based indices as metrics of conifer GPP face significant challenges in capturing seasonal to decadal changes of photosynthetic activity5,6. In contrast, solar-induced chlorophyll fluorescence (SIF), which is emitted by chlorophyll pigments as a byproduct of the photosynthetic process and can be measured via satellite instruments, has been shown to closely follow the seasonal cycle of photosynthetic production in evergreen forests7. The combination of newly available remotely-sensed high- resolution SIF data, in conjunction with measures of complex terrain characteristics (e.g., slope angle, aspect and elevation), represents a unique opportunity for understanding GPP over mountain biomes. In my proposed work, I will analyze GPP derived over the Sierra Nevada mountain range in California using ground- based flux tower data, biogeochemical models, and remotely-sensed SIF and reflectance-based data in order to test the hypotheses described below and in Figure 1. Hypothesis 1 (H1) - SIF is an improved way to measure GPP over montane conifer ecosystems compared to traditional reflectance-based remote sensing indices. High-resolution SIF from satellites provides extensive spatial data coverage, but satellite- Fig. 1: Satellite-based SIF will be analyzed against based SIF has yet to be analyzed for fine-scale reflectance-based indices and modeled GPP in mountains spatial and elevation gradients in complex to assess drought-induced stress. terrain. To test H1, I will analyze SIF data from the TROPOMI and OCO-2/3 satellite instruments over the Sierra Nevada range, comparing to GPP measured at eddy-covariance flux towers from the NSF-funded National Ecological Observatory Network (NEON) and Southern Sierra Critical Zone Observatory sites in the Sierra Nevada range. I will then compare SIF and traditional reflectance-based indices (NDVI, EVI, CCI) to quantify differences in their ability to predict seasonal and interannual GPP as a function of elevation and terrain characteristics. Hypothesis 2 (H2) - Characterization of mountain conifer forests response to drought can be improved with the aid of remotely-sensed SIF. The Sierra Nevada range has consistently experienced extreme drought conditions over the past decade; these forests’ productivity in response to drought can be estimated using biogeochemical models, but does the spatial resolution of a model limit its ability to resolve such dynamic processes over complex terrain? Based on insights from H1 and using SIF as a means of constraining modeled GPP, I will test H2 by comparing remotely-sensed SIF to GPP modeled using the Community Land Model version 5 (CLM5) over the Sierra Nevada region for timeframes with available high-resolution SIF data, detecting mismatch between satellite- and model-derived GPP over seasonal and interannual cycles during observed drought periods. Hypothesis 3 (H3) - SIF can provide information towards early warning capabilities for forest health in response to drought conditions. Assuming a direct linkage between forest productivity and physiological stress, remotely-sensed measures of GPP could act as highly localized indicators of forest health in drought- stricken regions. Using information gleaned from H1 and H2, I will analyze high-resolution SIF data with recent records of forest drought disturbances to quantify trends in SIF as they are correlated with drought events. Through this analysis I will uncover statistical relationships between SIF and drought stress in the context of variables such as elevation, aspect, and snow cover to determine the extent to which SIF can act as an early warning system for forest health over land-use management scales. Collaborations and Computing Resources: To complete this work, I will build on an existing NSF- funded collaboration between the University of Utah and researchers at California Institute of Technology (led by Prof. Christian Frankenberg) to access high-resolution, pre-processed topography-corrected SIF data products over the Sierra Nevada range. I will also work with the University of California–Irvine Innovation Center for Advancing Ecosystem Climate Solutions to engage regional Sierra Nevada stakeholders in scientific discussion. To accommodate the computational needs associated with my work, I will utilize dedicated group-access nodes (purchased by advisor Prof. John Lin) on two supercomputers maintained by the University of Utah’s Center for High-Performance Computing. Intellectual Merit – Global Carbon Budget: Remotely-sensed SIF has the potential to track forest productivity over global scales and is a promising tool for rectifying uncertainties in the carbon budget of mountains. My work will be among the first to examine high resolution SIF over complex terrain in order to address uncertainties in forest drought response. As heat and drought stress grow increasingly prevalent due to climate change, understanding the highly dynamic response of montane carbon stocks will be critical for improving terrestrial biosphere models that inform global climate policy. Forest Management: High resolution SIF can help diagnose features of forest wellbeing and tree mortality at scales meaningful to land use management. Through assessment of H3, I will examine how SIF can be used to provide early warning capabilities for near-term forest disturbances and allow land managers to adapt to rapid changes in forest health from critical stress events. These capabilities ultimately aid in emergency preparedness capabilities to mitigate damage from wildfires and bark beetle die-off. Broader Impacts – Stakeholder Engagement: The U.S.D.A. Forest Service Region 5 and Sierra Nevada Conservancy have a vested interest in central California’s forest health and wildfire management, motivated by public protection, forest resource care, and conservation advocacy. In addition, the California Air Resources Board is seeking accurate methods to estimate forest carbon stocks for implementation of carbon accounting policies. Through collaboration with UC–Irvine (see above), I will engage stakeholder representatives from these institutions through ongoing virtual correspondence and regularly held stakeholder meetings to disseminate my findings on forest drought response and early warning capabilities. Code Sharing: As I have already done in my previous publications, I plan to provide open access to R and python scripts to the entire research community produced through my personal Github webpage (github.com/lkunik). References: [1] IPCC 6th Assessment Report, (2021) [2] E. Tomppo, et al. (2021) Remote Sens. 13, 597. [3] D. Schimel, et al. (2002) Eos Trans. AGU. 83(40), 445–449. [4] M. Rotach, et al. (2014) Bull. Amer. Meteor. 95(7), 1021-1028. [5] K. Springer, et al. (2017) Remote Sens. 9, 1–18. [6] D. Sims, et al. (2006) J. Geophys. Res. 111, G04015. [7] T. Magney, et al. (2019) Proc. Natl. Acad. Sci. 116(24), 11640-11645.	Winner!
56	Background: The Bushveld Complex is located in South Africa and was emplaced approximately 2.056 Ga. It is the largest layered mafic intrusion in the world, covering an area of 65,000 km2 with a thickness ranging from 7-9 km1. The Bushveld is an important resource for the world, hosting major quantities of platinum and platinum group elements (PGE), titanium, iron, vanadium, tin, and chromium2. The mafic to ultramafic cumulate sequence of the complex is called the Rustenburg Layered Suite (RLS) and is divided into five zones: Marginal, Lower, Critical, Main, Upper, and Roof Zones. The Upper and Upper Main Zones (UUMZ) are genetically related to each other and are separated from the lower zones by a layer known as the Pyroxenite Marker (PM)3. The UUMZ hosts the most significant Fe, V, Ti, and P deposits2. The UUMZ is dominated by gabbro, anorthosite, and Fe-Ti-oxide rich rocks, which include magnetitite (magnetite and ilmenite) and nelsonite (magnetite, ilmenite, and apatite) mineral assemblages. Fe-Ti-oxide rich rocks make up the smallest proportion in the UUMZ but host the majority of economically significant minerals. The Fe-Ti-rich rocks are typified in 26 magnetite and 6 nelsonite layers documented in the western limb4; the same number of magnetite layers have also been mapped in the eastern limb3. The gabbro layers in the UUMZ are thought to have been formed by cooling and differentiation of the residual magma that produced the Pyroxenite Marker. However, the evolution and relationship between the anorthosite and magnetite/nelsonite layers is still poorly understood. Several models have been proposed to explain the genesis of these layers, including: immiscibility, fractional crystallization/mineral accumulation, disequilibrium crystallization, chamber rejuvenation and magma mixing, and hydrothermal enrichment2-6, 9. However, no consensus has been reached as to which model may most accurately describes the petrogenesis of the Fe-Ti-oxide rich layers because each authors’ interpretation of the data has in turn been challenged by other workers. Objective: To apply new techniques and ideas developing in magma chamber research to the magnetite layers in the Upper Zone of the Bushveld Complex and use the new datasets to test models for development and differentiation of UUMZ layers. The results will be applied to magnetite pipes that have similar mineral assemblages to the magnetite layers, but have contested origins. Hypothesis: Testing different previously proposed models for the origin of Fe-Ti-oxide rich magnetite and nelsonite layers in the Bushveld Complex will result in a new model that combines certain aspects of these end-member processes to more accurately define the petrogenesis of the oxide-rich layers. Methods: The western limb will be studied via samples from the Bierkraal cores and the magnetite layers in the eastern limb will be sampled during field-mapping. Bulk rock geochemistry analyzed with x-ray fluorescence (major elements) and LA-ICPMS (trace elements). Mineral compositions will be analyzed by EPMA and LA-ICPMS; the data will be integrated with photomicrographs collected using optical and scanning electron microscopy. Testing models: Immiscibility. Silicate liquid immiscibility occurs when a homogenous silicate melt separates into two compositionally distinct liquids with identical mineralogy but in differing proportions. In mafic layered intrusions, this process begins after considerable crystal fractionation, resulting in a crystal mush. If the permeability of the crystal mush is high, liquid may separate by gravity, producing nearly monomineralic layers5,6. The presence of Fe-rich silicate inclusions in minerals such as plagioclase and apatite is evidence for liquid immiscibility. If the Fe-Ti-oxide rich layers formed through immiscible processes, then the crystallized Fe-rich melt inclusions will have major and trace element content similar to the magnetite layers. Additionally, the host mineral that crystallized from the Si-rich melt will have lower REE, HFSE, P, Ti, and FeO contents than the conjugate Fe-rich inclusions7. Fractional crystallization/mineral accumulation. Fractional crystallization of magma will lead to a dense residual magma that will begin to crystallize magnetite and accumulate into magnetite-rich layers. The resulting magma after magnetite crystallization will have a lower density and rise buoyantly continuing this process. If fractional crystallization occurred, a fractionation trend will be recorded in minerals with increasing height in the section. Consequently, stratigraphically higher magnetite and anorthosite layers will have increased iron enrichment, lower plagioclase An%, lower Mg# in pyroxene and olivine, lower V content in magnetite, and higher whole rock SiO wt%4. Additionally, in a closed system with continuous 2 fractionation, incompatible elements become more enriched and compatible elements depleted with increasing stratigraphic height. In magnetite, this will be recorded as decreased Ti, V, and Cr (compatible elements) and increased Si and Ca (highly incompatible)8. Disequilibrium crystallization. Another idea proposed for the genesis of magnetite-rich layers is rapid crystallization in disequilibrium conditions in response to increased oxygen fugacity (ƒO ) towards 2 the base of a magma chamber. As magnetite crystallization progresses, ƒO is lowered. Vanadium (V) 2 partitioning between magnetite (mt) and clinopyroxene (cpx) can be used as a proxy for oxygen fugacity, as it is sensitive to changes in ƒO . If V /V increases, this corresponds to decreasing ƒO 2. Additionally, 2 mt cpx 2 if the Fe-Ti-oxide layers crystallized instantaneously in high ƒO , Mg, Al, and Si contents will be relatively 2 enriched in magnetite from these layers compared to disseminated magnetite in anorthosite/gabbro layers9. Chamber rejuvenation and magma mixing. If new magma was injected periodically during the emplacement of the UUMZ to form the distinct Fe-Ti-oxide rich layers, step-like changes in mineral composition through a vertical unit, rather than a smooth fractionation trend, will be observed. Also, if the new magma is more primitive than the final fractionation stages of the previous injection then compositional reversals will be observed moving up-section. Compositional reversals in magnetite will be seen as higher Cr and V contents followed by an abrupt change to lower content4. Hydrothermal enrichment. This model is particularly applicable to the magnetite pipes that have a vertical structure. If the Fe-Ti-oxide rich layers formed by hydrothermal enrichment rather than having a magmatic origin, magnetite will be depleted in Ti, Al, and HFSE, as hydrothermal fluids have generally low concentrations of these minerals due to their relatively low solubility. In contrast, magmatic magnetite will be relatively enriched in compatible elements. Silicon and Ca are two elements that are highly incompatible with magnetite, so if these are enriched in the samples, it suggests hydrothermal activity. Another characteristic of magnetite that is indicative of hydrothermal enrichment is if the Ni/Cr ratio is >1 (in silicate magmas this ratio is always less than one)9. In addition, photomicrographs of magmatic magnetite commonly show concentric compositional zoning in contrast to patchy textures common of hydrothermal minerals9. Intellectual Merit: The UUMZ contains world class deposits of important strategic elements including vanadium, iron, and titanium. There are limited global resources of these minerals and global demand is growing exponentially2. Understanding how these ore deposits form is critical both globally and to U.S. interests, as the country is dependent on foreign sources for many of these important commodities. There is still much debate about the formation of magnetite layers in the Bushveld Complex, even though they have been identified and studied for many years. Results from this research may be applied to other parts of the extensive Bushveld Complex, as well as to other layered mafic intrusions around the world. Broader Impacts: Impacts of this research of magnetite layers of the Bushveld Complex extend beyond Earth. One of the most exciting endeavors of the last century, sending humans to outer space, has propelled scientific curiosity perhaps more dramatically than any other scientific activity. As interest in deep-space exploration and establishing a human presence on Mars and other planets increases, so has the need to understand the processes of ore-deposit formation and exploitation. Lessons learned about the differentiation of magnetite layers in the Bushveld Complex may provide insight and strategies for sourcing iron and titanium (and other metals) that are crucial to permanent infrastructures, but are not cost effective to send from Earth. Results from my research will be shared with colleagues through publications and presentations at conferences, including the national GSA and AGU conferences. Additionally, one of my long-term goals is to provide educators with resources they can implement in their classrooms to encourage curiosity in their students and create lifelong learners who can contribute to scientific advancement. Applying ore- forming processes to ideas of colonization on other planets offers the potential for stimulating STEM activities that can be leveraged into K-12 educations to increase engagement in science. Potential projects topics could include what would be required to mine resources on another planet, requiring students to engage strategy and research, and in the process hopefully garner a lifetime of scientific curiosity. [1] Zeh et al. (2015) Earth Planet Sci Lett, v. 418, p. 103-114 [2] Fischer (2018) PhD Dissert., 129 p. [3] Scoon and Mitchell (2012) S Afr J Geol, v. 115.4, p. 515-534 [4] Tegner et al. (2006) J Petrol, v. 47, p. 2257-2279 [5] Cawthorn (2015) in Layered Intrusions, p. 515-587 [6] VanTongeren and Mathez (2012) Geology, v. 40, p. 491-494 [7] Veksler et al. (2006) Contrib Mineral Petrol, v. 152, p. 685-702 [8] Dare et al. (2014) Miner Depos, v. 49, p. 785-796 [9] Klemm et al. (1985) Econ Geol, v. 80, p. 1075-1088	Winner!
57	"the environment, our previous knowledge, and our underlying motivation in order to make goal-directed choices. Several brain regions are recruited in this decision-making process. The mediodorsal thalamus (MD) has been shown to be necessary for cognitive tasks like working memory and goal-directed decision-making1. The mediodorsal thalamus (MD) takes higher order ​ ​ feedback and sensory information and relays it to the orbitofrontal cortex (OFC) and basal ganglia (BG). The orbitofrontal cortex (OFC) is necessary for value-based decision-making and inferring2. The OFC sends projections to the striatum, the input nucleus of the basal ganglia, ​ ​ which is needed for action performance3. ​ ​ While there is evidence that both OFC and MD project to dorsal striatum 4, how and what ​ ​ information is being sent or modulated through these paths is less clear. I plan to use transgenes and recombinase technologies to limit the expression of a fluorophore or a calcium indicator in a cell-type and projection-specific manner to measure the interactions between these regions during a decision-making task. I hypothesize that distinct sensory and valuation information ​ processing is occuring in OFC and MD. These aims address how information in cortico-basal ​ ganglia-thalamic (CBGT) loops is being passed on and selectively used to control decision-making. Aim 1: Characterize the cortico-thalamic-basal ganglia circuit using anatomical tracing. ​ Previous research has focused on individual streams of information from the thalamus to prefrontal cortex5. However, ​ ​ there have been no studies examining overlapping OFC and MD collaterals in striatum. I seek to address how OFC and MD projections are overlapping. In order to understand how information is flowing through this particular CBGT loop, I will first conduct an anatomical study. I will perform stereotactic surgery in mice, which I have previously done in the Gremel lab6. I will inject two adeno-associated viruses ​ ​ ​ into the mice: a retrograde Cre-GFP in DS and a Cre-dependent mCherry in MD. After waiting for adequate ​ viral expression, I will visualize the neurons using fluorescence microscopy. The presence of mCherry labeled MD terminals in OFC would indicate that those MD neurons project to both OFC and DS. The same strategy will be used to look for OFC neurons that synapse onto both MD and DS. This anatomical information (Figure 1) will inform how to ​ ​ proceed with functional investigations; if the same neurons are synapsing in OFC and DS (or MD and DS), there may be an interesting mechanism controlling behavior. An undetermined portion of thalamostriatal neurons synapse onto inhibitory interneurons, so there is a possibility that they function as a clamp to perform gain control on the information coming from OFC. If I do not see overlap of collaterals in the microcircuitry of striatum, I intend to investigate how MD is contributing to holding information in a decision-making task. Aim 2: Examine the activity of MD and OFC neurons synapsing in striatum during a ​ ​ self-initiated decision-making task. To examine how MD and OFC are synapsing in striatum, I will use genetically encoded calcium indicators (GCaMP and RCaMP) as a proxy for synaptic activity in behaving mice. I will perform fiber photometry to measure calcium activity in two groups of mice, one group with MD and DS, and another with OFC and DS. I will use an axon-targeting GCaMP in the MD and OFC and RCaMP in the DS so I can simultaneously record from both populations (Figure 2). To look at specific ​ ​ cell types, I will restrict RCaMP expression in DS to either indirect or direct pathway medium spiny neurons (iMSNs or dMSNs). These methods will allow me to look at how the activity in the MD or OFC is related to the two striatal output pathways. Mice will be trained to hold down a lever for a specific duration in order to earn a food reward. I hypothesize that information may be accruing actively over the period of holding ​ down the lever, and this information may be maintained through the MD. Calcium transients will ​ be examined around lever press initiation/stop and reward delivery. I will compare the transients over time (days of learning) and between MD, OFC, and DS to examine decision-making computations that may be supporting behavior. We will use regression analyses to quantify the dependency between the activity of MD and DS and OFC and DS when the animal is deciding to let go of the lever. I expect that there is more correlation in MD-DS when the animal holds down the lever long enough to earn a reward. Intellectual Merit This work will provide insight on circuitry underlying decision-making. The findings will also inform how neurons integrate information and use that integrated information to generate actions. Results may provide insight into circuit motifs like gain control that allow for rapid problem-solving, potentially applicable to other neural circuits and artificial intelligence. Broader Impacts Findings may inform the development of treatment for disorders where appropriate action selection is disrupted- OCD, mood disorders, schizophrenia, and addiction, hopefully improving well-being. Treatment for these diseases may mitigate the cost of disability in the US and help our economy grow by including more people in the workforce. Moreover, funding this project directly ensures the full participation of myself, a woman with a disability, in STEM. Results will be shared in peer-reviewed publications and at conferences, and will also be shared with the general community by writing blog posts for NeuWriteSD7. I will also share my ​ ​ research and general scientific topics through demonstrations at K-12 schools and at community events with all ages. I am excited to have matched with a ""pre-scientist"" 6th grade pen pal, and we are exchanging letters about going to college, overcoming obstacles, and scientific careers. All of this outreach generates curiosity and better scientific literacy in the general public. References 1 ​ Hallassa & Kastner, Nat Neuro (2017). 2 ​Gremel et al., Nat Comm (2013). 3 ​Yin, Neuroscientist (2017). 4 ​Hunnicutt ​ ​ ​ ​ ​ ​ ​ ​ ​ et al., eLife (2016). 5 ​Parnaudeau et al., Biol. Psychiatry (2018). 6 ​Baltz et al., eLife (2018). 7 ​neuwritesd.org ​ ​ ​ ​ ​ ​ ​ ​"	Winner!
58	"Intellectual Merit: Inspired by the adaptability of biological organisms, soft robots have emerged to address some of the technical limitations of conventional rigid robots. Although rigid robots are remarkably capable at high-precision and load-bearing tasks, their stiff material properties, with a Young’s modulus in the range of 109 – 1012 Pa, inherently limit their ability to physically interact with their environment. In contrast, soft robots are composed from gels, fluids, and elastomers with a Young’s modulus of 106 – 109 Pa. These soft materials mimic the mechanical properties of biological tissues and can bend, stretch, and compress. This ability to conform is key for applications in human-robot interaction, biomedical devices, and space-restricted environments. For example, a rigid wearable accessory has limited placement locations that are both comfortable for the user and informative for the device. In comparison, a soft wearable device, that can compress and stretch, greatly increases compatible locations and therefore potential applications, such as biometric monitoring or activity tracking. These advantages have also been shown to enable successful applications in medicine, such as organ-assist sleeves, drug delivery, prostheses, and surgical tools [1]. However, soft robot applications still face prevalent barriers to improved functionality over rigid robot solutions. The same material properties that give soft robots their versatility also create challenges in actuation, control, sensing, and modelling. In contrast to rigid systems with a small number of joints, soft robots possess many more degrees of freedom due to their continuous elastic bodies. In order to fully understand their environment, soft robots require effective sensing tools in a stretchable format. However, soft and stretchable sensing solutions have only been recently developed for strain and pressure sensing. The lack of sensory information has resulted in the absence of sensor-based control and higher-level decision making that would be customary for a rigid robot [2]. The Soft Machines Lab at Carnegie Mellon University led by Professor Carmel Majidi made a breakthrough in soft robotic sensing capabilities by demonstrating a hybrid soft sensor skin with orientation, pressure, temperature, and proximity sensing processed on-board [3]. Finally armed with multimodal sensing to determine the soft robot’s environmental and internal state, a unique opportunity has arisen for the development of sensor-based control for soft robots. Proposal: As part of the team that developed the soft sensor skin in [3], I will build on our previous work and implement sensor-based control of a robotic arm and gripper using the sensor skin. This will serve as the first demonstration of the feasibility of sensor-based feedback control on this soft robot system. The approach can then be extended to a wider range of soft robotic systems, which I will further explore in my graduate research. I. Physical System: The sensor-based control system will be implemented on a robotic arm and elastic sensor skin adhered to a two-finger soft Fig. 1: Hybrid soft sensor gripper. The sensor skin will have two soft strain sensors made of liquid skin described in [3]. metal traces, a time-of-flight (TOF) sensor to measure distance, an IMU, and an on-board microprocessor to process sensor data. The strain sensors will be located on each of the inner fingertips to detect the presence of the object. The TOF sensor will be placed on the palm of the gripper, parallel to the scanning surface. The IMU will be placed next to the on-board processor, which will be located at the top of the gripper, to sense the gripper’s orientation. The sensor skin will be connected to a computer hosting a finite state machine to sequence behaviors. The computer will be connected to the robotic arm, so that the finite state machine can generate its movement. 1 II. Control System: The sensor-based control system will consist of two main components. i. TOF data processing: The gripper will scan the table in uniform rows, collecting TOF data to create a 2D array of distance measurements. This 2D array can then be analyzed as an image using OpenCV to identify the size, orientation, and location of each object on the table. The development of this TOF algorithm is crucial to the development of soft robot feedback control by allowing for rudimentary image processing when camera sensors are inaccessible. ii. Finite state machine: The robot arm and gripper will be controlled by a finite state machine (FSM) that uses sensor input to govern the state. The size, orientation, and location of the object will be determined by the TOF algorithm in the first state. After the object has been identified, the strain, IMU, and TOF sensor data will be used to sense the presence of the object in the grasp of the gripper. The presence or absence of the object would inform the system of a successful grasp, lift, transport, or release of the object. This information will be used to traverse the states of the system. The FSM will demonstrate basic autonomy with feedback control of a soft system. III. Testing: The performance of the sensor-based control of the robotic arm and gripper will be tested against an open-loop robotic arm and gripper in a grasping and placement task. Objects of varying size will be placed on a flat surface in front of the gripper. The gripper and robot arm will be programmed to have four potential actions: grasp, lift, move, and release. The open-loop system will use provided object locations and complete each of the actions strictly in sequence. The sensor-based control system will not have prior information about object locations; instead, it will use sensor data to determine the object locations after scanning the workspace. The sensor-based control system will also have the opportunity to decide whether to move onto the next action or repeat previous actions based on sensor data. The success rate of grasping, lifting, placing, and overall task completion for each system will be compared over multiple trials. One potential challenge with this system is noise from ambient light conditions affecting the accuracy of the TOF data. Because the system relies on the TOF data to locate and interact with the object, the accuracy of this data is crucial. A potential solution would be calibrating the TOF sensor to the specific environment that it will be operating in. Broader Impacts: This proposal addresses a key challenge in the control of soft robot systems by demonstrating a novel implementation of sensor-based control using a multimodal sensing skin. A successful demonstration will further push the boundaries of control and autonomy in the field of soft robotics, as well as provide a platform for more complex control architectures in the future. These advancements are necessary for ubiquitous soft robots in everyday life; for example, soft robot applications in the care and improved quality-of-life for the elderly. One practical application for this specific soft robot system consisting of a sensing skin, soft gripper, and robot arm would be to sort and grasp food objects in a food processing facility. The soft gripper would be better-suited to handling delicate food objects than a rigid gripper, which would reduce the amount of damaged food from handling errors. The sensor-based control system would allow for unsupervised operation, as expected in a modern facility. In addition, this project could be demonstrated in STEM outreach to inspire interest in STEM from K-12 students. Because the differences in performance of this system could be easily seen and understood with little technical background, and the task of grasping and placing is familiar, this demonstration would be particularly well-suited to a young audience. [1] M. Cianchetti, et al.,""Biomedical Applications of Soft Robotics."" Nature Reviews. Materials 3.6 (2018): 143-53. [2] T. Thuruthel, et al.,“Control Strategies for Soft Robotic Manipulators: A Survey,” Soft Robotics, Vol. 5, No. 2 (2018). [3] T. Hellebrekers, K. B. Ozutemiz, J. Yin and C. Majidi, ""Liquid Metal-Microelectronics Integration for a Sensorized Soft Robot Skin,"" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2"	Winner!
59	Title: Energy-information trade-off in vocal development Background: Vocal development is the result of interactions among several biomechanical and physiological processes, all of which are constrained by energy. Our attempts to understand how all these mechanistic pieces are coordinated over postnatal life is thus a formidable challenge. While the importance of energetic costs in development is obvious, we must also consider that these costs must be traded-off with information transmission: human infants and other animals use vocalizations (e.g., cries, contact calls) to solicit care from conspecifics. How these costs are traded-off during development has not been considered, yet it could provide a common, high- level framework across species within which low-level mechanistic findings may be interpreted. I propose to investigate whether a model that considers energy and information trade-offs best predicts the shape of vocal developmental trajectories in mammals; I will then test the model’s predictions with empirical data. Three different models could potentially explain the trajectory of vocal development, and each can be characterized mathematically using a cost function. The first model is through a constant and progressive change (Fig. 1a,d). The second model relies on a strong initial change followed by slow adaptation (Fig 1b,e). Both models can ac- count for associative learning. The second model can also account for bodily growth in at least 60 species3. The third model considers development Figure 1. Model schematics. (a) First model: linear as stepwise (Fig 1c,f). Using data from three dif- shift of the cost. (b) Second model: cost modeled ferent mammalian species—marmoset monkeys1,2, through recurrence equation4. (c) Third model: two bats5 and humans6,7, all of which exhibit vocal fixed costs balance each other. (d) Linear change. learning—I extracted four standard acoustic fea- (e) Gradual change. (f) Stepwise change. tures (call duration, dominant frequency, amplitude modulation frequency, and Wiener entropy) and calculated their principal component to generate one single acoustic measure. Using that measure, I found that the best model is the stepwise model. The adjusted R2 for the linear, gradu- al and stepwise model are shown in Fig. 2. The stepwise model is also the only one able to accu- rately predict the day of transition between immature and mature-sounding vocalizations. It will therefore serve as the model for investigating the constraints that shape vocal development. Hypothesis: The stepwise transition of vocal output is predicted to be shaped by physiological (energetic) and social (informational) con- straints. These can be manipulated inde- pendently to determine how each might influence development. Aim 1: Test whether energy changes the transition tim- ing in marmoset monkeys. One factor in- fluencing the energy required to vocalize is respiratory power, thus lighter air should reduce its energetic cost. I will fit the Figure 2. Model fitting. (a) Marmoset monkey. (b) Egyptian stepwise model with vocal data collected fruit bat. (c) Human. while infants were in a helium-oxygen (lighter air) during brief daily sessions over 2 months and compare it to data outside the helium-oxygen chamber (regular air). Aim 2: Test whether the in- formation transmission changes the transition timing in marmoset monkeys. Efficient infor- mation transmission is characterized by how well the infant’s vocalization can be used to predict the parent’s vocalizations. I will fit the stepwise model with infant vocal data recorded in ses- sions with high versus low levels of parental feedback over 2 months2. Methods: To achieve aim 1, I will study energetic manipulations using vocalizations recorded in a heliox chamber8. Heliox (20% O and 80% He) has a lighter mass than regular air, so the ener- 2 gy required to pump the air out of the lungs is lower. Marmoset infants are placed for 20 min in a chamber that holds 45 L of air, and their vocalizations are recorded. To achieve aim 2, I will use vocalizations from infants placed in a 20-min playback condition, in which infants receive audi- tory feedback from a closed-loop playback sys- tem11. The infants receive either the father’s or the mother’s mature calls. With the closed- loop system, we can control the amount of contingent playback they will receive, thus manipulating the rate at which infants can change the informational content of their vo- calizations. Figure 3. Schematics of energy cost manipulation. (a) Results Evaluation: I can use the stepwise Higher and (b) lower cost. (c) Simulation using costs model to predict the change in the transition from (a) and (b). day when either the energy or information cost is manipulated (Fig. 3). That change will be com- pared with the change obtained in the experiments. In aim 1, by decreasing the energy cost in the heliox chamber, the transition day should happen later. By decreasing the information cost in aim 2, the transition should happen sooner. If the predictions do not hold, then either the constraints of the model are not energy and information, or the way they constrain development is not the one proposed by the model. In the first case, I can investigate other possible constraints, for ex- ample, how differences in the environment affect development. In the second case, I can check if variations of the model (such as a different cost shape) can better explain the data. Intellectual Merit: The project has the potential to explain how vocal learning can happen in marmoset monkeys and likely other species, including humans and other vocal learners. It incor- porates different factors important for vocal development into one framework, highlighting the importance of the body and the environment into behavior. The stepwise model is also general- izable to other behavioral systems (e.g., locomotion), and thus potentially useful to describe the transitions we see when categorizing any behavior into different stages. Broader Impacts: Low socioeconomic status populations have a higher incidence of speech- language impairments, such as reduced vocabulary and phonological awareness9. A better under- standing of vocal development can meaningfully inform intervention programs. For example, it can be used to measure how diet (energy) versus social interaction (information) lead to healthy vocal development. The computational nature of my project allows me to involve undergraduate students early on in my research. This will be facilitated by the ReMatch program at Princeton, that encourages first and second-year undergraduates to do research. The subject is engaging due to the natural curiosity around how humans and other animals communicate. Moreover, I will disseminate my findings in scientific meetings as well as in forums accessible to the public (in- cluding video lectures). References: 1Takahashi et al., 2015. 2Takahashi et al., 2017. 3Renner-Martin et al., 2018. 4Fehér et al., 2009. 5Prat et al., 2017. 6Cruz-Ferreira, 2003. 7Brent et al., 2001. 8Zhang and Ghazanfar, 2018. 9Perkins et al., 2013.	Winner!
60	"between a treatment and control group – has come to be understood as the gold standard for scientific settings where the end goal is intervening in some process to achieve a desired end, as in genetic engineering, clinical trials or public policy design. This is for good reason: causal inference is a technique for identifying the precise impact of a given intervention on the target outcome, which is essentially impossible otherwise due to issues of confounding. Nonetheless a number of issues still plague causal inference as a method of inquiry. Perhaps the most important is failure to generalize. We see this everywhere from MPRA estimated gene expression not predicting measured expression in cells [1] to the numerous nudges that work well in laboratories and fail when scaled [2]. Often these issues of generalization are related to a shift in the underlying population tested in the lab and the actual population intervened on. This means there is a deep connection between understanding when we should expect treatments to translate to results and out-of-distribution prediction problems in the machine learning literature. The second major problem with causal inference it is not integrative: information from one experiment rarely if ever informs our understanding of another experiment on a similar population. A clear consequence is that causal inference has produced a fractured landscape of treatment effects without real theoretical connections, especially in the social sciences. Using latent representations of experimental units would allow for multitask learning which would effectively share information on treatment responses across treated units. Related Work Machine Learning and causal inference is an emerging intersection with tremendous promise. Most work in the literature is focused on understanding treatment heterogeneity within a given study. Usually this is done by building a model to predict the outcome for treated and control units, then using that model to predict counterfactual treatment or control outcomes for each unit and taking the difference. The distribution of these differences captures the degree of treatment heterogeneity, which is often of interest especially in medical contexts where it is important to know if treatment effects are driven by broad effects or much higher than average effectiveness in some sub-group. Within this literature the closest work to my proposal is [3], which attempts to learn representations to improve the quality of these counterfactual predictions but does not focus either on out-of-distribution predictions for understanding generalization or learning representations for multiple experimental treatments. Proposal Toward extending the literature on machine learning and causal inference to address the generalizability of treatments and allow sharing of information across treatment effects I propose to use learned representations of experimental units to allow for out-of-distribution prediction with calibrated uncertainty estimates and multi-task learning. Calibrated uncertainty in individual predictions should allow extrapolating from the experimental setting to the population of interest and looking at the confidence intervals to understand the expected range of outcomes. Multi-task learning, and in particular using a shared latent representation across experiments should enforce information sharing across different experimental settings, formally allowing the results of treatment in one experiment to inform the analysis of other experiments. Research Plan Much of my work up to this point has been on representation learning of regulatory DNA and of political beliefs. In the political context I have found that even without tuning, representations can provide more robust out-of-distribution prediction. Along similar lines I have found that multi-task learning of latent representations also improves the out-of-distribution predictions. In the genetics context my work has shown that Gaussian processes offer well calibrated uncertainty estimation on samples far from the training distribution. Aim I: Before digging deeper into method development I want to confirm these insights hold across other contexts. Does multi-task learning improve the quality of latent representations for out-of-distribution prediction in genomics as well as politics? Do Gaussian processes still provide well calibrated uncertainty if treated units are companies instead of basepairs? More generally I plan to build simulations to explore the dimensions of when and why these ideas hold up and hopefully to develop supporting theory. Aim II: After confirming the results from my past work I want to extend these insights to develop a framework for embedding causal inference in deep learning. To build the learned representations I plan to explore Variational Auto-encoders, Auto- Encoding Generative Adversarial Networks, and comparing to a baseline using multitask learning directly and taking the last shared layer as the latent representation. The core idea is to use these latent representations to predict the outcomes for each unit in the control and treated conditions. Because of their high-quality uncertainty estimation, I plan to use Gaussian processes to make these predictions. Of course, if the learning of the latent representations is completely independent of outcome prediction there will be no information sharing across tasks. So, I am going to leverage another property of Gaussian processes: their differentiability. I plan to split training into two phases. The first unsupervised phase will just focus on training the auto- encoder for learning representations. The second phase will optimize the latent space for the predicting the outputs for all experimental settings simultaneously, updating the auto-encoder by back-propagating through the Gaussian processes (in the figure these are the yellow arrows pointing to outcomes). Aim III: This framework is only useful if it actually works in practice. I want to conduct replications of several randomized trials that were first tested in the lab and then scaled. In particular I want to examine deworming studies from development economics [4], fixed/growth mindset work from the education literature [5], and α-1 adrenergic receptor antagonists for COVID-19 treatments [6]. The first of these failed to scale, and the second succeeded with limited effectiveness, and the third is an example where the experiment only involves older men but the target population for intervention is the general public. If my method correctly recovers the average treatment for these experiments, it would confirm the value in robustly extrapolating before taking the costly step of scaling treatments. Intellectual Merit Should my approach to out-of-distribution confidence intervals prove successful it would have significant implications for the machine learning literature. Similarly, if integrating information across experiments proves useful for estimating treatment effects that will be very significant for work in causal inference, transforming the way we think about randomized trials. Instead of one-off experiments we could engineer large models that integrate as many effects as possible to mutually improve our understanding. Even if my main approach does not work as expected, in the process of completing this research, I will certainly be able to contribute to our understanding of when out-of- distribution prediction is easy and when it is hard, and to the literature on learning representations. Broader Impacts Understanding when and how treatments effects will generalize when scaled up significantly is a crucial question in clinical settings and in public policy. If I am able to establish a framework that allows for more precise estimation of treatments when scaled it could greatly improve our understanding of who drugs are effective at treating, allowing greater patient understanding of expected outcomes and uncertainty. It would also improve the design of government programs, and the cost of designing government programs if extrapolation could substitute for running full scale experiments. References [1] de Boer, Carl G., et al. ""Deciphering eukaryotic gene-regulatory logic with 100 million random promoters."" Nature biotechnology 38.1 (2020): 56-65. [2] Rai, Tage S. ""Honesty “nudge” fails to replicate."" Science 368.6488 (2020): 279-280. [3] Johansson, Fredrik, Uri Shalit, and David Sontag. ""Learning representations for counterfactual inference."" International conference on machine learning. 2016. [4] Miguel, Edward, and Michael Kremer. ""Worms: identifying impacts on education and health in the presence of treatment externalities."" Econometrica 72.1 (2004): 159-217. [5] Yeager, David S., et al. ""A national experiment reveals where a growth mindset improves achievement."" Nature 573.7774 (2019): 364-369. [6] Konig, Maximilian F., et al. ""Preventing cytokine storm syndrome in COVID-19 using α-1 adrenergic receptor antagonists."" The Journal of Clinical Investigation 130.7 (2020)."	Winner!
61	for the Multiple Mirror Telescope There are planets where it rains rubies. Specifically, some planets orbiting alien suns show evidence of clouds made of corundum, which is the basis on earth for rubies and sapphires (Wakeford, 2016). This is a romantic discovery from observing exoplanet atmospheres, but more practical research looks for possible signs of life on other planets, be it bacteria or dog in an alien suit, in the form of combinations of molecules that indicate non-equilibrium chemistry (e.g. water, oxygen, methane, ozone). In ground based observatories, research of this nature is only made possible with adaptive optics (a system that corrects observations in real time by using wavefront sensors to observe the shape of incoming light). Infrared (IR) observatories, like the Multiple Mirror Telescope (MMT) have the potential to observe these signs of life, but at present lack the instrument sensitivity or wavelength range to achieve these goals. In collaboration with the NSF funded project MAPS (the MMT Adaptive optics exoPlanet characterization System) I will test, model, integrate, and perform on sky commissioning with two IR and visible pyramid wavefront sensors on the Multiple Mirror Telescope. This proposed upgrade to the Multiple Mirror Telescope provides observations of fainter targets yet to be observed, increases wavelength range, and tests new wavefront sensing techniques that inform the next generation of telescopes. Intellectual Merit : In the field of exoplanet astronomy, ground-based observations are limited by the Earth's turbulent atmosphere, which causes incoming light to a telescope to spread irregularly over a larger area. When the object is a faint point-like source, better adaptive optics systems enable observations of star-planet-systems that we could not have seen before. With adaptive optics (AO), observations are corrected in real time by applying a correction (calculated by wavefront sensors) with mirrors that deform to correct the atmospheric perturbation. Figure 1 demonstrates an example of how light appears after corrections by adaptive optics at Lick Observatory, as well as the predicted improvement to the AO system at MMT with the MAPS upgrade. Figure 1. Left : Lick Observatory image with and without applied correction to the wavefront. (Max, 2019) Right : Using Strehl (the ratio of peak intensity for an image with and without aberrations) as a metric, we can see the predicted performance upgrade due to AO on MAPS through J and H bands (the IR regime) (Morzinski, 2018). My proposal is unique because I plan to incorporate the two wavefront sensors in a single system with a dynamic choice of IR or visible wavefront sensing, drastically improving image quality for the final IR science images. The observer will choose which wavefront sensor will provide better correction for their observation; for a source that is significantly brighter in the IR or visible, collecting more photons means faster and more accurate corrections, which is especially vital when the atmosphere is moving in real time. To that end, the switch to pyramid wavefront sensors will provide an additional improvement to photon collection over the present Shack-Hartman wavefront sensor installed on MMT. Additionally, two new detectors will be used for the wavefront sensors: a CCID-75 visible detector (which is new to astronomy applications), and a SAPHIRA IR avalanche photo diode detector; both have reduced readnoise over the current detector used for wavefront sensing with MMT (Morzinski, 2018). As shown in Figure 1, the proposed improvements to the AO system will provide a 40-50% improvement in the observed intensity in the IR as compared to the existing MMT system. Plan of Work : (1) Run an initial analysis to select our exact infrared regime. Specifically I will predict if the J band (1.1-1.4 microns), H band (1.5 – 1.8 microns), or an overlap, will be better for our proposed targets. This will be informed by my work with the Exoplanet Characterization Tool Kit, with an emphasis on the atmospheric retrieval tools contributed by Mike Line (Fowler, 2018.) I will evaluate to what extent water, carbon monoxide, and carbon dioxide (the molecules we expect to find in our initial round of Jupiter-like targets) are observable in these proposed bands given varying host stars shining through varying planetary atmospheres. (2) Perform initial experiments to setup and test the two wavefront sensors in the optics lab facilities at the University of Arizona. Specifically I will integrate the two wavefront sensors in the Arizona lab facilities and find ways to programmatically take images and control hardware on the testbed. This work will be streamlined by my work on GLARE (the Generalized Lab Architecture for Restructured optical Experiments), a Python suite of automated experiment software and testbed- agnostic controllers for hardware common in optical testbeds (Fowler, 2020). (3) Perform further testing to reduce and correct for alternate noise factors including dark current, readnoise, optical ghosts, etc. Specifically, I will generate multiple images, isolate signs of these alternate noise factors, and test alternative hardware configurations and modes and/or calibration software to optimize final image quality. My work on the Wide Field Camera 3 Quicklook project (a codebase including a dark current and readnoise monitor) will inform the detection and removal of noise from these experiments. (4) Integrate the wavefront sensors into MMT alongside on-sky commissioning of the full adaptive optics system. Unique Resources : Many of the investigators of MAPS are at the University of Arizona, including Dr. Katie Morzinski (the principal investigator and an Assistant Astronomer) who will advise me for this project. The University of Arizona has two lab spaces that will support this project, as well as a vibrant instrumentation group to support and facilitate this work. The University of Arizona has unfettered access to MMT as well as 50% of its telescope time for calibration observations and experiments, and as it is local to the university at Mount Hopkins, we can actively iterate with MMT and the lab to test and improve new components. Broader Impact : The original NSF MAPS proposal includes a Winter School, a brief winter workshop aimed at graduate students, postdocs, and professionals to teach the science of exoplanet instrumentation. The Winter School is based on previous NSF Professional Development Program funded programs like Adaptive Optics Summer School led by the Center for Adaptive Optics. As part of this work, I will design a lab demonstration exploring the distinction between an IR and visible wavefront sensor. My experience as a teaching assistant and Software Carpentry instructor will facilitate creating and leading a lab for my colleagues, under the advisement of Dr. Morzinski who is leading the Winter School. Fowler, J. et al. “G.L.A.R.E..”, AAS Meeting #235, 2020 --- Fowler, J. et al. “ExoCTK”, AAS Meeting #231, 2018 --- M., Claire “Introduction to AO and the CfAO.” AO Summer School. 2019. --- Morzinksi, K. “MAPS: The MMT AO ExoPlanet Characterization System .” Cf AAO Retreat. 2018. --- Wakeford, H. R. et al. “High- Temperature Condensate Clouds in Super-Hot Jupiter Atmospheres.” MNRAS, (2016)	Winner!
62	The continued reliance on agricultural and petrochemical-based methods of production for many high-value compounds threatens future generations with shortages of essential manufacturing materials, organic solvents, biofuels, and pharmaceuticals. The application of synthetic biology to metabolic engineering has worked to address this growing concern by transferring requisite enzymatic pathways from native organisms to standardized chasses and manipulating them to both decrease dependence on non-renewable inputs and increase overall production yield. Despite continued efforts to improve the tunability and consistency of reaction progress within large-scale bioreactors, traditional controller-based methods of optimization remain stymied by excessive variability characteristic of biological systems. Intellectual Merit: Recent developments in the application of optogenetic tools to metabolic pathways have demonstrated potential in addressing the lack of tunability and irreversibility of traditional chemical-inducer based control schemes. Leveraging the implicit reversibility of optogenetic induction, Milias-Argeitis et al. (2016) developed an automated transcriptional control system to maintain a constant concentration of fluorescent protein as a proof of concept. While the proposed systems can be used to rapidly increase protein production, the tunability of the system is limited by the slow rate at which the proteins degrade. In the context of metabolic applications this delay could contribute to the non-optimal accumulation of toxic intermediates and decrease the applicability of feedback structures, reducing overall fermentation yield. The development of a reversible post-transcriptional control mechanism presents a novel, generalizable solution to this meaningful challenge. Hypothesis: A reversible, post-translational system for the control of selective protein degradation can be created using existing optogenetic toolkits to rapidly and precisely decrease protein concentration. Approach: A selective protein degradation tag is conjugated to the coding sequence of a target protein. The tag marks the target protein for degradation via ClpXP, a selective protease comprised of ClpX and ClpP subunits2. The reconstitution of these subunits is facilitated through heterodimerization of the cryptochrome Cry2 to the protein CIB13. Sufficiently orthogonal to the green light (535nm) and red light (672nm) utilized for transcriptional control, instances of Cry2 and CIB1 reconstitute in the presence of blue light (470nm) and spontaneously disassociate in its absence4. A B Fig. 1 Genetic circuit for proposed selective degradation scheme. (A) Representation of “slow response” transcriptional control used in Milias-Argeitis et al. (2016) updated with a protein degradation tag. (B) Representation of “fast response” post-transcriptional control featuring the reconstitution of the ClpXP protease in the presence of blue light to degrade tagged proteins. 1 Research Statement Kevin Fitzgerald Aim 1: Ensure conjugation of CIB1 tag to ClpX subunits has negligible impact on hexamer formation. The ClpX subunit is itself composed of six ClpX subunits. In the context of this project, each a ClpX subunit would be conjugated to a CIB1 dimerization domain and constitutively produced. a Although a small protein, it is critical to ensure that the conjugation of CIB1 to ClpX does not a interfere with the formation of hexameric ClpX. To achieve this aim, a library of mutant ClpX a subunits with CIB1 conjugated at different locations will be generated via rational design and screened for their ability to recombine via western blot. Utilizing a non-conjugated ClpP subunit, the functionality of structurally-promising CIB1-ClpX mutants will then be evaluated by their a enzymatic capacity to degrade tagged fluorescent proteins. Aim 2: Ensure ClpX/ClpP fusion is negligible in the absence of blue light and reversible following exposure to blue light. Wild type ClpX and ClpP subunits independently recombine via the formation of hydrogen bonds between key looping peptides on their exteriors. For the system to selectively degrade tagged proteins of interest, it is necessary to minimize any spontaneous recombination and subsequent functionality of conjugated ClpXP in the absence of blue light. Simultaneously, the utility of this light-based system is contingent on the reversible nature of the optogenetic reactions. Once background ClpXP functionality is minimized, it is also possible that the ClpXP complex formed upon initial Cry2/CIB1-mediated binding interactions will remain cohesive and functional despite the dissociation of conjugated light domains. It is therefore necessary to rationally generate and screen mutations within the ClpX/ClpP binding domains capable of both minimizing subunit binding affinities and maintaining enzyme functionality in the presence of blue light. Broader Impacts and Future Directions: Properly tuned to compatibly function with existing transcriptional control systems, a rapid, light-controlled protein degradation system would serve as a valuable tool in improving the sensitivity of optogenetic feedback systems in industrial fermentation processes. By effectively decreasing system lag, target concentrations of potentially toxic enzymes can be maintained more consistently despite excessive background noise. In the context of metabolic engineering this increase in control has the potential to improve both the speed and yield of fermentations. For those individuals relying on fermentation-based pharmaceuticals for the treatment of disease or fermentation-based biofuels for energy, even minor improvements in fermentation yield could decrease costs and improve accessibility to such essential compounds. In the future, the implementation of optogenetic protein controllers at each step in a metabolic process could serve to drastically improve the tunability and engineering capacity of large-scale fermentation processes. References: 1. Milias-Argeitis, A., Rullan, M., Aoki, S., Buchmann, P., & Khammash, M. (2016). Automatedo ptogenetic feedback control for precise and robust regulation of gene expression and cell growth. Nature Communications, 7(1). doi: 10.1038/ncomms12546 2. Baker, T., & Sauer, R. (2012). ClpXP, an ATP-powered unfolding and protein-degradation machine. Biochimica Et Biophysica Acta (BBA) - Molecular Cell Research, 1823(1), 15-28. doi: 10.1016/j.bbamcr.2011.06.007 3. Park, H., Kim, N., Lee, S., Kim, N., Kim, J., & Heo, W. (2017). Optogenetic protein clustering through fluorescent protein tagging and extension of CRY2. Nature Communications, 8(1). doi: 10.1038/s41467-017-00060 4. Kennedy, M., Hughes, R., Peteya, L., Schwartz, J., Ehlers, M., & Tucker, C. (2010). Rapid blue-light–mediated induction of protein interactions in living cells. Nature Methods, 7(12), 973-975. doi: 10.1038/nmeth.1524 2	Winner!
63	projects, and written reports, as well as quizzes and exams. In large enrollment courses, instructors often use multiple-choice questions as an assessment method because of the ease and perceived objectivity in grading.1 Although multiple-choice exams are useful for providing fast feedback about student performance, the multiple-choice item format has been criticized for primarily assessing low levels of cognition.2 Biology assessments that fail to target higher-order thinking can be detrimental to the student learning process because these low-level assessments limit the development of critical reasoning and problem-solving skills, do not promote the long- term retention of course material,3 negatively affect study habits,4 and hinder scientific inquiry.5 Bloom’s taxonomy is widely used in biology education research as a tool for evaluating student performance and guiding the development of instructional strategies.6,7 Bloom’s taxonomy consists of a hierarchy of cognitive skills: remember, understand, apply, analyze, evaluate, and create.8,9 This framework can be used to categorize the cognitive levels assessed by multiple- choice and constructed-response items on biology exams. The division of biology courses into introductory and advanced courses implies that the higher-level courses provide opportunities for students to gain a greater depth of conceptual knowledge and to practice the higher-order cognitive skills that are necessary for STEM careers. Although previous research has identified that introductory biology courses primarily assess the two lowest levels of Bloom’s taxonomy,10 there are few studies that analyze if assessments in 300- and 400-level courses target the higher- order thinking that is presumed in advanced biology courses. The advantages and disadvantages of multiple-choice and constructed-response items are well-studied,1 but there is little research on the extent to which the different item formats are used when assessing content knowledge and cognitive skills in introductory and upper-level biology courses. Multiple-choice items are traditionally associated with assessing the lowest levels of Bloom’s taxonomy and constructed-response items are often thought to target higher- level thinking, but there has not been extensive research in biology courses to determine if there is evidence to support these stereotypes about item format. There is a gap in the literature regarding the frequency with which multiple-choice and constructed response items are used in introductory and upper-level biology courses to assess higher-order cognitive skills. My research will fill this gap, highlight strengths of the current methods of biology assessment, and identify the areas where assessment can be improved to better reflect the knowledge and skills that are required for success in STEM careers. Research Questions 1) Is there a difference in the cognitive levels assessed in introductory and upper-level biology courses? 2) What is the relationship between item format and cognitive level assessed on undergraduate biology exams? 3) What decisions, processes, and methods are instructors using to design undergraduate biology exams? Methods To answer these research questions, I will survey exams from biology instructors at a range of undergraduate institutions. Biology instructors will be recruited for participation in the research through professional networks such as the Ecological Research as Education Network and the Society for Advancement of Biology Education Research. The collected exam documents will be reviewed using a directed qualitative content analysis, a process in which each question on the exam will be categorized by a Bloom’s cognitive level as well as by item type. I will mentor undergraduate research assistants and teach them how to use Bloom’s taxonomy to review exam items. I will use a Cohen’s kappa analysis to determine interrater reliability for consensus of the classifications of Bloom’s level and item type between the reviewers. To determine which factors predict the Bloom’s level of exam items, I will run ordinal regressions with item type and course level as predictor variables and instructor as a random effect. In the analysis of Bloom’s levels on individual exams, I will calculate a weighted average because items designed to assess higher-order thinking may tend to have a higher point value than items assessing lower-level cognitive skills. I will conduct semi-structured interviews with instructors to clarify the decisions, processes and methods used to design biology exams. The interview protocol will consist of three sections: 1) questions about possible constraints, such as large class size, that might limit the type of assessments administered in their courses, 2) participant familiarity with Bloom’s taxonomy or other frameworks for assessing cognitive skills, and 3) goals for exam design. This research focuses on exams because this form of assessment tends to reflect the types of knowledge and skills that students are expected to master in a course, but I acknowledge that there are assessment methods other than exams. There are some limitations to Bloom’s taxonomy as a framework because of its design for broadscale application in education research. These limitations will be addressed by using the Blooming Biology Tool, which is a modification of the Bloom’s framework tailored for the analysis of questions on biology topics.7 Intellectual Merit My experiences as a high school science teacher and as an Assessment Specialist at the Educational Testing Service provided the skillset that I will use to conduct the proposed research. Previous studies have identified that introductory biology courses primarily consist of items assessing low-level cognitive skills,10 but there are few studies that have examined either the assessment methods in upper-level biology courses or the relationship between item type and cognitive level assessed on biology exams. Broader Impacts One goal for this research is to strengthen the quality of the undergraduate biology education experience through identifying areas of assessment that can be improved. Students who are administered high-level items throughout their science courses are more likely to acquire deep conceptual understanding of the course material,2 so determining where assessments can be modified to target higher levels of Bloom’s taxonomy is a step in the process of promoting intellectual development in biology students. This research also addresses the disparity between the cognitive skills assessed on introductory biology exams and the cognitive skills required for solving real-world scientific problems. Although this research will be conducted primarily on assessments from American undergraduate institutions, biology exams are not unique to the United States, and the implications of this research will have international reach. A second goal of this research is to promote the advancement of biology education research, which will be accomplished through training undergraduate students in education research methodology, involving students in the process of qualitative and quantitative data analysis, and collaborating with students to present the research at science conferences. (1) Stanger-Hall, K. F. CBE Life Sci. Educ. 2012, 11(3), 294-306. (2) Martinez, M .E. Educational Psychologist. 1999, 34(4), 207-218. (3) Jensen, J. L. et al. Educ. Psychol. Rev. 2014, 26(2), 307-329. (4) Entwistle, A.; Entwistle, N. 1992, 2(1), 1-22. (5) Momsen, J. et al. CBE Life Sci. Educ.2013, 12(2), 239-249. (6) Bissell, A. N.; Lemons, P. P. BioScience. 2006, 56(1), 66-72. (7) Crowe, A. et al. CBE Life Sci. Educ.2008, 7(4), 368-381. (8) Bloom, B. S. et al. McKay: New York, NY, 1964. (9) Anderson, L. W.; Krathwohl, D. R. Allyn & Bacon: Boston, 2001. (10) Momsen, J. et al. CBE Life Sci. Educ. 2010, 9(4), 435-440.	Winner!
65	"LeeAnnM.Sager MazziottiGroup,DepartmentofChemistry,TheUniversityofChicago Goal: Iaimtodevelopamethodologytoexplorethedegreeofexcitoncondensationonaquan- tumcomputer,determinethepreparation(s)thatobtainmaximumexcitoncondensationforagiven numberofqubits,andprobethepropertiesofsaidexcitoncondensatestates. Introduction: Condensation phenomena has been an active area of research since 1924 when EinsteinandBosefirstintroducedtheirideal“Bose-Einstein”gas.1Theidenticalparticlescompris- ingthisgas(bosons)wereproposedtobeabletoaggregateintoasinglequantumgroundstatewhen sufficiently cooled.1 Later, London and Tisza attributed Bose-Einstein condensation (BEC) to be thesourceofsuperfluidity—thefrictionlessflowofzero-viscosityfluids—thathadbeenobserved in low-temperature liquid helium.2 In 1940, Pauli established the relationship between spin and statistics, demonstrating that particles with integral spin values obey Bose-Einstein statistics—are bosons—and hence may form a condensate.2 Extrapolating further, pairs of fermions—particles with half-integer spins—may interact such that the overall two-particle system has integral spin and is hence bosonic. In fact, recent experimental and theoretical investigation has particularly centeredaroundthecondensationsofonesuchclassofbosons: excitoncondensates.3 Excitoncon- densation is defined by the condensation of particle-hole pairs (excitons) into a single quantum state to create a superfluid. The superfluidity of electron-hole pairs involves the non-dissipative transferofenergy,whichhasapplicationsinenergytransportandelectronics.3 Motivation: While excitons form spontaneously in semiconductors and insulators and while the binding energy of the excitons can greatly exceed their thermal energy at room temperature, theyrecombinetooquicklytoallowforformationofacondensateinasimplemanner. Tocombat recombination,thecouplingofexcitonstopolaritons,whichrequiresthecontinuousinputoflight,4 andthephysicalseparationofelectronsandholesintobilayers,whichinvolvesimpracticallyhigh magneticfieldsand/orlowtemperatures,5areemployed. Thus,anew,more-practicalavenueforthe creationandstudyofexcitoncondensationisdesired;quantumcomputingofferssuchanavenue. A qubit is the basic unit of quantum computing (analogous to the classical bit); the qubit itself is a quantum system whose most-general state is a linear combination of its two basis states ( 0 | ⟩ and 1 ,theclassicalbitstates)withanappropriatephasefactor(eiφ)givenby6 | ⟩ θ θ cos θ Ψ = cos 0 +eiφsin 1 = 2 . | ⟩ 2 | ⟩ 2 | ⟩ eiφsin θ ! "" ! "" ! # $2 "" Ifaqubitisconsideredtobeaone-fermion,two-levelsysteminwhichth#er$eisaprobabilitypofthe fermionbeinginthelower-levelstate( 0 )andaprobability1 pofthefermionbeingintheupper- | ⟩ − level state ( 1 ) where p = cos θ 2, then a single qubit can represent two particle-hole paired | ⟩ | 2 | orbitals. Assuch,asystemofN qubitscanbeviewedasN fermionsinanN-folddegenerate,two- # $ level, particle-hole paired system. As explored in Ref. 7, such a model can demonstrate exciton condensationinsystemswithasfewas3fermionsin6orbitals(i.e.,athreequbitsystem). Resources: InthisworktheIBMQuantumExperiencedevices—whichareavailableonline— willbeused. Qiskitopen-sourcequantumcomputingsoftwarewillbeemployedforanalysis. Aim-1, Probe the Extent of Exciton Condensation of an Arbitrary Qubit State: In or- der to computationally probe the presence and extent of condensation behavior, I aim to mea- sure the largest eigenvalue of the 2G˜ matrix—a calculable, characteristic property of exciton condensation—on the quantum computer.8 The elements of the 2G˜ matrix are given by 2G˜i,j = k˜,˜l LeeAnnM.Sager Page1 ⟨Ψ |ψˆ i†ψˆ jψˆ ˜l†ψˆ k˜ |Ψ ⟩ − ⟨Ψ |ψˆ i†ψˆ j |Ψ ⟩⟨Ψ |ψˆ k˜†ψˆ ˜l|Ψ ⟩ where |Ψ ⟩ is the N-fermion wavefunction; i,j,˜ l,k˜ representspinorbitals;andψˆ andψˆarethecreationandannihilationoperators. Duetotheparticle- † hole pairing of each qubit, the spin orbitals denoted by i and j and the spin orbitals denoted by k˜ and˜ l must correspond to the same qubit to be non-zero, simplifying the matrix. In order to obtain the 2G˜ matrix on a quantum computer, these elements must be translated into the basis of Pauli matrices. The expectation values of the Pauli matrices can be obtained through direct measure- mentfromaquantumcomputer,andthematrixelementscanthenbecalculatedthroughuseofthe appropriateconversion. Thelargesteigenvaluecanbecomputedfromthematrixobtained. Aim-2, Determine Preparation(s) for State(s) with Maximum Exciton Condensation: A quantum state of qubits can be prepared on a quantum computer by the application of a unitary transformation,Uˆ ,suchthat Ψ (1,2,...,N) = Uˆ Ψ (1,2,...,N) describesthepreparationof i i i 0 | ⟩ | ⟩ anN-qubitstatefromtheinitialstate Ψ = 00 0 . Iaimtodeterminetheappropriateunitary 0 | ⟩ | ··· ⟩ transformation for a given number of qubits that corresponds with the maximum condensation of excitons—the largest eigenvalue. One particular N-qubit state that may be of interest on this searchistheGHZstate—thestateinwhichallqubitsareinthe 0 stateorthe 1 statewithequal | ⟩ | ⟩ probability (i.e., Ψ = 1 0 N + 1 N );6 this state is highly entangled and is hence an | GHZ ⟩ √2 | ⟩⊗ | ⟩⊗ idealcandidateforexcitoncondensation. # $ Aim-3, Probe Properties of Exciton Condensates: Any physical, measurable property of a system corresponds to a Hermitian matrix (Aˆ) such that the eigenvalues of Aˆare the possible out- comes of measurement of said property. The elements of these Hermitian matrices can be written intermsoftheexpectationvaluesofPaulimatricesandcanthereforebeobtainedforagivenqubit preparation. Fromthismatrix,theprobabilityofagivenmeasurement( Ψ a )andtheexpectation i n ⟨ | ⟩ valueofthatproperty( Ψ Aˆ Ψ )canbeobtainedwhere a istheeigenstatecorrespondingtoa i i n ⟨ | | ⟩ | ⟩ givenmeasurementa and Ψ isanN-qubitstatepreparedbytheunitarymatrixUˆ . Forexample, n i i | ⟩ the energetics of a prepared qubit state can be probed by obtaining the eigenvalues/eigenstates of thetwo-fermionreducedHamiltonianmatrixgivenby2K = 1 1 2 Zj + 1 1 . N −1 −2∇1 − j r1j 2r12 Intellectual Merit: This project aims to expand our understa%nding of exciton’condensation & phenomena. Shouldtheseapproachesprovesuccessful,areliableandfacilepreparationforexciton condensatestateswillbeachieved,andpropertiesofsuchcondensateswillbeabletobeprobedin astraightforwardmanner. Broader Impacts: The superfluidity of excitons in a condensate allows for the frictionless transportoftheexcitationenergy,releaseduponrecombinationoftheparticleandhole. Addition- ally, such superfluidity in a bilayer—with electrons in one layer and holes in another—allows for the frictionless transfer of charge as long as current is directed in opposite directions in the two layers—a phenomenon known as counterflow superconductivity.3 Understanding and exploiting the superfluid properties of exciton condensates may hence be instrumental in the effort to design wiresandelectronicdeviceswithminimallossofenergy,decreasingoverallenergyconsumption. 1 Einstein,A.KöniglichePreußischeAkademiederWissenschaften1924,261––267. 2 Vilchynskyy,S.I.;Yakimenko,A.I.;Isaieva,K.O.;Chumachenko,A.V.LowTemp.Phys.2013,39,724–740. 3 Fil,D.V.;Shevchenko,S.I.LowTemp.Phys.2018,44,867–909. 4 Fuhrer,M.S.;Hamilton,A.R.Physics2016,9. 5 Kellogg,M.;Eisenstein,J.P.;Pfeiffer,L.N.;West,K.W.Phys.Rev.Lett.2004,93,036801. 6 Kaye,P.;Laflamme,R.;Mosca,M.Anintroductiontoquantumcomputing;OxfordUniversityPress,2010. 7 Lipkin,H.J.;Meshkov,N.;Glick,A.J.Nucl.Phys.A1965,62,188–198. 8 Garrod,C.;Rosina,M.J.Math.Phys.1969,10,1855–1861. LeeAnnM.Sager Page2"	Winner!
66	Introduction: Grain growth is a critical process to both metals and ceramics processing, as grain size plays a major role in bulk material properties, such as fracture toughness. Abnormal grain growth (AGG) is a process by which the growth of a small fraction of grains is incentivized and they grow faster than their neighbors, resulting in a bimodal grain size distribution and heterogeneous bulk properties. Though work has been conducted in this field for decades1, the cause and mechanism behind AGG are still poorly understood. The process is particularly import to ceramic materials, as the superior thermal resistance of ceramics lends itself to extreme environment applications. At these elevated temperatures kinetics are accelerated, expediting grain growth and AGG – this is a particular challenge in alumina, which is very susceptible to AGG2. Processing of these materials also raises concern, as high sintering temperatures can have the same deleterious effect. As such, it is vital to understand how to control grain growth in ceramics processing and applications. Textured microstructures with enhanced mechanical properties in materials can be designed by controlling their crystallographic orientation during processing. One technique that has been explored is the application of a magnetic field during processing3,4. This project will investigate how texturing by applied magnetic field in alumina ceramics impacts grain growth through the use of electron and synchrotron X-ray based techniques, which allow us to track individual grains and grain boundaries. I hypothesize that a strong applied magnetic field during processing will reduce grain growth and mitigate AGG due the formation of low-angle and, thus, low-energy grain boundaries that have a low driving force to move. This will be validated by 3D microstructural characterization, allowing the measurement of grain boundary orientation and character for textured samples. These results will be relevant to industrial applications of alumina – including the manufacture of automotive parts and ballistic armor – as microstructure engineering can Figure 1. Preferential crystallographic orientation (ratio of 006 signal intensity to total signal intensity) improve the mechanical properties of ceramic materials as a function of applied magnetic field strength in and improve stability and lifetime. alumina with different slip solid loading4. Objective 1: Preparation of textured Al O samples by thermomagnetic slip-casting process: 2 3 Alumina samples for this experiment will be prepared via slip casting with high-purity α-Al O 2 3 powder. Alumina is chosen as a test material due to its impressive mechanical properties and applicability as a structural ceramic, as well as its susceptibility to texturing by magnetic field. A dispersant will be added to the slip to prevent the agglomeration of particles. The samples will be subjected to an applied magnetic field during casting – this texturing technique has been shown to induce the growth of preferentially oriented grains during annealing3,4. Figure 1 illustrates this effect in alumina. Samples will be cast under applied magnetic fields of 0-8 T with 0 T being a control sample. Once cast, alumina green bodies will be sintered to near theoretical density and annealed at temperatures above 1400 ºC for various times. Objective 2: Electron microscopy characterization of grain growth: From the bulk samples, centimeter-sized sections will be cut and polished for observation under a scanning electron microscope (SEM) to determine the initial average grain size by image analysis software employing the linear intercept method5. The samples will then be further annealed under identical time and temperature conditions, and the same method will be repeated to determine the grain growth in each sample. These results will provide a quantitative measure of grain growth as a function of applied magnetic field during processing and annealing time. Objective 3: 3D characterization of crystallinity and grain size distribution: The novelty of this work lies in the use of high-energy X-ray diffraction microscopy (HEDM) to characterize the samples. In this technique, a sample is placed in the path of an incident X-ray beam and rotated while diffraction patterns are collected. In post-processing, these can be indexed to generate a crystallographic map of the measured volume. From the bulk, millimeter-sized samples will be prepared. From 3D crystallographic maps measured by HEDM, true grain sizes (at a resolution of 1 µm) can be determined and a grain size distribution created. The non-destructive nature of this technique is extremely advantageous, as it will allow tracking of individual grains and boundaries across heat treatments. Thus, the slower movement of individual textured low-angle grain boundaries can be observed and quantified. Via 3D characterization of individual grains and boundaries, these results will verify a) the character and Figure 2. Set-up for HEDM synchrotron motion of the boundaries as a function of applied magnetic measurements, beamline 1-ID at Advanced Photon Source, Argonne field, and b) whether observed low-angle grain boundaries National Lab. Incident X-ray beam induced by texturing reduce AGG. travels along positive z-direction. Research Plan: The timeline for this project is one year. Slip casting will be done at Oak Ridge National Lab, which houses a commercially available thermomagnetic system offering up to 8 T magnetic field. The timeline for this step is one week, accounting for travel time, as slip casting is a well-known process that can be modified as needed. SEM will be conducted at the University of Florida, whose Research Service Centers house a TESCAN SEM. The polishing, sample preparation, and data analysis will take 9-10 months. Lastly, HEDM experiments will be run at the NSF-sponsored CHESS synchrotron’s Structural Materials beamline, at which HEDM will be available beginning in December 2019. The experimental design considers that each HEDM measurement takes hours, and beamtime slots are limited. As such, high-priority samples exhibiting strongly textured microstructure will be selected for synchrotron measurement. Broader Impacts: Beyond structural materials, microstructure engineering is essential to functional materials like oxide fuel cells and laser materials. The results of this project will offer quantitative insight into the use of an applied magnetic field to texture alumina ceramics – this fundamental study will serve as a bridge for future industry-specific studies. As such, I would enjoy sharing these results at conferences and with industrial collaborators directly. 1. Journal of the American Ceramic Society, 80(5), 1149–1156. 2. Scientific Reports, 6(37946), 2–11. 3. Scripta Materialia, 54(6), 977–981. 4. Science and Technology of Advanced Materials, 7(4), 356–364. 5. Journal of the American Ceramic Society, 91(7), 2304–2313.	Winner!
67	Background and Rationale: Cerebral Arteriovenous Malformations (cAVMs) are congenital vascular lesions that affect 0.01-0.50% of the population. The annual risk of hemorrhage in the cAVM nidus is on average 3%, but the risk can be as low as 1% to as high as 33% depending on patient-specific anatomies. Since the cAVM nidus has low resistance, it allows blood to shunt directly from arterial feeders (AFs) to draining veins (DVs) at high flow rates. As a result, if the hemodynamic stresses exceed the elastic modulus of the vessel wall, then a rupture may form and cause a hemorrhage. Embolization – the intravascular injection of embolic materials to AFs, is one of the most common interventional therapies to prevent hemorrhages by diverting blood flow away from the nidus. However, current methods to visualize blood flow patterns for embolization treatment planning are limited. Using the standard-of-care – 2D superselective angiogram sequences (2D+τ) and 3D rotational angiography (3DRA) – is challenging because the contrast agent reaches multiple regions of the nidus simultaneously, which prevents the identification of cAVM compartments and fistulae. Therefore, there is an urgent need to elucidate blood flow patterns in the cAVM to improve on treatment planning for embolization. One method to address this problem involves computational fluid dynamics (CFD) to simulate cAVM hemodynamics on patient-derived models. However, the length-scale of the nidus prevents CFD from being applied because the spatial resolution of 0.6mm in 3DRA is insufficient to capture the 0.1mm or smaller diameters of intranidal vessels. As an alternative, some studies have resorted to using electric network models, but these models are typically not based on clinical data. Instead of modeling every intranidal vessel in the nidus, I could model all the intranidal vessels collectively through a porous volume [1], which has been shown to be a good approximation of the nidus [2]. Simulating the density of intranidal vessels beyond the spatial resolution limitations of 3DRA is possible because voxel intensity is proportional to the amount of contrast agent in a vessel [1], which provides information on the internal geometry of the nidus. Based on Darcy’s law and mass conservation, there are seven parameters that characterize blood flow through the porous volume: porosity, permeability, fluid viscosity, fluid density, quadratic drag factor, and the velocity and pressure boundary conditions [1]. These parameters can be inferred from 3DRA images. However, existing models [1], [3] lack validation with other paradigms, such as in vitro testing, and the models are limited to Types IIa, IIb, and IV in the Yakes classification of cAVMs, which are not representative of the population since there are a total of six types. My research objective is to investigate cAVM hemodynamics for embolization planning through generalized patient-specific CFD models for each angioarchitecture type in the Yakes classification, with a porous volume representing the nidus, and validate in silico results using an in vitro flow loop. Specific Aim 1: Develop simplified cAVM digital phantoms for validation with varying spatial porosity distributions. I will design digital phantoms on SolidWorks (Dassault Systèmes, Vélizy- Villacoublay, FR) with one tubular AF and DV connected to a rectangular porous volume. The phantom will be meshed using a triangular grid of approximately 104-105 elements for a cell density of around 16 cells per voxel [1] using Gambit (Ansys, Canonsburg, PA). The velocity boundary condition at the AF inlet will be set according to phase-contrast magnetic resonance angiography (PC-MRA) images, while a zero-pressure boundary condition will be used at the DV outlet. Blood viscosity will be set to 4.00cP, density to 1060 kg/m3, and Re to 265. Fluent (Ansys, Canonsburg, PA) will be used to run the CFD simulation. To validate, I will confirm that the fluid behaves according to expectations where flow paths will mostly circulate in nonporous regions and that flow through porous regions will obey theoretical expectations from Darcy’s law. Specific Aim 2: Construct generalized patient-specific cAVM models. I will construct the boundaries of the cAVM and the AFs and DVs from 3DRA images. This will be done through segmentation (3D Slicer, Boston, MA), and I will obtain morphometric measurements using Analyze 12.0 (AnalyzeDirect, Overland Park, KS). Based on the segmented anatomies and patient- specific measurements, I will create generalized models for each cAVM angioarchitecture type on SolidWorks. Afterwards, I will mesh the models using a tetrahedral grid on Gambit. The porous volume parameters will be inferred from 3DRA images and then averaged. As before, I will set the fluid properties of blood based on nominal values. I will validate mesh convergence through a mesh independence study on Fluent by comparing coarse vs. medium, medium vs. fine, and coarse vs. fine meshes. Moreover, I will compare porosity distributions through CFD between the generalized models and five different patient-specific anatomies for each angioarchitecture type. Specific Aim 3: Develop a flow loop for in vitro validation. I plan to modify an existing flow loop in Prof. Ajit Yoganathan’s lab to simulate cAVM hemodynamics. For example, the flow loop has two pressure measurement probes, which is insufficient for cAVM simulation. I will make modifications to incorporate more probes to accommodate for all the AFs and DVs. From the models used in the in silico study, I will manufacture transparent rigid physical phantoms using an MR-compatible resin (Watershed 11122, DSM Somos, Elgin, IL). I will then implement the same inflow conditions from the computational study to the flow loop and compare flow field data through PC-MRA and digital particle image velocimetry for validation. Furthermore, I will also manufacture the five patient-specific anatomies for each angioarchitecture type in the Yakes classification from Specific Aim 2 and compare in vitro flow field results to in silico results. Timeline and proposed laboratory: I would like to work with Prof. Ajit Yoganathan (Georgia Tech) because of the close alignment of research interests. I anticipate that this study will take five years: one for Specific Aim 1, and two each for Specific Aim 2 and Specific Aim 3. Intellectual Merit: Currently, only Orlowski et al. [1], [3] have used a porous volume to simulate the cAVM nidus. My project expands on this model, in that there have been no published papers that uses a generalized CFD model based on patient-averaged data, an in vitro flow loop to validate and compare with in silico findings, and a porous model to simulate the cAVM nidus. As post-embolization complications are a major concern, my computational model can serve as the first step to a surgical planning software that meets this need. Broader Impact: With further exploration, the computational model proposed can be expanded to a two-fluid model for simulating the propagation and solidification of embolic therapies, which can provide pre-operative outcome prediction, potential increase in embolization session efficiency, and optimize interventional strategies. My project may also be scaled-up for interventional planning in other AVMs situated in the lung, muscle or bone, prognosis evaluation, and optimization of therapies. Finally, I will collaborate with clinicians at Emory University and University College London to ensure clinical utility and present my work at conferences. Feasibility: From my master’s thesis, I will be supported by University College London, University College Hospital, and Kings College London to obtain 3DRA and PC-MRA patient data. I will seek Institutional Review Board approval under Prof. Yoganathan’s support. [1] P. Orlowski, F. Al-Senani, P. Summers, J. Byrne, J. A. Noble, and Y. Ventikos, “Towards Treatment Planning for the Embolization of Arteriovenous Malformations of the Brain: Intranidal Hemodynamics Modeling,” IEEE Trans. Biomed. Eng., vol. 58, no. 7, pp. 1994–2001, Jul. 2011. [2] C. W. Kerber, S. T. Hecht, and K. Knox, “Arteriovenous malformation model for training and research,” AJNR Am. J. Neuroradiol., vol. 18, no. 7, pp. 1229–1232, Aug. 1997. [3] P. Orlowski, P. Summers, J. A. Noble, J. Byrne, and Y. Ventikos, “Computational modelling for the embolization of brain arteriovenous malformations,” Med. Eng. Phys., vol. 34, no. 7, pp. 873–881, Sep. 2012.	Winner!
68	Introduction Oligodendrocyte precursor cells (OPCs) are the progenitor cells responsible for forming mature, myelinating oligodendrocytes (OLs) in the central nervous system during development. While the majority of OPCs differentiate early in life, there is a small pool that generate OLs over the life span and can differentiate into OLs following white matter injury (WMI). Previous work has shown that this differentiation is impaired in aging, reducing the ability to recover from WMI.1 Significant upregulation of senescence markers in old vs young OPCs suggests senescence, a stress induced state in which cells no longer proliferate, contributes to the impaired ability of OPCs to differentiate2. Studies of senescence in other cell types has shown that it leads to reduced functional capacity and mediates the physiological consequences of aging. Thus, better characterizing OPC senescence and the mechanisms involved would greatly improve our understanding of the role of OPCs in brain aging. Although studies have identified some canonical senescence genes in OPCs, OPC senescence genes have not been completely characterized, making accurate identification of senescent OPCs more difficult. Furthermore, due to our lack of understanding of OPC senescence mechanisms, there is a need to identify novel regulators of senescence in OPCs. Understanding these mechanisms would greatly enhance both our understanding of OPC development, and our ability to promote CNS myelination in injury. Given this, I propose to identify an OPC-specific senescence signature, and to identify novel regulators of senescence via a genome-wide CRISPR- Cas9 knockout screen. I intend to carry out this project with two faculty experts in glial cell biology and genome-wide CRISPR screens. Research Plan: Aim 1: Define canonical and OPC-specific senescence markers in vivo by single-cell RNA-seq Rationale: A comprehensive OPC-specific senescence signature has yet to be established. Such a signature would allow for a more accurate identification of senescent phenotypes in different OPC populations and may provide insight into the mechanisms underlying senescence in OPCs. Experimental Design: OPCs will be taken from 20-24 month old mice and senescent cell clusters identified by flow cytometry using fluorescent antibodies to established senescence markers such as NOTCH3, B2MG and DEP1, which have been shown to have high levels of expression in senescent cells3. Following identification of senescent OPCs, RNA from both proliferating (non- senescent) and senescent OPCs will be extracted and single-cell RNA-seq performed, with non- senescent OPCs serving as a control to identify genes implicated in aging, but not senescence. Data will then be analyzed to identify differentially expressed transcripts in senescent OPCs, which will comprise our OPC senescence signature. This experiment will yield genes both up- and downstream of senescence and will allow for more accurate identification of senescent OPCs. In the future, candidates may be validated through in vivo knockout studies. Possible Outcomes: It is possible that with multiple senescence markers, we may miss OPCs which display lower levels of markers or do not display some at all. We can account for this in adjusting the sensitivity of our gating and analysis, or utilizing fewer antibodies for selection. Aim 2: Identify regulators of OPC senescence via a CRISPR-Cas9 knockout screen Rationale: Discovery of OPC-specific regulators of senescence will establish mechanisms underlying OPC senescence and can be used to inform the development of mechanistic in vivo studies. Experimental Design: To identify regulators of OPC senescence, I will conduct a genome- scale CRISPR-Cas9 knockout screen with a pooled lentiCRISPR library. Complex pooled DNA libraries will be combined, and delivered in lentiviral constructs in 4-8 week old OPCs, which should not display high levels of senescence. OPCs can be cultured and transduced at a large scale which allows for genome-wide screens. After transduction and selection, I will select senescent cells using the flow-cytometry approach described Fig 1. Experimental outline for a senescence marker- in Aim 1 with multiple antibodies against based pooled CRISPR screen. senescence markers. Sorted cells will then be analyzed to identify genes whose knockout increases or decreases senescence, yielding insight into mechanisms of OPC senescence. Possible Outcomes: If Aim 1 yields OPC-specific markers for which there are robust reporters or antibodies, these can be used in a parallel screen to support our initial findings. Future studies may also use the signature from Aim 1 to validate hits from Aim 2 in vivo. Intellectual Merit While senescence has been characterized in microglia and astrocytes, significantly less work has been done in understanding OPC senescence and its relevance in aging. Given that the efficiency of myelination has been shown to decline over time, this proposal, which aims to better characterize OPC senescence, will greatly contribute to our understanding of the effects of aging on oligodendrocyte development and myelination. Furthermore, characterization of OPC senescence opens up the possibility of mechanistic and in vivo studies, which will broadly increase our knowledge of the roles of OPCs, and how they change over time. Broader Impact This work has implications for the understanding and treatment of neurological diseases, as aging is linked to an increase in disorders such as dementia and Alzheimer’s. Dementia is linked to loss of white matter and decreased myelination, and myelin breakdown is implicated in Alzheimer’s, pointing to a critical role for OPCs. This work could eventually allow for the reversal of senescent phenotypes in OPCs, potentially leading to curative treatments. Additionally, understanding OPC senescence has important implications for demyelinating diseases such as MS, for which there are no remyelinating therapies. Recovery from such diseases is impaired by lack of understanding of OPC differentiation; thus, understanding blockades to differentiation such as senescence may inform future therapeutic development. Feasibility My previous work with oligodendrocyte development will significantly contribute to the success of the project. Furthermore, I am rotating in the lab of Dr. Ophir Shalem, whose lab has a strong and successful history of genome-wide CRISPR-Cas9 knockout screens, and who will provide the resources and mentorship need. I am also rotating with Dr. Chris Bennett, who has expertise in the isolation, culture, and sequencing of glia, and can offer the resources and mentorship needed for this project. Having access to a wide breadth of experts in my fields, as well as Penn’s world-class facilities and resources, also ensures the feasibility of this project. References: (1) Swenson et al. Translational Medicine of Aging 2019; (2) Neumann et al. Cell Stem Cell 2019; 3) Althubiti and Macip, Methods in Molecular Biology 2019.	Winner!
69	Introduction: Honey bees (Apis mellifera) are cornerstone pollinators and contribute nearly $20 billion to the U.S. agricultural economy each year1. Honey bee populations have drastically declined by an estimated 30-40% in the past three decades, and 2019 marks the largest winter hive loss ever recorded1. Bee decline threatens the U.S. economy and food supply, which has driven agricultural stakeholders and the scientific community to investigate reasons for honey bee deaths. A number of factors have already been identified, including habitat and foraging space loss, pesticide exposure, and infection by parasites, fungi and viruses. Bees have a commensal community of microbes aside from pathogens that includes bacterial, viral, and eukaryotic species, collectively known as the microbiome. Like in most organisms, the microbiome in bees plays an important role in nutrition and shaping host health through immunity and disease susceptibility. The extent to which the honey bee gut microbiome influences health outcomes remains unclear and for these reasons the scientific community is diligently working toward the characterization of the honey bee microbiota. Intellectual Merit/Background: 16S sequencing has revealed that remarkably simple and spatially organized microbial communities of about 8-10 bacterial phylotypes occupy the honey bee gut, consistent across geographic distributions of bees2. In-depth metagenomic sequencing and single-cell characterization at the strain level revealed that each phylotype spans considerable microbial genomic diversity, leading to substantial polymorphism within and between hives. Such variation between bacterial strains belonging to the same phylotype could result from functional diversification (due to niche partitioning) but also suggests co-divergence and adaptation with host lineages. Bidirectionally, the bee microbiome has been shown to play an important role in modulating the host physiology. Germ-free studies highlighted that native gut microbiota is able to stimulate the immune system in adult worker bees3 and the use of probiotics to effectively mitigate parasite effects shows promise4. However, the contributions of the gut microbiota to host immune pathways and the mechanisms by which the host responds to gut variation has yet to be investigated. Lastly, the genetic architecture of a honeybee colony makes any two daughter worker bees of sister queens mated with a single drone share ¾ of their genes on average5. Taken altogether with the consistent microbial phylotypes observed across colonies, this makes honey bees an ideal model for studying host-microbiome interactions. Using a combination of comparative genomics and field experiments, I aim to identify possible routes of honey- bee/microbiota co-diversification. I hypothesize 1) that the host genotype, diet, and environmental landscape shape the functional capabilities of the honey bee microbial community, and 2) this microbial community can acutely impact the innate immune system of adult bees, and that this community can be modulated by the addition of probiotics. I plan to test these hypotheses with the following aims: Aim 1: Determining the contribution of host genetic and environmental landscape in shaping the functional microbiome of the honey bee microbiota. I will rear several hives derived from single drone-mated sister queens from three different subspecies of A. mellifera: ligustica, carnica, and mellifera. These will be replicated in two geographically separated apiaries (collections of hives). Bees in each location will have access to the same respective landscape of flora to forage from and will also be fed identical nutritional supplements (protein supplementation, sugar syrup). Samples will be collected at the initial hive set-up, then once every 3 months for a year. They will be collected from nurse bees (who stay within the hive) and foraging bees (who leave the hive). Use of apiaries, acquisition of bees, and subspecies identification is enabled by collaboration with Dr. Ramesh Sagili of the Oregon State Honey Bee Lab. High-throughput shotgun metagenomic sequencing will be used to assess and compare the bees’ microbial structure at the phylotype and strain level between and within a) subspecies, b) nurses/foragers of each subspecies and at each location, and c) longitudinally, to assess patterns of functional microbiome congruence between the genetically and geographically distinct subgroups of bees. Additional data including winter survival rates per subspecies will be collected. Microbial samples from the flora the bees forage from in the different apiary locations will be collected in an attempt to ascertain the microbes they are exposed to outside of the hive. Aim 2: Comparative analysis of functional and spatial diversity in the A. mellifera gut microbiome following immune system challenge. Germ-free bee studies have found upregulation of antimicrobial peptides in hemolymph (blood analog in bees) to be associated with inoculation by specific gut microbiota members3. I will determine if altering the microbial community with probiotics can modulate the immune response of the bees and reduce fatalities due to Nosema ceranae (a parasite associated with bee depopulation⁶). To do so I previously collaborated with the Honey Bee Lab and performed a three-week in vivo experiment on the addition of probiotics during Nosema infection. Microbiome samples were harvested from the midgut and hindgut of single bees (Nosema localizes in the midgut). My pilot 16S sequencing confirmed that our methods are sensitive enough to detect distinct spatial microbial compositions (consistent with the literature). Shotgun metagenomics will be performed to compare strain-level functional diversity between experimental groups and gut regions, as well as determine impact of probiotic strains on composition and functional diversity. Functional pathways will be identified in each group and quantitatively compared to ascertain up- or downregulation of antimicrobial peptides and other immune-related genes in the context of infection or probiotic addition. mRNA- seq will be completed to quantify and compare host response to those pathways identified by metagenomics. Pathway predictions will also be confirmed via LC-MS/MS of antimicrobial peptides present in the bee hemolymph. Broader Impacts – Research Dissemination: The sequences and metadata generated by this project will be made publicly available through the online BeeBiome consortium2, where there is a pressing need for comprehensive honey bee microbiome data. Results regarding probiotic treatment of honey bee hives will be communicated to bee keepers through local and nation-wide beekeeping meetings and publications. I anticipate submitting a first author paper detailing my findings in spring 2020, as well as presenting my results at the International Society for Microbial Ecology 2020 meeting in Cape Town, South Africa. At OSU, I will utilize opportunities to share my research, such as the bi-annual Center for Genomic Research & Biocomputing conferences, where I have presented research previously. Broader Impacts – Science Communication: I will share my research with audiences from K- 12 children, community members, and faculty. I plan to involve the community in my research, by expanding this project to include bee gut and local flora samples donated by regional beekeepers. This community-driven, crowd-sourced approach is necessary to characterizing honey bee health outside of the lab setting. I also plan to involve high school AP Biology students during the collection and characterization of apiary flora (Aim 1). I currently serve as the Outreach Coordinator for the Micro Grad Student Assoc. and am in the process of developing several opportunities for local K-12 children to learn about honey bees, utilizing hands-on activities at the weekly Saturday farmer’s market. This combination provides an optimal platform for me to talk about my research and engage with community members on the themes of how humans rely on bees for our food production, and how the health of honey bees impacts us. 1. Pollinator Health Task Force. 2016. Report to Congress. 2. Engel et al. 2016. MBio. 3. Kwong et al. 2017. R. Soc. Open Sci 4. Khoury et al. 2018. Frontiers. 5. Johnstone et al. 2012. R. Soc. Bio Sci 6. Rubanov et al. 2019. Sci. Rep	Winner!
70	Stomata are small pores located on the epidermis of aerial plant tissues necessary for CO 2 assimilation and O release. Stomata are also the primary sites of transpiration, accounting for 2 nearly 90% of all water loss in rice. Plant species can modulate their stomatal density and conductance on newly emerging leaves in response to environmental stimuli such as CO , water 2 status, and temperature1–3. Despite many recent studies in dicots, limited attention has been paid to characterizing the genetic underpinnings of stomatal development and physiology in monocots, namely grasses. Grass stomata exhibit specialized anatomical and physiological attributes, such as subsidiary cells that flank the guard cells allowing for faster stomatal aperture rates4. Concerted efforts could provide insights into environmental adaptation mechanisms exclusive to monocots. The few studies that have characterized aspects of monocot stomatal biology have relied on mutant screens to identify key genes or have attempted to characterize homologs from the model plant organism Arabidopsis thaliana4–6. The use of quantitative genetic approaches as an alternative might reveal quantitative trait loci (QTLs) associated with this important trait. I will complete a genome-wide association study on a rice (Oryza sativa) diversity panel to characterize stomata-mediated drought response in rice. Currently available high density single nucleotide polymorphism (SNP) data allows for the resolution of discrete QTLs that are relevant in extant rice variation7. Further investigation of the most significant SNPs will enable the characterization of genetic variation involved in rice stomatal physiology and development in response to water deficit. Aim 1: Genome wide association study of stomatal traits in drought simulation A hydroponic platform incorporating polyethylene glycol 6000 (PEG)-induced drought stress will be used to yield a high-throughput and uniform assay of plant stomatal density and physiology in response to drought. The stomatal density differences will be measured in normal watering conditions and drought stress lines for each accession. A Li-6400 XT will be used to measure stomatal conductance of individual replicates. The optimized phenotyping platform will be applied to a rice diversity panel of 300 accessions. I will use principal component analysis to maximize coverage of total genetic diversity in the selected lines. Stomatal density and conductance differences between the two treatments will be used as the phenotypic parameters in the GWAS alongside a high-density rice array containing nearly 700k SNPs. Association mapping will be conducted using a custom python script that can account for subpopulation structure as a potential covariate. All loci above the significant p-value threshold will be further analyzed to identify likely candidate genes associated 1 with stomatal density and conductance modulations responsive to drought. I will then use haplotype analysis to determine the haplotypes and frequencies at the most significant QTL. Aim 2: Characterize candidate genes using CRISPR/Cas9 mediated knock-outs and ectopic overexpression The candidate genes most closely associated with the highest significance SNPs will be further characterized using CRISPR/Cas9 to induce mutations 8.Targeted mutagenesis will be executed in the Kitaake genetic background, in which I have already successfully produced knockouts. The short generation time of this accession makes it ideal for high-throughput research. Additionally, I will ectopically overexpress candidate genes with a strong promoter in the Kitaake background. The stomatal density and conductance of mutant overexpression lines will be measured to determine if these candidate genes play a role in stomatal development and physiology. Aim 3: Multiplexing knock-ins of drought adaptation alleles CRISPR/Cas9 and geminiviruses will be used to produce knock-ins of advantageous alleles in the homologous native location in the Kitaake background. Alleles selected will belong to the haplotype associated with the highest significance SNPs from the most drought tolerant accessions. Useful variation may exist in promoters, genes, or non-coding sequences. This variation will be leveraged using the high replication rates of geminivirus replicons to increase rates of homology-directed repair, with precise positioning enabled by CRISPR/Cas9 mediated double-stranded breaks9. Quantitative traits are governed by numerous QTL that contribute collectively to a phenotype10. Simultaneous knock-ins of alleles from drought adapted accessions into the Kitaake background will be confirmed and assayed for performance in drought. This approach highlights an avenue to leverage natural variation using targeted genome editing for allele swapping. Intellectual Merits: Grass stomatal adaptations are currently understudied. Rice can serve as a model for other monocot species in investigating novel environmental adaptations that have evolved relative to dicots. Access to high density marker sets coupled with transformation facilities can enable biological investigations that go beyond the scope of model plant organisms. Results may be translated to species such as hexaploid wheat, where GWAS and transformation is more challenging, to explore the conservation of adaptive mechanisms among grasses. Furthermore, multiplexing adaptive allele knock-ins could bypass the high time investment and linkage drag inherent to traditional breeding approaches, and be broadly applied to a range of traits for which there is existing GWAS data11. Broader Impacts: Improved understanding of drought tolerance mechanisms in monocots can enable eventual crop improvements. These advancements will be necessary to improve plant performance in the face of impending global climate changes. International collaborators will assist with field testing of the most promising edited lines, integrating a broad community of plant scientists. I will leverage my connections in NPR Scicommers and local science museums to share findings of this study with the public and discuss data at conferences, thereby engaging with all individuals about plant-environmental interactions in the context of climate change. References: [1]Hamanishi, E. T., Thomas, B. R. & Campbell, M. M. J. Exp. Bot. (2012). [2] Gray, J. E. et al. Nature (2000). [3].Zhu, J. et al. Forests (2018). [4].Raissig, M. T. et al. Science (2017). [5] Raissig, M. T. et al. Proc. Natl. Acad. Sci. (2016). [6] Hughes, J. et al. Plant Physiol. (2017). [7] McCouch, S. R. et al. Nat. Commun.(2016). [8]. Doudna, J.A. & Charpentier, E. Science (2014). 9.Wang, M. et al. Molecular Plant (2017). [10] Crowell, S. et al. Nat. Commun. (2016). [11]. Jacobsen, E. & Schouten, H.J. Trends Biotechnol. (2007). 2	Winner!
71	25% of all marine life.1 They provide invaluable services by protecting shorelines from storm surge, supplying food sources, and promoting eco-tourism.2 However, coral populations around the world are exhibiting an alarming decline due to climate change, specifically from ocean warming (OW). OW has severely diminished coral health through increased disease susceptibility and bleaching.3 To fully understand how corals might fare under future OW conditions, the potential for coral acclimatization over generations and life history stages must be assessed. Previous research on coral acclimatization has focused primarily on intra-generational acclimatization (IGA), which investigates if corals can adjust to new conditions within their lifetime. For example, Brown et al. (2002) found that when G. aspera were exposed to higher levels of solar radiation, they were less susceptible to bleaching.4 While IGA is crucial in elucidating coral resilience, the study of trans-generational acclimatization (TGA) in corals is essential to understanding the persistence of coral reefs in a warmer future. TGA occurs when the phenotype of the offspring is influenced by the environmental conditions experienced by the parents and/or previous generations.5 Epigenetic modifications, or heritable alterations in gene expression and cellular functions that do not involve changes to the original DNA sequence, are thought to play a role in TGA.6 Most studies on epigenetic modifications and TGA have focused on exclusively DNA methylation; for example, Strader et al. (2019) found that parental environments of S. purpuratus affected patterns of DNA methylation in offspring.7 However, other epigenetic markers, such as histone modification and chromatin remodeling, may be relevant to TGA in marine invertebrates, but have been seldom studied. I will address this knowledge gap by investigating multiple epigenetic mechanisms and outcomes of TGA in corals, over multiple generations and life history stages, in the context of OW. Additionally, I will examine coral physiological processes to better understand all aspects of coral TGA. Discerning the influence of epigenetics on TGA will contribute to the knowledge of coral resilience and susceptibility in an evolutionary and ecologically relevant context. The coral Pocillopora damicornis was chosen for this study, as it is an important reef-building coral in the Indo-Pacific region and is commonly used in laboratory experiments as a model coral. Aims and Hypotheses: Aim 1: Assess physiological effects of TGA in offspring at several developmental stages (larval, juvenile). Hypothesis 1: Both larval and juvenile offspring whose parents were exposed to OW conditions will have a higher tolerance to OW conditions than larval and juvenile offspring whose parents experienced ambient conditions. Aim 2: Compare the epigenetic modifications, specifically DNA methylation patterns and histone modifications, of coral parents and offspring during several developmental stages (larval, juvenile) to evaluate the acquisition and stability of TGA. Hypothesis 2: Parents exposed to OW conditions will produce offspring with differentially methylated genes and modified histones compared to offspring whose parents experienced ambient conditions. Research Methods: I will collect reproductively viable adult P. damicornis from the fringing reefs of Kaneohe Bay, Hawaii and experimentally expose them to OW conditions. To simulate current and future environmental parameters, experimental ambient/high temperatures will be defined as 26/30°C.8 Exposures will take place in mesocosm tanks with flow-through experimental treatment water in the Hawaiian Institute of Marine Biology’s seawater system for one month. Following the adult exposure, I will evaluate the photosynthetic efficiency (F/F ) of v m adult corals to assess their capacity to photosynthesize. Additionally, I will preserve adult tissue samples for later epigenetic analysis (see below). After the exposure and physiological assessment, I will induce adults from all treatments to spawn. I will collect larvae post-spawn 1 and experimentally expose them to ambient/OW conditions for one week. At the beginning and end of the larval exposure, I will measure lipid content and oxygen consumption from a subset of larvae in each treatment to determine energy reserves needed for metamorphosis and metabolic rate, respectively. Following the exposure, I will preserve another subset of larvae from each treatment for epigenetic analysis (see below). I will transfer the remaining larvae from each treatment into 10 L tanks with ambient flow-through seawater and plugs for settlement. After 6 months, I will experimentally expose the now-juvenile offspring to ambient/OW conditions for one month. At the end of the exposure, I will measure juvenile F/F ; following photosynthetic v m analysis, I will preserve juvenile tissue for epigenetic analysis (see below). Epigenetics: At the end of all exposures, I will collect tissue from juveniles/adults and a subset of larvae for DNA methylation and histone modification analyses. Using extracted genomic host DNA, I will assess whole genome DNA methylation using the MeDIP-seq approach. This method utilizes DNA immunoprecipitation and next-generation sequencing to estimate methylation levels of specific DNA regions. I will also use genomic host DNA for histone modification analysis. Histone modifications will be analyzed through the ChIP-seq method, which combines chromatin immunoprecipitation and next-generation sequencing to identify regions of the genome associated with these modifications. Intellectual Merit: Not only will this research considerably enhance knowledge of physiological and epigenetic processes in coral biology, but it will be one of the first studies to provide a deeper understanding of coral resilience over multiple generations and life history stages. The utilization of cutting-edge epigenetic analyses will help to define the contribution of DNA methylation and histone modifications to TGA, which is currently understudied in corals. Additionally, the cognizance of coral TGA potential in the face of anthropogenic stressors will allow scientists and reef managers to make more informed predictions about future reef health and population evolution. An increased knowledge of epigenetic mechanisms in corals will also supply a starting point to investigate TGA potential in other marine invertebrates who may be susceptible to climate change. Broader Impacts: The Graduate Research Fellowship will enable me to pursue important research opportunities and will equip me with the knowledge and abilities needed to succeed as a future governmental or non-profit research scientist. Moreover, the GRF will enhance my skills as a scientific educator and mentor to younger students. I intend to partner with local high schools in the greater University of Hawaii area to connect students with marine science and research. Through this partnership, I will provide opportunities for students to undertake independent projects within the context of my research. I will guide students through an integrated overview on how to conduct research projects from the initial proposal to the final written product. More specifically, I want to include low-income high school students during the research partnership. Low-income students can often be excluded from fully pursuing their interests in science, due to lack of financial and academic support. I hope to provide those students with research opportunities and support, so that they can receive an enriching experience. References: 1Reaka-Kudla (1997) Biod. II. 2Constanza et al. (2014) Glob. Env. Chan. 3Hoegh- Guldberg et al. (2007) Sci. 4Brown et al. (2002) C. Reefs. 5Torda et al. (2017) Nat. Clim. Chan. 6Eirin-Lopez and Putnam (2019) Ann. Rev. Mar. Scie. 7Strader et al. (2019) J. Exp. Mar. Bio. Eco. 8IPCC (2013) AR5. 2	Winner!
72	Background: Cancer is among the leading causes of deaths worldwide with approximately 38% of men and women diagnosed with cancer at some point in their lifetime. With the rising cancer 1 epidemic and the need for cheaper and more accessible drugs for people in developing countries, it is crucial to find new ways to develop pharmaceuticals. One sustainable method is engineering metabolic pathways of microorganisms such as yeast (Saccharomyces cerevisiae) or Escherichia coli to produce the precursor of a drug. One of most successful applications of this technique was achieved in Dr. Jay Keasling’s lab at the Joint BioEnergy Institute by producing the precursor of the antimalarial drug artemisinin, reducing the cost 30-60% and ensuring a continuous supply. 2,3 Engineering microorganisms to produce pharmaceutical products more efficiently can be applied to cancer drug development. Production of terpenoid and polyketide by engineered microorganisms would be particularly beneficial as small amounts of the molecules are produced via natural pathways and yields vary widely based on environmental and epigenetic factors. 4 Paclitaxel (brand name Taxol) is one of the most successful cancer drugs, and was first listed on the World Health Organization Model List of Essential Medicines in 2013. However, there was concern regarding the high cost of the drug , as it initially required the bark of six 100-year-old 5 Pacific yew trees to treat a single patient with breast cancer. Although alternate methods have 6 been developed to extract paclitaxel from needles of the European yew, synthetic biology tools can be used as a more sustainable alternative. Since paclitaxel is a highly decorated diterpenoid (Fig 1), its complicated structure makes it a good fit to be engineered from yeast due to the highly versatile DNA transformation system and well-defined genetic system of yeast. The objective of this project is to engineer a yeast strain capable of synthesizing paclitaxel which can be later optimized for commercial production. This will have an essential impact by reducing the cost of a crucial anticancer drug and providing valuable insight into the pathways required for the production of terpenoid and polyketide natural products from yeast. This project will focus on engineering taxadiene biosynthesis in yeast by utilizing glucose as the hydrocarbon source, studying and identifying cytochrome P450 oxygenation reactions in the pathway, and integrating these components to produce paclitaxel (Fig 1). Aim 1: Engineering of Taxadiene Biosynthesis in Yeast Taxadiene (taxa-4(5),11(12)-diene) biosynthesis in yeast is crucial to the production of paclitaxel but the levels of various precursors these organisms produce naturally are insufficient to make the process feasible. The diterpenoid precursor for taxadiene, geranylgeranyl diphosphate (GGDP), is necessary for a heterologous taxoid biosynthetic pathway but is produced in insufficient quantities in yeast due to competition for steroid synthesis with farnesyl diphosphate. I will introduce heterologous genes from the Sulfolobus acidocaldarius GGDP synthase instead of the native GGDP synthase from Taxus for improved production of GGDP A T axa-4(5),11(12)-d ien e (T P 450 O xygen ation R eaction s P aclitaxel axad ien e) B A im 1E n g in eerin g o f T ax ad ien eB io sy n th esis in Y east A im 2S tu d y C y to ch ro m e P 4 5 0 -D ep en d en t O x y g en atio n R eactio n s in P ath w ay A im 3In teg ratio n o f T ax ad ien eP ath w ay an d O x y g en atio n R eactio n s to P ro d u ce P aclitax el Figure 1. A: Taxadiene intermediate structure catalyzed via various cytochrome P450 oxygenation reactions to produce paclitaxel.7 B: Identified route for production of paclitaxel. and taxadiene as there is no competition for steroid synthesis. Introduction of genes from S. 4 acidocaldarius also results in substantial production of geranylgeraniol, further increasing taxadiene yields. Yeast will be transformed by the lithium acetate method on SC minimal 8 medium agar plates via CRISPR/Cas9 and select plasmids (pVV200 (tryptophane), pVV214 (uracil), pRS313 (histidine) and pRS315 (leucine)). Yeast will be cultivated for 48 hours with glucose as the carbon source, lyophilized, extracted with pentane, and analyzed by GC-MS. Aim 2: Study Cytochrome P450-Dependent Oxygenation Reactions in Pathway The oxygenation steps in the biosynthesis of paclitaxel have yet to be fully studied and identified. After taxadiene biosynthesis, oxidative modification of the olefin and the elaboration of side chains are needed to transform taxadiene into paclitaxel. After the taxadiene skeleton is formed, the olefin must be modified by several P450-mediated oxygenations and coenzyme A dependent acylations. Candidate genes for all but one of the seven steps after taxadiene synthesis are postulated, but the entire pathway has yet to be confirmed. Uracil-specific-excision-reagent 9 (USER) cloning will be utilized for site-directed mutagenesis of the identified genes and cytochrome P450s and USER primers will be designed using the online AMUSER tool. All intermediates will be characterized by GC/LC-MS. Aim 3: Integration of Taxadiene Pathway and Oxygenation Reactions to Produce Paclitaxel Once the taxadiene pathway and oxygenation reactions are identified and characterized, the pathways will be integrated via the lithium acetate method, CRISPR/Cas9, and site-directed mutagenesis to produce paclitaxel. Glucose will be used as the starter carbon source in yeast and will follow the native mevalonate pathway. The established genes to produce GGDP synthase will be introduced and the previously identified taxadiene synthase gene that has been codon optimized for improved yeast expression will be incorporated to produce taxadiene. Then, the 4 pathway developed with the cytochrome P450 oxygenation reactions will be introduced to produce paclitaxel. All intermediates will be evaluated using GC-MS. It is possible that some proteins synthesized in the complete pathway are insoluble or inactive in yeast, thus similar proteins will be determined or engineered to be active. Intellectual Merit: My knowledge from chemical engineering coursework and research with developing cell cultures, DNA analysis, and molecular modification of chemical structures gives me an essential, well-rounded training for fulfilling this project. This project will be the first time a polyketide synthase (PKS)-terpene hybrid has been produced in yeast and will mark an imperative step in the industrial production of PKSs and thus, in the field of synthetic biology. I will collaborate with members of the Joint BioEnergy Institute to learn the genetic technique of integrating genes using CRISPR/Cas9 and use my knowledge of DNA sequencing and GC to confirm the genes responsible for paclitaxel and determine the developed molecules at each step. Broader Impacts: Developing pharmaceuticals from microorganisms is an efficient and cost- effective way to produce the same high-quality compounds obtained from natural products. By engineering yeast to produce paclitaxel, a lower-cost, more sustainable drug could be developed for people suffering from lung, breast, or ovarian cancers who would otherwise not have access to the medicine. If successful, this project will provide a framework for synthesizing other PKS- terpene hybrid chemicals and pharmaceuticals from microorganisms. References: 1. NIH, NCI. 2018. https://www.cancer.gov/about-cancer/understanding/statistics 2. Hale et al., Am Soc Trop Med. 2007. DOI: 10.4269/ajtmh.2007.77.198. 3. Ro et al., Nature. 2006. DOI: 10.1038/nature04640. 4. Engels et al., Met Eng. 2008. DOI: 10.1016/j.ymben.2008.03.001. 5. Mendis. WHO Model List of Essential Medicines. 2011. https://www.who.int/selection_medicines/committees/expert/18/applications/Mendis.pdf?ua=1 6. Horwitz, SB. Nature. 1994. DOI: 10.1038/367593a0 7. Chang et al., Nature. 2006. DOI: 10.1038/nchembio836. 8. Kaiser et al., Cold Spring Harbor Laboratory Press. 1994. ISBN: 0-87969-451-9. 9. Jennewein et al., PNAS. 2004. DOI: 10.1073/pnas.0403009101.	Winner!
73	Motivation: Virus purification and concentration is critical for the production of vaccines and gene therapy vectors. Today, vaccines exist for 26 viral diseases1, but low yields and high costs of production prevent access to many vaccines. Similarly, production limitations will reduce access to viral gene therapy, which may provide cures to single-mutation genetic diseases. Although many technologies exist for virus particle purification, they each face unique roadblocks to developing rapid, high-yielding, and ultimately continuous processes. Chromatography is widely-used for virus purification, but suffers from low binding capacities, frequently inactivates viruses, and cannot operate continuously. Ultrafiltration is readily scalable and provides high throughput and recoveries, but fouling reduces flux and removal of contaminating proteins is often ineffective. Ultracentrifugation is difficult to scale up and has low yield, even though purities are high. Aqueous two-phase systems (ATPS), most commonly constructed with two polymers or a polymer and a salt, are proposed as a replacement to simultaneously purify and concentrate virus. Biologically-gentle purification is achieved by choosing the identity and concentration (described by tie-line length, TLL) of phase-forming components, ionic strength, polymer molecular weight, and pH so that the desired bioparticle partitions to one phase and contaminants to the other. Many viral particles have been purified in ATPS, with yields as high as 79%2, a significant result in an industry that often accepts overall downstream yields of 30%. Still, the vaccine industry is reluctant to adopt ATPS in part because each bioparticle requires a unique system for optimal recovery and predictive models are lacking to fill this critical gap. So far, attempts to develop a predictive model for bioparticle partitioning in ATPS can be grouped into three approaches. A first approach, the Collander equation, uses known partitioning of bioparticles in ATPSs to predict partition coefficient (K) in new systems3. Because of its reliance on data, this model cannot predict K for new bioparticles. A more robust method combines density gradient theory with thermodynamic and association models. It successfully predicted the mass transfer of an amino acid in ATPSs4, but cannot model partitioning of large bioparticles like viruses. A better solution by Chow, et al. uses surface and interfacial properties to predict partitioning5. Particle diameter, net charge, and contact angle with the ATPS components can predict K. No one has yet successfully combined a theoretical model with measurements of ATPS and particle surface properties to predict bioparticle partitioning. Chow, et al. verified their model empirically by successfully comparing K to TLL and pH, but did not attempt predictions with surface measurements. I propose combining novel methods developed in my lab to measure virus surface characteristics with Chow’s theoretical model to together predict virus partitioning in ATPS, a task which was not previously possible. Hypothesis: Partitioning of virus particles in aqueous two-phase systems can be predicted using surface chemistry properties measured at the single-particle level. Experimental Plan: I propose extending Chow, et al.’s models to predict partitioning of virus particles in polymer-salt ATPS. Chow’s model requires phase properties of ATPS and surface properties of the viral particle to predict K. Methods for characterizing ATPS are rapid and well- known. Turbidity measurements will be used to determine the binodal points and tie lines. Characterization of viral particles is much more difficult. While virus diameters and isoelectric points (pI) (related to surface charge of viral particles) may be found in the literature, no method to measure the contact angle between ATPS and viral particles currently exists. 1 Historically, virus characterization has depended on bulk solution measurements or amino acid sequencing. Instead, my lab has developed a novel single-particle method to measure virus surface chemistry using chemical force microscopy (CFM), shown in Fig. 1. In a recently submitted article6, we determined the pI of two model viruses using atomic force microscopy probes chemically functionalized to carry positive or negative charges. Adhesion to the viral particle was measured in varying solution pH and the pI determined6, giving a direct characterization of the viral surface and avoiding error from contaminants. A similar single-particle Fig. 1. CFM with characterization will be used to determine the parameters of Chow’s virus particles partition model. The first stage of this work will focus on how virus surface chemistry changes in the presence of ATPS components individually before combining them. First, virus-sized gold nanoparticles (AuNPs) coated in BSA or lysozyme will be immobilized on a gold surface. Then adhesion force between the AuNPs and probes modified with charged or hydrophobic ligands will be measured by CFM. These proteins are known to partition differently in ATPS, and since contact angle between these proteins and ATPS may also be determined by traditional sessile drop methods, comparison will confirm that CFM can be related to surface tension for well- defined systems before extending the method to viruses. Similar determinations of surface tension by CFM for non-biological surfaces have already been reported7. Then, two enveloped (surrounded by a lipid bilayer) and two non-enveloped viruses will be used to explore varying viral surface chemistry. Once a relationship between CFM and contact angle is established, the second stage of this work will characterize multiple ATPS and adapt Chow’s model to predict virus partitioning. To verify the model as a function of component concentration, three TLLs for three common polymer (PEG) and salt (citrate, phosphate, and sulfate) ATPS will be evaluated to show the model is robust for varying chemistries. Similarly, the pH of ATPS will be varied over the range of stability for each virus, typically 4.5<pH<7.5. The model will be complete when it predicts the K of a virus as a function of TLL or pH. Once this work establishes a reliable model for virus partitioning between the bulk phases in ATPS, partitioning to the interface, which frequently results in the irreversible aggregation and inactivation of virus particles, can be explored. Intellectual Merit: Developing predictive models for virus partitioning in ATPS will add to the understanding of viral surfaces and the driving forces behind ATPS, reducing the experimental cost of developing ATPS to purify new bioparticles in the future. Broader Impacts: By filling a critical gap in the literature, making ATPS research faster, and making ATPS more accessible to industry, this work will speed the development and production of vaccines and gene therapy, ultimately reducing outbreaks of viral and genetic disease and saving lives. In addition, I will mentor undergraduate researchers who will have the opportunity to develop contact angle measurements with protein-coated AuNPs. References: 1. WHO. Vaccines and diseases. 2019 2. Joshi, P.U., et al. Journal of Chromatography B, 2019. 1126. 3. Madeira, P.P., et al. Journal of Chromatography A, 2008. 1190(1-2): p. 39-43. 4. Chicaroux, A.K. and T. Zeiner. Fluid Phase Equilibria, 2019. 479: p. 106-113. 5. Chow, Y.H., et al.. Journal of Bioscience and Bioengineering, 2015. 120(1): p. 85-90. 7(38): p. 21305-21314. 6. Mi, X., et al.. Under review. 7. Drelich, J., et al. (2004). Journal of Colloid and Interface Science 280(2): 484-497. 2	Winner!
74	Graduate Research Plan Statement Title: Exploring critical zone structure and function in a tropical urban watershed through concentration-discharge relationships Introduction: Streams are recognized as integrators of the surrounding landscape. Stream water chemistry is thus an excellent indicator of broader critical zone (CZ) processes. The CZ is the space from the top of the vegetative canopy down to bedrock and lowest extent of freely circulating groundwater.1 The CZ framework provides a holistic approach to develop predictive models that describe processes at the earth’s surface, including the constraints on material export from the continents to fluvial networks.2 Concentration-discharge (C-Q) relationships in streams provide an integrated signal of sources and transport processes to examine how solutes and sediment respond to changing patterns of runoff.3 Studies of the CZ are mostly limited to pristine systems; however, C-Q relationships in urban streams may be more complex due to altered hydrology, impaired water quality and heterogenous subcatchments.4 With increasing pressures on urban landscapes, there is an urgency to understand hydro-biogeochemical processes of the urban CZ that govern water quality and quantity. Research in urban systems will be highly valuable to cities and communities and can better inform management practices and help improve urban infrastructure.4 I propose to characterize and compare C-Q relationships across two stream networks with differing levels of urban development through a series of whole-network sampling efforts capturing baseflow to storm events. I also propose to study the impacts of hurricane disturbance on C-Q trends through analysis of long-term water chemistry records. I will test three hypotheses on variability in C-Q relationships across stream networks associated with watershed urbanization (H1-H3): H1: Watershed urbanization drives greater variability in C-Q relationships across stream networks associated with increased impervious surface area. H2: C-Q relationships during storm events are more variable in the urban network due to heterogenous hydrologic signals, and ultimately depend on storm intensity. H3: Both urban and pristine stream networks show increased variability in C-Q relationships after hurricane disturbance, but the magnitude of variability is higher in urban watersheds. Study sites: This work will be conducted in two watersheds: the urban Río Piedras in metropolitan San Juan, Puerto Rico and the mostly undeveloped Río Espíritu Santo in El Yunque National Forest in Río Grande, Puerto Rico, as a reference site. The Río Piedras flows through the metropolitan area with highly modified channels and significant impacts from failing sanitary infrastructure.5 In contrast, the Río Espiritu Santo originates in the Luquillo Mountains and is mostly undeveloped except on its coastal plain with significant changes in water chemistry evident only after hurricane impacts.6 The University of New Hampshire´s (UNH) Water Quality Analysis Lab (WQAL) group, including myself, has extensive experience working in these two stream networks. These watersheds are also a part of the Luquillo Long-Term Ecological Research (LTER) site and Luquillo Critical Zone Observatory (LCZO), which have generated multi-decadal records of stream chemistry and discharge. The lab also has ongoing collaborations with research groups at the University of Puerto Rico (UPR), who have worked in the Río Piedras: Jorge Ortiz- Zayas’s Tropical Limnology Lab and Alonso Ramírez’s Aquatic Ecology Lab. Proposed approach: (H1) I will establish a synoptic sampling regime of 20 sites in each stream network, ranging from small headwater sites to larger mainstem and coastal sites. I will collect water samples as well as measure physicochemical parameters with a handheld multiparameter instrument at least 15 times in the span of 2 years. Samples will be analyzed for nitrate, ammonium, phosphate, dissolved organic carbon, anions, cations, dissolved greenhouse gases, and total suspended solids (TSS) at the UNH WQAL Lab. I will also take discharge measurements at each 1 Carla López-Lloreda NSF Graduate Research Fellowship Graduate Research Plan Statement site and date either using dilution gauging or acoustic velocity measurements, depending on stream size. I will target sampling dates that capture a range of flow conditions by monitoring the US Geological Survey (USGS) gauging stations at two sites within each stream network. This sampling will allow me to characterize spatial and temporal variability in C-Q relationships across the stream network. (H2) I will conduct targeted storm sampling at one USGS gauged site in each stream network with ISCO automated samplers. These samples will be analyzed for the same solutes as in H1, with a focus on TSS to calculate sediment flux during storm events. Real-time discharge data for these events will be obtained online through the USGS´s National Water Information System Interface. I will capture events of different magnitude to explore effects of storm magnitude and intensity on C-Q relationships and solute and sediment transport. I will use these results to evaluate specific patterns of hysteresis, which characterize the rising and falling limbs of a storm event, providing additional insights into material reservoirs within watersheds. (H3) I will use long-term chemistry data from sites in the Río Piedras and the Río Espíritu Santo, which have been sampled weekly for approximately 10 years for multiple solutes and are USGS gauged sites. I will quantify C-Q relationships before and after Hurricane María in September of 2017 which will allow me to examine the resilience of these watersheds to a major perturbation. Intellectual Merit: This work will provide insight into the interactions and mechanisms of sediment and nutrient production, pathways, and transport in a tropical urban watershed. Studying storm events and hurricanes is particularly important because they are “hot moments” of increased hydrological activity, which transport disproportionate amounts of solutes and sediments to streams and oceans compared to baseflow conditions.7 Understanding high flow processes is crucial, as major storm events and hurricanes are expected to increase in intensity and frequency with climate change.8 And though much C-Q work has been done on temperate systems, tropical streams supply disproportionate amount of sediments and solutes to the ocean and studying these systems in a global context is becoming increasingly important.9 This work also leverages research done by the Luquillo LTER and LCZO research networks. Broader Impacts: During this work, I will continue developing collaborations with local research groups in ways that will engage local underrepresented undergraduate students in fieldwork through my network and storm sampling efforts. I will work with UPR professors to offer independent research opportunities to students. This research will provide a useful framework for local government agencies to use in their nutrient and sediment management plans across the island, including restoration projects aimed to help reduce pollution and sedimentation and regain critical zone services in urban ecosystems.7 For this, I will engage local researchers examining sediment and nutrient loading and the local Puerto Rico offices of the Department of Natural Resources. I will also collaborate with local organizations working in the Río Piedras such as the San Juan Bay Estuary and the ENLACE project of the Caño Martín Peña. These projects have longstanding efforts in the Río Piedras watershed and extensive connections with local communities. In collaboration with these organizations, I will use this opportunity to teach communities about water quality in their watershed through a series of roundtables and by providing educational resources to disadvantaged groups that have greater exposure to poor water quality. References: [1] National Research Council (2001). Basic Research Opportunities in Earth Science. Natl. Acad. Press. [2] Brantley, S. L. et al. (2017). Earth Surface Dynamics, 5(4), 841–860. [3] Chorover, J. et al. (2017). Water Resources Research, 53, 8654–8659. [4] Kaushal, S. S. et al. (2014). Biogeochemistry, 121, 1–21. [5] Potter, J. D. et al. (2014). Biogeochemistry, 121, 271–286. [6] McDowell, W. H. et al. (2013). Biogeochemistry, 116, 175–186. [7] Kaushal, S. S. et al. (2018). Biogeochemistry, 141(3), 273–279. [8] Zimmerman, J. K. et al. (2018). Ecology, 99, 1402–1410. [9] Schlesinger, W.H. & Bernhardt, E. (2013). Biogeochemistry (3rd. ed.). Academic Press. 2	Winner!
75	Overarching career goal: Push the frontier of multiphase flows in extreme conditions that are subjected to high speed, strong turbulence, and large mass loading. For my graduate study, my aim is to unveil the underlying physical mechanisms of complex couplings between solid and gas phases in compressible particle-laden turbulence, which remains elusive for both industrial and astrophysical applications. The major challenge in this area is the lack of high-quality time- resolved experimental datasets that can illustrate the particle-gas and particle-particle interaction in extreme conditions. To address this issue, I plan to leverage a recently-developed ultra-high- speed diagnostic system to embark on understanding multiphase flows in this exciting new regime. Introduction and Background (Intellectual Merit): From the atomization in internal combustion to collisions and growth of dust particles in protoplanetary disks (Fig. 1(left)), particles in high-speed compressible turbulent environments is ubiquitous in nature and engineering applications. This two-phase interaction produces some key issues: (i) Multi-scale physics: particles interact with compressible turbulence with many length scales and coherent structures. (ii) The coupled interaction can lead to clusters of particles and droplets, resulting in enhanced collision rates and fast growth1. This is important for rain droplet growth in turbulent clouds, a type of incompressible turbulence. In this well-known regime, particles preferentially cluster in regions of low vorticity and high strain rate2, promoting collision but inhibiting mixing. This finding may have severe consequences in combustors, where mixing is desired for perfect combustion2. In compressible turbulence, there is a new structure – shocklet3. Rather than high vorticity or strain, it is a region of high pressure. Particles interacting with this new structure may alter the mean fields and turbulent characteristics of the flow4. A numerical study revealed that light particles cluster in very thin and long filaments on the shocklet surfaces5 (Fig. 1(right)), while larger inertial particles form dense clouds downstream of the shocklet. Hypothesis: A new particle clustering mechanism will become important in compressible turbulence due to the unique coherent structure – shocklet. Since shocklets are intermittent spatially and temporally, particles will interact with other low-speed eddies as they would in incompressible turbulence. The dynamic competition between Fig. 1. (left) Artistic illustration of protoplanetary dust. these two clustering mechanisms will pose an (right) Numerical simulation iso-surface displaying interesting new regime where the particle light particles (green) and eddy shocklets (orange)5. collision rate may be sensitive to small changes of the dimensionless groups, such as the turbulence Mach number, Stokes number, and Reynolds number. Aim 1: Upgrade current 2D experimental facility to a supersonic turbulent environment To generate a turbulent and supersonic environment, I will construct a facility to produce a 2D supersonic mixing layer using two opposing parallel supersonic streams, a method that has been used previously to visualize shocklets3. The two jets are separated by a vertical distance, enabling us to capture the interaction between the jets through the shear layer. Particles will be tracked using an in-house particle tracking code. I have access to different algorithms developed in the lab, which includes both 2D tracking and the 3D Shake-The-Box Method6. We have access to the Shimadzu HPV-X2 camera, which captures images at 10 million frames per second at exposure times 1 Graduate Research Plan Statement Juan Sebastian Rubio of 200 ns. With this camera I will take time-resolved measurements. We also own several Phantom high-speed cameras, which can take 40,000 images during one run. With this camera I can perform multi-scale measurements and conduct extensive statistical analysis of the flow. To obtain gas phase velocities, I will seed the flow with sub-micron sized particles and illuminate them with an in-house burst-mode laser (20 mJ per pulse at 100 kHz) and analyze the results using particle image velocimetry, a method I used previously both during my undergraduate research and at Los Alamos National Laboratory under the supervision of Dr. John J. Charonko. With this innovative experimental setup, I will obtain high-quality experimental images for further analysis. Aim 2: Investigate particle physics and its effect on the flow properties Because eddy shocklets are structures with strong compression Particle bow shocks regions, they can be visualized with either shadowgraph or Schlieren imaging methods. Thanks to the quasi-2D configuration of the proposed setup, shocklets will be visualized by the Schlieren method. I have already begun preliminary work to study particle motion using the underexpanded particle-laden jet facility in our lab. With the current experimental setup, I already visualized oblique and normal shocks using the Schlieren method. I also visualized the bow shocks that form around individual particles Fig. 2. Shadowgraph experiments (like eddy shocklets) at high frame rates (Fig. 2). Finally, the using the compressible particle- statistics of particle clustering will be evaluated using the Voronoi laden jet facility at Johns Hopkins analysis and the correlation between each of the particle’s location. depicting particle bow shocks. Aim 3: Study the turbulence by characterizing coherent structures - shocklets The complex coupling covers a multi-dimensional space. Compressible turbulence can be quantified by both the Reynolds and Mach number, which includes both the free stream and fluctuation velocity. Small particles can be characterized by the Stokes number, particle Reynolds number, and particle Mach number. I hypothesize that the particle Stokes number and particle Mach number are key parameters that may control the couplings between the two phases. A large Stokes number results in a large particle inertia, which may induce a strong particle-based shocklet. In addition, the particle mass loading is important. As in the incompressible case, the low mass loading may be key in particle-turbulence interaction, but the particle-particle interaction or particle-turbulence two-way couplings will not be important when the mass loading becomes large. I will spend part of my thesis to understand the behavior of particles in different parameter regimes. Broader Impacts: Improving our understanding of particle-laden compressible turbulence will have significant impact in numerous areas of science and engineering. It may provide new insights as to how the universe formed. To aid researchers around the world, I will store our experimental data in an open source repository. Outreach: I will mentor underrepresented undergraduate students on a semester basis, provide them with the opportunity of assisting me with experiments, and enable them to co- author in research papers. Through the Johns Hopkins SABES program, I will perform simple experiments to show students the importance of particle behavior in the environment. 1. Pan, Liubin, et al. The Astrophysical Journal (2011). 2. Squires, Kyle D., & John K. Eaton. Physics of Fluids (1991). 3. Papamoschou, Dimitri. Physics of Fluids (1995). 4. Chen, Chang H., & Diego Donzis. Journal of Fluid Mechanics (2019). 5. Yang, Yantao, et al. Physics of fluids (2014). 6. Schanz, Daniel, et al. Experiments in fluids (2016). 2	Winner!
76	Life Sciences: Systematics & Biodiversity Background and proposal: Complex body plans evolve through the acquisition of heterogeneous material properties. In the late 18th century, architects began to combine rigid and flexible materials in order to achieve new levels of structural complexity in response to the limited space within urban areas. An analogous process happened in animals throughout the Cambrian explosion. During this time, complex animal morphologies diversified with the appearance of both rigid and flexible materials, such as skeletons, muscles, and fluid-filled cavities. The relationship between rigid and flexible materials and the production of diverse complex morphologies in skeletonized animals, such as arthropods and vertebrates, has been extensively studied. However, little is known about this relationship in gelatinous animals. In jellyfish (Cnidaria: Medusozoa), the underlying material properties are not well understood but still thought to play a major role in the evolution of diverse morphologies. Jellyfish have a free- living medusa stage (medusoid) with a bell that is responsible for locomotion1. Much of the medusa bell shape appears to be constrained to a concave ellipsoid, conserved across >2,000 species of jellyfish1. In stark contrast to this relatively simple medusa morphology, a unique subclade, the siphonophores (Cnidaria: Hydrozoa), exhibits morphologies well beyond this norm. Siphonophores are colonial animals that use asexual reproduction to produce physiologically integrated chains of individuals, called zooids. Siphonophores swim using retained, specialized medusoid zooids (nectophores), which propel the colony. Nectophores display astonishing diversity of extremely complex morphologies2,3 (Fig. 1). In order to achieve this diversity, siphonophores have something typical medusae do not: ridges and facets. In other systems, the development of ridges and facets is dependent on the distinct underlying material properties4. Like free living medusae, nectophores are composed primarily of mesoglea, an expanded extracellular matrix, which is a mesh network comprised mostly of collagen fibers located between epidermal and gastrodermal epithelia5,6. Anecdotal evidence of physically handling siphonophores indicates that the mesoglea has a wide variety of elasticity and stiffness both within a nectophore and across species depending on the presence of ridges and facets. It is unknown to what extent the ridges and facets of nectophores depend on the material properties of the mesoglea, such as the density, elastic modulus, and viscous modulus. Typical medusae, without ridges and facets, have homogenous mesogleal material properties. I hypothesize that complex nectophore morphologies with ridges and facets require heterogeneous mesogleal deposition within the nectophore. If supported, the nectophores with complex ridges and facets will have both regions with elastic properties and regions with viscous material properties. In contrast, the nectophores that lack ridges and facets will have homogenous mesoglea with material properties comparable to typical medusae. If the core hypothesis is not supported, it would mean that material properties and nectophore morphology can vary independently. If this is the case, I propose two alternative hypotheses: (1) heterogenous mesoglea evolved first facilitating the evolution of ridges and facets, and (2) ridges and facets evolved first, and were secondarily reinforced by A: Phylogeny & Morphology heterogeneous mesoglea. To Ridges/Facets N N Y N Y N Y (Y/N) test these hypotheses, I will characterize nectophore morphology and material Other Rosacea Vogtia Sphaeronectes Lensia Stephalia Agalma properties to test for Medusozoans phylogenetic correlations. I will also use the phylogeny to model trait evolution of the Siphonophorae Figure 1: Phylogeny and medusa morphology. Adapted from Totton (1932). Research Proposal Lauren Mellenthin morphology and material properties. This study will help understand not only how diversity in morphology arises, but how it evolves. Are mesogleal ultrastructure and correlated material properties phenotypically integrated with nectophore morphology? Answering this question recognizes the role material properties have in achieving morphological diversity in gelatinous animals. This work will contribute to the growing interest in the scientific community that realize diverse material properties and their critical role in the evolution of complex body plans. Methods: I will use museum specimens from the Yale Peabody Museum (YPM), which has ~180 species with ~2-3 specimens per species to describe the ridges and facets of nectophore morphology. Using literature and YPM specimens, I will delineate the presence and absence of ridges and facets across siphonophore species. Freshly collected specimens will be used for mesoglea and material property analysis. I will use scanning electron and differential interference contrast microscopy to image the mesoglea. To analyze material properties, I will use small angle x-ray scattering and rheology. These techniques characterize the structure of the mesogleal mesh at sub-micron lengthscales, density, and viscous and elastic moduli. All tools are available at Yale. Using a well resolved phylogeny7, I will test if morphology changes in congruence with mesoglea and material properties using phylogenetic mixed models in R statistical software8 with the pglm package. I will also test the order at which morphology and material properties arose via ancestral state estimation with the ape package. Feasibility: A risk of this project is the unpredictability of fieldwork to obtain fresh material for measurements of material properties. However, working with experts in siphonophore biology in the Dunn lab and collaborators will alleviate this risk. My previous research experience with Dr. Dean Adams in comparative morphometrics is directly applicable for questions of trait evolution. Former work at the Field Museum and current involvement in YPM has prepared me for handling museum specimens and contributing to collections for future generations. Collaborating with Dr. Alison Sweeney of Yale will contribute to the biophysical aspects of this project and her mentorship reduces radical differences between fields of physics and organismal biology. Intellectual Merit: Using and adding to YPM specimens leverage an important collection for a novel interdisciplinary project. Nectophores are a diagnostic trait of siphonophores and are used to characterize species, therefore this work will enhance what we know about siphonophore identification using gross morphology. Nectophore biology has broader implications for understanding how different zooid types contribute to colony integration. This work will also inform how extracellular matrix development has influenced metazoan evolution at early nodes in the metazoan tree, potentially with implications for body plan evolution. The emerging field of soft robotics currently use animal biomaterial properties to understand efficient fluid mechanics. A major breakthrough was a medusa-like robot. However, nectophores utilize heterogeneous soft materials to integrate a variety of functions beyond self-propulsion, allowing engineers to design robots exclusively of soft materials with extensive functional repertoires. Broader Impacts: I plan to bridge the disparate fields of physics and evolutionary biology, by sharing this work. Additionally, by being a graduate affiliate of the Yale Peabody Museum, I am able to share my research and educate about natural history to a broader audience. I will take advantage of this unique platform that encourages global curiosity about ocean exploration and overall scientific curiosity to put my work in perspective. Importantly, these opportunities excite both the scientific and public communities about current interdisciplinary research. References: Megill, W.M., PhD diss., McGill University (1991)1, Carré, C., Carré, D. Ordre des siphonophores (1995)2, Totton, A.K. Siphonophora. (1932)3, Gibson, L.J. The Royal Society (2012) 4, Bergheim, B.G. Essays in Biochemistry (2019)5, Gambini, C. Biophys. J. (2012)6, Munro, C. Molecular Phylogenetics and Evolution. (2018)7, R Development Team. R Foundation for Statistical Computing (2008)8	Winner!
77	Core-collapse supernovae (CCSNe) are the spectacular explosions that accompany the deaths of massive stars. CCSNe have been the subject of ongoing research for decades but the explosion mechanism is still not fully understood. Proper treatment of the CCSN problem requires all areas of physics; in particular, general relativistic gravity, complex neutrino transport, turbulent magnetohydrodynamics, and nuclear physics. CCSNe are the primary engines of galactic chemical evolution; many of the elements heavier than H and He are synthesized here, those necessary to life in particular. Furthermore, a complete view of CCSNe is necessary for understanding the compact objects that arise from core-collapse, such as the binary neutron stars and stellar mass black holes that have been detected by Advanced LIGO and Advanced Virgo[1]. Intellectual Merit As current high-fidelity CCSN simulations lack the ability to properly predict electro- magnetic (EM) emission, I propose to investigate the explosion mechanism driving CCSNe by further developing the current CCSN simulation capabilities to pro- vide accurate EM predictions. While yielding far more insights into the processes at work in the CCSN, it will also allow for comparison with observational astronomy. This collaboration of full multimessenger signals is a yet untapped resource that could give new insights into the explosion mechanism. The proposed project is broken down into three stages: (i) upgrading our nuclear physics, (ii) getting 1D LC and EM information, and (iii) going to multiple dimensions. My background in stellar astrophysics and computational methods makes this project a natural next step. Upgrading our nuclear physics. In the hot interior of massive stars, material is said to be in nuclear statistical equilibrium (NSE), meaning that forward and backward reactions are balanced such that elemental abundances are given by relatively simple statistical relations. Current high-fidelity CCSN simulations assume that NSE is satisfied throughout the entire star. While this is a good approximation in the interior regions of interest most pertinent to the explosion, it breaks down quickly at large radii so that only the central regions can be accurately modelled. I will work to transition the equation of state (EOS) to the non-NSE regime in the FLASH code. This will allow for whole-star simulations and more accurate nucleosynthesis calculations. Inclusion of the outer regions of the star will also allow for modelling of the neutrino-driven wind – a supersonic outflow of stellar material powered by neutrinos emitted from the core of the star. This wind is proposed as a possible site of heavy element nucleosynthesis. We can then begin to meaningfully study the full nucleosynthetic signatures of CCSNe. Getting 1D LC and EM information. The ultimate goal of the study of the CCSN explosion mechanism is the ability to make predictions and understand observations. Armed with a more realistic EOS and accurate nucleosynthesis, I will study the EM signals emitted during a CCSN. To achieve this, I will use a new model for driving 1D explosions that includes the crucial effects of turbulence and convection and map simulation data into SuperNu[2], a multi-D Monte Carlo radiation transport code, to produce the EM signals. This is imperative as, to date, we have only one observation of a CCSN that includes any signals other than electromagnetic. The FLASH code is already capable of handling the gravitational wave (GW) and neutrino emission, so this extension will provide complete predictions of the multimessenger signals. With this, we can begin to make direct connections between physical conditions of the explosion and what is observed. An understanding of how, for example, uncertain nuclear physics affects the electromagnetic signals is crucial to the success of the CCSN problem. This will allow us to compare our findings to observations and, for the first time, connect observations of CCNSe with details of the progenitor stars. Going to multiple dimensions. The final goal of this project is the ability to run fully 3D CCSN simulations that for the first time include a proper treatment of the non-NSE EOS and EM information. This project will push the frontiers of current high fidelity 3D simulations and greatly enhance their explanatory and predictive powers. Due to the extreme computational resources required of these simulations, the simulations would begin during years 3-4 using computing allocations available to Dr. Sean Couch, the PI of the Michigan State University (MSU) research group. At this stage of the project, I will have the ability to study the full range of multimessenger signals from CCSNe including EM, GW, and neutrino signals in addition to the nucleosynthetic signatures of the explosion. With all of this in hand, we can make accurate and meaningful predictions of how the various physics that go into the CCSN impact the explosion, and how that in turn affects the observations. Broader Impact The work presented here will result in the advancement of our understanding of the CCSN explosion mechanism, galactic evolution, and ultimately, the origin of the elements including those that comprise life. The results of this work will promote constructive collabo- ration between theoretical stellar astrophysicists and observational astronomers. EM data produced through this project can be compared against observational data as a benchmarking tool and may also be used by observers to inform future studies. This project aligns with the goals of the DOE’s Scientific Discovery through Advanced Computing (SciDAC) initiative, as well as the National Strategic Computing Initiative, in the push to exascale computing. MSU houses the Joint Institute for Nuclear Astrophysics, National Superconducting Cyclotron Laboratory, Facility for Rare Isotope Beams, and a new De- partment of Computational Mathematics, Science and Engineering and as such it the ideal campus for this interdisciplinary work. In an effort to reach first generation students, I will create a chapter of Ask A Scientist at MSU utilizing my connections with the national organization. To best leverage the available resources, I plan to create collaborations with existing programs at MSU such as the 4-H Michigan Extension, MSU Science Fair, and first generation student mentor program. I will travel to rural Michigan schools to show first generation and low income students that a college education and career in science are options for them. Through this chapter of Ask A Scientist, these communities can continue to benefit after the conclusion of my graduate studies. Astronomy has amazing potential to transform both lives and communities1 and the NSF GRFP would give me the resources necessary to begin my career while using my research as a tool for change. [1] Abbott, B. P., Abbott, R., Abbott, T. D., et al. 2016, Phys. Rev. Lett., 116, 061102 [2] Wollaeger, R. T., van Rossum, D. R., Graziani, C., et al. 2013, The Astrophysical Journal Supplement Series, 209, 36 1 https://www.nature.com/collections/xtxtmqfrgf	Winner!
79	Background and Motivation: Observation of the 21cm hydrogen emission line has the potential to provide tremendous insights into the evolution of the universe, and is one of the most exciting frontiers in cosmology. Roughly 400,000 years after the Big Bang, the universe began cooling enough for neutral atoms to form in a period known as recombination. After recombination came a period known as the ‘dark ages’, during which the universe consisted mainly of neutral hydrogen. It gets that name because, although the universe was transparent, no stars had formed yet, so the only radiation was photons from the CMB and 21cm emission coming from the hyperfine spin-flip transition of neutral hydrogen. Thus far, researchers have been unable to directly observe this period of the universe. Eventually, gravitational collapse allowed the first stars and galaxies to form. Radiation from these galaxies then began ionizing the neutral hydrogen in a time period known as the Epoch of Reionization (EoR). Roughly one billion years after the Big Bang, reionization was complete, and the universe became observable again. My research aims at detecting the cosmological 21cm emission line, which will allow us to study the mechanisms driving the evolution of the early universe. Improving our understanding of this period of the universe is crucial to the field of cosmology. In the most recent decadal survey by the National Academy of Sciences, experiments aimed at detecting the cosmological 21cm signal were listed as the highest priority in radio astronomy [1]. There are many barriers to observing the 21cm emission line, including the weakness of the cosmological signal and instrumentation challenges. Most experiments searching for this signal use interferometers, rather than traditional dish telescopes, due to their higher resolution, which is particularly important for long wavelength radio waves. The signal we are searching for is 4-5 orders of magnitude weaker than the foreground sources, which means that our instruments must achieve an extremely high level of precision. With interferometry, calibration of the array can be extremely challenging, but without exceptionally precise calibration, systematic errors dominate the experiment and will prevent any attempt to recover the 21cm signal. There are two primary methods of calibrating an interferometer: sky-based calibration and redundant calibration. Sky-based calibration, performed here using Fast Holographic Deconvolution (FHD), relies on an a priori sky model to solve for the antenna gains. Incompleteness in current sky models has been shown to produce sufficient contamination of the power spectrum to obscure the desired signal [2]. Redundant calibration makes use of the redundancies in the arrangement of array antennas in solving the calibration equations. This method can largely be performed without the use of a sky model, and in fact an estimated sky model is actually produced during calibration without using prior knowledge [3]. My research will improve the redundant calibration pipeline and use the estimated sky model to inform the model used for sky-based calibration, thus increasing the precision of current calibration techniques. Research Project: For my research, I will work primarily with data from the Murchison Widefield Array (MWA), which is a low-frequency radio interferometer containing 256 tiles, each of which is composed of 16 dipole antennas. Specifically, my work will be based on phase II of the MWA, which has been running since 2016, and whose data is yet to be fully analyzed. The MWA provides unique opportunities for studying interferometric calibration because it is composed of two highly redundant hexagonal arrays and a pseudo-random extended array, which makes it workable for both sky-based and redundant calibration. I began working with this data set during my research as an undergraduate, where I created the first images of the sky model NSF Graduate Research Plan Statement Dara Storer produced through redundant calibration and uncovered sources of error that were propagating into the model. My undergraduate work was aimed at finding relative agreement between the estimated sky model produced through redundant calibration and the observed data. If achieved, this estimated sky model could be used to inform the sky model used for sky-based calibration, thus lowering the necessary precision of the a priori sky model. Through this research, I found evidence that positional errors in the antennas were propagating into the estimated sky model and contaminating the calibration solutions. As a first year graduate student at the University of Washington, I have already joined the radio cosmology group, led by Miguel Morales. This group is one of the leaders of the MWA and the Hydrogen Epoch of Reionization Array (HERA) collaborations, and has developed some of the most important data analysis pipelines for precision calibration. By joining this group, I have gained access to better resources and more robust software, which positions me well to study the calibration systematics I uncovered as an undergraduate. My research will proceed as follows: Phase I: I will run simulated data through the same pipeline I used on the real data in my undergraduate research, which will allow me to more precisely examine the systematics contributing to positional error propagation. I will subtract the model produced through redundant calibration of the simulated data from the a priori sky model given by FHD. This result will provide insight into significant sources of flux that may be missing from our sky model. Phase II: I will work with collaborators at the University of Melbourne, led by Professor Rachel Webster, to supplement any missing sources in the catalog currently being used for sky-based calibration. During my semester abroad in Melbourne I worked on a piece of code, PUMA, that is used to produce and combine source catalogs, so I am well prepared to study these catalogs further. Then, I will run the MWA Phase II data back through this adjusted pipeline, and reexamine the propagation of positional uncertainties. Phase III: I will work with the HERA collaboration as they begin collecting data from the telescope, which is expected to happen in Spring, 2019. I will compare the results from HERA with those from the MWA, which will allow me to better determine which systematics are specific to the instrumentation of the MWA, and which are due to the calibration pipeline. This research plan will allow me to systematically track and eliminate the propagation of position errors into the sky model and calibration solutions, which will greatly increase the precision of interferometric calibration, and bring us closer to a true detection of the 21cm hydrogen emission line. Broader Impacts: Observing the EoR will provide insight into what the early universe looked like and the processes that led to the formation of the first stars and galaxies. Understanding the early universe is fundamental to understanding the modern one, and measurement of the 21cm signal will have tremendous influence in almost every area of astrophysics. In addition to my research, I will work with Professor Morales’ group to continue their long history of supporting community college transfer and other historically underrepresented students through the CHAMP program, as detailed in my personal statement. [1] National Academy of Sciences, New Worlds, New Horizons in Astronomy and Astrophysics [2] Barry, N., Hazelton, B., Sullivan, I., et al. 2016, MNRAS, 461, 3135B [3] Li, W., Pober, J., Hazelton, B., et al. 2018, ApJ, 863, 170	Winner!
80	that microbes constantly pass through the human gastrointestinal (GI) tract. During transit, physical and chemical barriers such as peristaltic action, pH gradients, and intestinal enzymes protect the gut from microbial colonization. However, select bacteria can overcome these barriers by adhering to specific niches in the GI tract using surface proteins called adhesins (1). Adhesins locate bacteria in environments conducive to their growth, granting them a selective advantage. Therefore, both the surface properties and environment of a microbe are essential to their survival. While structural research regarding adhesins and their targets is expanding, little is known about the parameters governing microbial binding within the GI tract; describing these parameters therefore can lead to a fundamental understanding of bacterial attachment. Previously, complex microfluidic devices that simultaneously incorporate physiological characteristics, mechanical forces, and mammalian cell co-cultures have been used to study microbes in the GI tract (2, 3). While these are valuable models of in vivo activity, their complexity obscures analysis of how individual environmental conditions and the surface characteristics of microbes affect their attachment. To address this challenge, I have developed a modular platform to engineer the adhesive properties of live microbes and investigate their attachment under various conditions. This platform uses adhesive ligands conjugated to the bacterial surface as a proxy for adhesins, allowing tunable ligand density and specificity on bacteria. Using this system, the influence of bacterial surface properties on attachment can be analyzed in more detail than is possible with unmodified bacteria. Furthermore, by incorporating factors (either individually or in combination) such as bile salts, pH variation, fluid flow, and enzymatic activity, the influence of environment on bacterial attachment can be determined. I hypothesize that bacterial adherence in the GI tract is a consequence of both bacterial surface characteristics as well as environmental factors. By using a modular platform to control the microbe surface and its environment, the proposed approach can be used to study attachment of diverse bacterial species to biotic or abiotic surfaces under varying conditions. Aim 1: Determine how the specificity and density of adhesive ligands on microbial surfaces affect attachment. During my first year as a graduate student, I have developed a platform based on avidin and biotin binding to mediate the attachment of bacteria to targeted surfaces. Bacteria are first functionalized with biotin using N-hydroxysuccinimide (NHS) chemistry, which forms an ester bond between biotin and primary amines on the bacteria surface. Adhesive ligands (such as antibodies, as shown) are conjugated to streptavidin using amine-based chemistry and are then linked to biotin groups on bacterial surfaces following mixing (Figure 1A). To date, I have demonstrated that biotinylation of bacteria enhances binding to an abiotic streptavidin-coated surface (Figure 1B) and that attachment of Intracellular Adhesion Molecule (ICAM-1) antibodies enhances binding to live Caco-2 cells, a model cell line of intestinal 4000 3000 Biotin Targeting Ligand 2000 Bacteria 1000 0 0.0 0.2 0.4 0.6 Streptavidin Cell Density (OD600) ).u.a( ecnecseroulF 4000 TaTargrgeetteedd 3000 CCoonntrtrooll 2000 1000 0 Cell Density (OD600) ).u.a( ecnecseroulF A 5000 4000 3000 2000 1000 0 0.2 0.4 0.6 0 Control Targeted Figure 1. (A) Schematic of engineered microbe. (B) Concentration-dependent attachment of biotinylated GFP- expressing bacteria to a streptavidin-coated well plate. (C) Attachment of ICAM-1-targeted, GFP-expressing microbes to live Caco-2 cells compared to an unmodified control. ).u.a( ecnecseroulF C 5000 4000 3000 2000 1000 0 Control Targeted ).u.a( ecnecseroulF B enterocytes, in comparison to a non-targeted control (Figure 1C). To optimize this system, I will adjust the density and specificity of antibodies on bacterial surfaces. Antibody density is controlled by the extent of bacterial-surface biotinylation and will be varied using the ratio of biotin mass to bacterial density. The density of biotin sites on the bacteria surface will be quantified following fluorescent streptavidin probe attachment using flow cytometry. Next, the effect of antibody specificity will be determined with three anti-ICAM IgG antibodies (clones R6.5, 1A6, and 1A29) with varying specificity for ICAM on Caco-2 cells. Monolayers of Caco-2 cells will be grown in tissue culture treated well plates and incubated with GFP-expressing microbes that have varying antibody density and specificity. Bacterial attachment will be quantified using the fluorescence signal from attached bacteria, measured using a plate reader and analyzed for their spatial localization using microscopy. By enabling precise control over the surface characteristics of microbes, this aim provides clear insight into how these characteristics influence microbial binding. The antibody configuration (density and specificity) that provides the highest levels of attachment in Aim 1 will be further analyzed in Aim 2. Aim 2: Use Design of Experiments to analyze and optimize environmental parameters for microbial attachment. Design of Experiments (DOE) is a statistical 8000 approach to determine the sensitivity to and interactions between individual parameters that affect a system response (4). This approach 6000 will be used to screen four environmental factors for their relationship 4000 to microbial binding: bile salt concentration, pH, intestinal enzyme 2000 activity, and fluid flow rate. The factors will be tested at two levels that reflect the upper and lower limits experienced along the GI tract. 0 Microbial attachment will be determined using a GFP-expressing strain in well plates (for static studies) or a straight-channel microfluidic chip (for fluid flow studies) that I have previously validated for targeted microbial attachment (Figure 2). Attachment of microbes will be quantified both through automated particle counting in ImageJ and with fluorescence intensity in the well plates or microfluidic channels. Results from the screening experiments will be used to identify which of the factors significantly influence microbial attachment. A secondary study with the identified factors will be designed to determine a response surface model and a desirability profile for microbial binding in varying environmental conditions. This will be used to estimate the conditions that optimize microbial attachment, which will be validated experimentally. By optimizing the environmental conditions, this model can determine the location of the GI tract that is most conducive for a bacteria’s attachment. Broader Impacts. This interdisciplinary proposal applies techniques from materials science, engineering, microbiology, and biochemistry to analyze parameters that influence microbial attachment and represents a practical, tunable system for modifying the adhesive properties of microbes. Successful completion of this project will provide valuable insight into the mechanisms of microbial colonization of the human GI tract. More broadly, due to the ubiquity of microbes in the biomedical sector, modular platforms that can be used to provide mechanistic insights into a variety of microbes on both biotic and abiotic surfaces are sorely needed. As research is most impactful when communicated directly to the public, I will teach middle school students the profound role bacteria play in our lives and our environments through Morehead Planetarium’s SciMatch program. Additionally, I will broaden the reach of this research with a novel art/science collaboration designed to increase public scientific literacy in Chapel Hill. [1] Stacy et al. Nature reviews. Microbiology 14, 93-105 (2016). [2] Kim et al. Lab on a chip 12, 2165-2174 (2012). [3] Bhatia and Ingber. Nature Biotechnology 32, 760 (2014). [4] Anderson et al. Productivity Press (2007). slleC dehcattA Control Targeted Figure 2. Attachment of biotin-conjugated bacteria to streptavidin-coated channels under flow (1μL/min)	Winner!
81	Foreword: Because I am a BioMAT trainee with two years of NIH support, my adviser, Dr. Shuichi Takayama, allowed me to build my own project. Motivated by my interest in immunopathology, I independently created this project after becoming familiar with the work of our established airway disease collaborator, Dr. Rabindra Tirouvanziam at Emory University. My adviser plans to expand my idea into a full grant proposal that I will write with him. Motivation: Inflammatory airway diseases (IADs), including COPDa, CFb, and asthma, are the fifth leading cause of mortality globally.1 Chronic pulmonary inflammation can lead to fibrosis, recurrent infection, and loss of lung function that requires transplant. Neutrophils (PMNsc) are the perpetrators of this excessive inflammation, making them a target of therapeutics and motivating the study of mechanisms driving their pro-inflammatory conditioning. plug In vivo models are insufficient due to significant differences in the 1. generator function of inflammatory mediators and disease phenotype. They also do not offer the ability to systematically eliminate confounding variables, making in Apical vitro models attractive for mechanistic studies. In typical in vitro studies, channel investigators expose either blood PMNs or epithelial cell lines to a disease- Alvetex related stimulus (e.g. smoke, bacteria) in a static system and measure cells’ membrane responses. Despite these efforts, no therapeutics have yet halted the cycle of damage inflicted by PMNs, motivating more sophisticated mechanistic Basal channel studies to inform therapeutic strategies. In vitro models neglect fluid mechanical stress (FMS, caused by thickened lung mucus) and PMN transmigration into the lumen despite 1. plug 2. trans- evidence that severe, FMS-induced crackling sounds are one of the top three propagation migration 2 . clinical predictors of poor prognosis in CF and COPD,4,5 and PMNs that transmigrate into diseased lung fluids acquire a pro-inflammatory phenotype.2,3 These processes may be connected: thickened mucus causes apical ASL FMS-induced epithelial inflammation that recruits PMNs and makes them epithelium pro-inflammatory. Stressors are known to induce sterile inflammation in collagen many cell types.7 Bronchial epithelial cells produce exosomes, cytokines and Alvetex miRNA in response to compressive mechanical stress, and hypoxia-stressed endothelium epithelial cells release exosomes that activate proinflammatory signaling in basal ch. macrophages.8,9 I hypothesize that fluid mechanical stress induces sterile inflammation of the epithelium resulting in epithelial exosomes, miRNA and Fig. 1: Device design. Fig. 2: Cross-section. cytokines that contribute to the inflammatory phenotype of PMNs. The objective of my research is to establish a novel microfluidic model of pulmonary inflammation, incorporating FMS and PMN transmigration, to discover pro-inflammatory pathways that are inaccessible with current models. Aim 1: Design, develop and optimize the “lung inflammation-on-chip” microfluidic device. I am currently modifying the lab’s established lung device,2 which already includes FMS, to incorporate PMNs by adding a porous AlvetexTM membrane that our Emory collaborators use to model PMN transmigration.3 To generate confluent, primary epithelial and endothelial cells on the membrane, I will adapt established Transwell coculture methods: I will continuously flow media on both sides until confluence, which I will evaluate with trans-bilayer electrical resistance and staining for tight junctions.10 To differentiate the epithelial cells, I will remove media from the apical channel and flow 5% CO air for 14 days with media flowing in the basal channel. Liquid 2 plugs will be generated in the absence of neutrophil flow with PBS + 1.2 mg/mL of the surfactant Survanta; the target speed of the liquid plug is 2 mm/s at an air pressure of 1 kPa to model physiological conditions of sublethal stress.11 Liquid plug speed and pressure drop will be measured with established methods from our lab.2 Viscosity of the plug-generating fluid will be measured with rheometry. Computer-controlled electromechanical actuators will create the air flow switching that generates liquid plugs. To validate transmigration, neutrophils will flow on the basal side of undamaged epithelium and the apical side will be incubated with either a) RPMI control or, to induce transmigration, b) RPMI+100 nM LTB4 or c) patient airway surface liquid (ASL). PMN analysis described in [3], including flow cytometry and measurements of metabolism and bacterial killing, will validate that the model produces proinflammatory PMNs.3 Aim 2: Model stress-induced inflammation and infer novel inflammatory networks Epithelial cells will be exposed to 0, 6, 12 or 24 hours of liquid Liquid PMN plugging at a constant rate and pressure drop (5 plugs/min and 1 kPa), and plugging transmig. then PMNs will transmigrate through the stressed epithelium (see Figs. 1- collect apical & basal fluid 2 for diagrams and Fig. 3 for workflow). Exosomes will be isolated with an ExoQuick kit and lysed with 5% Triton X. 40 cytokines (including IL- PMNs : flow supern atant 8, CXCL1, IL-1β, IL-6, and IL-10) will be measured with Luminex assays, cyt, assays + exosomes from [3] and neutrophil elastase (NE) will be measured with ELISA for intra- and soluble cyto- extra-exosomal groups (extracellular NE activity is a predictor of lung e xosomes kines, m iRNA function in CF adults3). miRNA will be isolated from supernatant and e xosomal Lum inex, exosomal lysate with the miRNeasy Mini Kit. miRNA microarrays will cytokines, microarray, identify frequently occurring miRNAs in the supernatant and exosomal miRNA RT-PCR lysate. Quantitative RT-PCR will validate the microarray data.12 I will Fig. 3. Aim 2 workflow. compare the ASL and PMNs from models with stressed and unstressed epithelium, and I will include no-PMN devices as controls. I will also compare transmigrated and non-transmigrated PMNs from the same device. These comparisons will be made at all 4 timepoints so I can evaluate how the system evolves from healthy to diseased over extended stress exposure. To interpret my data, I initiated a collaboration with Dr. Kelly Arnold at Univ. of Michigan, whose lab specializes in analyzing COPD patient sputum and blood using data-driven computational approaches to infer cytokine networks driving cell behavior, tissue phenotype and disease progression. The Arnold lab will use systems biology computational methods to identify inflammatory PMN markers from my flow cytometry data and infer proinflammatory cytokine networks between epithelium and PMNs from the 40-plex Luminex assays. My hypothesis is correct if a) PMNs that transmigrate through stressed epithelium are pro-inflammatory (based on assays from [3] or cell surface marker expression) and b) proinflammatory cytokines or miRNA are produced by the epithelium in response to FMS that stimulate PMN inflammation. Broader Impacts: I am uniting leaders in microfluidics, immunology and systems-level data analysis to engineer a novel IAD pathology model and discover immunological mechanisms that will inform therapeutic design, ultimately reducing lung transplants and extending lifespans. This work is applicable to all IADs and the device can also be used to model immunopathology of pneumonia, lung cancer, or idiopathic pulmonary fibrosis. I will disseminate my results in publications, conferences and seminars with incarcerated people, and I will mentor undergraduates and minority high school students through ENGAGES. ● achronic obstructive pulmonary disease. bcystic fibrosis. cpolymorphonuclear leukocytes. Ref: 1“(COPD).” WHO, 2017. 2Tavana et. al (2011) Biomed. Microdev. 3Forrest et. al (2017) J Leukoc Bio. 1-11. 4Konstan et. al (2007) J Peadiatr 151:134-9. 5Jacome et. al (2017) Clin Resp J 612-620. 6Unpub., Tirouvanziam Lab 7Fleshner et. al (2017) Trends in Immuno 38(10):768-76. 8Park et. al (2012) Mech. of Allergy 130:1375-83. 9Moon et. al (2015) Cell Dth 6. 10Hermanns et. al (2004) Lab Invest. 84:736-52. 11Yalcin et. al (2007) J App Physio 103(5):1796:807. 12Ohshima et. al (2010) PloS ONE 5:e13247.	HM
82	Motivation: Hydrogenation is an especially important industrial reaction that finds uses in pharmaceutical, agrochemical, fragrance, and fine chemical synthesis. 10-20% of chemical reactions at Roche (the world’s third largest Biotech company) are catalytic hydrogenations1, and hydrogenation of N to NH consumes an estimated 2% of the world’s energy supply. 2 3 Supramolecular chemistry utilizing adjustable hemilabile ligands that approximate enzymatic activity has been an extremely active area of research, and hydrogenation can be improved significantly with respect to its chemoselectivity by tuning the catalyst to minimize over- hydrogenation. Mirkin et al. have constructed elegant ‘molecular tweezers’ that take advantage of chloride binding and supramolecular interactions to create switchable on/off catalytic turnover2. Miller et al. have recently published a PCN (phosphorous-carbon-nitrogen donor atoms) pincer ligand that takes advantage of macrocycle-cation interactions to enable both switchable and tunable catalytic activity2; however, this ligand’s catalytic potential has not yet been fully realized. Cation interaction with the macrocycle can speed the catalyzed reaction rate of olefin isomerization by over three orders of magnitude3. In situ control of catalytic activity via hemi-lability of crown ether/cation interactions will revolutionize homogenous hydrogenation catalysis. The ability to completely quench catalysis via addition of an anion, resulting in precipitation of a simple salt and re-coordination of the unoccupied crown-ether to the catalytic metal center, will give a new degree of control to hydrogenation catalysis. This will eliminate over-hydrogenation, a well-documented issue with isophorone, an essential polycarbonate precursor4 as well as other feedstocks. Enzymes are notable for their ability to use first-row transition metals to catalyze a wide variety of molecular transformations. These metals are generally more abundant, less toxic, and easier to dispose of than their heavier isoelectric counterparts. Replacing precious metals with ‘greener’ options is of vital importance to sustainable chemistry. Hanson et al. have reported an air- and water-stable Co(II) complex with a pincer ligand that is capable of hydrogenating alkenes, aldehydes, ketones, and imines at low pressures of gaseous hydrogen (1-4 atm) and low temperatures (>60°C)5. A Re complex with PNP was recently reported to activate N , but does 2 not have switchable or tunable properties6. Research Plan: Pincer ligands are attractive due to their thermal and oxidative stability as well as ease of modification. I hypothesize that installing a hemi-labile macrocycle to a PNP ligand will allow significant switchable and tunable activity of an established homogenous hydrogenation catalyst. The synthesis of a novel ligand is shown in Scheme 1 using commercially available starting materials and known methodology. The phosphine, macrocycle, secondary amine, and metal ions can all be varied to generate a large library of Scheme 1: Synthetic route for L. X= Halide; M= complexes for catalytic analysis. The double Co (II), Ir (II), Re (II), Rh (II); M’=Na+, Li+, Ca2+; crown ether moieties are hypothesized to R=Ph, Me, Cy. increase the tunability of catalysis even further than the published example by Miller et. al. Tunable steric bulk arising from the cation complexed crown-ethers, is hypothesized to allow for substrate specificity in hydrogenation. Coordination of a vacant crown-ether to the catalytically active metal will block the association of H that must take place for catalysis to occur; however, when a Group 1 or 2 cation 2 Kevin Michael Wyss – Research Proposal binds to the crown ether, it will detach from the Co(II) and allow H to access and be activated by 2 the transition metal (Scheme 2). My first goal will be to optimize of the synthesis and purification of L. Characterization of L will include single crystal XRD, mass spectrometry, and NMR. Determination of the stability of L to air and water, and its solubility, will be essential to further analysis of the complex. My second goal is to assess the ability of L to catalyze the selective hydrogenation of furfural. Furfural is a popular industrial feedstock that is a bio-based renewable building block for a wide range of polymers and fertilizers. Selective hydrogenation of furfural to furfuryl alcohol is a vitally important part of Scheme 2: Showing the switchable activity of L. functionalization, and often requires the use of noble metals10. It is presumed that hydrogenation using Ir(II) will occur readily for a variety of alkenes, alkynes, imines, and carbonyls through redox pathways as there is significant precedent for this in literature. A mechanism showing the hypothesized hydrogenation is shown in Scheme Scheme 3: A proposed mechanism for the 3. Besides furfural, it is hypothesized that this hydrogenation of furfural to furfuryl alcohol. same mechanism should allow for selective hydrogenation of substituted benzaldehydes and cyclohexenes, two other classes of molecules that that are of high industrial importance. Screening multiple L complexes with an array of substrates, under varying reaction conditions will identify trends to investigate in further studies. Addition of anions to the reaction solution is hypothesized to result in a switchable activity. The identity of the cation used to dislodge the crown-ether from the active site, is also expected to result in tunable rates of reaction, with Group 2 likely exhibiting the fastest rate as the fit best within the crown ether. I will then determine whether earth abundant Co(II) can substitute Ir(II) in the catalysis. Hanson et al. assert that the mechanism of hydrogenation proceeds through a Co(II) hydride intermediate, and that the coordination of a secondary amine to the metal center is essential5. It is hypothesized that although 3d catalytic pathways often favor one-electron processes due to size, as L still contains the secondary amine, hydrogenation via reductive elimination of the hydride intermediate can still occur. Use of deuterium-labeled H and solvents to probe the catalytic 2 mechanism will result in crucial fundamental research in this budding class of catalysis. Mechanistic study of catalytic cycles is an important area of fundamental research, and it is necessary to improve selectivity and turnover, or to design complementary catalysts. Broader Impacts and Intellectual Merits: Earth abundant metals are cheaper, safer, and greener alternatives when used as catalysts, and this is an important aspect of establishing sustainable chemistry as well as increasing the long-term economic security of the U.S. by decreasing dependence on unstable sources of precious metals. The work will also contribute to our fundamental knowledge of switchable and tunable catalysis which is a relatively new and unexplored area. 1) Curr Opin Drug Discov Devel., 2001, 4(6), 745-755. 2) Science, 2010, 330, 66-69. 3) J. Am. Chem. Soc., 2014, 136, 14519-14529. 4) J. Am. Chem. Soc., 2015, 137, 12121-12130. 5) J. Am. Chem. Soc., 2013, 135, 8668-8681. 6) J. Am. Chem. Soc., 2018, 140, 7922-7935. 7) ACS Catal., 2017, 7, 1720-1727. 8) Russ. J. Gen. Chem., 1986, 56(8), 1777-1781. 9) J. Organomet. Chem., 2017, 845, 82-89. 10) Sci. Rep., 2016, 6, 28558.	Winner!
83	Introduction: Conventionalseismicmomentresistingframes(SMRFs)aredesignedtoresist anddissipateseismicenergybytransferringtheloadsanddistributingpermanentyieldingtothe primary members, such as beams and columns. While this design philosophy performs well in providing strength and collapse resistance to a structure, damage to major structural members presents a major drawback in economical repairs. Therefore, most recent alternative designs not onlyseektodissipateseismicenergyandavoiddamageintheprimarymembers,butalsotomitigate damageintheconnectionsthemselves. Inessence,thedevelopmentofsuchaconnectionincreases seismicresiliencyinstructures,avoidingexpensiveanddisruptivereplacementofentiremembers whilealsolimitingexcessivedeformationsattheconnections. Onesuchmethodistheslidinghinge joint(SHJ):1 Figure1showsthe proposedSHJconnectionwithmodifications. Theflangesofthe beamareconnectedtoslidingplateshingedtothecolumn. Thus,asthecolumnandbeamrotate duringseismic events, theconnection’srotation causesthe platestoslip, andenergyis dissipatedin theformoffriction. Because the moment resistance is dependent on the friction provided betweenthe platesand flanges, challenges intheir design includeensuringthattheconnectionsstillprovideadequatestrength and safety under service. Additionally, the benefit in mitigating theneedtoreplaceconnectionsafterdamagecanonlyberealized by ensuring that the joint return to its original position. Since it is desired that the connection also be economical and simple to build, the proposed research will be the first to examine the performance of posttensioned (PT) strands as the self-centering mechanismcoupledtoaslidinghingejoint(SHJ-PT).Theproposed study will be focused around the achievement of the following primaryobjectives: 1)developmodelsfortheSHJconnectionand Figure1: SHJconnectionwith PTstrands,and2)performsimulationandanalysesonthemodels PTstrands1 and,resourcespermitting,experimentaltestingofthesubassembly. Hypothesis: Posttensionedstrands providean economical, easily-constructable, and geometri- callyflexiblemechanismofrecenteringfortheslidinghingejointconnections. Objective 1: Development of Models for SHJ and PT Strands The proposed connection consists of a beam and column connected with shear plates. The SHJ as described by Clifton (2005)2 willbeemployedasthemethodofenergydissipation. AsshowninFigure1,steelstrands paralleltothebeamareanchoredtothecolumnsinordertostressthebeam. Whiletheexperimental testing of full-scale structures under seismic loads is one method to examine response behavior, suchtestsarecostlytoperformandgenerallystillrequiresupplementalanalysestoassesslocalized materialresponse. Expandingonexistingmethods,3 ananalyticalprocedurewillbedevelopedto establishtherelationshipbetweentheconnectionrotationandthemechanicalresponsedevelopment in the PT steel strand. For a range of rotation induced by seismic loads, the response of the PT strandwillbedeveloped. SimilartoworkbyKhoo(2012),4 theSHJcanbemodeledusingasystem of rotational springs, thus providing the SHJ’s response to seismic load. When coupled with the previousmethod, thetotalstructuralresponseandbehavioroftheSHJ-PTcanbestudied; output 1GCCliftonetal.“Slidinghingejointsandsubassembliesforsteelmomentframes”. In: 2007. 2GCClifton. “Semi-rigidjointsformoment-resistingsteelframedseismic-resistingsystems”. PhDthesis. 2005. 3ConstantinChristopoulosetal.“PosttensionedenergydissipatingconnectionsforsteelMRFs”. In: (2002). 4Hsen-HanKhooetal.“Developmentoftheself-centeringSHJwithfrictionringsprings”. In: (2012). 1 strainsanddisplacementsfromthePTstrandsmodelarepassedtotheSHJspring-model’srotations andanalyzed (andvice versa)in orderto fullyunderstandthe interactionbetween thetwo systems. Additionally,finiteelementmethodanalysiswillbeperformedtocheckthatthestressesontheplate donotexceedthefailurelimitsoftheplates2. Objective 2: Perform Seismic Simulations and Analyses on Models Working with Dr. Pa- tricia Clayton (UT), I will numerically investigate the performance of the SHJ coupled with PT strands under earthquake simulations. Using the component models developed under Objective 1, building models will be constructed. These building models will be subjected to time-history analyses simulating a series of ground motions representing the maximum considered event and servicelevel earthquake. In thisobjective, thegoalsare toarrive atadequate responseresultsfrom whichaproceduretodeterminedesignparameterscouldbeestablished. Asametricofachievement, the results will be subjected to validation using existing test data from previous studies5,6. Upon the model structure’s achievement of data validation and providing sufficient response to bench- markloads,thecomputationalstudycanbeexpandedtoobservetheperformanceofthestructures utilizingvariationsofthedesignedconnection,suchasusageataspecifiedspacingsandbaysor with varying strengths of the sliding hinge plates. Resources permitting, testing of the proposed SHJ-PTconnectionsubassemblycanbeperformedusingthefacilitiesattheFergusonStructural EngineeringLaboratoryattheUniversityofTexas. IntellectualMerit: Thoughthereisexistingresearchonthedevelopmentoftheslidinghinge joint6,1,2,4 andtheanalysisandtestingofreplaceableconnectionsutilizingPTstrands7,8,5,efforts to couple these concepts in a design have yet to be extensively studied. Results9,4 from previous simulationsand physical testinghavedemonstrated theSHJ’sability to mitigate damage aswell as itspotentialtobecoupledwithself-centeringmechanismsandwarrantsfurtherresearchintonovel designs. Asitstands,theSJH-PT’sadvantagesare: 1)mitigationofdamagetoprimarymembers andlimitingdamageontheconnection;2)constructionusingconventionalmaterialandskills;3) abilitytoself-centerafterevents. Thesebenefitswarrantfurtherstudyintotheconnections’strength andserviceabilitypotentials. ThestudyalsoaimstoaddressconcernsbasedonpracticalityofPT strands in connections based on the effects of gap-opening and the compression that the strands induceinthebeamflanges1. BroaderImpacts: DrawbacksincurrentpracticeinSMRFsthatemployweldedconnections include seismic damage to beams and columns that require disruptive and costly replacement of thecomponent. ThesuccessfuldevelopmentofSMRFsusingenergy-dissipationmethodsmeans removingtheneedtoreplaceconnectionsafterseismicevents, leadingtoamoreflexiblerecovery processforstructuresafternaturaldisasters. Thisplaysahugeroleinpresentingretrofittingoptions toboostresilience forstructuresinareaswhere seismicrisksarerisingdue toenvironmental and industrial impacts. I will also work to increase the acceptance of the SHJ-PT connection in the structuralengineeringcommunitythrough1)publicationsinnotablejournals,suchasJournalof StructuralEngineeringandJournalofConstructionalSteelResearch,2)attendingconferencesinthe discipline,and3)workingcloselywithindustryentities,especiallythosealreadywithproprietary work in semi-rigid moment connections such as Simpson Strong-Tie and Skidmore Owings & Merrill,inordertofurtherdevelopandpopularizeenergydissipationbasedSMRFconnections. 5JamesMRiclesetal.“Posttensionedseismic-resistantconnectionsforsteelframes”. In: (2001). 6HHKhooetal.“Experimentalstudiesoftheself-centeringSlidingHingeJoint”. In: (2012). 7Ying-ChengLinetal.“Seismicperformanceofalarge-scalesteelself-centeringMRF”.in: (2012). 8PatriciaMClaytonetal.“Seismicdesignandperformanceofself-centeringsteelplateshearwalls”. In: (2011). 9GregoryAMacRaeetal.“Theslidinghingejointmomentconnection”. In: (2010). 2	Winner!
84	Improving Bounds on the Entropy of Odd Cycle Graphs Keywords: graph entropy, independent set, indistinguishability Introduction The entropy, also known as the Shannon capacity, of a graph is an important quantity in information theory, and can be used to study the zero-error capacity of a noisy communication channel. This channel can be represented as a cycle graph G in which each vertex represents a transmitted symbol and each edge indicates indistinguishability between symbols. A cycle graph is a graph which consists of a single cycle, i.e. a series of vertices connected in a loop. For instance, the cycle graph C (shown in 5 Fig. 1 with one of its independent sets in blue) represents a communication channel with five distinct symbols (herein called a, b, c, d, and e) in which adjacent symbols can be mistaken for each other due to noise in the channel. Figure 1. The The question posed is to determine the most efficient communication schema graph C with an 5 independent to transmit data with no errors and maximize precious band-width, and this indicated in blue. information density is encapsulated by the quantity known as graph entropy. Background Due to their graph theoretic properties, the entropy of all even cycle graphs is known. The same quantity is far more elusive for odd cycle graphs, however. In 1979, Lovász famously determined the entropy of C to be √5, but the entropy of C , for all odd p ≥ 7, is unknown. In a 5 p 2017 paper, Mathew and Östergård [1] used a stochastic search of independent sets guided by possible symmetries to establish the current best bounds on the entropies of C for p up to 15. p Even in the few short years since their research, computers have increased significantly, presenting the opportunity to further improve these bounds by using new algorithms, high performance computing, and theoretical results. Proposal To further improve the known bounds on odd cycle graph entropy, I propose using today’s increased computing power to run a variety of stochastic independent set search algorithms in parallel on high-performance computing clusters. Determining the entropy of a graph involves maximizing the size of its independent set, and the hope is that this search will yield at least a slight improvement in the previous bounds found in [1], particularly on the entropy of C . 7 Another source of potential untapped by Mathew and Östergård is the algorithmic Lovász local lemma, proven to succeed by Moser and Tardos in 2010 [2]. Because there is a natural family of local modifications to be made to a graph’s independent set, the lemma gives an algorithmic way to explore the space of independent sets. A third approach is to turn the problem of finding a graph’s entropy by constructing a maximal independent set into a boolean satisfiability problem (abbreviated SAT) and apply a SAT solver. Over the last decade, the field of SAT-solving has produced numerous sophisticated and effective methodologies, yielding a variety of strategies for approaching the entropy problem [3]. Methods I plan to focus initially on the entropy of C , in three stages: stochastic independent set search 7 algorithms, application of the algorithmic Lovász lemma, and use of multiple SAT-solving strategies. Noemi Glaeser Graduate Research Plan NSFGRFP 2018 In the initial stage, I will attempt to improve the bounds on entropy by writing code to probabilistically constructing independent sets. With various start configurations (an empty set, a random set, or a simple suboptimal construction, for instance), I will allow an iterative program to run for a limited time span, attempting to add points to the set. The method of adding points can be varied, e.g. allowing replacement of one point for another, or two points for another, but never removing more than two points at a time. Another approach is to simulate physics in the search, for instance by favoring the addition of points that produce more rigid configurations, which would result in the lattice structure suspected in optimal packings. Each algorithm will be optimized and modified to run in parallel on a high-performance computing cluster. Once the improvements from the initial stage have been exhausted, I will move on to applying the algorithmic Lovász local lemma to the problem. This involves a similar construction of the independent set, but one that allows the insertion of illegal points and adjusts the existing structure to restore a legal configuration. These changes propagate outward from the point of origin and are guaranteed by Moser and Tardos to eventually stabilize. Finally, the problem can be redefined in terms of a Boolean expression to be satisfied by the largest possible independent set. At this point, several free and open source third-party SAT solver algorithms, some of which are highly parallelizable, can be applied to the expression. The expression may also be rewritten in various ways, and the algorithms again applied, to improve the chances of a favorable result. The practice of applying SAT solvers is becoming increasingly effective in addressing well-known problems, notably in [4]. Conclusion Intellectual Merit Should these approaches prove successful, they can be applied to similar problems, particularly the entropies of C with p ≥ 9. The algorithms developed may also prove to be useful for other p problems in graph and information theory, in particular the outline of the iterative stochastic program, the physics-inspired approach to packing problems, and the novel application of the algorithmic Lovász local lemma. Broader Impact A more accurate understanding and estimate of the entropy of these odd cycles has direct implications for the definition of error-correcting codes. Knowing the entropy of C , for instance, 7 offers a constructive proof of the existence of a specific optimal information density, also yielding the construction of a 7-symbol encoding mechanism that realizes this density. This will lead to more efficient but still error-free communication through noisy channels, which could impact all digital communications, but in particular unreliable modes such as the satellite communication by phones, internet, and even space probes. [1] Mathew, K.A., & Östergård, P.R.J. (2017). “New lower bounds for the Shannon capacity of odd cycles”. Designs, Codes and Cryptography, 84: 13-22. doi:10.1007/s10623-016-0194-7. [2] Moser, R.A., & Tardos, G. (2010). “A constructive proof of the general Lovász local lemma”. Journal of the ACM, 57(2): 1-15. doi:10.1145/1667053.1667060. [3] Gong, W., & Zhou, X. (2017). “A survey of SAT solver”. AIP Conference Proceedings, 1836(020059): 1-10. doi: 10.1063/1.4981999 [4] Heule, M.J.H., Kullmann, O., & Marek, V.W. (2016). “Solving and Verifying the Boolean Pythagorean Triples problem via Cube-and-Conquer”. Lecture Notes in Computer Science: 228–245. doi: 10.1007/978-3-319-40970-2_15.	Winner!
85	[1], a major gap still exists between the skills of these graduates and the skills needed for industry success [2]. Two major components of this gap are programming skills (e.g. code style) and computational thinking (e.g. extending algorithms). My research will enable teachers to close these gaps by building a toolkit to integrate better-targeted problem types than those currently in use. Programming assignments in CS classrooms, primarily code tracing and code writing, lack the granularity to target students’ Zones of Proximal Development (ZPD, i.e. challenging but solvable problems). Code tracing questions involve reading and understanding code, but they may not trigger students’ mental models of concepts because they are tedious or too easy [3, 4]. Code writing questions are a large leap from code tracing, conflating many programming skills into a single solution (e.g. design, computational thinking, programming, code style) and supporting a wide space of disparate approaches and solutions. Extrapolating from earlier results [3], I propose the use of Parsons problems (e.g. [5]) to help students remain in their ZPD while learning computational thinking and programming through advanced CS concepts. Parsons problems involve unscrambling chunks of code, often individual lines, from a target program into a correct solution. Such problems provide similar learning gains to code writing problems – often 30% faster – for introductory assignments [3]. My research will enable teachers to integrate Parsons problems into existing curricula to better attain their teaching outcomes by providing a toolkit for these new problems. Preliminary Work Within my Ph.D., I have been studying how Parsons problems can be applied to improve CS pedagogy. In Spring 2018, I ran an exploratory between-subjects study which found that when students begin by solving a Parsons problem instead of writing code, they are more able to generate multiple alternative solutions, addressing a skill gap in the “ability to generate alternate solutions” [2]. In Fall 2018, I ran a within-subjects study, followed by a structured interview, teaching algorithms to students who had not taken an algorithms class. They were taught two algorithms by either writing code from a pseudocode specification (i.e. a language-independent solution) or by solving a Parsons problem. Students with a range of skills noted that Parsons problems let them focus on the logic of the algorithm, engaging them in computational thinking. Students found writing code from pseudocode to be either too complex, distracting their focus, or too trivial, not engaging with the algorithm. These results suggest that Parsons problems can support a variety of students in practicing their computational thinking. Approach In my thesis research, I will explore how we can leverage new problem types, e.g. Parsons problems, to supplement existing teaching tools for advanced CS concepts. I will run longitudinal studies on these new problem types by partnering with UC Berkeley professors who teach relevant classes with hundreds of students. I will then synthesize the results from these studies to develop a toolkit for teachers to help teachers easily adapt existing material into Parson problems and achieve their curricular desires. RQ1: How can Parsons problems support the development of computational thinking with algorithms? Based on my Fall 2018 study, I am now exploring how to help students further focus on computational thinking with new Parsons problems that flexibly blend pseudocode with code within or between problems. By using pseudocode, students are constrained to rely less on syntax and more on computational thinking. I will measure students’ learning gains of the taught algorithms as well as their performance on interview-style algorithm questions. RQ2: How can Parsons problems improve programming ability by teaching programming idioms? One major risk of supplementing existing assessments with Parsons problems is that it could hurt students’ programming skills by reducing their time spent practicing writing code. To overcome this pitfall, I will explore how Parsons problems can improve code writing by teaching programming idioms (i.e. common code and design patterns such as finding the five largest numbers in a list). There is a strong connection between students’ ability to write well-styled code and their ability to select and apply appropriate programming idioms [6]. However, complex idioms are often not explicitly taught. Results from my Spring 2018 study indicate that Parsons problems could support students exploring multiple solutions to a problem using different idioms, helping them compare when they are effective to use. They could also expose students to a range of problems where an idiom is applicable, helping students learn how it can be applied. I will run a formative study to better understand how students learn and select idioms. I will measure ABC scores of solutions, a well-established metric for code complexity, to evaluate students’ ability to efficiently apply idioms [6]. RQ3: How can instructors easily integrate Parsons problems into their teaching? Even if these new problem types are found an effective teaching tool, it must also be straightforward for instructors to generate and integrate them into the classroom. I will interview professors to explore the range of teaching methods used in classes and homework. These interviews will guide the design of a system to give teachers a powerful tool to target specific learning goals with more problem types. For example, a teacher could use think-pair-share in class to encourage students to share their problem-solving strategies by having students discuss which line of pseudocode should be placed next in a Parsons problem. Or, the large corpora of student solutions could automatically generate Parsons problems for teachers to modify and use. Resources UC Berkeley provides rare access to collaborate with teaching professors in large, innovative classrooms. Here, my research will change how thousands of students learn CS. Intellectual Merit My work will enable further research of teaching tools throughout the CS curriculum. My proposal explores how students learn and use computational thinking and programming idioms in complex problems through problem types and assessments. While there is a plethora of research on teaching introductory CS concepts, there is minimal research on how to teach advanced CS concepts, which are closer to real-world needs. The results of my work will empower teachers to create new resources to help students learn complex CS concepts. This research will be the first to explore the effectiveness of Parsons problems beyond introductory courses, creating new interactions and contexts for Parsons problems. This will inspire applying Parsons problems in new ways: incorporating them into more domains in CS curricula, using them for post-school learning such as API tutorials or system documentation, or new situations where engaging with multiple solutions is beneficial. Broader Impact My research is inspired by a desire to make CS concepts more accessible. Code writing problems are “one of the most significant reasons for giving up” by online learners in introductory classes [3]. These techniques will help improve learners’ self-confidence in these areas, with the aim of reducing impostor syndrome and attrition, as a step towards making CS programs more inclusive. The results of this research will be disseminated within top-tier publications in HCI and CS Education. My software engineering experience enables me to make the toolkits developed over the course of my research robust and publicly available. Together, these will help researchers and content creators make knowledge more accessible to a diversity of audiences. [1] https://nces.ed.gov/programs/digest/d17/tables/dt17_322.10.asp [2] Radermacher et al. “Gaps between industry expectations and the abilities of graduates,” SIGCSE ’13 [3] Ericson et al. “Solving parsons problems versus fixing and writing code,” Koli Calling ’17 [4] Denny et al. “Evaluating a new exam question,” ICER ’08 [5] https://js- parsons.github.io/ [6] Wiese et al. “Teaching Students to Recognize and Implement Good Coding Style,” L@S ’17	Winner!
86	Introduction: Habitat loss and fragmentation due to land development disrupt ecosystem services, degrade carbon stocks, and threaten biodiversity1. Restoring connectivity between habitat patches is an effective approach to conserve biodiversity in a fragmented landscape, but limited funding requires prioritization of reconnecting habitat patches with high diversity. The theory of island biogeography2 suggests that area of a habitat patch and amount of space between patches (hereafter the “matrix”) are both important for restoration3. Consideration of only patch and matrix area, however, is inherently a flat perspective: an abundance of life lives above- ground, around 40% of biodiversity in forests4. Habitat volume may better predict biodiversity than area, and if so should be an important metric for establishing restoration priorities. The concept of an ecosystem as a multidimensional space of discrete niches is well- established5, and although a species-volume relationship has been recently proposed, it has not been rigourously tested6. My previous research shows that distinct microhabitats exist in a multidimensional space within forests7, and evidence suggests that canopy height and structure, which in part determine the number of available niches, influence diversity8. Taller, more structurally complex canopies may provide more “vertical niche space” (VNS) for species to use. VNS crossed with patch area can be a metric for patch volume. Some evidence suggests that VNS is more important for determining alpha diversity (hereafter alpha) than patch area9, and high-volume patches (fig. 1B) are expected to maintain higher alpha than low-volume patches (fig. 1C). Patch isolation (mean distance to closest patch on all sides) determines how easily organisms can cross between patches, and therefore influences alpha3. Depending on the land use of the matrix (fig. 1E&F vs. fig. 1G&H), matrix VNS will vary, and high matrix VNS may also increase alpha of adjacent patches. In regions with immense biodiversity but rapid rates of deforestation, such as Madagascar10, careful consideration must be given to restoring land between high-diversity patches. Conservation International (a conservation NGO) has scheduled large-scale restoration work in the Ambositra- Vondrozo Corridor (AVC) of Madagascar, but baseline biodiversity and forest cover monitoring is necessary. Herpetofauna (reptiles and amphibians) are particularly threatened both in Madagascar11 and globally12. Herpetofauna are abundant at all forest heights and some species are persistent in even small forest fragments, making them an excellent model taxon to test a species-volume hypothesis. By integrating high-resolution remote sensing of forests from satellites and an unmanned aerial vehicle (drone) with on-the- ground herpetofaunal surveillance, I will test the species- volume hypothesis to inform both ecological theory and restoration efforts. Hypothesis: I hypothesize that volume of habitat patches and inter-patch matrix more accurately predicts alpha diversity than patch area and isolation distance alone. Aim 1: I will determine how well patch area and isolation of habitat patches predict patch alpha. Aim 2: I will then build upon the species-area theory by evaluating how well patch volume (area crossed with VNS) and matrix volume (patch isolation crossed with VNS) predict patch alpha. Aim 3: I will use forest structure and patch alpha to hierarchically prioritize restoration sites. Methods: To select sites, I will first use Landsat satellite-derived estimates of forest cover13 and ICESat LiDAR-derived coarse estimates of canopy height14, and will calculate patch area and isolation using the SDMTools R package15. I will choose sites that are accessible and encompass forest patches that vary in size and structure. To acquire high-resolution models of volume, I will monitor a total of ~84 km2 (40 days of flights, 0.7 km2 coverage each) via a senseFly eBee® drone. An equipped CANON Powershot® will record photographs that I will stitch together using Pix4D photogrammetry software to produce 3D orthomosaic point clouds. I will divide the study region into 30m2 cells and substract orthmosaic points from a digital elevation model to derive mean and variance in canopy height (synthesized into a single VNS index) for each cell16. For both patches and matrix, I will multiply VNS by the standardized cell area to calculate volume of each cell. I will record herpetofauna richness and abundance in forest patches across the region by conducting 30 vertical surveys (from forest floor to top of canopy), sufficient for species accumulation (Scheffers, personal communication). I will perform generalized linear models17 (GLM; pending data structure and distribution) to model alpha in relation to 1) the interaction between patch area and isolation and 2) the interaction between patch volume (sum of cell volumes) and matrix volume (sum of non-forest cell volumes within a 5-km buffer zone of a patch). I will then conduct three restoration prioritization analyses using the software Zonation, which will prioritize cells by accouting for desired habitat inputs while iteratively removing the least valuable cells. In analysis (A1) inputs will be patch area and isolation; in (A2), patch/matrix volume; and in (A3) alpha of patches. I will contrast site selection for restoration by analyzing the correspondence between output cells from A1 and A2 to cells with high diversity (A3). Resources: I will be advised by Dr. Brett Scheffers (University of Florida; UF), an expert on Malagasy herpetofauna, I am familiar with single-rope canopy access, and the members of the Scheffers lab are certified with 1,000s of hours of canopy access. I will build point cloud models using UF’s HiPerGator 2.0, the world’s third fastest university computer. Intellectual Merit: My proposed research extends a classic ecological theory, the species-area relationship, by combining cutting-edge tools with conventional field methods. If my hypothesis that habitat patch volume can predict diversity more accurately than area is correct, this study will contribute to Understanding the Rules of Life, one of NSF’s 10 Big Ideas, and my workflow will establish an efficient pipeline for estimating biodiversity. Once such drone methods are achievable via satellite, my study can be replicated without any site visitation. My diversity monitoring may also contribute novel data on critically endangered and data-deficient species11. Broader Impacts: My work will inform the ambitious forest restoration projects planned for the AVC (2019-2024) and I have communicated with Conservation International to monitor sites of mutual interest. Forest restoration will enhance ecosystem services, absorb carbon emissions, and mitigate flooding of local communities, and my forest structure data can be used to quantify carbon stocks for offsetting projects in the region. As part of USAID PEER funding (2017-2021) to Dr. Brett Scheffers, we will collaborate with a Malagasy graduate student to train communities to monitor on-the-ground carbon stocks, thereby assisting them with active protection of their forests. I will also facilitate a letter exchange between Malagasy students and the Riverside Elementary School in the US, with which I have established contact on the matter. Citations: [1] Foley, J. A. et al. (2005) Science. [2] MacArthur, R. H. & Wilson, E. O. Princeton University Press, 2001. [3] Laurance, W. F. et al. (2002) Conserv. Biol. [4] Ozanne, C. M. P. et al. (2003) Science. [5] Hutchinson, G. E. (1957) Cold Spring Harb. Symp. Quant. Biol. [6] Gatti, R. C. et al. (2017) Plant Ecol. [7] Klinges, D. H. et al. Amer Nat. in review. [8] Bergen, K. M. et al. (2009) J. Geophys. Res. Biogeosciences. [9] Basset et al. (2015) PLOS One. [10] Myers, N., et al. (2000) Nature. [11] Andreone, F. et al. (2008) PLOS Biol. [12] Böhm, M. et al. (2013) Biol. Conserv. [13] Hanse, M. et al. (2013) Science. [14] Simard, M. et al. (2011) J. Geophys. Res. [15] VanDerWal, J. et al. (2014). R package. [16] Lisein, J. et al. (2013) Forests [17] Webster, C. et al. (2018) Remote Sens. Environ.	Winner!
87	A Quantum Field Theoretic Approach to Disordered and Amorphous Solids The objective is to formulate a fundamental theory of defects in disordered solids using quantum field theory (QFT) methods, with the ultimate goal of understanding amorphous materials. Motivation: P. W. Anderson said in 1995, “The deepest and most interesting unsolved problem ​​ in solid state theory is probably the theory of the nature of glass,1” glass being the prototypical ​ ​ highly-disordered amorphous material. Defects describe disordered materials in a more concrete, physical way, and are central objects of study in materials science. They govern the deformation of solids and overall mechanical properties, but also significantly affect electronic, optical, and other functional properties.2 This alternative approach to disordered systems holds great promise. ​ ​ Due to their long-range nature and effects, extended defects in particular, including dislocations, grain boundaries, etc., are difficult to model and not well understood theoretically. Common computational methods like density functional theory (DFT) scale poorly with system size, making such defects too costly for simulation.2 Classical molecular dynamics is based on ​ Newtonian mechanics, so doesn’t take into account any quantum effects. While there exist empirical models for the effect of defects on these functional properties, they need many parameters, and none are fully ab initio. A radically different approach is needed. ​ ​ Introduction and Background: The classical theory of dislocations more or less fully explains mechanical properties of materials. However, there remain many open questions on how dislocations and defects in general affect functional properties, that remain so because the classical treatment is insufficient. I propose to describe extended defects in disordered solids as ​ quantum fields that give rise to quasiparticles, enabling a deeper understanding of defect interactions. Motivated by the example of phonons as quasiparticles that quantize the lattice displacement field, we define a new quantum field for dislocations and its associated quanta – the “dislon”3. Much as the phonon theory led to great advances in understanding the effect of ​ ​ lattice vibrations on photons and electrons, this new theory will lead to a similar paradigm shift. Initial Successes–the Dislon: A dislocation is an extended crystal defect. Critical to formalizing ​​ them is the Burgers vector b, representing the dislocation-induced lattice distortion and ​ ​​ associated displacement field u. This u is the classical field we turn into a quantum field for the ​ ​​ ​ first quantization. Conceptually, the constraint imposed by b is very important, as the only thing ​ distinguishing a phonon from a dislon, both being just quantized u at heart. ​ ​​ Already this theory has been successfully applied to supplant previously incomplete explanations of materials phenomena, such as the effect of dislocations on superconducting critical temperature T . The expression above is analytically derived and ​ ​c describes the competition between quantum​ vs. classical effects.3 It predicts the direction of T ​ ​ ​c change after increasing dislocation density, which affects Poisson’s ratio ν and Lamé parameters ​ λ, μ. It is somewhat surprising because the expression combines variables/parameters describing ​ ​ ​ both mechanical and electronic properties, not usually seen together in one formula. Research Plan: I plan to join the Energy Nano Group, led by Prof. Mingda Li, creator of the dislon theory. His group combines theory and experiment, and I will contribute to the theory side of research. With the dislon as the origin, we can organize our future work along three axes. Axis 1–Deepening: The dislon theory still has great potential to be applied to many other types ​ of interactions. One example is dislon-induced topological phase transitions, where we turn a topologically trivial material into a topological insulator by tuning its bandgap. I will include electron-dislon interactions into band structure calculations, which avoids the computationally 1 Haihao Liu Graduate Research Plan Statement October 2018 expensive supercells required for DFT. Another application is dislon-enhanced Anderson localization, the suppression of electron diffusion in disordered systems.4 Dislons give us a way ​ to explore this effect in crystalline materials, which thus far has been challenging to observe. There is still much work to be done extending the theory itself. Most promising is the incorporation of gauge symmetry, a classical theory of which already exists for dislons.5 The ​ goal is to quantize this, analogous to quantum electrodynamics (QED) for electrons. In the same ​ way photons arise naturally from QED as a force carrier, we would expect something similar from a quantum gauge theory of dislons, that can shed light on the nature of plasticity. Axis 2–Broadening: The quantization procedure outlined for dislons can be readily generalized to account for anisotropy and discrete lattices (current derivation is for continuum limit). The initial success with dislons means this is a promising approach to defects in general. In ​ conjunction to the work Axis 1 on 1D dislocations, I will generalize the quantization process to ​ 2D grain boundaries and 3D inclusions. An analytical theory allows computation of interfacial transmission coefficients for the Landauer formula from microscopic and no ad hoc parameters. ​ ​ Axis 3–Experimental: There will be a strong experimental component to inform and augment ​​ the theory development. Working with our collaborators at Oak Ridge, we will probe phonon- dislon interactions using advanced techniques like inelastic X-ray and neutron scattering, to see if measured thermal transport and relaxation coefficients match those calculated from dislon theory. Additionally, we will use our lab’s crystal growth facilities to synthesize materials with controlled dislocation density, to explore dislon-induced topological phase transitions. Intellectual Merit: With the dislon model, all dislocation effects are incorporated: strain, coulombic, and vibrational. We now have a systematic framework to investigate the effect of defects on functional properties, by including a defect Hamiltonian. This allows us to calculate constants such as deformation potential coefficient ab initio, which will lead to a deeper ​ ​ understanding of material properties far beyond any empirical model. We can also now use tools from QFT like Feynman diagrams to study complex electron-phonon-dislon interactions. One may ask, why should extended defects be quantized to begin with? The intuition is that extended (as opposed to point) defects have spatial extent, captured by the “field” part of QFT, and its resulting internal dynamic structure is described by the “quantum” part. We have shown this idea applies to dislocations, suggesting it applies generally to all extended defects. Broader Impacts: These theories will have a profound impact on the direction of condensed ​​ matter and materials research. Much work now is on trying to find new materials with desired properties. By understanding how defects can induce such properties, we vastly expand the family of known materials exhibiting them, without needing to find completely new materials. As a radical theory that sits at the intersection of quantum field theory and materials science, two disciplines with relittle crosstalk, there is much educational potential. Through talking with my colleagues in the materials science department, I aim to start bridging the gap between these two communities. I plan to help develop a course introducing QFT to engineers, that I could be a TA for, and eventually down the line, perhaps even write a textbook on this theory. So far, over 20 years after Anderson’s comments, we still only have the empirical model of electrons in glass developed by Mott.1 By understanding defects, we hope to ultimately have a ​ fundamental theory of amorphous materials, as a limiting case of infinitely many defects. Works cited: [1] Anderson P W, Science 267, 1615-16 (1995) [2] Mott N F and Davis E A, Electronic processes in ​ ​ ​​ ​ non-crystalline materials (2012) [3] Li M, arXiv preprint, arXiv:1808.07777 (2018) [4] Anderson P W, Phys Rev ​ ​ ​ 109, 1492-505 (1958) [5] Kadic A and Edelen D G B, A gauge theory of dislocations and disclinations (1983) ​​ ​ ​ 2	Winner!
88	Unraveling the process of polysaccharide utilization in complex bacterial ecosystems Intellectual Merit – Introduction: Bacteria do not exist in isolation in nature; they form complex communities in which they must recognize, compete over, and, share nutrient resources.1,2 The gut microbiome is an ideal model to study this type of ecosystem, as we know what nutrients these bacteria are exposed to and have access to powerful tools to study taxonomy and metabolism. Gut bacteria break down polysaccharide nutrients and convert them to health-beneficial end products of metabolism called short-chain fatty acids (SCFAs) that are taken up and used for energy by host intestinal cells.1,2 However, there is a gap in our understanding of what factors give rise to the emergent phenomenon of SCFA production. The leading hypothesis in the field is that communities that fail to respond to a given polysaccharide lack certain “keystone species” that are integral to metabolism.1 The search for keystone species has focused on primary polysaccharide degraders,1 but findings in our lab indicate that these are abundant even in samples otherwise characterized as non-responders (Fig. 1). In contrast with the notion of a single species as a keystone, I hypothesize that community-level polysaccharide metabolism is driven by the presence of core communities of multiple species that function as an assembly line. This would explain how samples can fail to produce SCFAs despite the presence of primary degraders. By shifting the focus to species that consume degradation byproducts, I will reveal novel cross-feeding interactions and develop a model to predict polysaccharide response from community composition. Our lab has a collection of stool samples from multiple healthy donors that have been characterized for SCFA production (Fig. 1), making me well situated to conduct this research. I will address my hypothesis in the Figure 1: Bacterial communities derived from following aims: stool samples from 9 donors (A-I) contain primary degraders of all tested polysaccharides.3 Intellectual Merit – Research Plan: Aim 1: Isolate the core communities from multiple stool donors. I will grow bacterial communities from stool samples on minimal media with inulin as the sole carbon source in our lab’s “artificial gut,” a set of eight bioreactors.4 Inulin is a well-studied polysaccharide, a polymer of fructose with a single terminal glucose residue. The species that persist after three weeks of growth in this media, as measured by 16S sequencing, will include all species that consume inulin or inulin byproducts. By reducing complex communities to only those species involved in inulin metabolism, I will test the hypothesis that core community composition is directly related to SCFA production. I will next quantify expression of genes encoding select carbohydrate active enzymes and transporters for all species present using RT- qPCR. I hypothesize that degradation will occur in waves, with primary degrader species upregulating genes of interest at early time points, followed by secondary degraders. While I expect all samples to exhibit a robust first wave of upregulation, I hypothesize that the magnitude of the second wave will correlate with SCFA production. If there is no correlation, this would support the alternative hypothesis that strain-level variation drives differences in polysaccharide response. In this case, I would expect the gene expression experiments to reveal an increased number of species that upregulate the genes of interest in SCFA-producing samples. Graduate Research Plan Statement Jeffrey Letourneau Aim 2: Differentiate primary, secondary, and tertiary degraders. Our lab has developed a microfluidics growth assay that allows for the isolation of single cells in droplets (Fig. 2). Species that grow in this assay are defined as primary degraders, since cross-feeding interactions are prevented. I will identify secondary and tertiary degraders by taking conditioned media from primary degraders, which will contain partially broken down polysaccharides and byproducts of their breakdown, and using this media as the sole carbon source in a subsequent assay. Conditioned media will be characterized using the lab’s GC and HPLC to measure SCFAs and polysaccharides, respectively. I hypothesize that while stool samples from all donors will contain some primary degraders, samples unable to produce SCFAs will lack key secondary and/or tertiary degraders. Growth will be determined by 16S sequencing to measure relative abundance and flow cytometry to calculate the total number of cells. The use of 16S will also allow me to determine which bacteria are involved at which stage of polysaccharide utilization. I hypothesize that the primary degraders will comprise of both generalists (Bacteroides ovatus) and specialists (Roseburia inulinivorans, for inulin), secondary degraders will include acetate producers (Bifidobacteria and Lactobacilli), and tertiary degraders will include acetate- and lactate-utilizing Figure 2: Droplets containing bacterial butyrate producers (Eubacterium and Anaerostipes). cultures each derived from single cells.5 Broader Impacts – Disseminating Research: To date, I have shared my research findings with diverse audiences from kindergarteners to faculty and will continue to use the results of this project as a means of promoting science education. I am making my research accessible to middle and high schoolers by teaching for a Saturday program called Duke Splash, where I have previously taught Intro to Microbiology and Bread Science. I am in the process of developing two new classes to debut this November, Nutrition Science and Data Science, both of which incorporate data from my own research on polysaccharide metabolism. At Duke, I have many opportunities to share my research, such as the monthly Duke Microbiome Center seminar, where I recently presented my findings on transcriptional memory of polysaccharides in gut bacteria. To reach undergraduates, I will be giving a talk at the annual University Scholars Program (USP) Symposium, the theme of which will be “(in)dependence.” With this theme in mind, I will be presenting on how humans are dependent on gut bacteria to help metabolize dietary fiber as it relates to my research. Previously, I spoke at the USP Graduate Research Seminar on my work as a rotation student. Beyond Duke, I plan to present my research at conferences such as the 2019 Keystone Microbiome Conference. Broader Impacts – Importance and Innovation: Emergent properties of complex systems can be difficult to deconstruct, and this project provides a methodology for doing so in a novel, high-throughput manner that can be applied in future studies. By determining how emergent polysaccharide metabolism phenotypes arise, I will address a key gap in our understanding of how microbial ecosystems respond to nutrients. These results may be applied in human diet research to predict individual polysaccharide response and in bioremediation to develop co-culture methods for effective degradation of pollutants. Moreover, my findings will raise exciting new questions related to evolutionary biology as to how polysaccharide utilization came to be an assembly line process. References: 1. Makki et al. 2018. Cell host & microbe. 2. El Kaoutari et al. 2013. Nature reviews Microbiology. 3. Villa. 2018. Unpublished. 4. Silverman et al. 2018. bioRxiv. 5. Bloom. 2018. Unpublished.	Winner!
89	Animals harbor a suite of innate fears, knowing them from birth in the absence of firsthand experience. These fears are rooted in evolution and are often species-specific. For example, mice respond defensively to odors related to foxes and cats, two of their most common predators. Alternatively, humans respond defensively to snakes and spiders, both of which can be highly-lethal and are endemic to East Africa, the original range of genus Homo. These factors strongly imply a genetic origin for these fears, though none has yet been investigated. Aversive responses have both behavioral and hormonal components. The two brain regions necessary and sufficient for innate aversive odor responses in mice are the cortical amygdala (CoA) and the amygdalo-piriform transition area (APir). The CoA mediates innate olfactory behavior, and is organized spatially based on the emotion a given odor evokes — neurons responsive to aversive odors are located in the anterior CoA, and neurons responding to all other odors are posterior.1 APir controls the hormonal stress response to innately aversive odors, stimulating secretion of corticotropin-releasing hormone (CRH) by the hypothalamus, raising peripheral corticosterone levels.2 Neither region has any other known functions. A distinct, spatial organization to neuronal populations that mediate specific behavioral functions, independent of individual experience, strongly implies genetic control over the developmental programs creating these pathways. For instance, a past study examining similar populations in other regions showed each one expresses a set of marker genes specific to their own population.3 Thus, neuronal populations in CoA and APir may similarly express their own sets of marker genes. I propose to identify the first set of marker genes for neurons controlling innate aversive responses to a set of specific stereotyped odors. Aim 1: Identify neurons specifically mediating innate olfactory aversion. Hypothesis: If these regions respond to innately aversive odors, then the specific responsive neurons within these regions should be identifiable based on the population’s activity in odor exposure. Method: I can mark these neurons using a transgenic mouse strain with neurons that Cre- dependently express eYFP if active within a transient, hours-long period after peripheral tamoxifen injection.4 I will identify innately aversive odor-responsive neurons by exposing mice to either water or trimethylthiazoline (TMT), a well-validated innately aversive fox odor, shortly after peripheral tamoxifen injection. The water- responsive group represents neurons active at rest, while the TMT- responsive group represents neurons activated by innately aversive odors. Differences in response between the two conditions should reflect regional activity differences. Anticipated Results: The eYFP- Figure 1. Preliminary Results and Research Plan. After odor expressing population should be exposure, cells (blue) in the amygdala express Arc (green). A. enriched in CoA and APir in the Amygdala neuron activity after water exposure. B. Activity after TMT-exposed mice compared to the TMT exposure. C. Proposed research plan. BLA: basolateral amygdala nuclei, CeM: centromedial amygdala nuclei. water-exposed mice. The regions NSF GRFP 2018 1 James R. Howe Graduate Research Plan they innervate, the BLA, CeM, and the hypothalamus, which control general aversive responses, should be enriched as well. Preliminary data corroborates these predictions (Figure 1A, 1B). Aim 2: Identify genes exclusive to innate olfactory aversion neurons. Hypothesis: Neurons active during TMT exposure should express a suite of marker genes not expressed in neurons active during water exposure throughout the brain. The eYFP-expressing activated neurons can be dissociated on ice via an optimized combination of RNA polymerase inhibitors, physiological solutions, and extracellular matrix-specific cold-active proteases to keep cells alive and eliminate gene expression artifacts.5,6 I can combine this technique with dissection and fluorescence- activated cell sorting to isolate single live eYFP-expressing cells from CoA with high fidelity.7 Method: I will use an efficient, well-validated, high-resolution form of single cell RNA- sequencing to precisely assay the expression of all genes in all isolated cells (Figure 1C).8 This approach closely resembles the method used in a recent series of experiments in the neuroscience literature.9 A custom computational pipeline purpose-built for this experiment will analyze the data. Machine learning algorithms will classify cells into groups based on similarities in underlying gene expression. Differential expression analysis will identify the most highly upregulated genes in each group of cells compared to all others. Gene ontology (GO) analysis will then identify these genes’ functions. RNAscope, a multiplexed single-molecule RNA fluorescent in situ hybridization platform, will externally validate these results.10 I collaborate with three groups across multiple disciplines and institutions to perform these techniques: in biology at the University of Cincinnati, biochemistry at UCLA, and bioengineering at UCSD. Anticipated Results: Using this framework, I expect to identify at least one group of neurons present in the TMT-responding population but not the water-responding population, with at least one corresponding suite of highly-expressed genes. These will both be confirmed via RNAscope. We expect these genes to display enriched neural development-related GO terms. Intellectual Merit: This would be the first study to identify and validate the heritability of innate behaviors, the specific neurons mediating these behaviors, and their underlying genes. Finding such genes would allow the targeted stimulation and genetic access of these neurons for the first time, making modification or simulation of specific odor responses (even in the absence of prior experience) possible, a valuable future research tool. Using such tools, researchers could create far more precise experimental designs, allowing researchers to answer more specific hypotheses than ever before. The availability of such novel technologies at the intersection of psychology, molecular biology, and sensory neuroscience will have many implications for studies in all three fields and will further stimulate interdisciplinary research incorporating aspects of all three. Broader Impacts: During this project, I will train undergraduate students from underrepresented communities at UCSD in molecular biology and behavioral techniques, as well as mentor them in research methods, both at the bench and away from it. The computational and molecular methods will be made open-source on GitHub and protocols.io. I will communicate the results in open-access peer-reviewed academic journals, and I will also write articles in popular media outlets throughout the project based on what I study and discover along the way. The marker genes identified in this project could lead to advancements in commercial research technologies and pharmacology, as each one could serve as a possible target for future drug development should any of these populations become relevant to certain research questions or diseases. References: 1Root et al. Nature 2014 515:269-273. 2Kondoh et al. Nature 2016 532:103-105. 3Kodama et al. J Neurosci 2012 23:7819-7831. 4Guethner et al. Neuron 2013 78:773-784. 5Wu et al. Neuron 2017 96:313-329. 6Adam et al. Development 2017 144:3625-3632. 7Hempel et al. Nat Methods 2007 2:2924-2929. 8Torre et al. Cell Syst 2018 6:171-179. 9Tasic et al. Nat Neurosci 2016 19:335-346. 10Wang et al. J Mol Diagn 2012 14:22-29. NSF GRFP 2018 2	Winner!
90	S.Moore Background Let L be a finite dimensional semisimple Lie algebra. A subset H ⊂ L is said to be a Car- tan subalgebra if H is a maximal toral subalgebra (a subalgebra in which all elements are ad- diagonalizable). In particular, H will be abelian, implying that every h ∈ H is simultaneously ad-diagonalizable. We call α ∈ H∗ a root of L if α (cid:54)= 0 and there exists nonzero v ∈ L such that [h,v] = α(h)v ∀ h ∈ H. The set of roots, R , is finite. Let S = {α ,α ,...,α } ⊂ R be a φ φ 1 2 n φ basisofH∗ suchthatanyα ∈ R canbewrittenasα = (cid:80)n c α withallc eithernonpositiveor φ i=1 i i i nonnegative integers. We call elements of S simple roots. If α ∈ R has all c nonnegative, then φ φ i αissaidtobeapositiveroot. WedenotethesetofpositiverootsbyP . ThesesetsS ⊂ P ⊂ R φ φ φ φ (together with some more data) are called the root system φ of L. For example, the root system of sl (C)(denotedA )hassimpleroots{α ,α }andpositiveroots{α ,α ,α +α }. 3 2 1 2 1 2 1 2 A(Bridgeland)stabilityfunctiononarootsystemisamap Z : P → H = {z = x+iy ∈ C | y > 0} φ satisfying Z(α + β) = Z(α) + Z(β) for all α,β ∈ P . As such, Z is uniquely determined by φ Z | . We also typically require that Z be generic, meaning that Z(α) (cid:54)= cZ(β) for any c ∈ R S φ wheneverα (cid:54)= β ∈ P . SeeFig. 1forexamplesofstabilityfunctionsonA . φ 2 For α ∈ P , the phase of α under Z α +α φ 1 2 is the angle from the positive real axis to α +α 1 2 α Z(α). For generic Z, this induces a combi- 1 α 2 natorial ordering of the elements of P via φ α α 2 1 decreasing phase. Two stability functions Z,Y : P → H are said to be combinatori- φ allydifferent ifZ andY inducedifferentor- Z Z 1 2 derings of P . In particular, Z and Z (see φ 1 2 Fig. 1) are combinatorially different stabil- Figure1: TwostabilityfunctionsonA . 2 ityfunctionsonA . 2 Our goal is to understand simple wall crossings of stability functions, which are defined as follows: Let Z and Y be stability functions of a root system φ. If the induced combinatorial orderingsofP underZ andY arethesameexceptthataconsecutivetripleτ,τ +ω,ω underZ is φ rearrangedtoω,τ +ω,τ underY,thenY issaidtobeobtainedfromZ byasimplewallcrossing. For example, Z is obtained from Z by a simple wall crossing. Intuitively, a simple wall crossing 2 1 arisesfromtakingapaththatconnectsZ andY inthespaceofstabilityfunctions. Suchapathwill necessarily cross through a non-generic stability function in which τ,ω, and τ +ω have the same phase. Notethatthespaceofstabilityfunctionsissimplyconnected,soanytwostabilityfunctions maybeobtainedfromoneanotherviaafinitenumberofwallcrossings. The aim of this project can be broken into two main goals: First, given a root system φ, we aimtocombinatoriallydescribethegraphofcellsofstabilityfunctionsonφseparatedbysimple wallcrossings. Second,wewishtodeterminetherelatedidentitiesamongmotiviccharacteristic classes of geometrically relevant spaces (see below). To accomplish these aims, we will use a 1 combination of methods from combinatorics and rational function identities for neighboring cells (see[RR]). IntellectualMerit Such wall crossings have impacts beyond the study of Lie algebras. In particular, to a root system φ, we may associate a Cohomological Hall Algebra C (defined by [KS]). This will be an φ infinite-dimensional, non-commutative, associative algebra whose product is denoted by ∗. Such algebras are motivated by string theory. To each α ∈ P , [RR] assigns a motivic characteristic φ class c0 ∈ C . The element c0 is a class in an equivariant cohomology (K-theory) algebra of a α φ α geometrically relevant space (such as the Grassmannian, or flag manifold). Such c0 have various α interestinginterpretations,suchasmotivicChernclasses,Chern-Schwartz-MacPhersonclasses,or stableenvelopesinOkounkov’snewtheoryrelatinggeometrytophysics(see[MO]). Supposethatwehaveasimplewallcrossingwhichpermutesτ,ω+τ,andω. By[RR],thisgives risetoanidentityc0 = [c0,c0]intheCohomologicalHallAlgebra. Notethatthisresultissimilar ω+τ τ ω in nature to wall-crossing formulas (also known as quantum dilogarithm identities) in Donaldson- Thomas theory [KS]. Such an identity gives a convenient way to calculate c0 for α ∈ P \ S , α φ φ as the commutator [c0,c0] is well understood for τ,ω ∈ S . For example, in A we can calculate τ ω φ 2 (cid:16) (cid:17) (cid:16) (cid:17) c0 = [c0 ,c0 ] = c0 ∗ c0 − c0 ∗ c0 = 1 + yb − 1 − b = (1 + y)b. That is, the α2+α1 α1 α2 α1 α2 α2 α1 a a a (cid:16) (cid:17) class c0 is obtained as the difference of the K-theoretic total Chern class 1 + yb and the α2+α1 a (cid:16) (cid:17) K-theoreticEulerclass 1− b . a DisseminationofResults I will present the results of this project in a variety of settings. Locally, I plan to present at the Triangle’s annual Assocation for Women in Mathematics conference (established last year) and at UNC’s Graduate Student Seminar. On a larger level, I plan to return to a national conference to presentaswell. Furthermore,resultswillbepublishedinarelevantmathematicaljournal. BroaderImpacts After graduating, I plan to become a professor. As described in my personal statement, a large focus of my career will be in mentoring undergraduates in research projects. I have previously mentored a high school student in a research project where she explored various non-Euclidean geometries. If awarded the NSF GRFP, I will continue building my mentoring capabilities by creatingaprojectforUNC’sDirectedReadingProgram. Thisprogramallowsgraduatestudentsto mentorundergraduatesthroughasemester-longreadingproject. MyprojectwouldbebasedinLie algebra,culminatinginanunderstandingoftherootsystemsassociatedtoeachsl (C). Iwillalso n becomeinvolvedintheMcNairScholarsProgramatUNCbyhelpingwiththeirresearchprogram overasummer. Thiswillallowmetheopportunitytohelpminorityundergraduateresearchersina variety of fields (not just mathematics) by providing them with critical feedback at various stages intheresearchprocess. References [RR] R. Rimanyi. Motivic characteristic classes in cohomological Hall algebras (Preprint). Available at http:// rimanyi.web.unc.edu/research1/,2018. [KS] M.KontsevichandY.Soibelman.Stabilitystructures,motivicDonaldson-Thomasinvariantsandclustertrans- formations.Availableathttps://arxiv.org/abs/0811.2435,2008. [MO] D.MaulikandA.Okounkov.QuantumGroupsandQuantumCohomology.Availableathttps://arxiv. org/abs/1211.1287,2018. 2	Winner!
91	Applications to Airborne Wind Energy Systems Introduction: While wind energy capacity has tripled in the past decade[1], the installation of towered wind energy systems in remote and deep-water offshore locations, as well as the ability to harness wind resources above 100m, is severely limited by tower and foundation constraints.[2] Airborne wind energy (AWE) systems solve this problem by replacing the conventional tower with tethers and a lifting body (usually a kite or wing). Two approaches to AWE systems have been adopted: (i) ground-based generators, where the lifting body is cyclically spooled in and out and power is generated on the ground[3], and (ii) airborne generators, in which the generator is a turbine attached to the lifting body.[2][4] Both technologies possess the potential for vastly increased energy generation through the execution of cyclic crosswind flight, which results in apparent wind speeds that can far exceed the true wind speed on a high lift/drag lifting body.[2] However, the successful implementation of crosswind flight requires a robust control framework for optimizing the crosswind flight path in the presence of disturbances. AWE systems executing crosswind flight are one of many control systems that execute repetitive (cyclic) motion, under a varying environmental profile, in order to maximize an economic metric (average net power output for AWE systems). Iterative Learning Control (ILC) presents a foundation for addressing this problem, allowing the controller to draw from previous iterations to inform decisions at the present iteration. However, traditional ILC techniques focus on tracking a prescribed path/trajectory in the absence of an external disturbance that can vary from iteration to iteration, rather than optimizing the path itself to maximize an economic metric in the presence of an iteration-varying disturbance. The proposed research will, for the first time ever, fuse techniques from library-based flexible ILC[5] and optimal control to arrive at a disturbance and performance-weighted ILC (DPW-ILC) framework that:  Bases its learning on an economic performance index, rather than setpoint tracking, and  Biases its learning to emphasize previous iterations whose conditions match the present. Research Plan: The proposed research will focus on a DPW-ILC formulation (Fig. 1), applied to an AWE system executing crosswind flight. This control formulation involves two critical features that occur in the iteration domain, the designs of which will encompass key components of the proposed graduate research: 1. Maintaining and managing a library of previous iterations and results: DPW-ILC is predicated upon maintaining a library of relevant previous iterations’ selected path geometries, corresponding performance (iteration-averaged power output for the AWE application), and associated measured Figure 1. Block diagram of the proposed disturbance (wind speed in the AWE application). As DPW-ILC control scheme, as applied to an iterations progress, a critical task will involve AWE system. managing the library size, maintaining those library entries that maximize a statistical measure of information for future ILC updates. 2. Economic DPW-ILC update: The DPW-ILC framework must identify those previous iterations that are most relevant to the present iteration in terms of having similar disturbance values (wind speed in the AWE application) and favorable performance (iteration-averaged power output in the AWE application). Quantification of a relevance index, which will be informed by related work in higher-order tracking ILC[7][8], will be a significant element in the creation of the DPW-ILC update. Formal stability and convergence analysis will be performed on the resulting DPW-ILC approaches. In particular, the following general questions will be posed:  Defining regret as the difference between the realized and optimal performance, how does long-term expected regret depend on the statistical properties of the external disturbance (e.g., variance, temporal length scale)?  How do the aforementioned regret bounds depend on the library size? These convergence analysis questions also lead to metrics against which designs can be evaluated (for example, a low regret bound that is robust to the library size is highly desirable). The DPW-ILC approaches will also be experimentally validated on a unique lab- scale, water channel-based platform. In particular, the authors of [8] designed a water channel- based setup for closed-loop flight testing of tethered systems, and [2] verified dynamic scaling between lab scale and full scale conditions. This existing platform, which has to-date been used to characterize AWE designs under constant flow profiles, will be extended to characterize real-world wind profiles, which will be dynamically scaled to the water channel level. Specifically, wind data from a Cape Henlopen, DE wind profiler will be scaled down to the water channel level to create low-frequency, iteration-to-iteration variations for initial validation of DPW-ILC algorithms. After the successful performance of this first round of experiments, high frequency, intra-iteration variation will be applied using wake generation devices upstream of the AWE model. Intellectual Merit: The DPW-ILC framework pioneered in this research will be the first to fuse economic ILC with library-based higher-order ILC, creating an entirely new avenue of research within the ILC community. Furthermore, the research will result in the first dynamically scaled experimental validation of AWE flight control strategies on a lab-scale platform, under realistic (and also dynamically scaled) wind profiles. Broader Impacts: The creation of robust, optimized control systems for AWE systems will render wind energy a viable alternative to diesel fuels[4] in remote, off-grid locations and a long-term solution for deep-water offshore locations. Furthermore, the control methodologies created through this research will be applicable to other engineered systems that execute repetitive (cyclic) motion, under a varying environmental profile, to maximize an economic metric. Examples include active exoskeletons, pick-and-place robotic systems operating under variable plant conditions, and even traditional wind turbines. [1] AWEA. https://www.awea.org/wind-101/basics-of-wind-energy/wind-facts-at-a-glance. [2] Cobb, M. et. al., “Lab-Scale Experimental Characterization and Dynamic Scaling Assessment for Closed-Loop Crosswind Flight of Airborne Wind Energy Systems” ASME J. Dyn. Sys., ’17. [3] Fagiano, L. et. al.,“High-Altitude Wind Power Generation,” IEEE T. Egy. Convers., Vol. 25, No. 1, ’10. [4] Altaeros, Inc., http://www.altaeros.com/. [5] Hoelzle, D. et. al., “Flexible iterative learning control using a library based interpolation scheme,” 51st IEEE CDC. [6] DiMarco, C., et. al., “Disturbance & Performance-weighted Iterative Learning Control with Application to Modulated Tool Path-based Manufacturing,” ASME DSCC ’16. [7] Cobb, M., et al, “Iterative Learning-Based Waypoint Optimization for Repetitive Path Planning, with Application to Airborne Wind Energy Systems,” 56th IEEE CDC. [8] Deodhar, N. et. al., “Laboratory-Scale Flight Characterization of a Multitethered Aerstat for Wind Energy Generation,” AIAA Jnl., Vol. 55, No. 6, 2017.	Winner!
92	I propose to study planetary system structures around low-mass stars by conducting radial velocity (RV) observations of known single-planet transiting systems observed by Kepler/K2 and the Transiting Exoplanet Survey Satellite (TESS). RVs and spectroscopy will be completed with MAROON-X1, an instrument with which I will have guaranteed clear-sky observing time as a student at the University of Chicago. I will address if there is a separate population of single planet low-mass stellar systems or if they have an inclined inner-most planet. This study will result in the publication of compelling individual systems as they are identified and a full statistical analysis once the full sample has been obtained. Background & Research Proposal: Planet formation models predict planetary systems form in the same orbital plane. However, 𝚹=15° there are several systems2 that suggest the inner-most planet may be inclined by a significant degree. Heavily inclined planets Figure 1: The inner-most planet would go undetected during exoplanet transit surveys (observing (black) lies 15° off the plane of the stellar flux over time), Figure 1. The analysis of Kepler/K2 system and does not transit, while an outer planet (purple) does. transiting exoplanet system yield an overabundance of single transiting planets, Figure 2.3 It is possible the single transits detected are the inner-most inclined planet and the remaining planets have a different orientation. The overabundance of single transiting planets is most notable around low-mass M-dwarf stars (70% of the Milky Way population). M-dwarfs are smaller and cooler than the Sun (< 0.7 M , T < 4000K). With nearly 300 current planet detections from NASA’s Kepler/K2 missions, Sun eff it is estimated each M-dwarf hosts at least 2 small planets (< 4 R ); TESS is predicted to find 500 additional planets Earth orbiting M-dwarfs.4 By studying planets around M dwarfs, I will be studying the dominant environment of planet formation in our galaxy. Follow-up RV measurements of planetary systems will allow us to identify planets that do not transit. I will be searching for non-transiting companions to known Figure 2: Comparison of the Kepler multi-planet yield (blue) to the best-fit transiting planets to understand the population of planetary models (red). Models under-predict systems with mutual inclinations. By conducting a large RV the number of singly transiting survey of transiting planets with the MAROON-X systems and over-predict the number of multiple transiting systems. spectrograph, under construction at the University of Chicago and to be commissioned on the Gemini North observatory in early 2019, I will answer the following questions: Do low-mass stars have 1 Seifahrt, A., Bean, J. L., Stürmer, J., et al. 2016 “Development and construction of MAROON-X,” in [Ground- based and Airborne Instrumentation for Astronomy VI], Proc. SPIE 9908, 990845 2 Montet, B. T., Morton, T. D., Foreman-Mackey, D., et al. 2015, ApJ, 809, 25; Rodriguez, J. E., Becker, J. C., Eastman, J. D., et al. 2018, ArXiv e-prints, arXiv:1806.08368 3 Ballard S. and Johnson J. A. 2016 ApJ 816 66 4 Barclay, T., Pepper, J., & Quintana, E. V. 2018, ArXiv e-prints,arXiv:1804.05050 !1 significantly inclined inner-most planets? If so, what physical mechanisms are responsible for causing the inclination shift? If not, why do these systems only host one planet? I will compile an initial catalog of M-dwarf systems from the Exoplanet Archive and the TESS Input Catalog prior to data release. Once data are available, I will run the TESS data through my developed software, ELLIE, to search for planet candidates. I will vet candidates, prioritizing stars in my catalog, and determine RV follow-up feasibility based on predicted planet radius and levels of stellar activity identified in the light curve. This process will be repeated until July, 2020, when the entire sky observable by MAROON-X has been observed by TESS. I am choosing MAROON-X to conduct this study because it is optimized for the red optical (peak emission of low-mass stars) and obtains spectra of the stars. By obtaining spectra of stars, I will be able to calculate accurate stellar parameters, following methods developed during my internship at NASA Goddard Space Flight Center. Additionally, MAROON-X has a predicted precision of <1 m/s, allowing for potential detections down to 1 M . By pursuing my PhD at Earth the University of Chicago with the MAROON-X instrument science team, I am guaranteed to obtain data, regardless of bad scheduled nights. I propose to conduct 5 full nights of observing per semester for 3 years, for a total of 240 observing hours. I will obtain mass and density measurements of transit detected planets in addition to identifying new planets. Measuring both parameters will prove essential when choosing planets for atmospheric characterization with the James Webb Space Telescope, currently set to launch in 2021. Obtaining both a radius (via the transit) and mass (via RV) measurements will help us better understand the compositions of relatively nearby planets, improving our understanding of planet formation around the most common stars in the Milky Way. Intellectual Merit & Broader Impacts: My years of previous research experience, including publishing papers and giving talks, have prepared me well for the challenges which lie ahead. I have the necessary knowledge of Python and other tools necessary to complete the project proposed here. I have experience efficiently and accurately analyzing large data sets5 and observations6, and vetting planet candidates.7 I am ready to undertake the project presented here. My current outreach experiences have prepared me for future plans to speak about my research at local K-12 schools. I plan on working with local teachers to improve STEM education and educator development at the elementary/middle school level by creating educational in-class astronomy activities as well as create a website that follows my work that teachers will be able to use in the classroom. Interactive tools will include how to create light curves and how the presence of different objects will change a light curve. I will attend the National Science Teachers Association national conference to give a demonstration on how to use my website. I also plan on taking advantage of events hosted in Chicago. Such events include Astronomy on Tap: Chicago, which brings the universe into a local Chicago pub, and Soapbox Science, which promotes women in STEM by placing them on a box in highly populated areas in downtown Chicago to talk about their work. 5 Feinstein, A. D., Montet, B. T., et al (in prep) 6 Feinstein, A. D., Schlieder, J. E., Livingston, J. H., et al, 2018, AJ (submitted) 7 Liang, Y., Crossfield, I. J. M., Schlieder, J. E., et al, 2018, ApJ 156 22; Crossfield, I. J. M., Guerrero, N., David, T., et al, 2018, ArXiv e-prints, arXiv:1806.03127 !2	Winner!
93	Graduate Research Plan Statement Reconstructing Morphologies of Distant Galaxies with the JWST Mock Catalog Keywords: Galaxy Evolution, Early Galaxy Structure, Near Infrared Imaging Abstract: In today’s universe, galaxies have simple morphologies (i.e. shapes and structures). They are ellipsoidal and symmetrical, often with an additional spiral component like our own Milky Way. However, there is strong evidence to suggest that galaxies in the early universe were clumpy and distorted, possibly driven by their assembly. The question then arises: how and why did galaxies change over time from asymmetrical structures to the objects we see today? The James Webb Space Telescope (JWST), NASA’s upcoming flagship space observatory, will revolutionize astronomers’ ability probe from the formation of the first galaxies over time until today. As a member of the JWST NIRCam team at the University of Arizona, I propose to create an analysis framework for the extraction of morphological information from the unique shapes and structures present in young galaxies. My proposed study will provide the necessary tools to understand the changing look of galaxies over cosmic time, fulfilling one of the primary aims of the JWST mission: understanding galaxy evolution from the first galaxies until today. Background and Motivation: The morphologies of galaxies are tied to the overall evolution of galaxy populations over cosmic time. For example, in today’s universe we use the shapes of galaxies to infer their recent histories; i.e. large spheroidal galaxies have undergone a merger with a galaxy of similar mass, whereas spirals have not (e.g. Mihos & Hernquist 1996). However, the deepest observations from the Hubble Space Telescope reveal that galaxies eleven billion years ago exhibit asymmetries and clumps in their images (e.g. Elmegreen et al. 2007, 2009). With few detections of distant galaxies, astronomers have been unable to fully characterize the evolution of galaxy morphology over cosmic time. Instead, the community has turned to complex hydrodynamical simulations, such as Illustris (Genel et al. 2014), to model galaxy evolution. While simulations can prove valuable for understanding certain astrophysical processes, they require tens of millions of CPU hours and can be difficult to compare to observations. Therefore, there exists an opportunity to provide methods to generate images of early galaxies with physical shapes that can be used to develop tools to extract morphological information from upcoming JWST data. Methods: The need to accurately simulate the data of the upcoming JWST Advanced Deep Extragalactic Survey (JADES) has led to the development of the JAdes extraGalactic Ultradeep Artificial Realizations (JAGUAR) mock catalog package (Williams et al. 2018). The package uses an empirical model for galaxy properties across cosmic time, including morphologies. However, the JAGUAR morphologies are modeled by simple symmetrical spheroids, an unrealistic assumption for early galaxies as shown in Figure 1. My first task will be to assign realistic morphologies to each mock galaxy. Here I will couple the results of hydrodynamical simulations, similar to Illustris, with existing observations of distant galaxies to inform the shape, structure, and composition of morphological components in each mock JAGUAR galaxy. By folding these in with JWST data simulation tools, I will produce physically motivated images of early galaxies with colors and luminosities that agree with observations of the distant universe. Following my creation of mock galaxy images, I will test techniques to extract morphological information from the data. Astronomers have attempted to tackle the problem of accurate characterization of distinct features in images with an ever growing suite of machine learning techniques. I will robustly examine existing techniques and find the approach that is best suited for categorizing asymmetric NSF Graduate Research Fellowship Plan Raphael E. Hviding Graduate Research Plan Statement and distorted morphologies of early galaxies. These include ensemble classifiers, such as a random forest decision trees employed in classifying galaxy mergers, e.g. Goulding et al. (2017), or deep learning algorithms trained on existing data sets, e.g. Sánchez et al. (2018). While both have been used to great effect, these procedures have only been tested in the local universe. I can therefore adapt and test candidate algorithms on my simulated images, measuring the method’s ability to recover input parameters. I will evaluate the efficacy of a broad set of techniques to find those that extract morphological parameters accurately for early galaxies over a wide range of cosmic times. The Steward Observatory at the University of Arizona is the ideal host institution for this endeavor, hosting the Principal Investigator of the NIRCam instrument on JWST (Professor Marcia J. Rieke) and the primary authors of the JAGUAR project (C. C. Williams and K. N. Hainline). I am therefore confident I can integrate my enhanced morphological modelling with current mock catalog efforts. Ultimately, I will produce a data analysis framework optimized for extracting morphological information from early galaxies validated through observations and simulations which will help determine why galaxies look the way they do today. Fig. 1: Young galaxies from current observations on the left (Bowler et al. 2017) compared with current models in the JAGUAR mock catalog on the right (Williams et. al 2018). Note the discrepancy in the level of structure. Conclusions and Broader Impact: My simulations of early galaxies and corresponding extraction algorithms will have immediate implications for the characterization of complex morphologies of young galaxies. These will be useful for any large-scale extraction of galaxy parameters in the early universe and essential for the success of the extragalactic component of the JWST mission. My tools will be made publicly available to the astronomical community to further the understanding of galaxy evolution to one day understand the nature and fate of all galaxies. In addition, I aim to bring the results of my research to the general public in Tucson. Given the intuitive nature of galaxy shapes, I aim to create lesson plans for the elementary school children I work with through Project ASTRO. By using my simulated images of galaxies, I will present a realistic view of the scientific capabilities of JWST in a fun and informative manner to inspire curiosity about extragalactic astronomy. Furthermore, simulated JWST observations of realistic images of galaxies are ideal for public lectures. By presenting a compelling visual narrative of watching galaxies change shape over time, I can communicate the main outcomes of my study that uniquely engages an audience. I plan to present my work at public talks that are held throughout Tucson, including the Steward Observatory weekly lectures and the Astronomy On Tap series. With support from the NSF Graduate Research Fellowship, I can develop useful and relevant tools for the astronomical community while taking advantage of the intuitive nature of galaxy shapes to bring high level cutting edge science questions to the general public. References: Agertz+, 2009, MNRAS, 397, L64 Elmegreen+, 2009, ApJ, 692, 12 M i h os+, 1996, ApJ, 464, 641 Bowler+, 2017, MNRAS, 466, 3612 Genel+, 2014, MNRAS, 445, 175 Sánchez+, 2018, arXiv: 1807.00807 Elmegreen+, 2007, ApJ, 658, 763 Goulding+, 2017, PASJ, 70, S37 Williams+, 2018, ApJS, 236, 33	Winner!
94	Kyle T. David Introduction. Arrow worms (chaetognaths) are a phylum of free-living predatory marine plankton. They are the second most abundant zooplankton group and represent a significant proportion of marine biomass1. Despite their abundance and ecological significance, arrow worms are very poorly understood. Charles Darwin2 commented on the “remarkable… obscurity of their affinities” and in the 174 years since, arrow worms have been placed in many different bilaterian groups, including nematodes, annelids, molluscs, crustaceans, arachnids, and chordates1. Most modern molecular analyses place arrow worms within protostomes (Fig. 1), but a consensus has not yet been reached3,4,5. Internal relationships within the phylum are similarly ambiguous1,6. I will broadly sequence arrow worm transcriptomes to determine relationships within and outside the group and use these transcriptomic data to elucidate the evolution of development within animals. 3,4,5 Figure 1. Four conflicting topologies that have all been recently recovered from molecular phylogenies Aim 1: Infer a Robust Chaetognath Species Tree. The inclusion of arrow worm transcriptomes to a larger protostome dataset will add significant power to phylogenetic analyses and resolve evolutionary relationships that have confounded biologists for hundreds of years1. Aim 2: Explore the Origins of Bilaterian Development. Though considered protostomes by most modern researchers, arrow worms possess deuterostome-like development (enterocoely, secondary Photo: Michael Le Roux mouth formation, radial cleavage). This makes arrow worms uniquely positioned to explore novel questions on the origins of development in bilaterians. If arrow worms are indeed the sister-group to protostomes (Fig. 1A), it is likely that deuterostomous development was present in the bilaterian ancestor. Alternatively, if arrow worms are instead nested somewhere within protostomes (Fig. 1B-D), it is likely these features are an example of convergent evolution. Methods. The Halanych Lab has an established history of collecting and sequencing transcriptomes of non-model marine invertebrates. Our lab has sequenced and annotated 59 transcriptomes listed on NCBI’s Sequence Read Archive (SRA). I will collect specimens of at least one representative from each of the 11 arrow worm families recognized by the World Register of Marine Species. I will be able to accomplish this through a previously established relationship with Dr. Janet Voight, an Associate Curator at the Field Museum of Natural History, who has access to these groups as well as with samples previously collected from my lab. I also aim to participate in the Graduate Research Internship Program (GRIP) available to GRFP fellows, which would allow me the opportunity to intern at the Smithsonian under Dr. Jon Norenburg in order to study and sample their collections. It may be necessary to collect from the field as well, which will be possible through research cruises like the Icy Inverts Antarctica Cruises with which my lab has a history of participation. RNA samples will be extracted, prepared, and sequenced through previously validated Halanych lab protocols3. The generalized bioinformatics pipeline is represented in Figure 2. I will use the skills I have learned from my recent participation in the Workshop on Molecular Evolution to infer maximum likelihood and Bayesian gene and species trees while using a variety of model assumptions and parameters in a comparative approach. Several deuterostomes (sea urchin, acorn worm, mouse, human) will serve as an outgroup. Figure 2. Simplified bioinformatics workflow for species and gene tree inference Intellectual Merit. There is currently only a single arrow worm sequence on the SRA. This project will increase the amount of genetic data for this poorly understood group by an order of magnitude. A well resolved tree will also provide a phylogenetic framework for understanding the evolution of several key features in animal evolution and provide evidence for the ancestral bilaterian state. Arrow worms are known to have many unique features including lamellar photoreceptors7 and mosaic hox genes8 in addition to a putative whole genome duplication event9. Increasing the availability of coding sequences in this group will allow myself and others to explore expansions/losses of several significant gene families (e.g., opsin and hox genes) and test for evidence of whole genome duplication within this enigmatic group. Broader Impacts. Results will be disseminated widely to expert (i.e., publications, symposia, talks) and non-expert (i.e., Skype a Scientist, outreach events, for details see Personal Statement) audiences. Through connections already established with faculty, I will also be able present my work as a guest lecturer through Auburn University’s Alabama Prison Arts + Education Project, which provides pre-college classes to prisoners. In 2016, the New York Times reported that inmates who participate in college programs have a 4% re-offence rate, creating a 500% return on investment in prison education initiatives. Alabama law does not allow prisoners to take remote classes meaning courses must be run on-site and in-person, something that would only be possible for me to participate in with GRFP support. All assembled transcriptomes and raw reads from this project will be made publically available on the SRA. I am committed to open-source software and will continue to upload all scripts required to reproduce analyses to my public repository (github.com/KyleTDavid). I will also mentor students through the NSF Research Experience for Undergraduates (REU) program, of which my lab is a participating member in computational biology. Students will receive a primer in basic programming skills and an introduction to phylogenomic workflows, as well as an opportunity to pursue independent projects. [1] Bone & Pierrot-Bults. (1991). Oxford University Press. [2] Darwin. (1844). Journal of Natural History. [3] Kocot et al. (2017). Systematic biology. [4] Marlétaz et al. (2006). Current Biology. [5] Matus et al. (2006). Current Biology. [6] Gasmi et al. (2014). Frontiers in zoology. [7] Goto et al. (1984). Cell and tissue research. [8] Papillon et al. (2003). Development genes and evolution. [9] Marlétaz et al. (2008). Genome biology. 2	Winner!
95	Gravitational Waves in Astrometry and Pulsar Timing Arrays Abstract: Gravitational waves are fluctuations in the fabric of spacetime and, in general ​​ relativity, they can be characterized by two polarizations states (commonly called the tensor E and B modes). A background of such gravitational waves can be detected through astrometric measurements and pulsar timing arrays, and the angular power spectra of these measurements for each polarization state is known. However, the capability of these experiments for localizing gravitational wave sources has not been studied in depth—this is of great importance since it will facilitate comparison with electromagnetic data. In addition, it is often assumed that a background of gravitational waves will conserve parity; however, this is not guaranteed since sources such as supermassive black hole mergers are known to produce circularly polarized gravitational waves that may give rise to chiral backgrounds. Thus, my intention is to determine how the locations of gravitational wave sources may be determined from astrometry and pulsar timing array data, as well as calculate the parity-breaking power spectra. Proposal: The advent of gravitational wave astronomy has provided a new window onto the ​​ universe for astrophysicists. Traditional barriers that are opaque to electromagnetic radiation can be penetrated by gravitational waves, which thus provide a novel way to research black holes, early universe processes, and other phenomena that are otherwise difficult to study using light. While it is possible to detect gravitational waves by direct observation, the existing interferometers of LIGO and Virgo are not suitable for studying the polarization content of gravitational waves, since at least five detectors are required to isolate the polarization states. Pulsar timing arrays and astrometric measurements may provide better constraints on the polarization content of gravitational waves. Light interacting with a gravitational wave will have its trajectory altered and this causes the apparent position of light sources to be deflected and the arrival of light pulses to be delayed. Thus, it is possible to study a stochastic background of gravitational waves using astrometric missions such as Gaia by observing how the positions of a field of stars changes over time, as well as by timing millisecond pulsars against each other to measure correlated signatures in pulse arrival times. The correlation functions and angular power spectra of such experiments have been calculated for each polarization state of gravitational waves by a variety of methods [1-3]. The power spectra will provide a starting point for analyzing a gravitational-wave background detected by astrometry or pulsar timing arrays; however, clearly there is more to a study of gravitational wave backgrounds than just the polarization content. I propose to investigate two key aspects of these gravitational wave experiments: their ability to localize sources and their ability to probe chiral gravitational waves. Neither issue has been studied in great depth—however, both projects will enhance experiments by facilitating comparison with non-gravitational-wave data and challenging the common, but possibly false, assumption that a background of gravitational waves will preserve parity. First of all, after detecting a background of gravitational wave, is it possible to localize sources of the signals? In other words, will the signal exhibit any preferred directions? I propose to study whether a gravitational wave background will exhibit such an asymmetry using bipolar spherical harmonics [4, 5]. The coefficients of the bipolar spherical harmonics can be constructed from the same quantities used to calculate the power spectra and should be able to show whether the signal will have a dipole asymmetry. In addition, it is sometimes assumed that this stochastic background of gravitational waves will conserve parity. Then the energy densities 1 Wenzer Qin NSF GRFP: Research Plan in the left and right-handed circularly polarized states should be the same, which implies that cross-correlation of tensor-E and B modes and the redshift-B mode cross-correlation will be zero. However, sources such as supermassive black hole binaries are expected to to emit circularly polarized gravitational waves, and in this case the detected background would be chiral and the cross-correlations would not vanish. Therefore, I propose to calculate the parity-breaking power spectra, i.e. the E-B and redshift-B cross-correlations for a chiral gravitational wave background. Since gravitational wave astronomy is a relatively new field, the results of these research projects will be useful for the development of pulsar timing arrays and astrometric surveys as gravitational wave detectors. The ability to locate gravitational wave sources from these experiments will be extremely useful, as it will allow us to compare measurements with electromagnetic data from the same region and therefore study gravitational-wave-emitting phenomena in greater depth. In addition, as stated earlier, for a chiral gravitational wave background, the EB and redshift-B cross-correlations will not vanish. Normally, if these functions were expect to be zero, then they could be used as null tests for systematic errors in these experiments; however, since we do expect some astronomical sources to emit chiral gravitational waves, subtracting these cross-correlations out of the data may result in the loss of real and important physics. Therefore, an expectation of what the cross-correlations should look like beforehand will be important for analyzing the data from astrometry and pulsar timing arrays, and that is what this project seeks to establish. I would like to carry out this research at the California Institute of Technology, since this topic of research overlaps significantly with the work of Sterl Phinney and Yanbei Chen. I would also be happy to work with Daniel Holz at the University of Chicago or Franz Pretorius at Princeton University, since their research also focuses heavily on gravitational waves. The NSF ​ fellowship will support me in my goals by allowing me to begin research in graduate school as soon as possible and spend as much time as I can developing the skills and knowledge necessary for my future career in physics research. Timeline: Years 1-2: Calculate the bipolar spherical harmonic coefficients from the redshifts and ​ position deflection of stars. Use these to determine whether the gravitational wave signal will exhibit a preferred direction/dipole. Publish results on how to use astrometry and pulsar timing arrays to locate gravitational wave sources. Year 3: Use the total-angular-momentum formalism to calculate the parity-breaking ​ cross-correlation power spectra. Publish the functions and their derivations. [1] L. G. Book and E. E. Flanagan, “Astrometric Effects of a Stochastic Gravitational Wave Background,” Phys. Rev. D 83, 024024 (2011) [arXiv:1009.4192 [astro-ph.CO]]. [2] L. O'Beirne and N. J. Cornish, “Constraining the Polarization Content of Gravitational Waves with Astrometry,” Phys. Rev. D 98, no. 2, 024020 (2018) [arXiv:1804.03146 [gr-qc]]. [3] D. P. Mihaylov, C. J. Moore, J. R. Gair, A. Lasenby and G. Gilmore, “Astrometric Effects of Gravitational Wave Backgrounds with non-Einsteinian Polarizations,” Phys. Rev. D 97, no. 12, 124058 (2018) [arXiv:1804.00660 [gr-qc]]. [4] A. Hajian and T. Souradeep, “Measuring statistical isotropy of the CMB anisotropy,” Astrophys. J. 597, L5 (2003) [astro-ph/0308001]. [5] L. G. Book, M. Kamionkowski and T. Souradeep, “Odd-Parity Bipolar Spherical Harmonics,” Phys. Rev. D 85, 023010 (2012) [arXiv:1109.2910 [astro-ph.CO]]. 2	Winner!
96	NSF GRFP Application – Research Proposal Research Question and Intellectual Merit: How does internal migration influence the geographic diversity of intergenerational income mobility (IIM1) in the U.S.? The IIM literature has seen a surge in activity, in part thanks to Chetty et al. (2014) (henceforth CHKS), who link several years of IRS tax data to investigate IIM in the U.S. on an unprecedented scale. Among their key findings is that the expected economic outcomes of a child vary drastically based on their commuting zone (CZ) of origin. Both CHKS and much of the literature that has followed it have focused on the importance of the characteristics of where an individual is from in influencing their expected income mobility as opposed to where (or whether) they go. This may be in part because CHKS themselves appear to put the issue to rest: they find that their IIM estimates do not change meaningfully after limiting their sample to individuals who stay in their original commuting zone, nor do they appear to be strongly correlated with CZ-level net migration rates. However, limiting the sample to stayers is insufficient to fully investigate the role of self- selected migration in forming the landscape of IIM in the U.S. if this sample is endogenously determined2, and focusing on more narrow migration patterns than net rates uncovers a more suggestive relationship. Figure 1 juxtaposes state-level IIM estimates with the college graduate outflow rate3 in each state. With few exceptions, the most income-mobile states in the country (namely, those in the rural Midwest and the Mountain States) also exhibit some of the highest rates of out-migration. Table 1 reveals this visual association to be statistically robust on a basic level after controlling for the most important correlates of IIM that CHKS identify. This project will more meticulously consider the importance of internal migration in generating spatial variation in IIM through the development and estimation of a structural model. In doing this, I will provide new insight on an oddity that has not been thoroughly probed hitherto: the fact that children from underprivileged backgrounds seem to fare the best when coming from some of the most remote and forgotten-about places in the country. Creating a formal model will also allow me to add to the relatively much smaller recent literature that carefully evaluates policy counterfactuals regarding IIM. Methodology: I intend to construct and solve a lifecycle model that follows the migration and child-rearing decisions of agents from high school graduation into early adulthood. The model will expand the classic Becker and Tomes (1979) framework to incorporate local labor market conditions and moving opportunities. Agents are born in a home CZ to parents of a certain income level, who also endow them with a set of inherited attributes and human capital investments. The children then choose whether to stay or move to a new location, after which they select how many children to have of their own and how much to invest in them. Investments in children are differentially costly across locations to reflect heterogeneity in public school quality. In this framework, local labor market quality will induce dual effects on IIM: stronger labor markets will improve the outcomes of stayers but will also depress incentives for agents to leave and find a better match. This may provide motivation for recent empirical findings that conventional measures of local labor market quality have little predictive power for IIM. 1 Measured as the expected national income percentile in 2011-2012 of a child born in 1980-1982 to parents who were in exactly the 25th national income percentile over the years 1996-2000. 2 If a highly income mobile CZ also has high rates of out-migration, and natives who stay do so because they received unusually good income realizations in their home, then the CZ will continue to exhibit high levels of IIM even after the sample restriction. The related-but-distinct thought experiment I consider is what would happen to IIM in the U.S. if those that would move from their home CZ are somehow restricted from doing so. 3 Measured by taking the sample of income-earning college graduates from the 1980-1982 birth cohorts in 2011- 2012 and computing the percentage of individuals born in a state who are observed living elsewhere. 1 Garrett Anstreicher NSF GRFP Application – Research Proposal Figure 1: IIM (Percentile) and College Table 1: OLS Estimates for Various Graduate Outflow (%) in U.S. States Covariates of State-Level IIM VARIABLES IIM College grad outflow 0.0899** (0.037) Share single mothers -0.728*** (0.240) Student-teacher ratio -0.336 (0.193) Constant 66.81*** (13.19) Observations 49 R-Squared 0.697 Table Notes: Standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.1. Non-displayed controls include share black, Theil segregation index, college graduation rate, labor force participation rate, high school graduation rate, violent crime rate, and Gini coefficients. Figure Notes: IIM estimates for top map from http://www.equality-of-opportunity.org/data/. Data for bottom map from 2011 and 2012 American Community Survey (Ruggles et al., 2017). The central mechanism I aim to capture resembles an intranational brain drain: parents from areas with cheap human capital and poor labor market conditions will face incentives to heavily invest in their children, who in turn will leave their home CZ. The process of leaving will allow the child to select their most compatible labor market, greatly increasing their chances of claiming a higher wage and bolstering the measured IIM of their place of origin. Broader Impact and Conclusion: In addition to motivating the high IIM of remote areas, I intend to evaluate the efficacy of various educational policies. An example is New York’s Excelsior Scholarship, which remits tuition under the stipulation that recipients stay in the state for some time following graduation. Such a policy may work well in increasing the supply of college graduates in states where opportunities are abundant such as New York, but it may not be nearly as effective in rural areas with more condensed wage distributions. Expanding this model to consider general equilibrium effects could also allow me to address myriad issues. How will geographic wage distributions in the U.S. evolve over time in response to self-selected migration flows? Will the brain drain I capture lead to further economic deterioration in the rural U.S., or will its declining living costs induce more highly skilled individuals to return? These are important questions that my model may be extended to answer. References Chetty, R.; Hendren, N.; Kline, P. and Saez, E. “Where is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States.” The Quarterly Journal of Economics, 129(4): 1553-1623, 2014. Becker, G. and Tomes, N. “An Equilibrium Theory of the Distribution of Income and Intergenerational Mobility.” Journal of Political Economy 87(6): 1153-1189, 1979. Ruggles, S.; Flood, S.; Goeken, R.; Grover, J.; Meyer, E.; Pacas, J. and Sobek, M. IPUMS USA: Version 8.0 American Community Survey. Minneapolis, MN: IPUMS, 2018. https://doi.org/10.18128/D010.V8.0. 2	Winner!
97	Measuring Rayleigh Wave Phase Velocity in the Antarctic Upper Mantle from Ambient Seismic Noise Background and Motivation Two compelling questions make the Antarctic region worth studying: 1) Why is there a significant age difference between West and East Antarctica? And 2) How exactly will global sea-levels rise in the future? Divided by the Transantarctic Mountains, West Antarctica is significantly younger than the East Antarctica craton (Hansen et al., 2014). Resolving the age difference between Antarctica’s two halves will help us understand the tectonic history and evolution of the Antarctic region. On the other hand, sea-level rise has serious implications on infrastructure displacement as we lose land surface area. Simulations of global sea-level rise have high uncertainty but could benefit from incorporating bedrock uplift, mantle viscosity, and geothermal heat flux (Gomez et al., 2015). Approaching both questions requires better constraints on mantle properties underneath Antarctica. Investigating Antarctica poses unique challenges for many traditional measurement techniques. About 98% of Antarctica’s land surface area is underneath a thick ice-sheet (Fretwell et al., 2013), and uncertainty in mantle viscosity convolutes anticipating changes in the Earth’s surface in response to ice-sheet melting and growth. These two factors make measuring geothermal heat flux impractical. Furthermore, traditional seismic tomography methods rely on earthquakes for seismic signals which are scarce in and around the Antarctic region. An emerging approach in seismic tomography is to use ambient seismic noise – signals primarily generated from interactions between Earthquakes 1966-2017 Figure 1: Topography map of ocean water waves and solid Earth – in addition to earthquake data to the Antarctic plate. Colors characterize mantle properties. This method has been published by represent bedrock elevation in Bensen et al. (2007) and has been widely adopted by seismologists meters and pink dots are earthquakes that occurred in with over a thousand citations. One study shows incorporating ambient 1966-2017. Tectonic plate noise can increase phase-velocity map resolution in the Indian Ocean boundaries are plotted as by 20% relative to maps generated by relying solely on earthquake white lines. data (Ma & Dalton, 2016). While there exists many studies employing ambient noise, the number of similar studies on the Antarctic region are relatively scarce. Proposed Methodology I will obtain long-period vertical component data from the Incorporated Research Institutes of Seismology (IRIS) for all active, unrestricted stations south of -55 degrees latitude which encompasses all of Antarctica. The data will be processed in 4 h stackable (i.e. able to be combined through addition via linearity) windows. To ensure a high signal-to-noise ratio in our phase arrival time measurements, I will discard 4 h windows that are not entirely full. Correlation measurements between all station pairs will be done in the frequency domain by computing the cross-spectrum ρ (ω) as done in Ekström (2014): ijk Kevin Trinh NSF Research Statement 𝑢 (𝜔) 𝑢∗ (𝜔) 𝜌 (𝜔) = 𝑖𝑘 𝑗𝑘 𝑖𝑗𝑘 √𝑢 𝑖𝑘(𝜔) 𝑢 𝑖∗ 𝑘(𝜔)√𝑢 𝑗𝑘(𝜔) 𝑢 𝑗∗ 𝑘(𝜔) The letter u represents a 4 h seismogram passed through a fast Fourier transform. ω is frequency, i and j are station indices, and k is a 4 h window index. The asterisk * denotes a complex conjugate. Performing an inverse fast Fourier transform on the cross-spectrum yields a cross- correlation in the time domain. Measurements of phase arrival times can be made from cross-correlations. These arrival times will be used to perform inversion and thus yield phase-velocities for many small, discrete regions in Antarctica. Earthquake data can be incorporated with ambient noise data to account for regions with low data count (i.e. not many paths traversing the discrete region). Additionally, the smoothing of our inversion will account for regions with little data and can be adjusted to yield the best results. I will conduct numerous tests to identify optimal smoothing parameters and relative weighing between ambient noise and earthquake-based data. These steps result in a 2D phase-velocity map and can be repeated to map varying depths of the Antarctic upper mantle. Anticipated Results The speed at which wave phases propagate through solid Earth is related to material properties such as temperature, composition, and partial melt. I expect to see West and East Antarctica to be dominated by slow- and fast-velocity anomalies, respectively, which should agree with past studies using p-waves to image the Antarctic mantle. Improved resolution in tomographic maps of the Antarctic upper mantle may help me observe undiscovered geological features such as cratons and oddly pronounced and heterogenous velocity anomalies. Proposed Timeline: Year 1: Download and process seismogram data from IRIS from 1900 to 2017. Year 2: Generate 2D seismic tomography maps. Year 3: Identify optimal smoothing parameters and relative weights. Repeat mapping process for varying depths of the Antarctic upper mantle. References Bensen, G. D., Ritzwoller, M. H., Barmin, M. P., Levshin, A. L., Lin, F., Moschetti, M. P., et al. (2007). Processing seismic ambient noise data to obtain reliable broad-band surface wave dispersion measurements. Geophysical Journal International, 169(3), 1239–1260. https://doi.org/10.1111/j.1365-246X.2007.03374.x Ekström, G. (2014). Love and Rayleigh phase-velocity maps, 5–40 s, of the western and central USA from USArray data. Earth and Planetary Science Letters, 402, 42–49. https://doi.org/10.1016/j.epsl.2013.11.022 Fretwell, P., Pritchard, H. D., Vaughan, D. G., Bamber, J. L., Barrand, N. E., Bell, R., et al. (2013). Bedmap2: improved ice bed, surface and thickness datasets for Antarctica. The Cryosphere, 7(1), 375–393. https://doi.org/10.5194/tc-7-375-2013 Gomez, N., Pollard, D., & Holland, D. (2015). Sea-level feedback lowers projections of future Antarctic Ice-Sheet mass loss. Nature Communications, 6, 8798. https://doi.org/10.1038/ncomms9798 Hansen, S. E., Graw, J. H., Kenyon, L. M., Nyblade, A. A., Wiens, D. A., Aster, R. C., et al. (2014). Imaging the Antarctic mantle using adaptively parameterized P-wave tomography: Evidence for heterogeneous structure beneath West Antarctica. Earth and Planetary Science Letters, 408, 66–78. https://doi.org/10.1016/j.epsl.2014.09.043 Ma, Z., & Dalton, C. A. (2016). Evolution of the lithosphere in the Indian Ocean from combined earthquake and ambient noise tomography. Journal of Geophysical Research: Solid Earth, 122(1), 354–371. https://doi.org/10.1002/2016JB013516	Winner!
98	rates of gun-related homicides and emergency department visits are notoriously higher in the U.S. than in other developed countries. Gun violence disproportionately affects low-income and minority residents; homicide is the first (second) leading cause of death for young black (Hispanic) men [1]. Early death and incarceration contribute to staggering numbers of “missing” black men (83 black men per 100 black women), creating other social problems in minority communities [2]. Therefore, decreasing gun crime would reduce inequality by improving outcomes for disadvantaged minority groups. In addition, reducing gun violence would benefit taxpayers by decreasing Medicaid and Medicare spending on gun-injury related health care.1 Therefore, governments have several reasons to examine potential policy solutions to reduce gun crime. Background: Revitalizing urban areas with green spaces could be part of a policy solution to reduce gun violence, although the effects of vegetation on crime are theoretically and empirically ambiguous. On the one hand, trees and shrubbery could provide hiding places for criminals and inhibit neighborhood surveillance by obstructing views of the streets. On the other hand, greener spaces could deter crime (1) by providing community gathering spaces, thereby placing more eyes on the ground and (2) by dampening criminals’ sense of aggression through the physiologically calming effects of nature [3]. Understanding how green spaces affect criminal activity has important consequences for safety in urban neighborhoods. Intellectual Merit: A naïve comparison of gun crime between more and less green areas potentially includes selection bias, since more affluent, less crime-ridden areas also tend to be greener. Even a comparison across time for areas that become green can produce biased estimates if a confounding factor like gentrification of a neighborhood simultaneously increases green spaces and decreases crime. To circumvent such endogeneity, my research would exploit vacant lot renovation programs as close-to exogenous variation in green spaces, thereby identifying the causal impact of greening spaces on gun crime rates: a policy-relevant parameter. By combining this policy-induced variation with objective, detailed data on gunshots and geographic imagery and data across several American cities, my proposed research seeks to uncover how greening urban spaces affects gun violence: one important category of crime. Studying this effect is challenging with typical, reported crime data since renovating lots might affect reporting rates (if more people are present to report crime, for instance) and actual amounts of criminal activity. My study will address this issue by using new data from ShotSpotter gunshot sensors (described below). Furthermore, existing studies on the effects of greening vacant lots on crime rely on relatively sparse crime data in a single locality [4] [5]. In contrast, I would utilize data on a long time horizon, with more frequent observations,2 across several counties and municipalities.3 Therefore, I could contribute longitudinal and more precise, generalizable estimates of the effect of greening spaces on gun crime to the existing literature. Data: ShotSpotter is a technology that captures incidents of gunfire using audio sensors, providing comprehensive data on these events including precise geographic coordinates and timestamps. A key advantage of these data is that they are not subject to reporting bias or underreporting, thereby providing a more objective measure of gun crime: my outcome measure [6]. In order to measure 1 Medicaid and Medicare picked up nearly half of the costs of caring for Chicago survivors of gunshot wounds among costs between 2009 and mid-2016 analyzed by the Chicago Tribune. 2 According to Carr & Doleac, 911 reports of shootings (reports of assault with a dangerous weapon) capture just 12% (2-7%) of all gunfire incidents recorded by ShotSpotter. 3 ShotSpotter has been employed in at least 90 U.S. counties or cities since 2000, of which I currently have access to over 1,500 locality-months’ worth of data representing 27 unique localities. the treatment (greening of vacant lots), I will utilize cities’ databases on vacant lot renovations where readily available.4 To allow for analysis in cities where such databases are not available, I will use machine-learning algorithms on high-resolution digital aerial imagery and LIght Detection and Ranging (LIDAR) data to classify areas as green spaces, trees, or other objects of urban spaces, akin to methods described by Zhou & Troy [7]. While I have not worked with these kind of geographic data before, my background in machine learning coupled with support from GIS experts at my university’s library would allow me complete this step of my project. I will combine these data in ArcGIS to create a lot-time level panel dataset.5 Methods: The first part of the empirical analysis involves an event study, difference-in-differences (DD) approach before and after greening of vacant lots occur. Control observations will be vacant lots that were not renovated. The primary outcome measure will be the number of ShotSpotter- detected gunfire incidents within a specified radius from each lot. I will check the results using different radii from lots both (1) as a robustness check and (2) to determine how close to a green lot one should live to experience its effects: a question that remains underexplored in the current literature. Regression analyses will include year and month fixed effects (FE) and lot-, Census block-, or city-level FE. In specifications with city FE, I will control for Census block-level covariates available from the American Community Survey (ACS) including median income, home ownership rates, racial demographics, and other variables that could be associated with crime levels. I will cluster standard errors at the lot level: the level of treatment. Further analyses would adjust for spatial correlation. In addition to basic DD estimates, I will also employ methods of synthetic control to establish comparison groups that better demonstrate common pre-trends. A potential concern with this quasi-experimental approach is that greening spaces might just push the same amount of crime to other areas of a city not-yet renovated. However, this phenomenon would attenuate my estimates of the treatment effect since crime would increase in control areas relative to treatment areas.6 To further investigate the prevalence of this phenomenon, I could look at citywide impacts before and after periods of lot restoration. Assuming encouraging findings, I would also complete a cost-benefit analysis. I would gather information on the average cost to turn a vacant lot green and maintain it, and associated administrative costs. On the benefits side, I will use estimates of the social costs of gun crime (criminal justice claims, loss of life, and other costs [8]) to estimate costs avoided because of decreased gun violence. More broadly, I will translate my findings into units like dollars and lives saved that are salient and easily interpretable for policy-makers. I will also submit my findings to peer-reviewed publications and present at crime, urban policy, and economics conferences. This research has important implications for determining the potential of urban renewal policies, such as vacant lot restoration programs, to reduce crime. If vacant lot restoration programs are effective at curbing gun violence, minority populations would disproportionately benefit. References: [1] CDC data. [2] Wolfers, J., et al. (2015, April 20). The New York Times. [3] Kuo, F. E., & Sullivan, W. C. (2001). Environment and Behavior, 33(3). [4] Branas, C. C., Cheney, R. A., MacDonald, J. M., Tam, V. W., Jackson, T. D., & Ten Have, T. R. (2011). American Journal of Epidemiology, 174(11). [5] Garvin, E. C., et al. (2013). Injury Prevention, 19(3). [6] Carr, J., & Doleac, J. L. (2016). Brookings Research Paper. [7] Zhou, W., & Troy, A. (2008). International Journal of Remote Sensing, 29(11). [8] Gani, F., Sakran, J. V., & Canner, J. K. (2017). Health Affairs, 36(10). 4 For example, Branas et al use such a database to study Philadelphia, PA. 5 A lot-day dataset is possible for cities in which the exact renovations dates are known. Otherwise, I will create a dataset using whatever courser time level lot greening could be detected using aerial imagery and LIDAR data. 6 To determine the effects of spillover effects more exactly, future randomized trials could consider varying the proportion of lots renovated within a city among a sample of several cities.	Winner!
99	Catalyzed by Amino Acids in Enzymes Background: Proton-coupled electron transfer (PCET) reactions refer to the sequential or concerted transfer of both a proton and an electron. PCET reactions are indispensable to life, occurring throughout cellular respiration and photosynthesis, and they even drive the photocycles of many light-sensing photoreceptor proteins. These proteins often accomplish this chemistry through the use of tyrosine and tryptophan residues, which are oxidized to their corresponding radical species that subsequently play a role in the reaction mechanism. PCET reactions involving tyrosine and tryptophan are of particular importance to the biological processes mentioned above. Motivation: Despite the emerging prevalence of tyrosine (Y) and tryptophan (W) mediated PCET reactions, much of the basic science remains unknown. The local environment of these residues varies from coordination to a metal ion, such as iron or manganese, to being buried in a hydrophobic core. How does the local protein environment stabilize and direct the reactions of these radicals without causing oxidative damage to the protein? Are there any common trends in the existing enzyme systems that utilize tyrosine or tryptophan residues for facilitating catalysis? These questions are difficult to study experimentally due to turnover times dictated by conformational changes, high redox potentials, and the reactive nature of the radical intermediates. To overcome these limitations and isolate a single, well-defined active site, a small (cid:68)-helical protein with a buried Y or W residue (denoted (cid:68) X, where X refers to the 3 catalytic amino acid), has been created. Without significant perturbation of the protein structure, the native Y/W residues can be reversibly oxidized, and the redox potential can be tuned by incorporating Y/W chemical derivatives.2 Proposed Study: I will use the well-characterized (cid:68)X systems of our experimental collaborators 3 as a base to build an extensive computational model, with the goal of understanding the role of the solvated protein environment and electronic effects on amino acid mediated PCET reactions. To address these issues, I will use two main approaches, which will provide the necessary information for the culminating study on PCET kinetics. In my first approach, I will investigate the effect of the protein environment by computing the redox potential of Y/W derivatives using density functional theory (DFT) in both implicit solvent and with electrostatic embedding to include the (cid:68) X protein environment. In my second approach, I will use molecular dynamics (MD) 3 to probe the role of water and protein conformational changes in this system. These two approaches will provide the information needed to compute the PCET rate constant.3 Aim 1: I will assess how the protein environment influences the redox potential by computing the redox potentials of a series of amino acid derivatives with implicit solvent and in the protein environment through electrostatic embedding. The partial charges representing the protein environment will be obtained from the MD simulations in Aim 2. The experimentally measured relative redox potentials from our collaborators on tyrosine and its Table 1. Mean unsigned errors (MUE) of the derivatives in the (cid:68) X scaffold will be used to 3 relative redox potentials with respect to p-cresol. benchmark our methods. The influence of the As shown, the mean unsigned error (MUE) is protein has been viewed as a constant shift below the accepted error threshold of 1 kcal/mol. compared to aqueous values, and finding theoretical Data includes a series of fluorinated tyrosine evidence to support or disprove this assumption, as derivatives. The calculations were performed at well as an explanation, would be valuable. the DFT B3LYP/6-31++G** level with the PCM solvation model.1 Preliminary data (Table 1) for implicit solvation studies reproduces the ordering and magnitude of the experimental redox potentials in the protein very well, suggesting that the protein is uniformly affecting each chemical species and cancels out when computing relative redox potentials. Aim 2: I will characterize the role of protein conformational motions and water accessibility in radical formation and decay. Water serves as the proton acceptor in many PCET systems, and access to the often-buried redox-active residues may be gated by conformational changes. MD studies will be used to build a hypothesis of the mechanism of water entrance into the hydrophobic interior of the (cid:68) X proteins. The water exchange dynamics and probability distribution will be 3 assessed from calculation of the radial distribution function and the average residence time of water in the hydrophobic pocket. In addition, the protein conformational changes associated with water entering and leaving the pocket will be analyzed. If water is found at a high occupancy, the representative conformations for water will be examined in Aim 1. Aim 3: I will predict the rate constants for these PCET reactions using a vibronically nonadiabatic PCET theory and will probe the electronic effects as the substituent on the Y or W is changed. The information calculated from the previous aims will be used as input to the analytical rate constant expression for vibronically nonadiabatic PCET. The underlying PCET theory is related to Marcus theory for electron transfer reactions,4 but the transferring proton is also treated quantum mechanically to include hydrogen tunneling.3 In the past, this theory has been applied to enzymes such as soybean lipoxygenase,5 illuminating the role of protein motions on the large kinetic isotope effect observed experimentally. Application to the (cid:68) X system will provide an unprecedented 3 opportunity to conduct a systematic study on a series of substituted Y and W residues in a controlled protein environment. These results will allow a much deeper understanding of biological PCET reactions. Intellectual Merit: The proposed work aims at advancing our fundamental understanding of biologically relevant PCET reactions through theoretical examination. The (cid:68) X protein system 3 will allow the study of the protein environment and electronic effects in tyrosine and tryptophan residues, which can serve as a model for understanding important radical chemistry in DNA synthesis, cellular respiration, and photosynthesis. The knowledge gained from a well-constructed computational model can motivate the synthesis and incorporation of unnatural tyrosine and tryptophan analogs into biological systems, thereby tuning their energy transfer capabilities. Additionally, the incorporation of unnatural amino acids into proteins for use as fluorophores or mechanistic probes is growing rapidly. With these advancements, an understanding of these probes and their chemical reactivity in the protein environment will be crucial to successful protein design. Broader Impacts: To aid experimental collaborators and non-specialists in understanding this work, I will develop a biological PCET module for our webPCET Java server.6 The webPCET server is freely accessible and provides an overview of PCET and related research by illustrating the theoretical underpinnings through interactive calculations. By construction of this module, I will be improving the accessibility of theoretical chemistry to biochemists and enzymologists, who may otherwise shy away from mechanistic and chemical studies. 1a) Becke, A. D. Phys. Rev. 1988, 38 (6), 3098-3100; 1b) Miertuš, S.; Scrocco, E.; Tomasi, J. Chem. Phys. 1981, 55 (1), 117-129; (c) Becke, A. D. J. Chem. Phys. 1993, 98 (7), 5648-5652. 2) Martínez-Rivera, M. C.; Berry, B. W.; Valentine, K. G.; Westerlund, K.; Hay, S.; Tommos, C., J. Am. Chem. Soc. 2011, 133 (44), 17786-17795. 3) Soudackov, A.; Hammes-Schiffer, S., J. Chem. Phys. 2000, 113 (6), 2385-2396. 4) Marcus, R. A. J. Chem. Phys. 1956, 24 (5), 966-978. 5) Li, P.; Soudackov, A. V.; Hammes-Schiffer, S. J. Am. Chem. Soc. 2018, 140 (8), 3068- 3076. 6) webPCET Application Server, Yale University, http://webpcet.chem.yale.edu (2009); Hammes-Schiffer, S.; Soudackov, A. V.	Winner!
100	Bio-production of synthetic rubber using engineered Escherichia coli Introduction: In a society with both a growing dependence on energy and a depleting reservoir of fossil fuels, it has become increasingly important to design chemical syntheses that are sustainable, renewable and cost-effective. One synthesis of particular concern is that of isoprene, a precursor to synthetic rubber, since the current production relies on finite petrochemical sources.1 Recent advances in synthetic biology and metabolic engineering have made it possible to biosynthesize isoprene using glucose extracted from plant biomass, a renewable feedstock. Despite these advances, previously studied synthesis pathways report low product yield due to poor catalytic activity, making them economically unfeasible for large scale production.2,3 In order to overcome this roadblock, I propose to use a keto acid-mediated pathway to biosynthesize isoprene. Keto acids can be used as a selection in directed evolution (DE), a vital tool in the enhancement of enzyme activity.4 In order to employ DE however, enzyme specificity must be high enough for the desired conversion. Computational techniques, such as docking and funnel metadynamics, can be utilized to elucidate key amino acids involved in binding and catalytic active sites. These amino acids can then be mutated to enhance enzyme specificity. In this work, I focus on the conversion of citramalate to butanoic acid, a key step in the synthesis of isoprene, by expressing carnitine-CoA ligase (CaiC) and carnitine-coA transferase (mvaE) in E. coli. I hypothesize that engineering specificity and activity in heterologous CaiC and mvaE enzymes using a combined theory-experiment approach will increase butanoic acid yield, making the biosynthesis of isoprene more feasible for scale-up. Research Aims: The primary objectives of this project are to (1) manipulate specificity and (2) increase activity of CaiC and mvaE to optimize conversion of citramalate to butanoic acid, which can then be converted to isoprene through a series of organic reactions (Fig. 1). Figure 1: Workflow to sustainably produce isoprene from plant biomass by designing enzymes, supplementing experiment with modeling to engineer protein design, and using results from computation to inform experiment. Preliminary results: During the summer of 2018, I obtained preliminary data by working on the upstream synthesis of isoprene using engineered E. coli in Dr. Kechun Zhang’s lab at the University of Minnesota through the NSF-funded Center for Sustainable Polymers. Specifically, I was able to produce citramalate from glucose via citramalate synthase (CimA) using a keto acid-mediated pathway (Fig. 1). The next phase of the project is to convert citramalate to butanoic acid, the next intermediate in the biosynthesis of isoprene. Methods: (Aim 1) CaiC and mvaE enzymes are known to perform the desired reduction on carnitine, a molecule of similar structure and functional group composition to that of citramalate.5,6 I will use computational modeling to design more specific enzyme active sites for citramalate. The crystal structure for mvaE will be obtained from the Protein Data Bank. I will Kristen C. Vogt NSF GRFP Graduate Research Plan then build a homology model for CaiC based on the crystal structure of L-carnitine CoA- transferase (CaiB) and generate force fields describing carnitine and citramalate from quantum chemical calculations. I will run molecular docking of CaiC and mvaE enzymes with citramalate followed by molecular dynamics (MD) simulations to equilibrate systems. I will then use funnel metadynamics to calculate protein-substrate binding free energy to provide an estimate of the binding affinity between both enzymes and substrates.7 Insights gained from these simulations will inform key amino acid mutations to improve enzyme specificity. (Aim 2) Next, I will use directed evolution to improve enzyme activity using a keto acid- pathway and growth-based selection.4,8 I will use error-prone polymerase chain reactions (PCR) to create mutant libraries of CaiC and mvaE enzymes. PCR products will be cloned into a DNA backbone, electroporated into E. coli, and plated on Luria broth (LB) plates containing 100 g/mL Ampicillin. I will measure the total product formed from enzyme conversion using a 4- aminoantipyrene assay.8 Enzymes with readings 50% greater than parental standards will be selected for rescreening. Once both enzymes are optimized for activity and specificity, their DNA sequences will be ordered, replicated using PCR, ligated into a DNA backbone, and transformed into E. coli. I will then run fermentation for 48 hours in a 37 C thermoshaker running HPLC and OD every 12 hours to monitor butanoic acid production.9 600 Resources and support: In order to address these aims, I will work in collaboration with the Zhang and Truhlar groups at the University of Minnesota (UMN) to develop the experimental and theoretical components of isoprene synthesis, respectively. Access to supercomputing time through NSF’s XSEDE will allow for the proposed computational simulations. Intellectual Merit: The combined theory-experiment workflow outlined in this proposal is used to overcome low yield by improving enzyme activity and specificity in heterologous CaiC and mvaE enzymes. This methodology can be applied in any biosynthetic reaction to synthesize novel non-natural metabolites at higher yields over varying conditions.7 Even if highly accurate free energies are challenging to obtain from simulation, mechanistic information obtained from MD can be used to inform the next stage of experiment. Future directions of this project include converting butanoic acid to isoprene and investigating gene knockouts to further increase yield. Broader Impacts: The synthesis of isoprene, a commodity used in countless goods, such as adhesives, tires, and shoe soles, is currently unsustainable due to reliance on limited petroleum resources. Biosynthesizing isoprene using fermentation offers a renewable and cost-effective alternative. Increasing yield will make it feasible to utilize this technology in large scale production to move away from harmful, depleting syntheses and towards a more sustainable future. I plan to regularly present results from this work at conferences and make publications available to the public via open-access publication methods. Lastly, because parts of the above project, such as using recombinant DNA technology and running fermentation, lend themselves to undergraduate research, I will mentor and engage undergraduates in order to provide them with access to authentic research experiences early in their careers. 1. Singh, R. Org. Process Res. Dev. 2011, 15 (1), 175–179. 2. Zhao, Y.; Yang, J.; Qin, B.; Li, Y.; Sun, Y.; Su, S.; Xian, M. Appl. Microbiol. Biotechnol. 2011, 90 (6), 1915–1922. 3. Yang, J.; Nie, Q.; Liu, H.; Xian, M.; Liu, H. BMC Biotechnol 2016, 16. 4. Atsumi, S.; Liao, J. C. Appl. Environ. Microbiol. 2008, 74 (24), 7802–7808. 5. Eichler, K.; Bourgis, F.; Buchet, A.; Kleber, H. P.; Mandrand-Berthelot, M. A. Mol. Microbiol. 1994, 13 (5), 775–786. 6. Hedl, M.; Sutherlin, A.; Wilding, E. I.; Mazzulla, M.; McDevitt, D.; Lane, P.; Burgner, J. W.; Lehnbeuter, K. R.; Stauffacher, C. V.; Gwynn, M. N.; et al. J. Bacteriol. 2002, 184 (8), 2116–2122. 7. Limongelli, V.; Bonomi, M.; Parrinello, M. PNAS 2013, 110 (16), 6358–6363. 8. Bloom, J. D.; Labthavikul, S. T.; Otey, C. R.; Arnold, F. H. PNAS 2006, 103 (15), 5869–5874. 9. Kuhn, J.; Müller, H.; Salzig, D.; Czermak, P. Electronic Journal of Biotechnology 2015, 18 (3), 252–255.	Winner!
101	humanscalestructuresoutofmodularcomponentsinrealworldenvironments. IntroductionAutonomousunderwatervehicles(AUVs)fittedwithspeciallydesignedgripperswill build concrete retaining walls and artificial reefs with modular blocks. Blocks will be designed to providepassiveerrorcorrectiononbothpickupandplacement,makingthesystemrobusttonoise fromlocalizationandcontrol. Localizationwillbeachievedwithspeciallydesignedinfrastructure partiallyembeddedintheblocksthemselvestoprovidecertaintynearassemblyareas. Underwater construction is both dangerous and expensive because specially trained human divers must build and maintain structures by hand. At moderate depths, extreme measures must be taken to protect divers including the use of hyperbaric chambers to treat decompression sick- ness. Small mistakes or equipment failures during a dive are disastrous, often resulting in death. Atextremedepthsbuildinginpersoniseffectivelyimpossibleduetothereducedcapacityofpres- surized air tanks at high pressure. The substantial cost and risk of using divers on site leads us to favor assembly on land and then transportation to the final location which constrains the types of structuresthatcanbebuilt. Though autonomous underwater construction technology could open a frontier of possible ap- plications, no autonomous system exists to date. Teleoperation based solutions have received at- tention in the literature including a rubble leveling robot [2] and a back hoe [1]. Teleoperation eliminates the need for accurate localization, but it requires that large amounts of data are passed back to the human operator at a high rate. Communication underwater is often achieved using either a physical tether or acoustic networking. Managing long tethers is difficult and acoustic communicationislowbandwidth,limitingthepossiblerangeofteleoperationbasedsolutions. Research Plan My initial system will be built around a BlueROV2 underwater robot which is availableintheDartmouthroboticslab. TheBlueROV2isa10kilogram,0.5meterlongrobotwith 6degreeoffreedommotion. Itisanattractiverobottobasethesystemonbecauseitincludesawell maintainedcodebaseandseveralsensorsforlocalizationincludingtwoinertialmeasurementunits, pressuresensors,twocompasses,alowlightHDcamera,andashortbaselineacousticpositioning system. IwillutilizevisualmarkerscalledARTagsforvisualpositioninginformation. Research Question 1 How can we best exploit accurate localization information when it is avail- able, and smoothly switch to coarse grained techniques when it is not? Approach My preliminary experiments on localization suggest that sensor accuracy varies depending on the robot’s position and velocity. I will empirically model the noise and accuracy of the sensors under relevant cir- cumstances, then design a sensor fusion algorithm which exploits the most reliable information at every instant. Research Question 2 Given some desired structure, how can we design a feasible and robust build plan for the structure? Approach To design a feasible build order for a structure, itwillbenecessarytoconsiderthephysicalconstraintsofboththebuildingblocksandtherobot’s maneuvering capabilities. I will model the build ordering constraints induced by the structural andmaneuveringconstraintsasaconstrainedoptimizationproblem. Theobjectivefunctionofthis problemwillencodetheabilityoftherobottoexploitlocalizationinformationthroughoutthebuild. My initial work on build order optimization in 3D printing can be generalized to suit this applica- tion[3]. ResearchQuestion3Howcanweautomaticallyreacttobuilderrorscausedbychanging environmental circumstances? Approach To gain insight into the failure modes of the system, I willtestitrepeatedlyonaselectedfewbuildplansinboththepoolandLakeSunapee,NH.During each test, I will record and categorize the failures that occur. Based on the most common types of errors, I will develop automatic recovery mechanisms. For example, if a block is not fully seated onanotherblock,therobotcouldgentlynudgethemintoplace. Iftherobotpassesthroughacloud ofsilt,itcouldstopandwaitforthecloudtosettlebeforecontinuingthebuild. ExperimentPlanFirst,Iwillisolateeachsensoranddetermineitsaccuracylimitations. Toestab- lishtheaccuracyoftheIMUandARTagreadings,Iwillutilizeanindustrialroboticarmavailable inthelabtocollectaccuracyandnoisedatabasedonknownprogrammedmotions. Toevaluatethe positional accuracy of the sensor fusion algorithm on land, I will utilize a highly accurate VICON positioning system as ground truth. With the sensor quality models in hand, I will begin iterating on the sensor fusion algorithm. Preliminary debugging style experiments can be conducted in a 6 foot diameter water tank in the lab. Frequent field tests in the athletic pool and nearby lakes such as Lake Sunapee will inform further iterations. Simple tests such as repeatedly moving a block back and forth on a platform in calm clear waters will help isolate the quality of localization data. Build order and error recovery algorithms will be tested by selecting several simple build plans and repeatedly testing them in varying environmental circumstances. The ultimate goal will be to successfullyexecuteabuildplanneartheshoreoftheCarribeanseaduringoneofthelab’syearly tripstotheBellairsResearchInstituteinBarbados. Intellectual Merit Underwater construction will require the co-development of localization in- frastructure and sensor fusion strategies to rapidly instrument a build site. Rapidly deployable localization infrastructure will have applications in any autonomous robotic system attempting to achieve manipulation tasks in a remote or harsh environment. Rather than attempting to jump to unaided manipulation in totally unstructured environments, I will be taking the realistic approach of exploring the minimal amount of additional infrastructure required to complete manipulation tasks. This work will also advance the evaluation of the trade offs between computational and physical complexity in autonomous robotic systems. For example, designing blocks which more successfullycorrectforerrorcouldallowmanipulationbehaviorstobemoresimplewhilerequiring amorecomplexblockgeometry. Developingtechniquestorigorouslyevaluatetradeoffsbetween computationalandphysicalcomplexityforcomputational-physicalsystemsisanimportantstepin designingrobustroboticsystemsforrealworldenvironments. Broader Impacts A system which can robustly place modular components on one another could enable the mostly autonomous creation of retaining walls or artificial reefs. As ocean levels con- tinuetorise,retainingwallswillbeincreasinglyimportantindevelopedcoastalareas. Bystacking componentsofartificialreefs,wecouldenablelargerscalereefrestorationactivities. Asthetech- nology advances, it could enable more subtle applications as well. It could enable us to more efficientlybuildoffshoreenergyinfrastructureorenableustoscaleunderwateragriculture. This project is a valuable opportunity for young researchers to gain hands on experience with robotics and software development, preparing them for a productive career in an exploding field. During the conduction of the preliminary study, I worked with two high school students who con- tributed directly to this research. The students gained exposure to robotic software design and mechanical modeling. I will continue using this work as an opportunity to mentor driven young researchers. [1]TaketsuguHirabayashietal.“Teleoperationofconstructionmachineswithhapticinformationforunderwaterappli- cations”.In:AutomationinConstruction15.5(2006).21stInternationalSymposiumonAutomationandRobotics inConstruction. [2]T.S.Kimetal.“Underwaterconstructionrobotforrubblelevelingontheseabedforportconstruction”.In:2014 14thInternationalConferenceonControl,AutomationandSystems(ICCAS2014). [3]S.LensgrafandR.R.Mettu.“Animprovedtoolpathgenerationalgorithmforfusedfilamentfabrication”.In:2017 IEEEInternationalConferenceonRoboticsandAutomation(ICRA).	Winner!
102	Keywords: Aging, stress response, RNA biology, Drosophila Background: Aging is characterized by the accumulation of cellular damage, the physiological decline of tissue and an increased susceptibility to disease resulting from the failure to maintain homeostasis in the face of endogenous and environmental stresses [1]. A key mechanism underlying protein homeostasis (proteostasis) in response to stress is the assembly of RNA stress granules (SGs). When stress dissipates, SGs disassemble and cells return to homeostasis, thus SGs dynamic behavior offers a potential molecular mechanism that links aging and cellular stress resilience. Interestingly, changes in SG dynamics have been identified in age-dependent neurodegenerative disorders, yet SGs remain unexplored in normal aging. SGs are non-membrane bound organelles that assemble in the cytoplasm of cells when translation initiation is inhibited or during stress (e.g. heat shock, osmotic pressure, oxidative stress) [2]. SG formation has been shown to increase fitness during stress [3]. During transient stress, SGs stabilize mRNA and delay the aggregation of proteins linked to neurodegeneration [4- 5]. SGs preferentially sequester long, poorly translated RNAs as well as a diverse set of proteins such as nuclear pore complexes, RNA binding proteins and others varying by cell type and stressor [6-7]. SG assembly is rapid: a dense core is formed by an established network of protein- protein interactions, nucleated by G3BP1, followed by the assembly of a dynamic shell comprising RNA and RNA binding proteins that trigger liquid-liquid phase separation. After stress subsides, SGs spontaneously disassemble and allow sequestered factors to return to their functions [8]. When SGs fail to disassemble, such as during chronic stress, they disrupt, not maintain, proteostasis and facilitate protein aggregation [9]. Two previous studies found that SG components aggregate with age, but it is unknown how normal aging alters the nucleation, stability, or disassembly of SGs [10-11]. I hypothesize that (1) the dynamics of SGs will be altered throughout aging and (2) the composition of SGs will correspondingly be altered by age. Aim 1: Determine the dynamics of SGs during aging in response to stress Using a Drosophila model where Rasputin (RIN), the homolog of G3BP1 and the only protein required for SG formation, is endogenously tagged with GFP (RIN-GFP), I will visualize SG formation in the fly brain [12]. Drosophila share over 60 percent of their genome with humans, providing a translatable and practical model to study SGs throughout aging (lifespan averages 100 days). To determine if age impacts SG dynamics (e.g. assembly and disassembly) in fly brains, I will dissect adult fly brains and immunostain for GFP. Using a confocal microscope, I will quantify the distribution and sizes of RIN-GFP puncta in various regions of the fly brain. Drosophila will be dissected at five time points across aging (1, 20, 50, 80, 100 days). To capture the altered dynamics of SGs between types of stress, heat shock and oxidative stress through paraquat ingestion will be used to stress flies just before dissection. Controls will include age matched Drosophila that will not be subjected to stress. To distinguish between SG assembly and disassembly, flies will be dissected just after stress (assembly) or two hours after stress (disassembly). Ten to twenty flies will be studied per time point and treatment. Expected outcomes, potential pitfalls and alternatives. Preliminary experiments show that SG assembly declines with age in flies (data not shown) thus I expect less robust assembly of new SGs for older Drosophila. I expect most SGs to disassemble one hour after stress for young Drosophila. For old Drosophila, I expect less dynamic SGs or slower disassembly. The assembly mechanism of SGs varies slightly by stressor. Oxidative stress has canonically been used to elicit SG formation [2]. The dynamic behavior of heat shock induced SGs could defy expectations based on research into SGs generated by oxidative stress. Previous research shows some SG aggregation in aging [10-11]. If SG disassembly does not visibly change with age, though, changes in SG composition could alter SG dynamics in other ways. These experiments will show for the first time how aging fly brains respond to stress by assembling SGs. Future research will examine the relationship between altered SG dynamics and phenotypes of aging (e.g. behavior). Aim 2: Determine the composition of SGs during aging in response to stress SG dynamics are impacted by the composition of SGs. For example, the recruitment of protein kinases ULK1 and ULK2 and the ATPase VCP to SGs is required for SG disassembly [13]. Age-related alterations to SG composition are likely responsible for the expected decline in dynamic SG formation during aging. SGs will be isolated using immunoprecipitation (IP) of GFP tagged RIN protein/RNA complexes from the neurons of the same experimental Drosophila and controls described above. IPs will be optimized using IgG controls to ensure specificity of RIN-GFP complexes. Mass spectroscopy and RNA-sequencing will be used to identify the proteins and RNAs comprising SGs throughout the various stress and aging conditions. Key SG markers will be cross-referenced. The results will be analyzed using statistical models to identify which proteins and RNAs are enriched or depleted in specific conditions and age time points. Expected outcomes, potential pitfalls and alternatives. The differential recruitment of proteins to SGs has physiological impacts on cells during the stress response and can alter SG dynamics. For example, if SGs formed during old age increasingly sequester nuclear pore complexes, proteostasis is more likely to be disturbed in aging [6]; similar logic applies to other pathways. RNAs sequestered to SGs mediate protein recruitment but do not impact global translation [8]. By linking the composition of SGs with different assembly and disassembly phenotypes, the impact of age on the mechanism of SG-supported stress resilience can be more fully understood. To follow up candidates identified by IP/mass-spec, overexpression or knockdown of genes in Drosophila will be used to identify the specific role of proteins in the alteration of SGs dynamics in the stress response throughout aging. Intellectual Merit and Broader Impacts: For the past three years, I worked with Drosophila studying RNA biology in aging and the cellular stress response. Through my research, I optimized IP procedures to isolate SGs and prepare them for analysis by mass spectroscopy and RNA-seq. To analyze this data, I created statistical models using the R programming language. Given SGs role in stress resilience and neurodegeneration, understanding the impact of age on the dynamics of SGs is important. These findings will help generate new hypotheses about the role of the stress response in aging. Additionally, researchers are pursuing treatments to the neurodegenerative disease amyotrophic lateral sclerosis that eliminate SGs in humans. SG elimination could significantly impair the ability of neurons to survive especially during aging. The proposed experiments will provide key insights into new strategies to maintain cellular homeostasis in the human body, specifically the brain, and extend the health span as well as lifespan. As part of my research, I am responsible for leading and training a small team of undergraduates in our study of RNA biology in aging. In addition to effectively communicating my research at two international conferences and to my community, I helped review current research on the role of SGs in protein aggregation for an upcoming publication. References cited: [1] Rieraetal et al., 2016. Annu. Rev. Biochem. 85, 35-64. [2] Parker et al., 2009. Mol. Cell 36, 932-941. [3] Riback et al., 2017. Cell 168, 1028–40. [4] McGurk et al., 2018. Mol. Cell 71, 703-17. [5] Bley et al., 2015. Nuc. Acids Res. 43, 23. [6] Khong et al., 2017, Mol. Cell 68, 808–20. [7] Markmiller et al., 2018, Cell 172, 590–604. [8] Jain et al., 2016. Cell 164, 487–98. [9] Zhang et al., 2019. eLife 8, 39578. [10] Lechler et al., 2017. Cell Reports 18, 454–467. [11] Moujaber et al., 2017. Bioch. Acta 1864, 475–486. [12] Anderson et al., 2018. Human Mol. Genetics 27, 1366-81. [13] Wang et al., 2019. Mol.Cell 74, 742-57.	HM
103	Investigation of the Catalytic Properties of Cerium(IV) Oxide in Metal Oxide Laser Ionization-Mass Spectrometry Imaging Background: Matrix-assisted laser desorption/ionization-mass spectrometry imaging (MALDI- MSI) is an emerging and powerful analytical technique, which allows the spatially resolved characterization of a wide range of analytes within biological specimens.1 Metal oxide laser ionization (MOLI) is a recently described variation on MALDI in which a metal oxide, rather than an organic acid, is utilized as the matrix.2 255 m/z Unlike the other metal oxides, Cerium(IV) Oxide (CeO ) demonstrates a unique property of laser 2 induced fatty acyl catalysis when applied to 281 phospholipids and energized by standard lasers m/z found in MALDI-TOF MS instruments, as seen in Figure 1. This property of laser-induced catalysis by Figure 1. MOLI-MS using CeO 2 on POPC CeO provides a considerable opportunity in various 2 biological and clinical applications in which fatty acid profiling may be needed.3,4 Beyond clinical applications, CeO -based materials also have a 2 variety of applications as a catalytic system in fuel cells, thermochemical water-splitting, organic reactions, and photocatalysis.5 Because of the involvement of CeO in a variety of fields, and the 2 potential it has to impact future technologies, a further investigation of the biological catalysis properties of this compound are warranted. Preliminary Results: The Cox group in Colorado has headed the investigation for the use of MOLI techniques in the identification of bacterial species. This group discovered that CeO could be utilized 2 for identification with improved stability and reproducibility compared to other metal oxides.6 Previously, MOLI has not been used in conjunction with MSI in order to induce fatty acyl catalysis directly from tissue for possible bacterial detection. While MSI is found to be a promising technique, there are only very few research groups that currently have the instrumentation available to conduct MSI studies. At Harvard Medical School, the Agar group has been Figure 2. MOLI-MSI of control working in the field of MSI for over a decade. During my time in mouse brain: A) 255.4 m/z & the group this summer, I developed and optimized a technique for B) 281.5 m/z CeO deposition on biological tissue, which is currently being 2 prepared for submission. This technique describes the deposition of CeO for MOLI-MSI, such 2 as in Figure 2, as well as possible clinical applications. Although MOLI using CeO has shown considerable promise, the mechanism for which 2 fatty acyl catalysis occurs when laser energy is applied to CeO is still unknown. Most 2 commonly, when MOLI-MS is used, analyte ionization of phospholipids typically occurs by protonation due to interactions with the Lewis acid/base sites on the metal oxide. However, only no protonation occurs with CeO -induced catalysis, indicating that the mechanism of cleavage is 2 unique. It is postulated that much of the catalytic activity of CeO arises from oxygen vacancy 2 defects in the surface which occur at MALDI-like conditions (high temperature, low pressure).6 Proposed research: My preliminary results have developed a novel technique for the application of CeO to clinical problems. However, it has left unanswered a critical question 2 about the mechanism of fatty acyl cleavage that occurs when CeO is used for MOLI-MS/MSI. 2 Without understanding the mechanism of catalysis, it is difficult to fully interpret the mass Madison McMinn | 2018 NSF GRFP Research Statement spectra generated by this method. My future research plans consist of three specific goals, detailed below. By achieving these goals, I plan to elucidate the mechanism of catalysis that is unique to CeO , when it is used with biological samples. 2 The first goal is to expand beyond phospholipids, and study compounds that also contain fatty acid chains, such as diacylglycerols, sphingolipids, triglycerides, acyl-carnitines, acyl- coenzyme A thioesters and other acyl-bound biomolecules. I am interested in determining if the cleavage of fatty acid chains is unique to phospholipids, or if CeO can also induce this property 2 on other compounds. This will serve to determine the depth and breadth of this application. Also, if certain compounds are unable to undergo catalysis by CeO , structural differences can be 2 identified and studied. For these experiments, I plan to use commercially available lipid standards to evaluate catalysis in a simple, and direct way. The second goal is to apply the knowledge gained in the first goal of this proposal to complex systems. Since isolated and purified compounds do not exist naturally, it is critical to see how effective this technique is when applied to a complex biological specimen. For this, I plan to correlate mass spectra obtained with a typical MALDI matrix and with CeO . I aim to 2 correlate the known lipid composition with the fatty acid composition, to see if certain species are more prone to cleavage when present in a complex sample. This would be relevant in MOLI- MSI with heterogenous tissue samples, where lipid compositions can vary greatly throughout the specimen. The third goal is to investigate the surface chemistry of CeO using experimental and 2 computational methods. I plan to perform studies where CeO particles of varying sizes are 2 probed by electron/neutron diffraction, since X-ray diffraction is not an ideal technique for this material, due to the low scattering power of oxygen. These studies aim to determine if a greater number of oxygen defects contributes to improved catalytic cleavage. Diffuse Reflectance Infrared Fourier Transform Spectroscopy will be performed to study the surface morphology of CeO before and after it is subjected to MOLI-MS. Once the surface structure of CeO is well 2 2 understood, computational studies can be performed using density functional theory calculations. Intellectual Merit & Broader Impact: Elucidating the mechanism of CeO -induced fatty acyl 2 catalysis will allow scientists to use my developed MOLI-MSI technique and the MOLI-MS database for bacterial identification by the Cox group with increased confidence. Also, the information obtained from my first research goal has the potential to advance knowledge in fields that use fatty-acid containing molecules, such as cosmetics, nutrition, and metabolomics, in addition to biological applications. Furthermore, elucidation of the catalysis mechanism of varied metal oxides, and what induces this variance, can contribute fundamental knowledge to the field of catalysis chemistry, especially metal oxide catalysis. In regard to the broader impact in biological applications, fatty acid profiling of tissue specimens has been an extensive area of study, dating back nearly a century. Most current approaches use many time-consuming steps and, more concerningly, result in the loss of spatial relationships between these molecules. Preserving these spatial relationships is critical in the analysis of a wide variety of diseases, including many cancers, which demonstrate heterogenous tissue distribution. We have shown that MOLI-MSI using CeO can provide in situ fatty acyl characterization of biological tissues 2 while preserving regional distribution. By better understanding the chemistry of CeO induced 2 fatty acyl catalysis, a more informed interpretation of resulting MS spectra and therefore the tissue composition can be appreciated. This work will lay the groundwork for a potentially new clinical and translational diagnostic approaches. 1.Reyzer ML, Caprioli RM. J Proteome Res. 2005, 4(4), 1138-1142. 2.Schwamborn K, Caprioli RM. Nat Rev Cancer. 2010, 10(9), 639-646. 3.Voorhees KJ, Saichek NR, Jensen KR, Harrington PB, Cox CR. J Anal Appl Madison McMinn | 2018 NSF GRFP Research Statement Pyrolysis. 2015, 113, 78-83. 4. Choe SS, Huh JY, Hwang IJ, Kim JI, Kim JB. Front Endocrinol (Lausanne). 2016, 7(30). 5.Hodson L, Skeaff CM, Fielding BA. Prog Lipid Res. 2008, 47(5), 348-380.	Winner!
104	Evidence from the surface and atmosphere of Mars indicates that Mars was once a much wetter world. Today, we know that nearly all the water has been lost [6]. Understanding the evolution of water on Mars and its atmosphere is critical to answering questions such as: did life ever exist on Mars, or does it now? What can Mars tell us about possible futures for Earth or evolutionary pathwaysofexoplanetatmospheres? IntellectualMerit To approach these questions, we need new models that capture the complex chemistry and dy- namics governing escape of water. Thermal escape of hydrogen (H) is the main loss process for bothwaterandtheatmosphereasawhole[6]. BecauseHescapesmoreefficientlythanitsheavier isotope, deuterium (D), understanding variations in the D/H ratio is the key to understand- ing loss of Martian water. However, the most recent atmospheric escape studies that included D chemistry only considered global averages of D/H, and were 18+ years ago [7, 10]. Since then, D/Hmodelingworkhasstalled,andnostudieshaveinvestigatedoutgoingfluxofD[6]. Twenty years of data from the Hubble Space Telescope, ground-based telescopes, and Mars or- biters, landers, and rovers have augmented our knowledge of the D/H ratio on Mars. We now know it varies with season, altitude, and geographical location ([9], and references therein). This variability is apparent in atmospheric water vapor, which can form clouds and be transported by dust storms. Studies have shown that planetary boundary layer (PBL) water ice clouds can de- crease the total water column by up to 15% on timescales of a few days [4]. Orbiter data [3, 5] shows that dust storms boost water in the mesosphere, which was demonstrated by Chaffin et al. [2]toenhancelossofHwithinweeks. Iproposetousethesenewdataasinputsandconstraints inthefirststudiesinnearlytwodecadesoftheroleofD/HinMartianatmosphericloss. ResearchGoals 1. Explainhowseasonal,altitudinal,geographicalD/Hvariationsaffectatmosphericloss. 2. Understandtheeffectsofplanetaryboundarylayercloudsonatmosphericloss. 3. Quantifythecontributionofduststormstoatmosphericlossenhancement. ModelingMethodology,Preparation,andCurrentResults Due to computational resource limitations, 1D photochemical models are required to simulate the martian atmosphere on time-scales of 105+ years. Though limited in space, 1D models can pro- vide context for end-member cases of more expensive 3D calculations. During my first year in graduate school, I expanded a 1D photochemical model built by my advisor, Michael Chaffin, doubling the number of chemical pathways modeled and adding deuterium chemistry. Fol- lowing Chaffin et al. [2], the model solves a photochemical system of coupled partial differential equations. Modificationstoaddressresearchgoalsrequireonlyminorchangesasfollows. 1. New,high-precisiondatatoconstrainD/HinaltitudeandtimeisforthcomingfromtheESA Trace Gas Orbiter to [8]; we can model this data as a time- and altitude-depndent function. Spatialvariancecanbeestimatedwiththis1Dmodelbyindividualruns. 2. Bothdiurnalandseasonalvariationsinwatervaporabundanceduetocloudsandduststorms can be included with time-dependent calculations of the water vapor profile, which is pre- scribedinthemodel. CloudaltitudescanbeestimatedusingCuriosityrovermoviesandthe MarsRegionalAtmosphericModelingSystem(MRAMS)[1]. IrecentlystudiedtheeffectsofDchemistryandchangestothetemperatureprofileandwater Eryn M. Cangi Research Proposal 2018-2019 NSF GRFP application vapor mixing ratios on escape by calculating the fractionation factor f, which represents how efficiently D escapes with respect to H. (A fractional value 0.xx means that D escape is 0.xx% as efficient as H escape). Selected results are shown in Figure 1. This is the first effort to model differential escape of H and D in ∼18 years. Our results show that prior calculations greatly overestimated the relative escape of D due to systematic inaccuracies in atmospheric temperature measurementsandphotolysisratecoefficientsavailableatthetime. Isharedtheseresultswithcol- leaguesatthe50thmeetingoftheAmericanAstronomicalSocietyDivisionforPlanetarySciences (DPS),andamcurrentlycompletingworkonwritingtheseresultsinalead-authorpublication. Figure 1: Fractionation factor (percent efficiency at which D escapes with respect to H) for 6 model runs, comparedtotworeferences. Labelsindicateadjustments to three temperature profile control parameters: T , surf T , and T . “↓T ”, e.g., means the temperature at tropo exo exo theexobasewasloweredforthatmodelrun.Adjustments of±25%tothemeanprofileweretested.Thelowsurface temperaturecaseproducedanunstableatmosphere(pho- tochemicalsystemhadnosolution)andwasdiscarded. BroaderImpact The public imagination is already captivated by Mars, as a possible habitat for extraterrestrial life and a target for future crewed missions. As mentioned in my personal statement, next year I will join CU-STARS, a departmental program that brings extracurricular science lessons to Coloradopublicschools. ExcitementaboutMarsfromallages,andthefactthatatmosphericloss can be explained without complicated jargon, makes my research an excellent topic for reaching out to schools around Colorado. In terms of engaging the wider public, I also plan to make my research available to the public by giving talks at University of Colorado’s Fiske Planetarium, a method of outreach where I can draw on my earlier training in theatre. To understand possible futuresforMars,weneedtounderstanditshistory;myfirsttalkwilldiscussthehistoryofmartian atmospheric escape and implications for hypothetical future terraforming efforts (and the ethics thereof). I believe in making it easy for the public to access and understand the science their taxes pay for; in addition to presenting at conferences and publishing papers, I maintain a personalwebsitewithexplanationsofmyresearchforbothlaypeopleandfellowscientists. MarsresearchcontributesnotonlytoMarsscienceandmissions,butalsotoexoplanetaryscience. Compared to the requirements to understand exoplanet atmospheres, Mars is a cheap and readily available laboratory. It is valuable not only for scientific opportunities, but because for decades, it has captivated disparate groups of people. Now more than ever, humanity needs goals that unify usandremindusthatweareallinthistogether;myworktounderstandtheevolutionofwaterand theatmosphereonMarswilladvancethosegoals. References: [1]Campbell,C.,etal.2018,AAS/DPSMeetingAbstracts,300.03. [2]Chaffin,M.,etal.2017,Nature Geoscience,10,174-178. [3]Fedorova,A.,etal.2017,Icarus,300,440-457. [4]Haberle,R.M.,etal.2017,The AtmosphereandClimateofMars. [5]Heavens,N.,etal.2018,NatureAstronomy,2(2),126-132. [6]Jakosky,B.,et al.2018,Icarus,315. [7]Krasnopolsky,V.2000,Icarus,148. [8]Villanueva,G.L.,etal.2018,AAS/DPSMeeting Abstracts,303.09. [9]Villanueva,G.L.,etal.2015,Science,348(6231),218-221. [10]Yung,Y.,etal.1988,Icarus, 76(1).	Winner!
105	Background. How does motivation drive learning? Evidence abounds that reward, motivation, and curiosity can all enhance learning and memory1; these findings have far-reaching implications for education. However, a fundamental problem undermines our ability to apply this research in classrooms: extrinsic reinforcement can actually decrease intrinsic motivation2. In other words, although rewards like candy, stickers, and money are often used as incentives for students, these secondary reinforcers may decrease internal motivation, curiosity, and fulfillment. In the brain, dopamine pathways are strongly implicated in reward and motivation1. Dopaminergic cells in the ventral tegmental area (VTA) project to the hippocampus and surrounding medial temporal lobes3, influencing memory by enabling the brain to prioritize and remember important information4. Moreover, high-reward contexts increase sustained VTA activation and memory encoding5. Past studies of intentional encoding strategies have shown the importance of elaborative and self-referential processing6, but have yet to link these methods to dopaminergic modulation. Importantly, it remains an open question whether cognitive strategies can act upon dopaminergic pathways to enhance memory, either immediately or over time. Cognitive neurostimulation, the volitional modulation of one’s own brain activity through mental imagery and thoughts, offers a promising method of enhancing motivation. However, past research has found that without guidance, individuals struggle to self-motivate and modulate VTA activity7. Neurofeedback provides individuals with real-time information about their own brain activity. Past studies in the Adcock lab have successfully used neurofeedback to train participants to self-activate the VTA; this activation is associated with self-reported motivation7. A day later, trained participants retained the ability to self-activate, even without neurofeedback. Intellectual Merit. Thus far, no study has shown that self-activation of the VTA can influence memory encoding or consolidation. This missing link is essential for elucidating neural mechanisms of motivation and memory, as well as extending cognitive research to education. The proposed research will take a novel approach to address this gap in the literature by embedding neurofeedback training within a memory task. The present study seeks to: (1) Train participants to drive intrinsic motivation and self-activate the VTA, (2) Test whether self- activation enhances memory encoding and consolidation, and (3) Identify effective motivational strategies with a data-driven approach. I hypothesize that with neurofeedback, participants will learn to self-motivate and drive VTA activation, thus enhancing subsequent memory. Methodology. Fifty healthy participants will be recruited to participate in a study at the Duke University Brain Imaging and Analysis Center, which houses a GE Premier 3T MRI scanner. First, participants will complete two trait motivation surveys, the Motivational Trait Questionnaire8 and the Behavioral Inhibition-Approach System9 (BIS/BAS). In the scanner, I will collect fieldmap, T1-weighted structural, and functional scans (TR=1s, voxels=2x2x2 mm3). Participants will complete three kinds of tasks: Activate task. Participants will be instructed to self-motivate by using personally-relevant thoughts and mental imagery (e.g., one past participant reported success with visualizing a cheering crowd7). Using PYNEAL software, previously developed in the Adcock lab, I will calculate real-time VTA activation and inform participants with a dynamic thermometer display. Watch task. Participants will passively view the thermometer display, but will be informed that fluctuations are random, not neurofeedback. On these trials, the thermometer display serves as the control task, equating visual input with the Activate task. Encode task. On each trial, participants will try to memorize a series of 7 object images (2 sec. each), sampled randomly without replacement from a set of 336 images. In total, participants will complete 8 runs of 6 trials each. A random half of the trials will begin with the Activate task (20 sec), and the other half will begin with the Watch task (20 sec). NSF-GRFP Graduate Research Plan: Alyssa H. Sinclair 2 The Encode task (20 sec) will conclude every trial. Between runs, participants will verbally describe the motivational strategies employed on preceding trials. After the MRI scan, participants will be randomly assigned to either the Same-Day group (memory test immediately after the scan, n=25) or the Next-Day group (memory test 24-hours later, n=25). In a behavioral testing room, participants will complete a recognition memory test of the images from the Encode task (336 old images, 168 novel images), and rate confidence on a 5-point Likert scale. Analyses. In summary, I will employ a 2x2 design (Task: Watch, Activate X Time: Same-day, Next-day) to address the following questions: 1. Does VTA self-activation enhance memory? I expect that within-subjects, average memory accuracy (d', signal detection theory) and event-related VTA activation will be greater on Activate than Watch trials. Moreover, trial- wise VTA activation will be parametrically related to memory for the stimuli encoded on a given trial. Previous work in the lab has detected similar neuromodulatory effects on single trials10. 2. Does self-activation of the VTA influence memory encoding and/or consolidation? If VTA-activation enhances consolidation, then the Next-Day group will exhibit greater disparity in memory accuracy between Activate and Watch trials (relative to the Same-Day group), because consolidation is time- and sleep-dependent. Within the Next-Day group, I will compare memory accuracy for Activate and Watch trials to control for sleep-consolidation effects that are independent of VTA-effects. An alternative hypothesis is that VTA-activation will improve memory equally in both groups, reflecting selective effects at encoding. 3. What cognitive factors drive motivation and enhance memory? Using the trait motivation survey data, I will test whether individual differences in personality (e.g., higher scores on trait motivation and the BIS/BAS Reward Responsiveness subscale) predict success on the Activate task and subsequent memory accuracy. Moreover, I will employ a data-driven approach to identify the self-reported motivational strategies that most effectively increased VTA activation (e.g., verbalizations, various types of mental imagery). Importantly, the existing literature on motivation and reward is constrained by a limited set of strategies that are imposed by experimenters. Informed by my fMRI findings, I will develop future behavioral studies that test the efficacy of the diverse motivational strategies that participants intuitively employ. Broader Impacts. Motivation is a core component of learning. Critically, low-income and disadvantaged students exhibit low intrinsic motivation, which predicts poor academic outcomes11. In 2016, a staggering 29.7 million American children (41%) lived in low-income families12. In classrooms, fostering intrinsic motivation can improve learning outcomes and student retention9. Extrinsic reinforcers, such as monetary rewards, can have restricted and short- lived effects; in contrast, intrinsic motivation predicts long-term student success10. The proposed research seeks to empower individuals to drive intrinsic motivation and self-activate motivational brain systems, thus engaging the brain to improve learning. With a novel neurofeedback approach, I will identify cognitive strategies that effectively act upon dopamine systems to enhance memory. In future behavioral studies, I will directly test whether these strategies can successfully bolster motivation and memory without neurofeedback. The present program of research seeks to uncover accessible and non-invasive methods of fostering intrinsic motivation and improving memory, thus broadly benefiting learning and education. References: 1. Wise, R. A. Nat. Rev. Neurosci. 5, (2004). 2. Butler, R. Br. J. Educ. Psychol. 58, (1988). 3. McNamara, C. G. & Dupret, D. Trends Neurosci. 40, (2017). 4. Lisman, J. E. & Grace, A. A. Neuron 46, (2005). 5. Murty, V. P. & Adcock, R. A. Cereb. Cortex 24, (2014). 6. Kirchhoff, B. A. Neurosci. 15, 166–179 (2009). 7. MacInnes, J. J., Dickerson, K. C., Chen, N. & Adcock, R. A. Neuron 89, (2016). 8. Heggestad, E. D. & Kanfer, R. Int. J. Educ. Res. (2000). 9. Carver, C. S. & White, T. L. J. Pers. Soc. Psychol. 67, (1994). 10. Adcock, R. A., et al. Neuron 50, (2006). 11. Schultz, G. F. Urban Rev. 25, (1993). 12. Koball, H. & Jiang, Y. NCCP, (2018).	Winner!
106	Nature versus nurture? Maternal responses to infant distress calls Introduction & Significance: The brain has the extraordinary capability of attributing different levels of importance to the different types of input signals it receives. For example, hearing one’s name, even at very low volume, elicits strong neural signals, as the brain has learned the relevance of that particular sound. The process by which this occurs is known as synaptic plasticity, which allows the brain to alter its connections based on experiences associated with particular sensory inputs. Synaptic plasticity is usually experience-dependent, and a class of molecules known as neuromodulators have the role of strengthening specific neural circuits dependent on particular situations. Here I examine the action of one important neuromodulator, oxytocin, which is involved in a multitude of social interactions, including maternal care. Maternal behaviors are observed in all mammalian species, including mice. As infants, mouse pups are especially helpless, relying on their mother (called a ‘dam’) for all of their needs. Pups become scattered from the nest as the dam moves around, and must communicate with the dam that they have become isolated. To do so, they emit isolation ultrasonic vocalizations (USVs), triggering the dam to respond by retrieving the pup and returning it to the nest[1]. While dams retrieve pups with high accuracy, virgin female mice that lack prior experience with pups fail to exhibit this behavior, generally neglecting the calls of a nearby pup[2]. However, after being cohoused with a dam and pups for several days, virgin female mice can learn to retrieve pups with comparable accuracy to the dam[2]. A virgin’s acquisition of this pup retrieval behavior is accelerated by administration of the neuromodulator oxytocin [2]. This behavior can be eliminated by inactivation of the left auditory cortex (A1), which contains a significantly higher amount of oxytocin receptors (OXTR) than the right A1[2]. My proposed graduate research is concerned with the specific features of isolation USV stimuli that cause A1 to recognize the behavioral relevance of these sounds. Specifically, I propose to examine perceptual attributes of the isolation USV encoded by the maternal A1 that enable the dam to recognize and respond to this sound, as well as the role of oxytocin-dependent plasticity in acquiring USV-induced pup retrieval behavior by inexperienced virgins. Recent work demonstrated that human A1 distinguishes screams from conversational speech by an acoustic quality known as ‘roughness,’ defined as the rate at which the volume of sound changes[3]. By detecting roughness, human A1 rapidly engages subcortical structures to assess danger[3]. I hypothesize that similar acoustic perceptual features allow the maternal A1 to distinguish, and attribute behavioral relevance to, the sound of nearby pup isolation USVs, and that learning of pup retrieval behavior through experience relies on oxytocin-dependent synaptic plasticity that strengthens the A1 response to such perceptual features. Aim 1A: Which acoustic features differentiate isolation USVs from other vocalizations? To understand how the USV response is encoded in the dam A1, I will chronically implant electrode arrays into adult female mouse A1, and obtain single-unit recordings in response to natural, pup- evoked isolation USVs from a speaker. This Figure 1: Example single-unit recordings from process (Figure 1) allows clear visualization of tetrode implants in A1. An increase in activity is temporally-precise spike activity in A1 as it observed when isolation USV stimulus begins relates to sensory input from isolation USVs. Additionally, by observing behavior that dams exhibit when hearing the isolation USV stimulus, Katherine Furman – Graduate Research Statement I can visualize how A1 activity correlates to both sensory input and behavioral output (in the form of pup retrieval). Using the modulation power spectrum (MPS), which can visualize sounds two- dimensionally on both spectral and temporal domains[3], I can examine the portions of acoustic space in which these naturally-produced isolation USVs reside. By comparing this to the MPS of adult USVs (which are behaviorally neutral to the dam) I will isolate which acoustic features are unique to the pup isolation USV and have behavioral relevance to A1. Aim 1B: Are these acoustic features relevant to A1? If pup isolation USVs contain specific acoustic ‘roughness’ features distinguishable by the maternal A1, synthetically manipulated USVs which lack these features should result in a weakened response compared to natural pup- evoked isolation USVs. Using MATLAB I will synthesize audio clips which mimic pup USVs, but lack the spectral/temporal features previously identified as unique to isolation USVs. I will then play these to maternal animals, measuring behavioral and neural responses to calls with similar statistics as USVs, but varying in their frequency, temporal modulation (rhythm), and roughness. Using single-unit recordings from electrode arrays in A1, I will observe neural activity of A1 in dams when they are exposed to synthetically manipulated USVs played from an ultrasonic speaker. If I’ve successfully identified the differentiating feature(s) making isolation USVs unique from other mouse vocalizations, I expect that dam A1 neurons will exhibit a stronger, more temporally-precise response to hearing pup-evoked isolation USVs (as was observed in [2]), than synthetically manipulated USVs. I also expect that pup retrieval behavior will be significantly diminished, if not eliminated, when exposed to synthetically manipulated USVs. Aim 2: Are these specific elements dependent on oxytocin signaling? To test the hypothesis that oxytocin promotes maternal pup retrieval by strengthening the A1 response to unique spectral/temporal features of isolation USVs, I will observe the changes in A1 activity as a result of changes to endogenous oxytocin systems. Using transgenic Oxy-Cre mice will allow targeted expression of light-sensitive opsins in oxytocin-releasing neurons, the activity of which can be manipulated with light of specific frequencies. I will express channelrhodopsin-2, which is able to activate neurons in response to blue light, in pup-naïve virgin female mice. These mice will be exposed to pup isolation USVs concurrently with optogenetic stimulation of oxytocin-releasing neurons. By continuously pairing isolation USV audio with stimulation of endogenous oxytocin over a number of days, I hypothesize that the A1 activity of the naïve female will change to mimic the strong, temporally-precise response observed in dams. Intellectual Merit & Broader Impacts: With the help of the NSF GRFP, I will be the first to identify the specific acoustic features, over both spectral and temporal domains, which are unique to pup isolation USVs when compared to other mouse vocalizations. By identifying the A1 single-unit activity displayed in response to isolation USVs, and by identifying the changes in activity when crucial isolation USV features are eradicated from the stimulus, I aim to observe the specific activity patterns recruited by A1 in attributing behavioral relevance to infant-related sounds. By observing the changes in neural activity induced in pup-naïve virgins after optogenetic stimulation of oxytocin, I will be able to observe the unique form of neuromodulatory plasticity evoked by oxytocin in A1 which allows experience-dependent learning of maternal pup retrieval behavior. By examining maternal auditory processing in such depth, the field can better understand the interplay between auditory input and oxytocin to yield behavioral output. References: 1. Ehret (2005) Infant rodent ultrasounds – a gate to the understanding of sound communication. Behav Genet 35:19. 2. Marlin et al. (2015) Oxytocin enables maternal behaviour by balancing cortical inhibition. Nature Katherine Furman – Graduate Research Statement 520:499 3. Arnal et al. (2015) Human screams occupy a privileged niche in the communication soundscape. Curr Biol 25:2051.	HM
107	An investigation of thermal effects on Anax junius nymph growth Background: Earth’s climate is changing, and scientists are already observing impacts on biota across many taxonomic groups.1 Odonata are known to be useful biological indicators of environmental change at a macro-scale, including climate change.2 Odonata are highly temperature-sensitive, with direct effects on their physiology (e.g., developmental rate) and other life-history traits (e.g., phenology).1 Additionally, distributional and phenological records for Odonata are extensive, so they are excellent model organisms for studying the impacts of climate change on animal distributions, life history strategies, and development. Finally, the fossil record and historic data show that Odonata have survived rapid and dramatic climatic transitions in the past. However, present-day rates of climate change are substantially greater due to anthropogenic causes.3 Historically, Odonata have proven to be resilient and adaptable, but their current response is unknown. In sum, Odonata are considered sentinels of climate change, and there is growing interest in examining changes in their phenology and physiology as the climate warms. Odonata have been closely researched in the field. Studies using Odonates in applied research areas, such as climate change, are beginning to gain attention, but overall, research in this area is lacking.4 I believe that Odonata would be an exceptional research subject to help us understand how freshwater organisms are responding to warming temperatures. Shifts in air temperatures will influence lentic water temperatures through convection and by changing evaporation rates.5 Odonata are likely to reflect the mismatches between water and air temperatures due to climate change, demonstrating a potential temporal decoupling between aquatic and terrestrial species.5 Understanding this response in Odonates is particularly important, because they play an important role in structuring food webs, especially in fishless ponds which harbor unique biodiversity among macroinvertebrates, and are quite numerous across the landscape in many glaciated regions.6 Research Proposal: I propose an investigation to examine the effects of warming temperatures on larval dragonflies using the species Anax junius (the common green darner dragonfly, Order: Odonata, Family: Aeshnidae), in laboratory experiments. Although lab and field observations of temperature effects on larval Odonate development have been done independently, this proposed study will allow for a comparison between the lab experiment and data collected in the field in order to see if the lab findings in the lab hold up in real ecosystems. This proposal aims to 1) determine a range of temperatures that allow for optimal growth conditions for A. junius nymphs, and 2) compare the laboratory results to water temperature and A. junius emergence timing observed in the field by students and citizen scientists in order to better understand the effects that climate change has on aquatic ecosystems. Preliminary Work: For the past three summers, I have worked with my mentor Dr. Emily Schilling at Augsburg University on projects studying dragonflies from the family Aeshnidae. Our studies have focused on species showing evidence of modified life history strategies as an adaptive response to climate change. Through this research, we have developed relatively simple, cost effective and trustworthy sampling methods for nymphs, exuviae, and adults, that can easily be replicated by others. Methods and Materials: Aim 1) For this study, I have selected Anax junius, because this species is common throughout North America and adults are easily identified in the field (as opposed to other Aeshnids), which need to be observed in hand for species identification. Additionally, A. junius is known to be migratory, meaning that this species’ distribution covers a large geographic area. For the laboratory component of my study, I will set up fifteen 20-gallon tanks (30”x12”x12”), each containing ten A. junius nymphs. Each tank will have a heater to regulate the temperature and a HOBO Dissolved Oxygen Data Logger to monitor the dissolved oxygen differences amongst the temperature treatments. Tanks will be supplied with emergence supports and covered with mesh to capture emerging adults. There will be five temperature treatments (10°C, 15°C, 20°C, 25°C, and 30°C), with three replicates of each. Nymphs will be measured for their head-width-to-wing-sheath ratio three times per week in order to monitor growth, the number of days it takes each nymph to emerge will also be recorded. All molts, deaths, and emergences will be documented each measuring period. Aim 2) In order to get students and the general public more involved in STEM, I am going to enlist the help of volunteer scientists to broaden the scope of my data set. I will do this by contacting high schools and universities with NSF grants, and by posting ads on social media. I will also contact the Dragonfly Society of the Americas to enroll citizen scientists. For the recruited volunteers, I will let them choose a pond to sample and send them a temperature logger that continuously records water temperature, dip-nets for sampling dragonfly nymphs, and a guide on nymph and adult identification for A. junius. Lastly, I will create a web page where volunteers can easily upload their data and observations from their field sites. By using citizen scientists, I will be able to sample a larger geographic area than would be possible on my own, and collect data from multiple regions simultaneously. Broader Impact: By examining the data I receive from citizen scientists around the country, I will be able to gain insight as to which regions in North America are seeing the most dramatic changes in water temperature and gain a better understanding of the biological response to this environmental change. It is important to determine regions of concern so conservation planning can be prioritized in those areas. All humans and a large proportion of earth’s biodiversity require fresh water to survive. That is why research focusing on freshwater ecosystems is essential. Since climate change is a global issue, it is important to involve people in climate- related research that can ultimately inform how we protect freshwater ecosystems, arguably our most precious ecological resource. By engaging students and citizens in science, by allowing them to be a part of the data collection process, I hope to get more individuals interested in helping preserve and conserve the limited resources we have on Earth for generations to come. References: [1] Hassall, C., D. J. Thompson. 2008. The impacts of environmental warming on Odonata: a review. International Journal of Odonatology 11(2): 131-153. [2] Bried, J. T., C. Hassall, J. P. Simaika, J. D. Corser, J. Ware. 2015. Directions in dragonfly applied ecology and conservation science. Freshwater Science 34(3): 1020-1022. [3] Pritchard, G. & M. Leggott. 1987. Temperature, incubation rates and the origins of dragonflies. Advances in Odonatology 3: 121- 126. [4] Bried, J. T., M. J. Samways. 2015. A review of odonatology in freshwater applied ecology and conservation science. Freshwater Science 34(3):1023-1031. [5] Matthews, J. H. 2010. Anthropogenic climate change impacts on ponds: a thermal mass perspective. BioRisk 5:193-209. [6] Schilling, E.G, C. S. Loftin, A. D. Huryn. 2009. Macroinvertebrates as indicators of fish absence in naturally fishless lakes. Freshwater Biology 54(1):181-202.	Winner!
108	Regardless of signs of recovery from the economic crisis of 2007-2008, economic issues remain salient, and many Americans continue to experience stress due to their economic situation. Economic stress consists of both subjective and objective evaluation of one’s financial and employment-related stress1. My research will focus on four forms of economic stress: income inadequacy, financial fragility, underemployment, and job insecurity. Financial stress occurs when individuals perceive their personal financial situation to be insufficient to afford their needs and wants (perceived income adequacy) and are unable to cope with unexpected expenses (financial fragility), consequently leading to financial strain. While the unemployment rate in the United States is relatively low (below 5%), many workers are still experiencing employment-related stress due to underemployment and job insecurity. Underemployed workers hold jobs that insufficiently use their skills, abilities, education, or qualifications and may also receive less hours and pay than desired. Job insecurity is an individual’s subjective evaluation of the “perceived threat to the continuity and stability of employment as it is currently experienced”3. Employee perceptions of job insecurity have been empirically shown to increase stress, reduce mental, physical, and work-related wellbeing, and predict organizational commitment and turnover intention at work2-6. Perceived job insecurity may induce more stress than actual job loss or unemployment because the anticipation of job loss may prevent coping strategies to manage the stress. Along with unemployment, job insecurity has been a popular focus of economic stress research. However, there has been less concentration given to occupational health impacts of broader financial issues (e.g. 19). Economic stressors linked to the Great Recession, such as job insecurity, are associated with increased somatic symptoms experienced by individuals and a greater likelihood of alcohol abuse as a potential coping mechanism7. Relationships have also been found between perceived job insecurity of men and psychotropic drug use8. Additionally, empirical evidence suggests that illicit drug use may be a coping strategy for recessions and unemployment9. To my knowledge, research has not examined opioid use as a mediator of the relationships between economic stress and occupational health. For decades, the United States has struggled with an opioid epidemic. Opiates are drugs derived from the opium poppy plant that chemically interact with opioid receptors on nerve cells in the brain and in the nervous system to produce pain relief and pleasurable effects10. Opioid overdoses are driven by synthetic opioids (i.e. fentanyl), semi-synthetic opioids (i.e. oxycodone), and heroin and have led to four times more of American opioid-related deaths in 2015 compared to 199911-12. Long-term opioid use frequently starts with the treatment of acute pain13. Prescribed opioid pain relievers are the prescription drugs most often misused, resulting in a 72.2% increase in deaths due to synthetic opioids – not including illegally produced fentanyl – from 2014 to 201514. The number of opioid prescriptions written in 2012 would have been enough for every adult in the United States to have one bottle of opioid painkillers11. This longstanding problem has led to many societal consequences. The overuse and misuse of opioid substances costs the United States 80 billion dollars each year in healthcare, criminal justice, and productivity costs15. While the consequences of opioid use have been well-researched in other social domains16- 17, the relationships between opioid use and the workplace have received less attention. Current research shows that opioid use is related to absenteeism and work productivity18. My research will bridge this important gap in the literature on economic stressors, opioid use, and work-related outcomes. Gwendolyn Watson NSF Research Proposal Broadly speaking, my goal is to establish a stream of research that will answer the following questions: (1) What is the nature of the relationship between multiple economic stressors (job insecurity, fragility, underemployment, and income adequacy) and opioid use? (2) What are the causal mechanisms linking opioid use and economic stress? (3) How does opioid use relate to occupational health outcomes? Particularly I am interested in the impact on employee work engagement such as organizational commitment and turnover intentions. Opioid Use Work engagement Economic Stressors Retention Organizational Job insecurity Commitment Financial fragility Occupational Health Outcomes Performance Underemployment Other outcomes Income inadequacy This project will be a short-term investment with long-term goals. My short-term goals are to focus on the first two questions to establish the relationship between economic stressors and opioid use, as well as to identify causal factors to help explain the relationship. I will be analyzing a series of archival datasets and in-process data collection to test these relationships. Currently, my advisor Dr. Bob Sinclair is administering a longitudinal study online through Amazon’s Mechanical Turk (MTurk) that includes the variables of interest for my research (economic stressors, opioid use, and multiple occupational health outcomes). The first wave of this data collection is complete with over 700 participants. I also have access to multiple additional sources of survey data including other unpublished MTurk data sets, studies of retail employees, nurses, and larger population surveys. To test these initial relationships, I will conduct Multiple Regression Analyses and Structural Equational Modeling as applicable to each dataset. Having access to many existing and in-progress datasets will facilitate my short-term productivity and provide me with experience so that I can collect my own data in the future My long-term goals are to continue expanding on economic stress and opioid use to link their relationship to organizational and occupational health outcomes. I would like to further explore the workplace implications and potential interventions if opioid use is found to be a coping mechanism for managing both employment-related and financial economic stress. My current resources for data sets and collection will allow me to pursue my research interests and become a scholar in this area of this research. The NSF Graduate Research Fellowship will help me establish a program of work that will advance knowledge and have broader social impact. The proposed program of work will advance knowledge by bridging gaps in the literature concerning the causal links among economic stressors, opioid use, and work-related outcomes. In the long run, I hope to establish a program of multidisciplinary collaborative research connecting my work in applied psychology with scholarship in economics, public health, and business. Understanding the intricacies of the relationships between the variables of interest will have broader social impact as we will be able to identify potential antecedents of opioid use to create social or organizational interventions. This knowledge will help organizations better understand how to address issues of economic stressors and opioid use and ultimately mitigate negative occupational health outcomes. REFERENCES: [1] Voydanoff, P. (1990). Journal of Marriage and the Family.[2] Probst, T. (2004). Sage Publications Inc. [3] Shoss, M. (2017). Journal of Management [4] Cheng, G. H.-L., & Chan, D. K. S. (2008). Applied Psychology: An International Review [5] De Witte, H., Pienaar, J., & De Cuyper, N. (2016). Australian Psychologist [6] Probst, T. M., Stewart, S. M., Gruys, M. L., & Tierney, B. W. (2007). Journal of Occupational and Organizational Psychology [7] Vijayasiri, G., Richman, J. A., & Rospenda, K. M. (2012). Addictive Behaviors [8] Lasalle, M., Chastang, J. F., & Niedhammer, I. (2015). Journal of Psychiatric Research [9] Nagelhout, G. E., Hummel, K., de Goeij, M., de Vries, H., Kaner, E., & Lemmens, P. (2017). International Journal of Drug Policy [10] American Society of Addiction Medicine. (2016). [11] Heavey, S. C., Chang, Y., Vest, B. M., Collins, R. L., Wieczorek, W., & Homish, G. G. (2018). International Journal Of Drug Policy [12] Sarpatwari, A., Sinha, M., & Kesselhei, A. (2017). Harvard Law and Policy Review [13] Shah, A., Hayes, C. J., & Martin, B. C. (2017). MMWR: Morbidity & Mortality Weekly Report [14] Rudd, R., Seth, P., David., F., & Scholl, L. (2016). MMWR: Morbidity & Mortality Weekly Report, 65(50): 1445-1452. [15] Florence, C.S., Zhou, C., Luo, F., & Xu, L. (2016). Medical Care [16] Centers for Disease Control and Prevention (2014). [17] Zibbell, J. E., Asher, A. K., Patel, R. C., Kupronis, B., Iqbal, K., Ward, J. W., & Holtzman, D. (2018). American Journal of Public Health [18] van Hasselt, M., Keyes, V., Bray, J., & Miller, T. (2015) Journal of Workplace Behavioral Health. [19] Leana, C., & J. Meuris. (2015). The Academy of Management.	Winner!
109	Investigating morphological variation in Siphonophores Background & Proposal: Historically, Siphonophores have been mistaken for jellyfish due to their transparent bodies, long tentacles, and stinging nematocysts [1], [2]. Like many other cnidarians (e.g., corals), Siphonophores are colonial animals and are made up of multiple animal bodies, calledzooids, which arise from the same embryoand function together as one organism. Within the Siphonophore, zooid types are arranged in a specific pattern, which is repeated across the organism and determined at the growth zones of the Siphonophore [4]. Siphonophores have a high degree of functional specialization and precise organization within the colony, which sets them apart from most other animal species [3]. Though Siphonophores are a diverse group, we lack an understanding of how the organizational pattern of zooid type differs across species, and to what degree this morphological variation of patterns is conserved. Understanding the conservation of pattern type, will inform us of the functional specialization structures that are indicative of their survival. To answer these questions, I will use geometric morphometric methods to compare differences among Siphonophore species. This approach builds on recent work done in the Casey Dunn lab at Yale, draws directly from myexperience in Dr. Dean Adams’s lab,and is motivated by my own interest in complex trait evolution. Last year, the Dunn lab published a transcriptome-based Siphonophore phylogeny and used it to reconstruct the evolutionary history of changes in Siphonophore sexual systems, life history traits, habitats, and zooid types. In their comparisons of zooid type, Munro et al. [1] used only the binary characterization of presence/absence of each zooid type, making this study void of any zooid organizational pattern classification. Previous studies have also suggested that organizational patterns of zooid type are species-specific [5].These organizational patterns have never been examined from a phylogenetic perspective. I am interestedin extending the work done in Dr. Dunn’s lab by quantifying morphological variationof zooid types to determine their evolutionary history and organizational pattern within the Siphonophore colony.Understanding the evolution of zooid types is key to unraveling the mechanisms behind coloniality and functional specialization. Broadly, this study will improve our understanding of complex traits in non-model organisms from which we lack critical information about their basic biology. The aim of my study is to determine the evolution of organizational patterns and variation of zooid specialization in Siphonophores by applying novel methods to quantify three-dimensional data.To quantify the morphologicalvariation of zooid type beyond presence/absence descriptions, I will use micro-computerized tomography (CT) scans to characterize organizational patterns. This project will analyze traits of Siphonophores that is currentlynot understood within the scientific community. Methods:I will collect at least three specimens for each of the 33 species analyzed in the phylogeny produced by Munro et al [1] to quantify zooid morphology. Specimens will be collected via blue water SCUBA diving or remotely operated vehicles from the Monterey Bay Aquarium Research Institute. Collected samples will be stored and preserved in solutions of formaldehyde, as standard procedure [6]. In the Dunn lab at Yale, I will stain samples using osmium tetroxide to enhance the visualization of body structures and then use the micro-computerized tomography scanner available on site to scan collected specimens. Using CT scans, I will obtain images of x-rays for every species. To quantify zooid structures from these scans, I will develop a novel landmark scheme appropriate for use in Siphonophores and obtain these data using the programAvizo™ (Fig. 1). I will then perform multivariate computational analysis for these landmarks using thegeomorph package[7] in R [8]. To test for correlations between zooid morphology and phylogenetic history, I will perform a phylogenetic regression for Procrustes shape variables, which will identify patterns of zooid shape variation across the Siphonophore phylogeny. I will then determine the rate of evolution for zooid shape and organizational pattern by performing morphological disparity tests. These results will indicate the tempo and mode by which these morphological structures have evolved and the degree of conservation across species. Feasibility: In the Dunn Lab, I will work with experts in evolutionary and Siphonophore biology. At Yale, having access to the largest Siphonophore collection to date would allow me to assess all preserved specimens to incorporate into my research. My past research experience in preparing and maintaining museum specimens, as well as operating and analyzing data from a CT scanner will allow me to successfully complete this project. In the Casey Dunn Lab, I will apply methods from my work with Dr. Dean Adams’s lab including geometric morphometrics, biostatistics, and phylogenetics to complete this project. Intellectual Merit: The collections from this study will illuminate our understanding of the diversity across the Siphonophore phylogeny. It will also aid in the development of new techniques to maintain and preserve non-model specimens for theYale Peabody Museum. I will use morphological data to reveal how Siphonophore phenotypes have dispersed throughout their evolutionary history. Applying computational approaches to compare morphology has been an ongoing limitation for research in evolutionary biology. This study will further develop these approaches by using a novel combination of techniques such as multivariate analyses and CT scanning. Major outcomes of this study will be the identification of species-specific organizational patterns, as well as a greater understanding of phenotypic plasticity of zooid types. These results will inform biologists on the evolution of coloniality, functional specialization, and the morphological specificity of zooids. All CT scans, specimens, and codes from these analyses will be openly accessible to scientists via data sharing platforms. Broader Impacts: Throughout my dissertation, I willparticipate in public outreach and the education of young scholars in science by giving a series of presentations about my experience as a scientist, research methods, and results at theYale Peabody Museum. At Yale, I will continue to participate in the Society for the Advancement of Chicanos/Hispanics and Native Americans in Science and begin working with Pathways to Science programs to help low-income, first-generation, and underrepresented students pursuing science. I plan to engage students in these programs and the public by using the unique context of museums. I will work jointly with curators to help build interactive exhibits by providing field video blogs, preserved specimens, and topics that expand upon the direct implications of my work into more generally societally relevant fields such as declining biodiversity and global climate change. Importantly, these proposed exhibits not only give a diverse public face to scientists, but also use the museum to help spark scientific curiosity in the public. References:1) Munro et al.Molecular Phylogeneticsand Evolution127(2018):823-833. 2) Cooke et alClinical Toxicology3(1970):589-595. 3) Mackie, G.O.LowerMetazoa(1963)329-337. 4) Goetz FE.Nanomia bijugawhole animal and growth zones from http://commons.wikimedia.org/wiki (2018). 5) Dunn, C.W., Wagner, G.P.,Dev Genes Evol216(2006):743-754 6) Holst et al.Journalof Plankton Research38(2016):1225-1242. 7) Adams et al. Geomorph(2018) R package version 3.0.6 8) R CoreTeamR(2013).	HM
110	Rationale: Coral reefs provide services totaling $10 trillion despite covering only ~0.3% of the ocean floor1. Their evolutionary success relies on the association between coral animals and symbiotic algae. Corals provide shelter and nutrients for symbionts which in turn supply sugars and O to their hosts2. Corals host symbionts within the symbiosome, an intracellular space 2 defined by the coral-derived symbiosome membrane. This membrane is thought to allow corals to regulate delivery of nutrients to the symbiont but the specific transport mechanisms are mostly unknown. Alarmingly, human-caused ocean warming, acidification, and eutrophication disrupt this symbiosis leading to the expulsion of symbionts (known as ‘bleaching’), decreased coral fitness, and death2. However, the lack of mechanistic knowledge of healthy symbiosis impairs our ability to understand why bleaching occurs, identify resilient and vulnerable species, and design conservation strategies. The mechanisms that deliver nitrogenous molecules (N ) to m symbionts are particularly important. Healthy corals must provide symbionts with enough N for m the repair of photosystem proteins and other basic functions; however, excess N could result in m symbiont overgrowth and bleaching2. Thus, corals must possess yet unidentified mechanisms to regulate N delivery to symbionts. m I propose to study the mechanisms controlling N delivery to symbionts and to m characterize responses to environmental stress in two coral species with differential susceptibility to eutrophication. In other animal models, NH moves across membranes via Rhesus channels 3 (Rh). When paired with an acidification pathway, NH gas combines with H+ to form NH + that 3 4 is trapped on the other side of a membrane3. Coral Rh is an ideal candidate for transporting NH 3 across the symbiosome membrane for N delivery for three reasons: (1) an “Rh-like” gene is m upregulated in anemones upon symbiont acquisition4, (2) corals acidify the symbiosome using V-H+-ATPases, which would favor NH + trapping in the symbiosome5, and (3) NH + is 4 4 symbionts’ preferred N source6. I hypothesize that (1) corals supply N to symbionts via Rh in m m the symbiosome membrane, (2) N supply is controlled via transcriptional and translational Rh m regulation and changes in Rh localization, and (3) future ocean conditions can bypass the Rh pathway resulting in bleaching. Preliminary Results: I cloned the first coral Rh from Acropora yongei (ayRh) (MH025799), developed anti-ayRh antibodies, and confirmed ayRh protein expression via Western Blots. I found that ayRh is more abundant in the symbiosome membrane during daytime compared to the night via immunofluorescence microscopy (IFM) (Fig 1). Aim 1: Establish ayRh Transport and Function. In vitro: I will express recombinant ayRh in Xenopus oocytes and measure its transport kinetics. Oocytes injected with ayRh cRNA or scrambled cRNA (controls) will be incubated with the radiolabeled NH /NH + analog [14C]methylammonium, 3 4 and uptake rates will be measured with a gamma counter. Since some vertebrate Rhs can also transport CO 7, I will 2 determine ayRh CO permeability by measuring CO -induced 2 2 changes in oocyte pH with the pH-sensitive dye SNARF1. I will run statistics in R and Prism™. I predict that ayRh is NH - and not CO -permeable, supporting my hypothesis of 3 2 Figure 1. Diel Rh localization to the Rh-mediated N m delivery. If ayRh transports both, I will SM (n = 50) (p = 0.0246*). adjust my hypothesis and explore the role of Rh in providing carbon and N for symbiont photosynthesis and metabolism. In vivo: I will explore the m correlation between Rh abundance and N transport rate in isolated coral cells hosting m symbionts5. I will measure Rh abundance by Western Blot and NH + uptake rates from seawater 4 using spectrophotometry. I predict a direct relationship between Rh abundance and capacity for N transport. All materials are already available in my collaborating and host labs. m Aim 2: Characterize Coral Rh Regulation. I will expand on my preliminary results (Fig.1) to identify mechanisms that regulate Rh abundance in the symbiosome membrane. In addition to A. yongei, I will work with Stylophora pistillata, which is more resilient to N eutrophication8. This m comparative approach may unveil species-specific mechanisms that confer resilience in polluted oceans. Transcriptional and translational Rh regulation will be tested using qPCR and Western blotting in coral samples taken during day and night timepoints. Rh’s subcellular localization and dynamics will be assessed in unprecedented detail via IFM on a super-resolution confocal microscope. Building on preliminary experiments, I will sample every three hours over a two- day period. Furthermore, I will use the highly specific photosynthesis inhibitor DCMU to determine if the presence of Rh in the symbiosome membrane depends on photosynthetic activity or simply on the presence of light2. I will automate IFM data collection and quantitative analysis with ZENTM software; I will use my coding experience to create custom workflows to achieve high throughput and bias reduction during analysis. I will run statistics in R and Prism™. I predict the Rh pathway is present in both coral species, that Rh trafficking to and away from the symbiosome membrane depends on photosynthetic activity, and that Rh mRNA and protein abundance will remain relatively constant reflecting basal turnover rates. Aim 3: Establish Rh Responses to Stress. To determine the effects of future ocean conditions on Rh, I will grow A. yongei and S. pistillata in three conditions: (1) control, (2) elevated N (10 m μM NH Cl), and (3) elevated N and CO (10 μM NH Cl, 1000 μatm CO ). I will collect 4 m 2 4 2 samples at 12:00 and 24:00 daily over a 70-day period (10 days of control, 30 days of treatment, and 30 days of recovery in control conditions) and rapidly analyze Rh expression and subcellular localization as described above; this method will also allow me to quantify symbiont density to estimate bleaching. Additionally, I will study symbionts’ photobiology using respirometry and PAM fluorometry and genotype symbionts to explore potential effects of symbiont strain. I will run statistics in R and Prism™. I predict the Rh pathway will be initially downregulated in both experimental treatments. I also predict that corals in elevated N and CO conditions will m 2 undergo the highest degree of bleaching due to larger loss of host control over symbiont growth; these effects will be more pronounced in eutrophication-sensitive A. yongei. Finally, I predict the Rh pathway will gradually return to normal during recovery and reestablishment of symbiosis. Intellectual Merit/Broader impacts: This study will characterize a novel N transport m mechanism in coral symbioses and develop much-needed biomarkers to evaluate species-specific vulnerability to environmental stress and early detection of bleaching. It also has the potential to reshape our understanding of coral symbioses by establishing a novel diel regulatory mechanism that traffics proteins to and from the symbiosome membrane. I am well qualified to conduct this research based on my experience with IFM, molecular biology, coral biology, and computer science. In my PhD, I will continue to mentor undergraduates through my tutoring program, many of whom are Latina females, and I will expand my program to low income high schools. Results from my project will be presented to the scientific community through peer-reviewed papers and conferences, and to the general public in youth activities, lectures, and exhibits through Sally Ride Science and the Birch Aquarium (which hosts 450k visitors annually). My career goal is to be an R1 professor and these activities will shape my future outreach and education programs. References: (1) Global Environ Change 2014, 26, 152-158. (2) Microbiol Mol Biol R, 2012, 76, 229-261. (3) Transfus Clin Biol 2006, 13, 85-94. (4) G3-Genes Genom Genet 2014, 4, 277-295. (5) PNAS 2015, 112, 607-612. (6) Mar Biol 1983, 167, 157-167. (7) Membranes 2017, 7, 61. (8) Mar Biol 2000, 19, 103-113.	Winner!
111	The Magnetic Origins of Solar Coronal Plumes Sun: corona — Sun: magnetic fields —Sun: UV radiation I. Introduction The corona is a diffuse cloud of plasma that surrounds the Sun that is ~104 times hotter ​ ​ than the photosphere at the surface of the Sun. The mechanism for coronal heating is one of the most sought after solutions in solar physics, as it could explain the origin of the solar wind, a stream of charged particles that bombards Earth and other planets. In order to determine a solution to the coronal heating problem, solar physicists study structures that arise in the corona as a consequence of the ever-changing solar magnetic field, like coronal plumes. Plumes are sporadic, fountain-like structures that are rooted in a strong patch of dominant-polarity photospheric magnetic flux, surrounded by a predominantly-unipolar magnetic field. Plumes are located in the least active regions in the solar corona: in either coronal holes or in quiet regions. Studying the formation of plumes could shed light on how the corona is heated, as there may be a fundamental mechanism that heats plumes that may be the same in other coronal structures. Several observations have been presented regarding how plumes form and disappear, but none have succeeded in determining the mechanism that generates plumes. In the Summer of 2017, to further investigate plume evolution, I tracked the lifetimes of six coronal plumes, three in quiet regions and three in coronal holes using SDO/Atmospheric Imaging Assembly (AIA) 171 Å images and SDO/Helioseismic and Magnetic Imager (HMI) magnetograms with Dr. Sanjiv Tiwari at an NSF REU program at the University of Alabama in Huntsville and NASA Marshall Spaceflight Center. We based our study on two previous studies, one by Raouafi et al.1 and Wang et al.3. Raouafi et al.1 infer from observation that plume heating ​ ​ ​ ​ ​ ​ is a consequence of magnetic reconnection at the base, whereas Wang et al.3 observe that plume ​ ​ heating is a result of convergence of the base magnetic flux. Both papers suggest that the base flux in their plumes is of mixed polarity, most of which is unobservable due to the spatial resolution of current instruments. Raouafi et al. and Wang et al. both suggest that this is the primary mechanism responsible for sustaining plume heating, and do not consider other contributing factors. I specifically investigated whether or not a critical magnetic field strength is ​ necessary for plume production, and determined from that a critical field strength of 250-500 Gauss, along with base flux convergence and divergence, is necessary for plume formation. While these preliminary results were extremely promising, indicating that a critical field strength may be necessary for plume production, further work needs to be done in order to confirm them. II. Specific Aims As a graduate student, I will extend this study to include coordinated high-resolution spectroscopic data from the Interface Region Imaging Spectrograph (IRIS) and the Extreme Ultraviolet Imaging Spectrometer (EIS) on the Hinode spacecraft in conjunction with SDO/AIA and SDO/HMI data. We will also extend our SDO/AIA observations to include those in 193 Å, 211 Å, and 304 Å emission. This increased emission coverage will allow us to observe plumes from the upper layers of the corona down to the chromosphere, the region between the surface of the sun and the corona where smaller-scale jet eruptions occur. Including spectroscopic data will allow us to further investigate smaller-scale jet eruptions in the chromosphere and transition region by observing cooler emission lines, and will allow me to create dopplergrams, which show the redshift and blueshift of spectral lines, in order to observe how flow patterns evolve throughout each plume’s lifetime. In order to determine a solution to the problem of underlying mixed polarity, we plan to propose observations to the Daniel K. Inouye Solar Telescope (DKIST) when it comes online in 2019. As the largest solar telescope in history, the increased spatial resolution of DKIST will allow us to resolve regions as small as ~20 km2, thus giving us ​ ​ the opportunity to observe minute emergence of opposite-polarity regions. To ensure that any conclusions of this study are statistically significant, we will expand our dataset to include a larger sample of 100 coronal plumes, 50 in quiet regions and 50 in coronal holes. III. Preparation and Relation to Career Goals This project allows me to continue my already fulfilling work on coronal plumes with Dr. Tiwari, who has since moved to working at the Lockheed Martin Solar and Astrophysics Lab (LMSAL) in Palo Alto. Stanford is an ideal setting at which to conduct this research, as its close affiliation with LMSAL would allow me to continue working with Dr. Tiwari, as well as collaborate with other solar physicists at LMSAL. Extending this project to include spectroscopic data and ground-based telescope observations would give me the skills to conduct more comprehensive observations of the solar corona, and increasing my previous study’s sample size would give me the statistical and computational skills I need to conduct further, more rigorous studies on larger datasets. This fellowship would give me the freedom to focus on obtaining a conclusive result, thus bringing me another step closer to a career in solar physics research, education, and outreach. IV. Broader Impacts With DKIST beginning operations in 2019, and the Parker Solar Probe, a mission to collect in-situ data on the corona, planned to launch in 2018, a wealth of data will soon be available for scientists to learn more about our nearest star. For most of my undergraduate career, I was unaware that there were so many unanswered questions about our nearest star. During my time as a graduate student, I look forward to involving younger students in this research in order to help the field of solar physics gain more exposure among young scientists. I have been inspired by the Pre-Major in Astronomy Program in place at the University of Washington, which is a mentorship program aimed at involving underrepresented undergraduate students in research early in their careers. I hope to do similar work by mentoring high school students in the greater Bay Area through Stanford’s Science in Service program. V. References 1Raouafi, N.-E., & Stenborg, G. 2014, ApJ, 787:118 — 2Tritschler, A., Rimmele, T. R., ​ ​ ​ Berukoff, S., et al. 2016, Astronomische Nachrichten, 337, 1064 — 3Wang, Y.-M., Warren, H. ​ ​ P., & Muglach, K. 2016, ApJ, 818:203	HM
112	The nature of dark energy is one of the biggest mysteries in physics and astronomy today. To quote Michael Turner, dark energy is “a problem for the 22nd century discovered by accident in the 20th century.” The Dark Energy Spectroscopic Instrument (DESI) is a massive next- generation survey that will attempt to constrain the dark energy equation of state. DESI will measure the spectra of over 30 million galaxies and determine their redshift. These redshifts, combined with measurements of Type Ia supernovae (SNe Ia), provide the strongest measurements of cosmological distances, our way of knowing the expansion rate of the universe. DESI offers the potential to spectroscopically observe ~105 supernovae (SNe) including many SNe Ia [1]. Interestingly, while SNe Ia are used as standard candles, their origins are not fully understood. Evidence exists for two types of progenitors: a degenerate white dwarf accreting matter from a giant companion star, or two coalescing white dwarfs [2]. Identifying a large population of SNe Ia will help answer the progenitor question and provide constraints on dark energy. My background in handling large astrophysical datasets (see: Personal Statement) has prepared me make valuable contributions in this area. I propose to develop an efficient computational procedure to identify SNe Ia in the DESI survey. In order to obtain accurate, unbiased distance measurements from SNe Ia, spectroscopic measurements are crucial in calibrating corresponding photometric observations. In particular, DESI will be able to spectroscopically complement future large-sky surveys such as the Large Synoptic Sky Telescope (LSST), which will begin its science observing in 2021 but collect immensely more data (expected ~1 million transient alerts per night with ~1 million SNIa observed over a decade [2]), and the Zwicky Transient Factory. In doing so, we can understand just how “standard” these standard candles are and provide complementary redshift coverage. Further, it is to our advantage to identify SNe Ia in real-time and send out alerts for followup observations. Moving forward, generalized data-analysis pipelines that are able to handle enormous quantities of data will be fundamental to the success of future experiments. I propose the following analysis and timeline to develop such a computational procedure: 1. (year 1) Identify SNe Ia in galactic spectra. 2. (years 2-4) Extend the identification algorithm to detect and classify galactic spectral anomalies (outliers) in general. 3. (final year) Develop an automated pipeline that will run in real time to identify transients. In step 1. I will focus on identifying SNe Ia in the DESI catalog. This will require construction of spectroscopic models of galaxies and applying a statistical test to a large sample of galaxy spectra to look for deviations from the model expectations. Spectra with significant deviations (anomalies) will be tested further by fitting a SN Ia spectral template to see if the anomaly is in fact a supernova. I propose to develop several complementary tests to identify SNe Ia spectra. Previous studies have searched for SNe Ia in the Sloan Digital Sky Survey (SDSS) catalog using singular-value-decomposition of a large sample of galaxy spectra to construct a basis of eigenspectra. The eigenbasis is used to fit galaxy spectra, and the residual spectrum is searched for features corresponding to a SNe Ia (note that the model in this case, i.e. the individual eigenspectra, does not represent a physical object - only the linear combination has a physical interpretation). Using this method, Graur & Maoz [3] report 90 SNe Ia in SDSS data release 7. A first alternative approach involves constructing a χ2 statistic (or likelihood test) of SNe Ia templates with the observed spectra, defining anomalies based on the χ2 goodness of fit. The Ryan Rubenzahl NSF Research Proposal University of Rochester advantage is that no unphysical basis is being used. As a sanity check, I will cross-correlate my sample of SNe Ia with those found in previous studies [3]. With this sample, I will calculate an estimate of the SNe Ia rate to help answer the progenitor question and make first-order distance estimates using SNe Ia redshifts to build a framework for future photometric calibrations. While useful for cosmology, this method should be capable of identifying more than just SNe. In step 2. I will generalize this procedure to allow the classification to include any number of other astrophysically interesting phenomena. As a first step, this will involve adding templates for each source of interest, for example a two-galaxy-spectrum model to represent sources of strong gravitational lensing, which offer excellent tests of the general theory of relativity. Once the algorithm is efficient at detecting several different classes of objects, I will drop all a priori assumptions. In this case, the proposed algorithm will be general and unassuming of any particular class of object, allowing the potential of discovering new phenomena. A successful algorithm will also identify bad spectra either due to instrumental issues or other errors, learn what those patterns look like, and avoid or even correct for them in the future. At this stage, a plethora of astrophysical phenomena beyond just SNe may be observed and studied. Previous studies have tried general approaches to anomaly detection using predictive learning algorithms such as a random forest (decision tree), and have even found larger yields than targeted identification algorithms [4, 5]. I will study these approaches and develop a machine-learning algorithm to classify spectroscopic anomalies with DESI. I will work to boost the efficiency as well as the yield of outlier spectra while minimizing false-positives. In step 3. I will maximize the efficiency of the algorithm so that it can be used in real-time in a pipeline. These approaches will each be tested and trained on existing spectroscopic data obtained by SDSS. I will also test simulated spectra generated for DESI. Simulated data in particular will give an estimate of the classification purity of the pipeline for the various phenomena we are aiming to observe. Once DESI is online, the learning algorithm will be applied to the real data, being continuously trained as the dataset grows. An integral part of my work on this project will be engaging the public in active participation. The citizen-science project Galaxy Zoo Supernovae demonstrated that members of the general public are remarkably good transient-spotters: 93% of supernovae were correctly identified by the public with no false-positives [6]. I will make the results of my pipeline, including sample data, available to existing citizen-science platforms which have a proven track record of engagement and popularity with the public. Participants will enjoy hands-on involvement in the data analysis alongside lessons detailing the qualitative astrophysics of supernovae and how their signature can be seen in galaxy spectra through the heavy elements produced in the explosion. The data collected by citizen scientists can support actual results by providing confirmation or rejection of suspected outlier spectra. My findings, combined with the citizen-science results, will lead beyond DESI and into future LSST analysis and other data- intensive astronomy projects. References [1] DESI Collaboration, 2016, Final Design Report I [4] Baron & Poznanski, 2017, MNRAS, 465, 4530 [2] Maoz & Mannucci, 2012, PASA, 29, 447 [5] Buisson et al., 2015, MNRAS, 454, 2026 [3] Graur & Maoz, 2013, MNRAS, 430, 1746 [6] Smith et al., 2011, MNRAS, 412, 1309	Winner!
113	Key Terms: mechanotransduction, micropipettes, chemotaxis, phagocytosis, β integrin 2 Introduction: Future medical innovation will require a detailed knowledge of the causal sequences of events in biological processes. Currently, much of the understanding of these processes comes from correlative studies, whereas cause-effect relationships are less often explored. For instance, in immune cells, several signaling pathways are associated with dramatic, global bursts in cytosolic calcium concentration, but it remains unclear which pathways trigger the calcium burst and which depend on it. In human neutrophils, these bursts are correlated with several mechanically demanding processes, including β -integrin-mediated cell arrest1, the onset 2 of active cell spreading on immobilized IL-82, and the acceleration of β -integrin-mediated 2 phagocytosis3. On the other hand, my prior work in the Heinrich Lab has shown that pure (i.e. adhesion-free) complement-mediated chemotaxis neither causes nor requires such global calcium surges4 (see Fig. 1B). We further demonstrated that unphysiologically high levels of chemoattractant can cause calcium bursts, but contractile forces stalled or even reversed pseudopod formation in such cases. These findings imply a close connection between calcium bursts and mechanical behavior. The purpose of this project is to examine the cause-effect relationship between changes in cytosolic calcium concentration and mechanical responses of human neutrophils to chemotactic and phagocytic stimuli on a single-cell basis. Background: Store-operated calcium entry (SOCE) is considered the dominant mechanism for calcium bursts in human neutrophils. In this paradigm, ligation of certain receptors, such as G- protein coupled receptors (GPCRs), triggers a signaling cascade that leads to the depletion of intracellular calcium stores (usually via IP production). This prompts a calcium influx from the 3 extracellular space through channels such as Orai1. However, our own findings and several earlier studies indicate that this view of SOCE is incomplete, as GPCR ligation can cause chemotaxis without triggering a calcium burst4,5. Furthermore, shear force on high-affinity β - 2 integrins is known to mediate calcium influx1, which implies that mechanotransduction is important for SOCE in neutrophils. It also remains largely unclear which cellular activities depend upon the elevated calcium levels after store release and calcium influx. Calcium bursts often precede F-actin-mediated spreading1-3, but this connection is not fully understood. I hypothesize that β -integrin-mediated 2 mechanotransduction is key to inducing a global calcium signal in human neutrophils, which controls a mechanistic switch between two distinct modes of cytoskeletal organization and dynamics. I will primarily use single-cell, single-target micropipette experiments (Fig. 1A) to quantify aspects of the mechanical response (e.g. cell morphology, cortical tension, or surface area) while monitoring intracellular calcium concentration using a calcium-sensitive dye (e.g. Fluo-4 or Fura-2). This will be supplemented by data from Fig 1. A: Experimental setup. B: Neutrophil other biophysical experiments, as well as shows a calcium burst during phagocytosis, but mathematical modeling. not during pure chemotaxis4. Aim 1. Uncover specific mechanical or biochemical cues that are necessary and/or sufficient to induce calcium bursts. Before micropipette experiments, human neutrophils will be treated with an actomyosin inhibitor (e.g. latrunculin A, cytochalasin D, blebbistatin) or a β - 2 integrin (LFA-1 or Mac-1) blocking antibody. Our analysis of the calcium response will reveal the roles of the respective molecules in calcium burst induction. I will also use reflection interference contrast microscopy (RICM) to measure the contact areas of neutrophils spreading on glass with a known ligand density, determining if a threshold of engaged receptors can trigger a calcium burst. Application of a measurable force on β integrins on a neutrophil using atomic 2 force microscopy (AFM) will allow me to explore whether force can directly stimulate a calcium burst or if there is a synergistic effect between force and number of integrins engaged. Aim 2. Characterize the mechanical and morphological changes that require calcium store release and/or calcium influx. I will conduct micropipette experiments after depleting extracellular calcium with EGTA or emptying internal calcium stores with thapsigargin. In similar experiments, I will block IP -dependent store release using a PLC inhibitor (U73122), or 3 use murine neutrophils with a deficient calcium influx (Orai1+/- or Orai1-/-, collaboration with Dr. Scott Simon, BME Dept.). These experiments will indicate the relative importance of calcium store release and calcium influx for mechanical changes such as elevated cortical tension or surface area expansion. I will also collaborate with Dr. Soichiro Yamada (BME Dept.), using confocal microscopy to simultaneously image actin arrangement and calcium concentration in a neutrophil-like cell line (PLB-985) transfected with GFP-actin and loaded with Fluo-4. I will assess actin structure and dynamics following calcium bursts in these cells. Aim 3. Incorporate global calcium signaling into an established computational model of neutrophil phagocytosis. I will collaborate with Dr. Samuel Walcott (Math Dept.) to build on the computational model developed by Herant et al.6, which accurately describes the phagocytic behavior of neutrophils. The model predicts a key role for cytoskeletal membrane anchors, connections that form between integrins and F-actin via adaptor proteins. The assembly of these complexes is associated with elevated calcium levels1,3. I will incorporate the calcium- dependence of cytoskeletal membrane anchor strength into the model and leverage this revised model against the phagocytic behavior of neutrophils in the above experiments. Intellectual Merit: My three years of experience with micropipette experiments and quantitative data analysis have prepared me well for this important work. This project will fill a fundamental knowledge gap regarding one of the most dramatic signaling events in the life of a neutrophil. In addition, clarifying the sequence of molecular events leading from receptor engagement to calcium burst to protrusive force generation may elucidate similar mechanisms in other cells. Broader Impacts: An improved quantitative and mechanistic understanding of immune cells will strengthen the foundational knowledge for novel medical treatments such as immunotherapy. With an understanding of the cause-effect relationship between calcium signaling and immune cell motility, new therapeutic targets for immunodeficiencies and autoimmune diseases could also be identified. Furthermore, because calcium bursts are easily detectible and are strong indicators of immune cell activation, my research may inform the development of future diagnostic tools. I will include undergraduate students in this project, share findings in publications and at international conferences, and inform the public by creating and sharing videos online (www.youtube.com/heinrichlab). References: [1] Dixit, N. et al. (2011) J Immunol. 187(1):472-481 [2] Beste, M.T. et al. (2015) Ann Biomed Eng. 43(9):2207-2219 [3] Dewitt, S. and M.B. Hallett (2002) J Cell Biol. 159(1):181-189 [4] Francis, E. and V. Heinrich (2017) Revision under review. [5] Laffafian, I. and M.B. Hallett (1995) J Cell Sci. 108(10):3199-3205 [6] Herant, M. et al. (2011) PLoS Comput Biol. 7(1):e1001068	Winner!
114	"Development of a pressure-sensitive kinetic blood-brain-barrier Aβ clearance model Introduction: Alzheimer’s disease (AD) is a chronic neurodegenerative disease and the main cause of dementia worldwide. Recently discovered, the glymphatic system is a waste clearance pathway that exchanges the central nervous system (CNS)’s fluids which has been shown to be one of the main facilitators of the clearance of beta-amyloid (Aβ), one of the hallmark pathology of Alzheimer's disease (AD).1,2 This system carries the soluble Aβ peptide to be discharged through the blood-brain barrier (BBB) by specialized transporters and has been shown to become impaired in ageing, leading to its toxic accumulation in the brain.3 Furthermore, AD is often implicated with cardiovascular alterations affecting blood pressure and though their treatment has been correlated with reduced incidence of AD and slower cognitive declines in AD patients, their mechanistic relationship remains unclear.4 Although, Aβ clearance has been mainly characterized in vivo in animal models, they also involve the interplay of other cellular and enzymatic clearance mechanisms.4 Thus, a validated dynamic in vitro model would provide a platform to quantify and predict the yet unexplored effects of abnormal blood and cranial pressures in the BBB’s clearance of Aβ which are critical to advance our understanding of how one of the most prevalent vascular changes correlated with AD, affects the main clearance gate protecting us from it. The objective of this proposed research is to build a mathematical model from the kinetic characterization of pressure effects in the transport of Aβ through a microfluidic blood-brain-barrier device to predict the effects of pathologic changes in cerebral perfusion pressure in the clearance of Aβ from the brain. Approach: The proposed project will execute the following aims: (1) to adapt and validate current microfluidic BBB technology for this project, (2) to experimentally characterize the in vitro BBB’s Aβ kinetic transport as a function of protein load and pressures, and (3) to develop a mathematic model of the Aβ efflux across the brain in health and in disease. Aim 1: Microfluidic models have recently succeeded in recreating and mimicking the selective boundary properties of the BBB; one example includes the NeuroVascular Unit (NVU) model developed at Vanderbilt University by Dr. Wikswo’s lab (Fig. 1), involving two distinct chambers representing the blood and the brain sides, separated by a three-dimensional and sequential culture of an immortalized line of the human neurovascular cells building the structure of a complete BBB.5 Our device will build upon this model to allow for the controlled monitoring of pressure and flow Fig. 1: Wikswo’s lab blood- rates while applying its ability to recreate the human BBB’s brain-barrier-on-a-chip device.5 heterogeneity and structural complexity. Firstly, the devise will be equipped with loading port valves, a variable pulsatile pump to mimic vascular perfusion pressure and a syringe pump to maintain an accurate pressure and simulate glymphatic flow on the brain chamber. The boundary’s integrity under these dynamic conditions will be validated via transendothelial electrical resistance measurements (TEER) or alternatively via fluorescent microscopy to confirm proper endothelial tight junctions. Thereafter, pH, temperature, viscosity, and salt content on both compartment fluids will be matched to their human physiologic conditions, as protein behavior is sensitive to these factors. Aim 2: Commercially-available Aβ and Aβ peptides, will be infused at different 1-40 1-42 concentrations separately on the brain compartment, at different pressures under physiologic mean blood and glymphatic system flow rates to measure the BBB’s transport rates at which Aβ leaves the brain compartment. Right after running through the devise the solution will be sampled and Sergio Rodriguez Labra | Research Statement Aβ measured by ELISA. After multiple trials, a differential mass balance on the two compartments will be used to calculate two diffusive rate constants, one for the Aβ going from the brain to the blood side, and a reverse rate to account for potential Aβ reuptake into the brain compartment, allowing for the overall rate of Aβ transport to be calculated for different Aβ concentrations and pressures. To validate the physiologic operation of both of the known Aβ transporters, LRP1 and RAGE, in the cells, their commercially-available natural ligands will be similarly run as a control, comparing their transport rates to their physiologic literature values.6 Aim 3: Using the calculated Aβ transport rates, statistics on R will be used to confirm the data’s power and a mathematic model on MATLAB will be built to describe the Aβ efflux as a function of Aβ concentration and the pressures across both chambers, thus, quantitatively correlating the relationship between Aβ transport at different healthy and pathologic peptide burdens in the human brain, and under physiologic and abnormal pressures on both sides of the BBB. The developed model will be validated testing its predictions on the quantitative change in Aβ Fig. 2: Possible effects of pressure accumulation in the microfluidic device after sudden or chronic in A clearance for a fixed A load. pressure changes likely to be produced by stroke, traumatic brain injuries, and cardiovascular diseases. This will produce a predictive computational model of the decrease in BBB-mediated Aβ clearance with ageing and disease. Broader Impact: The development of a mathematical model focused on cranial and blood pressure will allow for the estimation of Aβ accumulation in the brain due to reduced BBB clearance and predict the increased risk to acquire AD after people develop hypertension, hypotension, stroke, or traumatic brain injuries, when it is much easier to take preventative and therapeutic measures than trying to arrest an evolved AD later on. Furthermore, by considering Aβ concentrations, this adaptable predictive model will be able to incorporate future Aβ biomarkers’ data for personalized medicine diagnostics using the patient’s own biometrics. Finally, the improved microfluidic device will serve as a platform for characterizing the effects of abnormal blood pressures in the arterial pulse-driven glymphatic system, thus, describing its synergistic dysfunction in disease, and for studies describing the pharmacodynamics of brain- penetrant drugs in altered vascular perfusion or cranial pressure conditions. Studying the link of the discussed highly prevalent clinical conditions has important repercussions in everyone’s everyday lifestyle decisions. To generate awareness, I will use my continued participation at the Society of Hispanic Professional Engineer’s national conferences and the blog I will manage during grad school to present and break down the significance of my findings to the general public, inspiring them to pursue similar projects in STEM. 1 Nedergaard, Maiken. ""Garbage truck of the brain."" Science 340.6140 (2013): 1529-1530. 2 Tarasoff-Conway, Jenna M., et al. ""Clearance systems in the brain - implications for Alzheimer disease."" Nature Reviews Neurology 11.8 (2015): 457-470. 3 Kress, Benjamin T., et al. ""Impairment of paravascular clearance pathways in the aging brain."" Annals of neurology 76.6 (2014): 845-861. 4 Hamel, Edith, et al. “Neurovascular and cognitive failure in Alzheimer’s disease: benefits of cardiovascular therapy.” Cellular and molecular neurobiology 36.2 (2016): 219-232 5 Brown, Jacquelyn A., et al. ""Recreating blood-brain barrier physiology and structure on chip: A novel neurovascular microfluidic bioreactor."" Biomicrofluidics 9.5 (2015): 054124. 6 Deane, R., et al. “Clearance of amyloid- peptide across the blood-brain barrier: implication for therapies in Alzheimer’s disease”. CNS & Neurological Disorders-Drug Targets 8.1 (2009): 16-30"	HM
115	Zack Morrow Introduction and Previous Work At the molecular level, density functional theory (DFT) describes the electronic state of a system through functionals, which are mappings whose domain is a function space [1]. Existing chemistry software packages (e.g., Gaussian) use DFT to provide evaluations of a potential energy surface (PES) at molecular coordinate locations. In order to drive simulations of molecular dynamics, one needs potential energy in order to solve for the reaction path. However, the evaluation of the true PES at the internal coordinates is far too expensive to carry on a dense grid over the full domain. Sparse grids provide a means to keep the computational cost in check. We compute and store evaluations of the true PES only at the sparse points. We then build a sparse interpolating polynomial of the PES as a surrogate, which we evaluate on the full grid over the domain. The evaluations of the interpolant are significantly less expensive than the evaluations of the true PES; the main expense occurs in evaluating the true PES at the sparse interpolation points, which is done at the front end and stored for later use. Previous work in our group by James Nance investigated the Smolyak construction of sparse grids and its application to molecular dynamics and surface-hopping problems [2, 3]. Nance worked in collaboration with the research group of Prof. Elena Jakubikova in NC State’s Department of Chemistry, our current collaborators. The sparse grid code that Jakubikova’s group utilizes today is the code written by Nance as part of his thesis. Proposal: Intellectual Merit Nature tends to an energy-minimizing state, and accordingly, stable molecular geome- triescorrespondtolocalminimaofthepotentialenergysurface(PES).Inordertoconstruct the PES itself, we first take q to be a fixed geometry and find the electron energy levels E i by solving (e.g., with Gaussian) the time-independent Schr¨odinger equation, a well-known quantum mechanical relationship encoding the energy of a system: Hˆ Ψq = E Ψq, i ∈ {0,1,2,...}. (1) i i i ˆ Here, H is the molecular Hamiltonian operator (connected to the total energy of the sys- tem), the eigenvalue E is the energy of electronic state i under geometry q, and Ψq is the i i wavefunction corresponding to E (connected to the probability that a given electron in a i molecule under geometry q has energy E ). i Now, as a function of all admissible geometries q, we denote E (q) to be the PES i corresponding to energy state i. Again, stable molecular geometries correspond to local minima on the PES; moreover, transitions of a molecule from energy state i + 1 to state i occur when the molecular geometry corresponds to a local minimum on E (q). The i+1 reaction path to an energy-minimizing configuration is the solution to q˙ = −∇E (q), (2) i which, pictorially, is the path of steepest descent on the PES from the initial geometry to a local minimum. Using Equation (2), we can simulate numerically how molecular geometry evolves in time after excitation to higher energy states and subsequent relaxation. Since the true PES E (q) is far too expensive to compute on the full grid, we generate a i set of sparse grid points and utilize the sparse interpolating polynomial Es(q) as a surrogate i for the true PES in our simulations. Currently, we use a MATLAB code to manage the generation and evaluation of the sparse interpolant Es(q), and the dynamical simulation i routines run on university desktops and laptops. However, the simulation code will soon be migrating to the XSEDE platform, a geographically distributed computing cluster on which it is infeasible to use MATLAB. We are therefore replacing the MATLAB sparse-grid manager with an open-source C++ sparse-grid manager called Tasmanian, developed at Oak Ridge National Laboratory, which additionally comes with a Python wrapper [5]. I have replicated previous PES results using the new Tasmanian package and am cur- rently working to integrate it fully into our dynamical simulation codes. One difficulty to resolve is that Tasmanian currently does not compute gradients internally—a problem when using Es(q) in Equation (2). To boost accuracy and exploit parallelizability, gradi- i ents are best evaluated inside the sparse-grid manager itself. I will spend next summer at Oak Ridge working to add gradient-computation routines to the Tasmanian package, which must meet rigorous Department of Energy software standards. In the meantime, I am familiarizing myself with the high-performance computing environment at NC State, in addition to becoming as comfortable with C++ as I am with Python and MATLAB. Beyond next summer, I will very likely need to refine the gradient routines, and I will also continue my literature review on quantum chemistry and the analysis of sparse grids. Proposal: Broader Impacts Sparse grids as a general category have ready applications in any field where compu- tational cost is a major concern. In the realm of chemistry, computationally tractable methods of handling high-dimensional DFT-driven dynamical simulations have a wide ar- ray of benefits to society, including public health, renewable energy, and national security. The research I propose to undertake can advance the state of the art of simulations in pharmacological modeling, photochemistry, conversion in solar cells, nuclear chemistry, and nuclear power [3, 4], to name a few. Additionally, the gradient routines that I will add to the Tasmanian code will be useful to all who use the publicly available Tasmanian package to manage sparse grids in their research. Conclusion Sparse grids are powerful tools, making simulations that were once intractable become feasible. I possess the drive, prior computational laboratory experience, and proven aca- demic background to carry this project through to completion and to communicate my results to both specialist and non-specialist audiences. [1] Hohenberg,P.andKohn,W.“InhomogeneousElectronGas”.Phys. Rev.136.3B(1964),B864–B871. [2] Nance, J. and Kelley, C. T. “A Sparse Interpolation Algorithm for Dynamical Simulations in Com- putational Chemistry”. SIAM J. Sci. Comput. 37.5 (2015), S137–S156. [3] Nance,JamesD.“InvestigatingMolecularDynamicswithSparseGridSurrogateModels”.PhDthesis. North Carolina State University, 2015. [4] Peherstorfer,Benjaminetal.“SelectedRecentApplicationsofSparseGrids”.NumericalMathematics: Theory, Methods, & Applications 8.1 (2015), pp. 47–77. [5] Stoyanov, M. TASMANIAN Sparse Grids. Tech. rep. ORNL/TM-2015/596. ORNL, 2015.	Winner!
116	Improving the production of biofuels by understanding metabolic pathways in microalgae The objective of the proposed research is to quantitatively assess the metabolic capabilities of microalgae to identify inefficiencies that limit cell growth and lipid synthesis. The long term goal is to create a strain of microalgae that is maximized for biofuel production. Currently, plant biodiesel is the main source of biofuels. However, the demand for oil has outpaced the amount of biodiesel that can be produced in this manner. Microalgae are a viable alternative to plants due to their ability to produce up to 370 barrels of oil per hectare, which is more than 100 times greater than the oil produced through soybeans- the main crop for biofuel.1 Even so, efforts to enhance their biofuel- producing capability is still needed in order to realize their industrial viability. One of the biggest challenges with algae biofuels is the understanding of mechanisms that influence the production of triacylglycerol (TAG), a precursor to biodiesel. The objective of the proposed research is to develop a platform to identify reactions that limit TAG production in the central carbon Figure 1: Metabolic network for Pt metabolism of algae (Fig 1). The The colored boxes indicate different cellular compartments. microalgae, Phaeodactylum Dark yellow circles indicate the metabolites that can be tricornutum (Pt), is ideal for biofuel measured using GC-MS and LC-MS. While circles indicate production: 1) they efficiently fix metabolites that cannot be measured. Red dotted lines indicate atmospheric CO 2- responsible for that reactions exist between TP, ACA and TAG. absorbing at least 25% of the total amount of carbon dioxide processed by the seas, 45-50 billion tons of organic carbon 2 2) they accumulate up to 45% of their dry cell weight in TAG 3. In recent years, advances in genomic tools for Pt such as genome editing4 and stable delivery vectors5. allow us to further enhance the ability of Pt to produce TAG. However, the genes that correlate to these metabolic inefficiencies are currently unknown so gene alterations are conducted through educated guessing. Stable isotope tracers, such as 13C, are added to biological systems to track patterns of isotope incorporation into numerous products synthesized by the cell, including TAG. Coupling tracers with Isotopically Nonstationary Metabolic Flux Analysis (INST-MFA), the metabolic fluxes in central carbon metabolism as well as the transport rates across cellular compartments (Fig 1) can be determined. With this knowledge, reactions that divert carbon away from TAG can be identified and subsequently targeted for gene editing. Through this method, we will be able to create a strain of Pt that is optimized for TAG production. 1 Amy Zheng NSF Research Proposal The proposed work will be broken down into two major tasks: 1) Describing the central carbon metabolism of Pt using INSTA-MFA 2) Highlight metabolic targets for deletion and assess its effect on TAG production. Task 1: Developing flux map of central carbon metabolism in Pt The objective is to find which reactions contribute to TAG production using INSTA-MFA. INSTA-MFA combines of computational and experimental methods in order to describe the metabolism of an organism. This is due to the infeasibility of measuring all metabolites within an organism. Therefore, the metabolites that cannot be explicitly measured (white circles in Fig 1) must be interpolated using a model. In the computational portion, a model is compiled from literature and biochemical databases focusing on the central carbon metabolism. Central carbon metabolism the main focus of our model because these pathways contain the major carbon reactions. Carbon enters the organism as CO and is turned into biomass or TAG.6 TAG is synthesized from ACA and TP 2 (red dotted line Fig 1). We have created this model for Pt which includes all of the measurable metabolites and the major carbon cycles such as the Calvin, Tricarboxylic Acid Cycle and Pentose Phosphate Pathway (Fig 1). On the experimental side, metabolites are tracked by feeding the 13C tracer as sodium carbonate (yellow circles Fig 1), allowing us to track the pathway of the carbon through the organism. The metabolites containing the tracer in the organism will increase over time as it becomes incorporated into the major cycles. This pattern of incorporation over time is called the Metabolite Labeling Data (MLD). In INSTA-MFA, MLD is combined with the computational model to understand the labeling pattern of unmeasurable metabolites. The results gives us the fluxes through all of the reactions present in the model, also known as a flux map. Using the flux map, we can identify bottlenecks, which are genes corresponding to reactions that direct CO away from TAG.6 2 Task 2: Highlight metabolic targets for deletion and assess its effect on TAG production The objective of the second area is to produce the diatom with the maximum amount of TAG production by targeting the genes correlated to the bottlenecks. From our simulation created in Area 1, we can identify reactions that improve TAG production through gene alteration. However, there is a gap in knowledge on how these gene mutations affect the metabolism of the diatom. Using INSTA-MFA, we can predict which genes limit TAG production by looking for bottlenecks. Then, we can delete those genes with help from our collaborators at Colorado State University, who have developed methods for genome engineering in diatoms, to produce desired mutants. After the mutants have been synthesized, we can assess the changes in metabolism and identify additional bottlenecks by using INSTA-MFA again. Through this iterative process, we can create a strain of diatoms that produce the maximum amount of TAG. Broader Impacts: Within the scientific community, this project will help us understand how CO is incorporated into photosynthetic organisms. On an industrial scale, increasing the 2 efficiency of biofuel production in Pt will eliminate our reliance on fossil fuels. INST-MFA allows us to understand the metabolism of Pt by combining 13C isotope experiments and simulations to create a unique flux map. From this map, we can identify genes that can be mutated to increase TAG production. By understanding how metabolism within Pt functions, we can create a diatom that produces the maximum amount of TAG physically possible. Sources: [1] Chisti Biotechn Adv 25, 294-306 (2007). [2] Young et al.Metab Eng 13, 656-665 (2011). [3] Falkowski, et al. Aquatic Photosynthesis. 2nd edn, (Princeton University Press, 2007). [4] Hu et al. Plant J 54, 621- 639 (2008). [5] Weyman et.al Plant Biotechnol J. 13, 460-470 (2015) [6] Cheah et al. Systems Biology 25-70 Wiley (2017). 2	Winner!
117	I propose to develop new theoretical and computational methods for investigating activated chemical processes. Specifically, I will develop a method for calculating derivatives of rate constants, transport properties, and other dynamical timescales with respect to temperature (T), pressure (p), and chemical potential (µ). I will use these to calculate activation energies (E ) and activation volumes (∆V‡) of transport properties in CO -expanded liquids (CXLs). a 2 Motivation: There is a growing interest in green alternative solvents for use in catalytic reactions.1 Onesuchalternativemedia, CXLs, areofparticularinterestduetotheirincreased safety, cost-effectiveness, and transport properties when compared with traditional organic solvents.2 Using a CXL can give up to a five-fold reduction in the amount of solvent needed for a reaction compared to the neat solvent, while significantly increasing mass transport important for catalysis where reactions are often diffusion-limited. Diffusion in CXLs has generally been seen to be monotonic with changes in p, T; however a separate, computation- ally expensive, vapor-liquid coexistence simulation is currently required at each phase point (p,T) before transport calculations can be run with molecular dynamics (MD). Introduction: Instead, I propose a direct method by which the entire T-, p-, and µ- dependence of transport properties in CXLs may be evaluated from simulations at a single phase point. Traditionally, the T-dependence and activation energies of transport properties are calculated from a series of simulations at different temperatures and evaluating their Arrhenius behavior. While this is generally satisfactory, there are systems in which calcu- lations over large temperature ranges are difficult or inconvenient, as in the case of CXLs where small changes in T and p change the composition of the liquid phase. Preliminary Work: A general method has been devel- opedinpreviousworkbywhichtheT-dependenceoftrans- port properties and their activation energies can be ex- tracted from simulations at a single temperature. This is achieved by launching non-equilibrium MD trajecto- ries from different points along a single NVT trajectory. This was originally applied to the reactive flux correla- tion functions and was later generalized by our group to work for any rate constant, transport property, or dynam- ical timescale calculated from a time-correlation function Figure 1: The ratio of the energy- (TCF).3−4 This also allows for decomposition of E into weighted mean-squared displacement a (MSD (t)) to the mean-squared dis- kinetic and potential energy contributions, providing oth- H placement (MSD(t)) is presented in erwise unobtainable mechanistic insight into the E . a red and is equal to E at the long a,D With this method, the first derivative of transport time limit, presented in blue. properties with respect to state variables (e.g., T, p) can be calculated. This has been successfully applied previously to the diffusion coefficient (D) to calculate the E of diffusion of bulk water, as pictured in Figure 1. A typical Arrhenius a calculation finds E should be 3.5 kcal/mol while this direct method found a value of 3.48 a kcal/mol. The second derivative is also similarly calculable with respect to these variables. For a system with a monotonic dependence on T, and p, as in the case of CXLs, these derivatives and a single value of the transport property are all that is needed to determine its value at other state points. This greatly reduces the number of simulations necessary to evaluate the T-dependence and drastically cuts down the computational expense while also gaining additional insight into the decomposition of the E . a 1 Graduate Research Plan Ezekiel A. Piskulich Aim-1 Application to the NpT Ensemble: With the NpT ensemble, fluctuations occur in both energy and volume that allow for the calculation of both E and ∆V‡. Using a a similar derivation to our previously published works, we have been able to show that ∆V‡ = k T∂ln(D) = lim (cid:104)δV(0)[r(t)−r(0)]2(cid:105) . Experiments and simulations have previously D B ∂p t→long (cid:104)[r(t)−r(0)]2(cid:105) used an ”Arrhenius-like” plot of ln(D) vs p to calculate ∆V‡; however, these calculations require that measurements be taken across an enormous range of pressures (1- 10,000 bar) to distinguish a measurable change in D with respect to p. Yet in some cases ln(D) is not linear in p. This should allow us to probe this non-linearity which is generally inaccessible using traditional methods. Additionally, cross-derivatives, such as ∂Ea, can be calculated, ∂p allowing us to extract the p-dependence of E from simulations at a single p. a Aim-2 Application to the µVT ensemble: In the µVT ensemble, the first derivative of a transport coefficient or other dynamical timescale with respect to the chemical potential can be written as ∂CAB(t) = β(cid:104)δN(0)A(0)B(t)(cid:105) . In practice, this derivative can be calculated ∂µ µVT byrunninganNVT simulationinwhichasub-volumehasbeendefined, allowingthenumber of molecules in the sub-volume to fluctuate. This provides a means by which changes in rate constants, diffusion coefficients, or other dynamical timescales with respect to the chemical potential can be calculated without separate simulations at different compositions. Aim-3 Application to CXLs: I propose to apply this method to a variety of CXLs of industrial importance, including CO -expanded ethylene oxide (EO) and methanol (MeOH) 2 which are involved in a non-phosgene route for the industrial commodity chemical, dimethyl carbonate.6 The methods described in the previous aims will be used to probe the T-, p-, and µ-dependence of their transport properties and other relevant dynamical timescales from simulations at a single point. Ideally, this will allow for the calculation of the entire surface of diffusion coefficients, reorientation times, and other dynamic timescales for these CXLS without requiring more than a single phase coexistence simulation. Intellectual Merit: The methods proposed within this work are simple to apply, signif- icantly decrease computational costs, and provide deeper insight into the mechanisms of transport properties. Additionally, they provide a convenient means by which first (and higher-order) derivatives with respect to T, p, and µ may be calculated. This will allow for entire dependence of these properties on these macroscopic variables to be calculated without requiring expensive phase coexistence calculations at each phase point. Broader Impacts: The EO–CO and MeOH–CO system is a green alternative solvent 2 2 consideredfortheproductionofdimethylcarbonate, animportantprecursorinpolyurethane production and will be investigated in collaboration with chemical engineers at the Center for Environmentally Catalysis.6 I will also be co-organizing the 2019 Liquid Gordon Research Seminar for graduate and postdoctoral students. I will continue to mentor undergraduate students throughout the course of my studies. Furthermore, I will pursue the outreach program proposed in my personal statement. The methods proposed in this work can be extended to solve a wide variety of chemical problems. [1]P.AnastasandN.Eghbali. Chem. Soc. Rev.,39,pp. 301-312(2010). [2]P.Jessop,andB.Subramaniam Chem. Rev.,107,pp. 2666-2694(2007). [3]O.O.MeseleandW.H.Thompson. J.Chem. Phys.,145,134107 (2016). [4] Z.A. Piskulich, O.O. Mesele and W.H. Thompson. J. Chem. Phys., 147, 134103 (2017). [5] K. Krinicki, C. Green, and D. Sawyer. Faraday Discuss. Chem. Soc., 66, pp. 199-208 (1978). [6] Z.A. Piskulich, B.B. Laird and W.H. Thompson. Fluid Phase Equilib., Submitted, (2017). 2	Winner!
118	INTEGRATING CONNECTED PEDESTRIANS IN INTELLIGENT INTERSECTION CONTROL SYSTEMS (IICS) Keywords: Automated Vehicle, Pedestrians, V2X Connectivity INTRODUCTION Broader Impact: With approximately 1.3 million road traffic deaths worldwide, roadway safety is a major health problem that affects humans around the globe. Nine out of ten of those serious roadway crashes are due to human behavior. Emerging automated and connected vehicle technologies have potential to transform the transportation system by removing fatal human errors. Using a variety of sensors and terrain information, automated vehicles (AV) and connected vehicles (CV) will have the ability to communicate with infrastructure (V2I), surrounding vehicles (V2V), and all roadway users (V2X). While embracing technology, engineers must consider all road users; not just automobiles. Complete Street policies adopted across the country and require streets to be planned, designed, and maintained to enable access for users of all ages and modes of transportation. Advances in AV and CV technology must integrate all modes transportation to achieve a safer and more efficient transportation system. Intellectual Merit: AV technology removes the ability for pedestrians to visibly communicate (i.e. eye contact) with a driver. One possibility of keeping pedestrians connected, proposed in the project, is based on connected pedestrian detection. With a large portion of the US population in possession of a “smart” or connected device, pedestrians may have the ability to connect with the infrastructure and vehicles. Pedestrians would eventually be to indicate which road they wish to cross to get the right-of-way. AV and CV vehicles in possession of pedestrian data can make safe and informed decisions to improve efficiency at an intersection. Although it is not feasible to have 100% saturation of connected pedestrians, this project develops the framework for connecting pedestrians into an intelligent intersection control systems. RESEARCH PLAN The goal of my Ph.D. research is to optimize intersections with AVs, CVs, and pedestrians. With the resources and support from the University of Florida (UF), I will develop, test, deploy, and analyze a pedestrian-integrated intelligent intersection control systems (IICS). My research expands ideas from my current undergraduate collaborations with Dr. Lily Elefteriadou, director of the UF Transportation Institute, Dr. Ruth Steiner, an Associate Professor at the UF Department of Urban and Regional Planning. Broader Impact: During my research process, I will include undergraduates in my team, mentor engineering students, and present the project to multiple audiences. A diverse undergraduate research team will assist my project with accessible tasks, exposing them to the research setting, and facilitating a passion for transportation engineering by working with cutting-edge technologies. I will continue mentorship in established programs at the UF ASCE student chapter and College of Engineering. I will disseminate my work through multiple publications in Transportation Research Board and other scientific journals. I will present my work to undergraduates at their club meetings and research fairs. Practicing engineers will gain access to my work through my involvement in professional societies (ASCE) and start to integrate AV and CV technologies into their long-range plans. Intellectual Merit: Recent publications developing system control algorithms incorporating AV and CV technologies intelligent intersection control systems (IICS) assume a mixed-traffic of conventional, connected, and autonomous vehicles in undersaturated conditions [1,2]. With my Rebecca Kiriazes Graduate Research Proposal established connections at the UF, I can procure all necessary equipment including autonomous vehicle, detectors, IICS, and a testing location. OBJECTIVE 1: INTEGRATING PEDESTRAINS IN ALGORITHM (FIGURE 1) The central computer will receive each vehicles and pedestrian arrival information. An algorithm will compute the parameters of the AV or CV to minimize travel time delay while considering the estimated movement of the connected pedestrian and conventional vehicles. The optimized AV or CV parameters determine the intersection signalization and the AV or CV trajectory [1]. Challenges in developing the algorithm include predicting pedestrian movement with the presence of AV, CV, or Figure 1: Intelligent Intersection Control System (IICS) with conventional vehicles [3]. Connected Pedestrians OBJECTIVE 2: PEDESTRAIN AND IICS COMMUNICATION Previous research in smartphone-based detection and localization has been developed for visually impaired pedestrians [4]. Smartphone-based connectivity will be established in the IICS. OBJECTIVE 3: TESTING AND EVAULATION Full scale testing will take place to establish working connection between pedestrian, vehicles, and infrastructure [2]. Multiple scenarios with varying frequency and volumes will be tested and the results will be recorded and established performance measures will be analyzed. OBJECTIVE 4: ROADWAY SAFETY EVAULATION Additional testing will determine the intersection effectiveness of the perceived pedestrian safety. Response time and post-crossing participant survey will be analyzed to increase the accuracy of the IICS pedestrian prediction and inform future designs of vehicle-to-pedestrian communication [5]. From this testing, a series of pedestrian safety workshops will be presented to inform the public on safe interactions with AVs and CVs. CONCLUSION Broader Impact: This proposal strengthens the connection between transportation engineering and urban planning in development of emerging technologies. It is expected that results of the proposed project will facilitate the adoption of connected and autonomous vehicle technologies. Adopting these technologies will result in a safer and more efficient transportation system. Intellectual Merit: This project will encourage the academic community to investigate vehicle- to-pedestrian issues including non-connected pedestrians, data security, moral dilemmas, and the effect AV will have on mode choice. REFERENCES [1] Pourmehrab, M. et al. 2017 Unpublished. [2] Omidvar, A. et al. 2017. Unpublished. [3] Marisamynathan, S. et al. 2014. Journal of Traffic and Transportation Engineering. (103-110) [4] Murali, V. et al. 2013. IEEE Int Conf Multimed Expo Workshops. July 2013. (1–7) [5] Clamann, M. et al. 2017. Transportation Research Board. (AND10).	Winner!
119	Motivation: Humans are inherently good at cooperation in teams, and our ability to work together enables us to overcome challenges that a single individual would otherwise be unable to complete. The same can be said for autonomous robotic systems: for many applications a team of robots working together can complete a task more efficiently than an individual robot working alone. Another advantage of multi-robot systems is that the agents can be mechanically simpler than a single, general purpose agent required to complete the same set of tasks. Introducing heterogeneous robot types into the team allows specialized agents to focus on the tasks they are good at, and in many cases, increases the efficiency of the team. Specialization improves teams, but introduces a new challenge: if a team is confronted with a task that no members of the team have the expertise to complete, the team will fail. The current solution to this challenge is to create large teams with a high diversity of agents, but this solution is inefficient and leaves highly specialized agents in the team under-utilized. Humans solve this problem intuitively: if the team does not have a capable member then one is recruited from an outside source, and when that task is complete and that person's skill set is no longer required, they are released from the team. Consider a situation where a child is lost in a theme park and security for the park sends out a quadrotor-based search team to look for the child. The quadrotors may be able to fly over the park’s walkways and above open areas searching for the lost child, but might determine that the child may have entered an area that they are unable to investigate. In this case the search team should be able to recruit the help of other agents in the area, like concession service robots on the ground or park employees, to search the areas inaccessible to the quadrotors. Then when the search is over, the recruited agents can return to their previous assignments. Background: This question of dynamic team building is largely unexplored in robotics, but represents a necessary functionality as the diversity of robot systems increases. Previous research in this problem domain has focused on defining this challenge and presenting assessment metrics [1]. However, formal methods for coordination of these types of teams are limited. In 2017, I worked with researchers at Oregon State University on the design and testing of a novel distributed coordination and task planning algorithm for heterogeneous robot teams called Distributed Monte Carlo tree search (Dist-MCTS). By generalizing the tasks, reward functions, and agent abilities, Dist-MCTS remains agnostic to the type of agents in the team, enabling coordination of teams composed of agents with different abilities. Simulated trials showed that Dist-MCTS teams earned 47% more cumulative team reward than teams coordinated using a distributed auction-based approach. While it is effective at organizing complex teams, this algorithm does not allow for dynamic team formation. Research Proposal: I propose to explore the challenge of creating dynamically formed teams by modifying the Dist-MCTS high-level planner and implementing it in a larger framework for the dynamic formation of heterogeneous multi-robot teams. Two essential extensions must be made to the Dist-MCTS algorithm before it can be used for dynamic formation of teams. These focus on scalability and p olicy estimation. A primary restriction of the current Dist-MCTS algorithm is scalability. If too many agents are added to the team (20+) or the number of tasks increases beyond a certain threshold (100+), the planning space becomes too high-dimensional and the solution quality declines. To address the challenge of scalability, I will incorporate autonomous sub-teaming and task space segmentation into the existing algorithm. This adaptation reduces the complexity of the planning operations for all agents across the macro-team. Extensibility is a key requirement for applications to dynamically formed teams because the planning space has the potential to expand rapidly during complex missions. The second I will make to Dist-MCTS is the incorporation of an adaptive task selection model for inclusion of independent agents like humans or unknown robots in the team. Previous work [2] has demonstrated preliminary results for a method to estimate the policy of an independent agent like a human and use this to inform the actions of multi-agent teams. This is necessary because it cannot be guaranteed that the recruitable agents in the operating environment of a dynamically formed team will be able to communicate with the team. Policy estimation for these independent agents will allow the existing team to coordinate itself around agents without requiring direct communication and enable teams to be formed of robots that are not built by the same research group or manufacturer. Methods: I plan to develop a novel algorithm for coordination of dynamically formed teams in three phases: algorithm design, simulation testing and refinement, and h ardware validation. I. The first phase will focus on integrating the modified Dist-MCTS in a larger software framework and developing the components necessary to facilitate dynamic team formation. This will include creating a method for assessing newly discovered tasks and determining a functional set of protocols for recruiting new agents to the teams. II. With the new planner in development stages, I will test and refine the system with a modified version of the simulator used to assess the Dist-MCTS algorithm. A key point in this phase will be investigating how teams should recruit and release members as tasks are discovered and completed. III. With a refined result, I will apply this high-level planner to multi-robot teams in a real-world hardware trials in unknown, dynamic environments to assess its validity in application. For a complete test of this algorithm, these hardware trials will be focused on not only validating the team’s basic functionality, but in testing its applicability to complex multi-robot task domains like collective construction. Broader Impact: This proposal addresses a key challenge in the design and operation of collaborative multi-robot systems, and will provide a platform for a variety of other potential research topics including rapid deployment of heterogeneous teams, and optimization of agent structures in such teams. Dynamic formation of multi-robot teams will revolutionize robotics by increasing the versatility of the multi-agent teams and the complexity of possible missions. As advances in robotics continue and more robots become integrated in everyday life, these types of teams will become increasingly useful in a wide variety of application domains. One application for these types of teams could be in STEM education, where students learn about how robots interact with both the real world and each other through demonstrations involving these teams, allowing them to draw parallels between human teamwork and robot teamwork. [1] Jones, E. G., Browning, B., Dias, M. B., Argall, B., Veloso, M., & Stentz, A. (2006). Dynamically formed heterogeneous robot teams performing tightly-coordinated tasks. In Proceedings 2006 IEEE International Conference on Robotics and Automation, ICRA 2006 (Vol. 2006, pp. 570-575). [1641771] [2] L. Milliken and G. A. Hollinger, “Modeling user expertise for choosing levels of shared autonomy,” in Proc. Planning for Human-Robot Interaction Shared Autonomy and Collaborative Robotics Workshop, Robotics: Science and Systems Conference, 2016.	Winner!
120	Question Answering Alvin Wan Keywords: visual question answering, attention, computer vision I. Introduction II. Proposition Visual Question Answering (VQA) is a task To broaden the scope of information that fuses both computer vision and natural incorporated, I propose running object language processing, where a computer detection on the original image to extract all must answer questions about a provided identifiable entities; this can be achieved image. State-of-the-art approaches leverage with a pretrained model, such as YOLO9000. attention networks in deep learning, which The set of attributes obtained from help to focus the neural network on portions captioning the image as well as describing of the image. In particular, these approaches these objects could then be fed into external see improved performance when applying information queries. attention to both the question and the image Provided that attention has improved 1. Further improvements are supported by performance when applied to questions and use of external knowledge bases. images alike, coupled with the fact that leveraging external information likewise Related Work improves performance, I propose applying attention to external information as well. Attention networks can currently leverage Once performance has been verified, I then external information by captioning the treat previous background in the sequence of image, extracting a set of attributes, and questions, as external information. This then supplanting the final answer-generator with allows the VQA bot to utilize previous summaries of facts related to those context. attributes. However, what if the provided In preparation for real-time inference and caption already ignores important context in memory constraints, we will additionally the image? A caption may ignore a beach consider size and space-efficient methods for ball in the background, but a pointed accomplishing these improvements, so that question may not. What’s more--what if the bot is able to build conversation. additional context in the conversation is needed? The user may ask for “the number Evaluation of people wearing jeans” and “of those ​ people, the number of people wearing red A number of public benchmarks provide ​ shirts”. I propose broadening the scope of adequate testbeds for both more higher-level information incorporated, when querying for processing and more rote, simple questions. external information, and applying attention The COCO-QA and DAQUAR benchmarks to all the data retrieved, to build more would serve this purpose, additionally stateful, conversational VQA bots. providing comparison with existing state-of-the-art methods. Other benchmarks that address subsets of the VQA task, such as image captioning, exist for intermediate evaluation. 1 Lu et. al. “Hierarchical Question-Image Co-Attention for Visual Question Answering“ (2016) III. Research Plan architecture and information retrieval both will be optimized by minimizing Year I: Question Attention, Broadened fully-connected layers, using convolutional Information Retrieval layer reductions, and rigorous memory profiling. Attention networks should be Objective: Test a number of varying trained and validated as part of an online implementations for raw information-based learning algorithm. attention. Methodology: Employ object detection and IV. Conclusion image tagging to increase the quantity of information queries. We can use both Intellectual Merit - Should this work ​ shallow and deep featurizations to produce a succeed in proving an effective method of rich set of data descriptors. Then, apply information distillation for incorporation, attention to external sources to determine visual question answering as a field could both quality and relevance of external take several steps forward in studying not information--we can begin by modeling just computer vision for images, or natural relevance as a binary classification problem, language processing for questions, but then by regressing to continuous-valued distributed systems for large-scale relevance. information retrieval and processing. Year II: Stateful Visual Question Broader Impacts - One application of an ​ Answering improved bot, with enhanced visual question answering capabilities, is personal assistants Objective: Utilize previous conversation for the visually-impaired. Considering context to aid in question answering. computer-aided descriptions are substitutes Methodology: Use attention networks to for the user’s vision, the quality of visual determine what types of external information question answering is highly-valued. are needed, whether the information However, the challenge is more complex pertains to objects, an intangible quality, or than simply answering a number of isolated previous conversation context. Attention questions; the bot must be able to leverage networks should successfully discern the conversation context itself i.e., previous relevance of samples collected prior to questions, the user’s behavioral tendencies, learning. This work may extend to year and finally, external information that may three, as various modalities of external make answers more accurate--for example, information may require significantly more if a price tag is shown but the store-wide data processing and filtering. discount is announced online. Thus, with more refined methods of incorporating Year III: Towards more Conversational outside data, the bot could see major Bots improvements in its value for the visually-impaired. Objective: Run such evaluation and answering in real-time. Methodology: This interest directly stems from my current work in reduced model sizes and real-time inference for self-driving cars. In a similar manner, we cannot always rely on network connections for high-level, fast-paced conversation. Neural network	Winner!
121	"Energy Agency estimates that by 2025, 70 million electric vehicles (EVs) will be on roads worldwide. As a result, research on increasing efficiency, reliability, and practicality of EVs and associated systems will be of increasing importance. Described herein is a wireless power transfer (WPT) approach for the dynamic charging of EVs. The goal is to address two challenges; reduction of component stress during high-power, high-frequency operation and electromagnetic field containment. Literature Review: Dynamic charging, also known as roadway charging, consists of embedded roadway coils that transfer power to receiving coils on vehicles in motion (see figure) and has been demonstrated to increase the range of EVs. However, power transfer levels of 50-100kW are required to maintain the EV battery state of charge while the vehicle is in motion [2]. Another challenge is the health implications of electromagnetic fields emanating from the coils. The Society of Automotive Engineers (SAE) J2954 standard sets limits on stray electromagnetic fields [3]. Therefore, their reduction must be addressed if dynamic charging is to achieve mass adoption. Objectives: To develop a high efficiency, high power multi-coil WPT system that will allow for localized electromagnetic field production between source and receiving coils. Hypothesis: A reflexive WPT system comprised of: coils with embedded capacitors, saturable inductors and custom magnetic geometries, will enhance field containment capability and allow for efficient, high power dynamic EV charging with minimal electromagnetic field emissions. Research Plan: Stage I-Initial Investigations: A WPT testbed based on a reflexive field containment compensation approach [4] will be fabricated. This topology achieves field containment by exploiting the receiving coil’s reflected capacitive reactance to neutralize the source coil’s inductive reactance. The source coil’s current is thus attenuated when the coils are uncoupled, thereby reducing emanated fields. Based on my recent findings [5], saturable inductors will be used to enhance the field confinement effect. The goal of this stage is to achieve a power transfer of 10kW at an efficiency above 90% while maintaining a 30-fold field attenuation factor between uncoupled and coupled conditions. Stage II-Coil Embedded Capacitors: At power transfer levels of 50-100kW, the high voltages at the terminals of the inductor coils stress the resonant tank capacitors making the SAE J2954 mandated high frequency (~80KHz) operation challenging to implement. Previous studies have demonstrated this power transfer level only at lower frequencies [6]. Self resonant coils have been shown to eliminate the need for capacitors [7], however these are limited to use in MHz level resonant frequencies and lower powers. To address this, coil enclosures will be designed and built with polyethylene dielectrics spacers that allow for embedding large parasitic capacitances to reduce the voltage stress on the external resonant tank capacitors and allow for efficient high frequency operation. Stage III-Additive Manufacturing Based Magnetic Structures: Custom magnetic structures will be implemented in the coils to allow for optimum channeling of flux during operation to further reduce stray fields and improve coupling. This will be preceded by extensive finite element analysis of the desired magnetic structure geometry. The magnetic structures will be fabricated using an additive manufacturing process recently demonstrated at Oak Ridge National Laboratory [8]. A supplier for the required magnetic nano-alloy powder (FeNbSiCuB) and polyphenylene sulfide mixture has already been identified. Stage IV-Optimization: This stage will focus on the challenge of optimizing system performance at ~100kW for high efficiency operation, conformance to SAE standards and EV testing. Soft switching will be employed on the Silicon Carbide based inverter while processing minimal reactive power to improve efficiency. The test EV will be modified to carry the receiving coil and necessary power electronics to interface with its rechargeable batteries. Based on the SAE standards, shielding for the receiving coil will be designed to prevent fields from penetrating the EV cabin. An array of segmented source coils will be embedded on equal intervals of test track. As the EV drives along the track, the power transfer and field containment capability of the system will be characterized and compared to simulation figures. Timeline and Collaboration: The proposed duration of the project is three years and will be performed under the supervision of Dr. Srdjan Lukic at North Carolina State University’s (NCSU) NSF funded Future Renewable Electric Energy Delivery and Management Engineering Research Center (FREEDM ERC) in collaboration with Dr. Tim Horn of the Center for Additive Manufacturing and Logistics (CAMAL). Being one of the nation’s leading centers for additive manufacturing, CAMAL’s facilities have specialized equipment capable of fabricating magnetic structures from magnetic nano-alloy powders. The systems level integration will be performed on the EcoPRT EV mentioned in the personal statement. Anticipated Results: Through the use of saturable inductors, custom magnetic structures, and coil embedded capacitors, high frequency power transfer at 50-100kW power levels with high field attenuation factor between uncoupled and coupled conditions will be demonstrated. Noting that a WPT system is a transformer with a large air gap, this project strives to meet a theoretical system efficiency of 98+%, or comparable to plug-in charging. Intellectual Merit: This research plan focuses on developing novel WPT systems that enable dynamic charging and, more generally, on pushing the boundaries of high power WPT by developing better models of complex coil designs with integrated passives. Additive manufacturing will allow for novel magnetic structures that are optimized for specific applications, and this methodology can be utilized in other high power high frequency applications. Broader Impacts: The mass proliferation of EVs is essential in addressing both climate change and the dependence on fossil fuels. Through the implementation of WPT systems for EV roadway use, the proposed project will assist in making feasible a system that will allow for the efficient and reliable increase in range and thereby practicality of EVs. I will regularly present findings from this research effort at scientific conferences and high profile journals. With NCSU being located in the Research Triangle area, I will share findings with industry to facilitate industrial proliferation of the technology. (1) Clean Energy Ministerial, Electric Vehicle Initiative & International Energy Agency (2017). Global EV Outlook 2017. (2) Z. Pantic, et. al., ""Inductively coupled power transfer for continuously powered electric vehicles,"" 2009 IEEE Vehicle Power and Propulsion Conference, pp. 1271-1278. (3) Wireless Power Transfer for Light Duty Plug- In Electric Vehicles and Alignment Methodology, no. SAE Standard J2954, 2016 (4) K. Lee, et. al., ""Reflexive Field Containment in Dynamic Inductive Power Transfer Systems,"" in IEEE Transactions on Power Electronics, vol. 29, no. 9, pp. 4592-4602, Sept. 2014. (5) A. Dayerizadeh, et. al., ""Saturable Inductors for Superior Reflexive Field Containment in Inductive Power Transfer Systems,"" 2018 IEEE Applied Power Electronics Conference and Exposition (APEC), Accepted. (6) G. Jung et al., ""High efficient Inductive Power Supply and Pickup system for On- Line Electric Bus,"" 2012 IEEE International Electric Vehicle Conference, Greenville, SC, 2012, pp. 1-5. (7) A. Kurs, et. al., “Wireless power transfer via strongly coupled magnetic resonances,” Science, vol. 317, no. 5834, pp. 83–86, Jul. 2007. (8) U.S. Department of Energy (2015). Electric Drive Technologies: 2015 Annual Report."	Winner!
122	"Today almost 2 million people in the U.S. face functional impairment due to limb loss and amputations with more than 185,000 amputations occurring every year. Neural prostheses are devices that aim to return full functionality to patients through brain-machine interfaces (BMIs). Emerging neural prostheses decode the user’s intention from neural signals recorded directly from the brain and create a closed-loop sensory feedback system through neural stimulation as shown in figure 1. The primary components of this technology are the neural signal interpretation algorithms and the neuromodulators (implanted neural recording and stimulation hardware). Recent advances in neural decoding techniques indicate the achievable high accuracy of BMIs. Previous work by Dr. Rajesh Rao demonstrates the viability of an unsupervised hierarchical k-means clustering algorithm to predict human behavior from brain recordings [2]. Figure 1: Closed-loop Feedback System [1] Advances in neuromodulators include recent work by Dr. Jan Rabaey on OMNI a wireless, low- power modular and distributed closed-loop neuromodulation device for chronic use [3]. Proximity of neuromodulators to neural tissue results in stringent power restrictions; any neural decoding algorithms must occur on external machines. Neuromorphic computing systems, hardware designed to function like biological neurons, are highly capable of enhancing brain- machine interface functionality. Classification algorithms on neuromorphic processors use 2 or more orders of magnitude less energy than on existing digital hardware. Implementing a neuromorphic processor in neural prostheses has the potential to improve BMI power consumption and mobility by eliminating data transmission and external hardware. A need for the consolidation of these 3 components - neural decoding, neuromorphic computing and neuromodulation - in hardware for BMI implementation has been clearly identified [4]. Proposition Throughout my undergraduate degree, I have conducted research on neuromodulators’ hardware- software interface, gained experience in neural signal processing and learning algorithms (such as SVMs, PCA and hyperdimensional computing), and worked on and gained an understanding of neuromorphic computing for adaptive learning hardware. I had the experience of working with three distinguished professors - Dr. Jan Rabaey, Dr. Rajesh Rao and Dr. Hugh Barnaby - on these distinct yet complementary areas. If I have the honor of receiving the NSF GRFP, I propose to fuse my knowledge in each to implement complex adaptive learning algorithms on neuromorphic hardware integrated with a neural recording and stimulating system. Research Plan Year 1: Evaluation and integration of neuromorphic hardware models with neural recording and stimulation devices. Cutting-edge neuromorphic chips such as, but not limited to, Intel’s Loihi, IBM’s True North, and the commercially-available Intel Curie module (if unable to obtain the former two) will be evaluated on power consumption, learning capabilities, applicability to neural decoding and accessibility. Consequently, an interface between the neuromorphic processor and the neuromodulation system including data transmission to and from each module will be designed. Year 2: Implementation of adaptive learning algorithm on neuromorphic hardware for neural signal decoding/interpretation. Various filters and learning algorithms will be evaluated including, but not limited to, k-means clustering, support vector machines, and spiking neural networks given potential performance on neural data and constraints of the selected processor. This involves research, design, implementation and testing of the algorithms on the neuromorphic processor to determine functionality and competence. Year 3: Testing, validation, improvement and finally study of system to advance knowledge of possibilities/limitations of neuromorphic applications in neural engineering. This process will involve testing of the system as a whole, ensuring data transmission between the neuromodulator and neuromorphic processor and functionality of the algorithm on the processor. Once the system is finalized, studies will be conducted to determine accuracy, speed, and power consumption of the system signifying operability in brain tissue. Further study: Research beyond the initial 3 years will include 2 years of in vivo studies to test and validate the system’s recognition of motor intention from neural data in animals. Facilities To conduct this study, I will work with Dr. Jan Rabaey at UC Berkeley. He has expressed considerable interest in working with me if I receive the NSF GRFP. I will integrate the neuromorphic hardware model with his group’s previous work on OMNI, the neuromodulation device. The neuromodulator is the primary component of my proposal on which the neuromorphic processor and learning algorithm are built. Working with Dr. Rabaey in UC Berkeley and continued mentorship from Dr. Barnaby and Dr. Rao on the neural decoding and neuromorphic modules will provide the resources I need to meet the goals of this proposal. Intellectual Merit and Broader Impacts Previous work claims that a neuromorphic neural interface will eliminate the need for external machines and significantly reduce power consumption enabling the possibility of a fully- implanted system. Potential for success has been demonstrated through simulations and modeling but the lack of a direct hardware implementation limits understanding of the true feasibility and impediments of utilization of neuromorphic processors in a closed-loop feedback system particularly regarding accuracy, speed and power consumption. This project will develop enabling technology that will advance the knowledge of the capabilities and limitations of neuromorphic applications in neural engineering. The work produced from this project will be presented at national and international conferences. Medically, the highly competent neural interface this research addresses has the potential to change lives through applications in bypassing spinal cord injury, deep-brain stimulation, and engineering plasticity for neurorehabilitation. Socially, this interdisciplinary project promotes collaboration between academic institutions. My experience with TYE taught me that the drivers of interest in engineering are role models and incredible technology. When I talk to middle and high school girls about my research - mind-controlled prosthetics - their eyes light up with possibilities. Throughout my graduate career, I will continue mentoring young girls to pursue engineering and will use this research to spark excitement for engineering in young minds. [1] S. J. Bensmaia et al., “Restoring sensorimotor function through intracortical interfaces: progress and looming challenges,” Nature Reviews Neuroscience, vol. 15, no. 5, pp. 313–325, 2014. [2] N. X. R. Wang, R. P. N. Rao et al., “Unsupervised Decoding of Long-Term, Naturalistic Human Neural Recordings with Automated Video and Audio Annotations,” Frontiers in Human Neuroscience, vol. 10, 2016. [3] A. Moin, J. Rabaey et al., ""Powering and communication for OMNI: A distributed and modular closed-loop neuromodulation device"", 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2016. [4] F. Broccard et al., “Neuromorphic neural interfaces: from neurophysiological inspiration to biohybrid coupling with nervous systems,” Journal of Neural Engineering, vol. 14, no. 4, p. 041002, Feb. 2017."	Winner!
123	impending possibility of “The Big One”—a M8 or greater earthquake along the San Andreas Fault System (SAFS) in California. While strike-slip systems exist across the world, the SAFS is unique in that it terminates at the Mendocino Triple Junction (MTJ), which is the only known example of a modern FFT system—a tectonic triple point bounded by two transform (F) boundaries and a subducting trench (T)[1]. The MTJ formed at approximately 30 Ma when the Pacific-Farallon ridge system was subducted beneath the North American Pacific Trench, creating the northward migrating triple point, and changing the tectonic regime of the plate boundary from a convergent style to a current strike slip style at the latitude of the SAFS.[1]. This change has had drastic effects on the Pacific Coast ever since, resulting in high levels of deformation and seismic activity. The project proposed herein seeks to better understand this geologically complex and socially meaningful system, by aiming to identify and characterize the first recognized ancient equivalent of the MTJ at the southwestern extent of the Paleozoic-era Norumbega Fault System (NFS) of Maine. My PhD work will investigate a ridge-subduction model for the Norumbega Triple Junction (NTJ) put forth by Kuiper 2016[2] by 1) utilizing field and microstructural analysis to evaluate whether dextral faults that curve in a direction opposite to Riedel faults in the NFS are equivalent to stepover faults in the SAFS and 2) testing whether these faults are progressively younger to the southwest, indicating a southwestward migration of the NTJ, analogous to the modern northward migration of the MTJ. If the southern terminus of the NFS is an appropriate type locality for the FFT style triple junction, then the region may provide insight into the evolution and behavior of strike-slip tectonics in the SAFS and around the world. The identification of such an analogue would be particularly significant given the strong outcrop exposure of structural features long the Maine coast in contrast to the more veiled features of the modern Mendocino Triple Junction. Regardless of model success, this work will provide a furthered paleogeographic model for the evolution of New England—a complex and meaningful region in its own right. Context Along its ~300 km extent from SW New Brunswick to S Maine, the mid-Paleozoic NFS is a NE-trending right-lateral transpressive system that parallels the Appalachians[3]. The NFS terminates to the SW, along high grade metamorphic rocks and migmatites associated with the Nashoba terrane of Eastern Massachusetts[4]. The youngest age of partial melting in the Nashoba (~360 to 380 Ma) is coincident with ages of dextral shear in southern parts of the NFS obtained by Ar/Ar age-dating[5,3]. These coeval ages suggest that while dextral movement occurred along the NFS in Maine, convergent style tectonics were still taking place in the nearby Nashoba terrane, indicating a triple point existed between the two. Furthermore, work done by Gentry et al., 2016 illustrates a lack of dextral NE-trending subvertical lineaments, shear zones, and faults in Massachusetts. These observations suggest an abrupt southern termination of the NFS, not unlike the modern MTJ placement against the northern Cascade arc volcanoes in the SAFS[6,7]. Hypotheses Splay-shaped faults in the NFS and SAFS play a key role in testing the Kuiper 2016 model. These structures look like expected Riedel-shaped faults, but curve in a direction opposite to the expected Riedel orientation [2]. In the SAFS, these features are known to be associated with dextral activity, and are possibly either slip-transfer faults from the SAFS to the Mendocino Triple Junction or linkage faults between various strands of the SAFS[8]. If the subducted ridge model is correct, the Norumbega Triple Junction would have moved SW, mirroring the current northward migration of the MTJ. If this is the case, the splay-shaped dextral faults in the NFS should be younger to the south, paralleling the age pattern inferred from the slip-transfer and linkage faults in the northern SAFS[8]. Alternatively, such features in the NFS could be original sinistral (Riedel- expected-orientation) structures which were subsequently reactivated, resulting in dextral overprinting. This alternative will be investigated alongside the work proposed herein. Methods and Work Plan Beginning in 2018, I will add to my background of coursework in high temperature geochemistry, structure, and geodynamics through graduate-level work in microstructure, kinematics, and geochemistry. During this time, I will identify optimal sites for mapping based on high quality LIDAR imagery in collaboration with previous workers and the Maine Geological Survey. Over the next two summers, I will mentor an undergraduate field assistant and conduct detailed structural mapping at sites across southern Maine, with the goal of identifying pre-NFS convergent folds displaying overprinted dextral shear indicators such as S-C fabrics, shear bands, and stretching lineations, all of which indicate the formation of the Norumbega Triple Junction.[9] Following sample acquisition, Backscattered Electron Imaging (BSE) analysis will be utilized to reveal spatial relationships between mineral assemblages and deformation fabrics sampled along the NFS (extending from the north towards hypothesized younger southwestern extents). This will allow for the selection of mineral candidates for U-Th- Pb age-dating. Analyses will be conducted using either an electron microprobe (EMP), or a laser ablation system coupled to a high-resolution, single collector inductively coupled plasma mass spectrometer (LA-ICPMS). Given that the EMP provides only an elemental age and has a smaller spot size than ICPMS, the exact methodology will depend on the size of the domains chosen for analysis. Both methods have been shown to display strong enough precision to resolve geological events in the study region (1-3% for the EMP and <3% for LA-ICPMS).[10] Results from each of the two field seasons will be synthesized and used to inform a workplan for future field and analytical work. Final results will take the form of my PhD dissertation and associated peer reviewed publications. Results will also drive the outreach campaign outlined in my Personal Statement. While this project is ambitious, my prior educational, professional, and research experiences pair with the resources available at Colorado School of Mines and elsewhere to ensure that I have the skillset, perspective, and support necessary to succeed. Broad Impact The affirmation of a ridge-subduction model for the NFS not only has the potential to establish a type locality for the enigmatic FFT style triple junction, but moreover would inform scientists seeking to decipher the kinematics of the currently active MTJ by allowing for interpretations from well exposed outcrops along the NFS to be applied to the SAFS—extending our base of knowledge about an active and dangerous natural system. Even if the model cannot be confirmed, the completion of an in depth structural history of the region will enable a stronger understanding of the mid-crustal dynamics associated with the evolution of the New England Appalachians. Finally, the completion and communication of this work will not only be scientifically relevant, but will enable me to build upon the goals outlined in my personal statement by establishing a career that will impact public discourse and geoscience education. References[1] Furlong, K.P., and Schwartz, S.Y., 2004, Annual Review of Earth and Planetary Sciences. v. 32. [2] Kuiper, Y.D., 2016. Geology, v. 44. [3] West, D.P., 1999, Geological Society of America Special Paper, v. 331. [4] Goldsmith, R., 1991, United States Geological Survey, Professional Paper 1366 E–J. [5] Buchanan et al., 2014, Geological Society of America, Abstracts with Programs, Vol. 46, No. 2. [6] Gentry et al., 2016, Geological Society of America, Abstracts with Programs, Vol. 48, No. 7.[7] Nicholson et al., 1994, Geology, v. 22. [8] Wakabayashi et al. 2007, Tectonophysics, v. 392. 9] Swanson, M.T.,1999, Geological Society of America Special Paper, v. 331. [10] Neymark et al., 2016, Economic Geology, v. 111.	Winner!
124	coral and is a crucial reef-building coral. Due to mass mortality from white band disease (WBD) in the Caribbean, however, A. cervicornis is currently designated as critically endangered by the IUCN Red List of Threatened Species. While the etiological agent of this disease is unknown, my lab recently discovered that a bacterium associated with the disease is strongly stimulated by nutrient pollution. In early 2017, I assembled the genome of this obligate intracellular parasite of A. cervicornis and, based on its phylogenetic position and genome content, I hypothesize that it is responsible for WBD and the destruction of Caribbean Acropora. Yet we do not yet know its mechanisms of transmission and disease development (pathogenesis) and further experiments are needed to confirm its role as the agent of disease. Using a combination of comparative genomics and field experiments, I aim to evaluate the gene expression, biogeography, and evolution of this bacterium to help prevent and manage this disease on coral reefs in the future. Background: WBD has been observed in the Caribbean, Red Sea, and the Pacific, but its mechanisms of pathogenicity are uncharacterized. Transmission experiments suggest that WBD is caused by bacteria,1 with species of Rickettsiales implicated as possible etiological agents,2,3 yet these taxa are present in both healthy and diseased coral microbiomes. Recent studies4,5 led by my advisor at Oregon State, Dr. Rebecca Vega Thurber, indicated that exposing corals to nitrogen stimulates the growth of an intracellular bacterium (order Rickettsiales). A strong negative correlation was found between the abundance of this taxon and coral growth (r2=0.9987, p<0.001) in the presence of nutrients.5 In A. cervicornis, this taxon increased from <11% of the microbial community to ~88% after 8 weeks of enrichment. In an unpublished study, tissue homogenates were generated from diseased and healthy A. cervicornis. The diseased homogenate, which caused a sixfold increase in mortality of exposed corals, was found to have a relative abundance of >50% Rickettsiales, while these species comprised only 0.1% of the healthy homogenate. While known pathogens in the genus Rickettsia clustered together in a 16S rRNA phylogenetic tree, I discovered that the intracellular parasite of A. cervicornis clustered most closely with uncultured symbionts of marine invertebrates, primarily corals and sponges.6 Based on strong statistical support for the distinction of these intracellular symbionts of marine invertebrates from other Rickettsiales, I proposed a new genus, Marinoinvertebrata gen. nov., in a publication in preparation for the ISME Journal.6 I named the newly-discovered parasite M. rohwerii sp. nov. after the lab that first observed it.3 Given their abundance in corals exhibiting reduced growth and signs of WBD, I hypothesize that: (1) Marinoinvertebrata spp. are members of the healthy coral microbiome but have evolved mechanisms of pathogenesis (encoded by virulence genes) that are modulated by environmental factors. (2) The increased expression of these genes due to nutrient exposure, in tandem with weakened host immune function caused by pollution, leads to coral disease. I will address these hypotheses through two central questions: Q1: How do mechanisms of pathogenesis and environmental response differ in species of Marinoinvertebrata from distinct regions? (Aim 1) I will compare the genome of M. rohwerii to genomes of disease-causing Rickettsia to identify homologs to virulence genes. I will also compare the pathogenicity of this organism with that of related species from different regions. Q2: How does concurrent exposure to nutrients and infection by Marinoinvertebrata spp. alter host physiology and induce disease? (Aim 2) I will conduct nutrient enrichment experiments on six genotypes of A. cervicornis to induce growth of Marinoinvertebrata spp. I will track parasite abundance using quantitative PCR as well as changes in microbiome composition, host/pathogen gene expression, growth rate, and disease progression. Lastly, I will use nanoscale secondary ion mass spectrometry (NanoSIMS) to trace parasite nutrient assimilation. Research Plan: Aim 1: Via the KAAS server,7 I will use reference genomes of pathogenic Rickettsia to guide my search for homologous virulence genes in the assembled genome of M. rohwerii. Using this method, I previously uncovered a complete Type IV secretion system in this genome, which is involved with host infection and genetic exchange in related bacteria8, as well as the NtrY-NtrX two-component system involved in sensing extracellular nitrate levels. However, additional virulence and environmental response genes present in Rickettsia may have more distant homologs in Marinoinvertebrata spp. As part of the Global Coral Microbiome Project and Tara Pacific, I have access to microbial community data from hundreds of Caribbean and Indo-Pacific coral samples and have identified species of Marinoinvertebrata in samples from Australia, Saudi Arabia, and Colombia. I will assemble the genomes of these species and determine whether they also possess virulence genes, and whether these are modulated by environmental-sensing genes. Aim 2: I will expose six different genotypes of A. cervicornis (raised in nurseries at Mote Marine Lab (MML)) to various levels of inorganic nitrogen to stimulate Rickettsiales proliferation as shown previously.4,5 Three of these genotypes are sensitive to WBD and three are resistant, as determined by Dr. Erinn Muller, who will be my host at MML. From the genome of M. rohwerii, I will generate quantitative PCR markers to track its absolute abundance throughout the enrichment experiments. Coral fragments will be sampled weekly with the help of students from MML’s after- school program, and high-throughput RNA sequencing will be used to assess Rickettsiales and host gene expression. I will track coral health through growth, photosynthesis, and respiration rates and changes to the host microbiome using 16S rRNA amplicon analysis. Finally, I will spend the summer of 2019 working with Dr. Xavier Mayali of Lawrence Livermore National Laboratory, using NanoSIMS to isotopically trace whether M. rohwerii scavenges nutrients from the host, from symbiotic algae, or from the environment. Intellectual Merit: Our ability to directly alter host-microbe interactions using nutrient enrichment provides a reliable model to ascertain which genes play a role in disease initiation and host response. Beyond its implications for coral disease, the opportunity to reconstruct the genome of a novel pathogen is rare and may uncover new mechanisms of transmission, especially when this pathogen is traced throughout different regions and coral hosts. As coral reef fish are crucial to the economy of many tropical regions, it is critical to combat the rapid destruction of their habitat by disease. The PCR primers I develop for this experiment can be used to quantify disease progression and track the spread of M. rohwerii, and our understanding of the host and pathogen transcriptomes will contribute to further studies on antibiotic treatment of WBD. Lastly, as MML’s coral nurseries were damaged by Hurricane Irma, my research will inform recovery efforts as genotypes most resistant to Marinoinvertebrata infection will be used in new nurseries. Broader impacts: Given my extensive advising and leadership experience, the robust educational programs already established by MML will provide an excellent framework to involve the local community in my research. Through MML’s Research-Based After-School Program for Students, I will work directly with high school students passionate about ocean conservation to introduce them to lab- and field-based research by helping them develop short-term experiments on how nutrient dose effects parasite growth. Dr. Muller and I will lead volunteer teams to propagate new nurseries and learn about the effects of coral disease. These teams will “adopt” their own corals to observe over time, increasing their personal investment in the reef. I will also work with the Oregon Coast Aquarium as a Scientific Interpreter to lead demonstrations showing how nutrient pollution negatively effects the health of all marine organisms, not just coral systems. Citations: [1] Gignoux-Wolfsohn et al. (2012) Sci. Rep. [2] Miller et al. (2014) PeerJ. [3] Casas et al. (2004) Environ. Microbiol. [4] Zaneveld et al. (2016) Nat. Commun. [5] Shaver et al. (2017) Ecology [6] Klinges et al. (in prep.) ISME J. [7] Moriya et al. (2007) Nucleic Acids Res. [8] Cascales et al. (2003) Nat. Rev. Micro.	Winner!
125	Introduction: Organic (polymeric) semiconductors (OSCs) are readily processible, leading to numerous advantages over traditional inorganic semiconductors.[1] These advantages include the ability to be fine-tuned for properties such as solubility, thermal processing, and optoelectronic properties, which allows OSCs to be leveraged for a variety of applications.[2] Through this molecular design, the charge carrier mobility of OSCs has been improved over the past three decades by seven orders of magnitude.[2] OSCs have been targeted for use in many future technologies, such as organic thermoelectric devices and organic solar cells, which can be used as alternative, green energy sources. One challenge in realizing this potential is the inherent structural and electronic disorder in polymers used as OSCs. In analogue to inorganic semiconductors, OSCs can be doped to increase electron or hole conductivity. These dopants take the form of small acceptor/donor molecules, which can be introduced into the polymer matrix a variety of ways. These methods include mixing the polymer and dopant in solution (solution doping), immersing or casting the dopant solution on top of the previously-cast polymer film (sequential doping), and subliming the dopant into the previously-cast polymer film (vapor doping). Although solution doping can be utilized for scale- up procedures such as roll-to-roll processing, solution doping leads to polymer aggregation and eventual precipitation, which leads to poor quality films. This causes a lower conductivity than as sequential and vapor doping, which have better local and long range ordering.[3] Despite the progress in doping methods, the dynamics for molecular doping into polymer films are not well known. This is due to the complexity of the charge transfer mechanism of molecular dopants in OSCs. In consequence, I propose to develop an easy-to-use platform to study the dynamics of OSC doping in situ, which will reveal both fundamental doping mechanisms as well as efficient and effective doping methods. The Institute for Molecular Engineering (IME) at The University of Chicago is a vibrant place for collaboration, and in conjunction with the Rowan group (synthetic) developing novel polymers for OSCs and the de Pablo group (computational) modelling advanced doping mechanisms, this platform would provide a method for probing the dynamics and optimization of a polymer/dopant pairing. Further understanding on the dynamics of OSC doping will inform further fine-tuning of OSCs and their development for use in novel, alternative-energy sources. Preliminary Results: In order to develop a platform for studying the dynamics of doping, I have used a model polymer-dopant pairing of Poly(3-hexylthiophene) (P3HT) and 2,3,5,6- Tetrafluoro-7,7,8,8-tetracyanoquinodimethane (F4TCNQ). P3HT has been widely studied as a conductive polymer due to its processability and ability to form an ordered semi-crystalline morphology. The conductivity of doped P3HT has been shown to increase six orders of magnitude from pristine to highly doped states.[5] Our custom-built chamber allows for measurement of in situ conductivity response to doping (Fig. 1), which is crucial for this platform. I have performed ) m c /S ( y t iv it 112 ... 050 )m c/S( ytivitcudnoC 111111 000000 ------ 1654321 00 R R Ru u un n n 1 2 3 Dopi1 n0 g1 time (s) 102 R RR u uu n nn 2 31 F c o P s l oao n 3i gmg n H -u d p lT our lee g-tc h F s t i r .4o pe T 2 n le oIC: n td N sa e oQtI s ta fn e s pt h oaa o nrks a swi et etnu e s t c regime, showing u A B d n 0.5 Onset Linear Saturation growth from 2E-6 to 2 Figure 1: (A) Chamber holds electrode array with o C regime regime regime S/cm over nine 0.0 minutes of vapor tungsten contacts. (B) Chamber sits above 0 200 400 600 doping. Data collected subliming dopant, with contacts for in situ conductivity measurements. Doping time (s) 14 Sept., 16:00 CST. 1 Mark DiTusa NSF GRFP Research Statement initial conductivity measurements on P3HT/F4TCNQ polymer-dopant pairing spun coat onto an interdigitated electrode array (IDA) we designed in the Pritzker Nanofabrication Facility at UChicago. These samples were vapor doped in our chamber for over nine minutes while in situ conductivity measurements were made. Dopant mass, surface area, and doping temperature were held constant between measurements. These measurements showed a profile that increased six decades in close agreement with multiple literature values.[1,4,5] These measurements were shown to be consistent over multiple runs (Fig. 2). My measurements also showed three doping regimes: an onset regime, a linear regime, and a saturation regime. These three regimes, which has not been previously reported in literature, will be individually probed for evidence on the dynamics of molecular dopants during the doping process. I have also conducted Grazing-Incidence Wide- Angle X-ray Scattering (GIWAXS) on pristine P3HT and P3HT doped with F4TCNQ at the 8-ID- E beamline in the Advanced Photon Source (APS) at Argonne. This technique showed us detailed information about the crystallinity of the film and how the dopant disrupts this structure. These measurements indicate that, for P3HT, the polymer backbone spacing is reduced and sidechain spacing increased by the presence of molecular dopant. Research Plan: With the platform for measuring conductivity in situ complete, I plan to use this platform in conjunction with complementary characterization techniques to study novel polymer-dopant systems. These polymer-dopant systems will be provided by our collaboration with the Rowan groups within the Institute for Molecular Engineering at UChicago. This project also allows me to be trained further at the Pritzker Nanofabrication Facility to produce the interdigitated electrode arrays necessary to measure polymer-dopant properties in situ. I will employ a complementary group of characterization techniques to probe these systems. UV-Vis-NIR spectroscopy in conjunction with FTIR spectroscopy will be used to identify charged species in films at all three doping regimes, allowing us to measure the concentration of charge carriers in the polymer films. Raman spectroscopy and microscopy will be used to measure and map the distribution of charge carriers in our systems in different doping regimes. Conductive Atomic Force Microscopy will allow us to map conductivity on the surface of our systems. These techniques will occur at the Materials Research Center at UChicago, an NSF MRSEC facility. Our relationship and proximity with Argonne Nation Laboratory allows us to utilize advanced techniques to characterize the polymer-dopant system. Synchrotron (high energy) X- rays are required for this project to probe the small length scales of our polymer films. As previously detailed, GIWAXS obtains measurements of the local crystallinity of the polymer film. Beamline 8-ID-E at the APS also has expertise in X-ray Photoelectron Correlation Spectroscopy, which allows for in situ measurement of the slow dynamics of the doping system. Resonant Soft X-ray Scattering at beamline 29-ID-D obtains morphology over large length scales (10-1000 nm). Conclusions and Broader Impacts: The data I will obtain from in situ measurements and characterizations will give us a more complete picture of how molecular dopants infiltrate and modulate conductive polymers throughout the entire doping process. Through our collaborations with the Rowan and de Pablo groups in the Institute for Molecular Engineering at UChicago, we will be able to use our platform with novel polymer-dopant systems, aiding in the discovery of systems with high conductivities. This, in turn, will allow for the improvement of organic semiconductor devices, furthering the development of OSCs for use in critical (opto)electronic applications such as thermoelectrics and organic photovoltaics. Receiving the NSF GRFP will allow me to realize these goals and to present at public outreach events about the need for alternative energy sources and how my research brings these technologies closer to realization. 1. Adv. Mater. 2017, 1703063 2. MRS Commun. 2015, 5 (3), 383-395 3. J. Mater. Chem. C. 2016, 4, 3454-3466 4. Macromolecules. 2017, 50 (20), 8140-8148 5. Phys. Rev. B. 2015, 91, 085205 2	Winner!
126	"ranging applications. Robotic systems are perfectly suited for repetitive or dangerous construction tasks, from brick laying and creating city infrastructure, to building extraterrestrial or disaster- relief structures. This latter application presents a formidable challenge in robotics: to enter an unknown and uneven environment and autonomously build pre-designed or adaptively designed structures to stabilize the site and prepare it for human presence. Already, there are many designs for emergency relief structures and space infrastructure that are well-suited for this type of autonomous construction, but existing robotic systems are not able to build them. There are two primary approaches to improving construction capabilities. The first approach is to create complex systems that rely on high-accuracy perception, complete representation of surroundings, and precise movement and control. While this method could be capable of building a greater variety of structures, it has problems with cost, computation, and robustness that current technology cannot address. The second approach relies on distributed systems that absorb high levels of error through mechanisms and control, rather than trying to perform perfect actions. This second approach can take great inspiration from biology. Termites, which build mounds millions of times their own size to house their colonies, and beavers, which cobble together dams to adapt their environment for their habitation, demonstrate the extraordinary potential of these systems. Additionally, the compliant or underactuated mechanisms that compose many animals show the potential for error-absorption and robustness in material choice and mechanism design. In this vein, some recent work has explored the use of robot collectives for construction. These systems take control and communication methods from biology, such as distributed decision-making based on the state of the built structure, called stigmergy, and communication through pheromones. In [1], a multi-robot system built simple structures with blocks using stigmergy, and mimicked ""tagging"" the structure with pheromones by using color-coded LED's. In [2], a multi-robot system relied on a stored blueprint to build complex structures much larger than an individual agent, and relied completely upon stigmergy for representation of the current built structure. Even more similar to termite construction, [3] presents an algorithm for construction of a ramp using amorphous material similar to the soil used by termites. These robotic systems rely upon simple agents and control policies to absorb errors and robustly and efficiently build. Other work takes inspiration from the underactuated and soft mechanisms present in nature to make compliant robots. These soft robots are extremely effective at absorbing error; their flexibility simplifies control and damps out unwanted perturbations. However, soft robots are presently limited by difficulty of manufacture and poor performance of soft actuators. There exists a large body of work developing soft robots, but these concepts have not been applied to construction tasks. Research Plan: My aim is to enable collective construction that is robust and effective on uneven terrain. It will consist of a mechanical design phase and an algorithm design phase, supported by a foundation of tools to evaluate system success objectively. I. Metrics: First, I aim to develop a novel metric to quantify error tolerance in a multi-robot construction system. Presently, most metrics that would apply are within the realm of control theory: metrics such as the size of zone of attraction, or the degree to which an action in the system is passively stable. These are effective in characterizing aspects of a collective construction system on the micro-level, in terms of individual actions, but fail to characterize the system on a macroscopic scale. I will create and validate a metric that effectively characterizes the response to disturbances and errors of a collective construction system. This will allow comparison of my future work with its biological counterparts, and will give the necessary basis for verifying a collective construction system. II. Physical System: I plan to create a physical robotic system capable of robust collective construction on uneven terrain. My research group at University at Buffalo, under Nils Napp, is the foremost group exploring autonomous construction on uneven terrain, a broad and applicable subset of the autonomous construction field. One approach that is almost untouched within the field is the use of soft, compliant, or underactuated robots and robot components to increase error tolerance. In the field of behavioral robotics, soft robots are ideal: their simplified control and adaptable structures make them perfectly suited. However, the lack of effective untethered actuation systems make them underutilized. I intend to use soft components in my robotic system, focusing on passive mechanisms and flexible-rigid interfaces to increase error tolerance without relying upon soft actuation. Additionally, I will extend research I performed in the past year, developing a robotic platform for construction over uneven terrain. I will modify it to be highly modular, allowing soft components to be swapped in and out easily, and enabling new directions of research involving self-charging and even self-healing agents. III. Control System: I will develop new control methods to improve coordination and efficiency in collective construction applications. Presently, there exist few control systems capable of controlling multiple robots towards a construction objective over uneven terrain. One of the only examples, presented in [3], makes non-navigable terrains navigable through deposition of expanding foam. With the Napp group at UB, I extended this in a paper submitted to ICRA this year, which modified the algorithm to work with a single-agent system and discrete objects: filled bags. I intend to generalize this work to construct any arbitrary structure, by developing new planning policies for the order of voxel construction based upon the structure shell and other parameters. Additionally, I will incorporate policies for a range of materials: continuous materials such as foam, and discrete amorphous objects such as filled bags. Broader Impact: The broader impact of the project lies in its motivation: autonomous construction can be used to save lives in disaster areas, and to build structures on different planets. Robots can stabilize buildings and make rubble navigable in areas of disaster, build levee walls to prevent flooding, build emergency shelters, and build structures on the moon and mars. Additionally, the project could have a parallel initiative in educational outreach, by incorporating high school and undergraduate students in important roles. Because the core of the project is small, highly modular robots whose abilities are augmented through collaboration, undergraduates and even high school students can take part in construction of the robots while learning about robotics and experiencing the environment of a research lab. Further, these modular robots can be easily integrated into demonstrations in schools to foster interest in robots and STEM in general. The coordinated robots could inspire elementary, middle, and high school students to become involved with robotics, and could form the foundation for a program to increase college attendance in STEM fields. [1] M. Allwright, N. Bhalla, C. Pinciroli, M. Dorigo, M. All- wright, N. Bhalla, C. Pinciroli, and M. Dorigo, “Towards Autonomous Construction using Stigmergic Blocks,” 2017. [2] J. Werfel, K. Peterson, and R. Nagpal, “Designing Col- lective Behavior in a Termite-Inspired Robot Construction Team,” Science, vol. 343, no. February, pp. 754–758, 2014. [3] N. Napp and R. Nagpal, “Distributed amorphous ramp construction in unstructured environments,” Springer Tracts in Advanced Robotics, vol. 104, pp. 105–119, 2014."	Winner!
127	Vehicles Background and Motivation: Solar-assisted electric vehicles (SEVs) are the ultimate zero-emission vehicles since they do not contribute to greenhouse gas emissions. However, due to the low power output of the photovoltaic array, widespread use of SEVs can only be realized if the vehicle system is highly energy efficient. Such rigorous optimization for limited power applications will lead to universal efficiency increases in vehicles of all kinds. One major boost in vehicle efficiency is to utilize in-wheel motors. For example, Protean Electric’s in-wheel motors were found to increase the 244 mile range of the Tesla Roadster by 14%.1However, this ​ ​ pivotal technology also increases unsprung mass; in Protean’s case, 30 kg at each wheel. The addition of unsprung mass causes more energy losses in the shock absorbers due to increased vibrational forces in the suspension.2 It also adds complications traditional vehicle control for ​ ride comfort and safety.3 I would like to propose a comprehensive approach to maintain ride ​ comfort and vehicle safety while harvesting suspension energy loss so that in-wheel motors can become the enabling technology for SEV’s. Research Method: While working at the Advanced Transportation Energy Center (ATEC) and ​ Future Renewable Energy and Electricity Distribution Management (FREEDM) Center at North Carolina State University (NCSU), I will develop multi-level control for an adaptive regenerative shock absorber (ARSA), which will lead to improvements in electric vehicle efficiency. An existing high efficiency ARSA will be optimized to make it comparable in weight to traditional shock absorbers. The novelty of the design will be the adaptive control strategy, which adjusts ARSA parameters to changing road conditions to optimize safety, comfort, and efficiency. The first layer of the control strategy will determine if an energy consumptive active or energy regenerative semi-active control should be used. The second layer will use an adaptive algorithm to adjust ARSA parameters based on vehicle response to road excitation. At the third level, an energy optimization strategy will ensure that the most energy is captured based on the corresponding mode of operation. The dynamic nature of the suspension will require quick response from the control scheme so the controller will be implemented in a closed-loop with negative feedback. Intellectual Merit: The SEV is a highly interconnected system and understanding the ​ relationships between its subsystems can increase its overall efficiency. This work will result in more intelligent suspensions that allow in-wheel motors to make better electric vehicles, including those assisted or driven by solar-power – the pinnacle of sustainable transportation. The potential of the ARSA is recognized among automotive suspension engineers worldwide.4,5,6 Previous work has resulted in the development of suspension control algorithms ​ optimized for specific conditions or objectives.7 An adaptive, multi-level control scheme will ​ allow all benefits of the ARSA in dynamic situations. Facilities: NCSU is the ideal place for this research due to the extensive resources available and ​ established collaborations. At FREEDM, I will have access to the dSPACE vehicle chassis simulator, which serves as the vehicle model, to quickly simulate the ARSA and develop control logic via software-in-the-loop (SIL). Once my design is constructed, I can integrate it with the ​ Lab Rapid Assessment Tool (LabRAT), a bare-skeleton vehicle made for bench testing components, at ATEC. ATEC also has an all-wheel drive dynamometer which I will use to get control data for comparison to simulation and road tests. The solar car team, SolarPack, will provide access to the prototype SEV for full integration of the prototype ARSA. Connections through SolarPack, such as the Haas Formula 1 racing team, can be leveraged to arrange premier testing facilities for accurate practical experiments. Timeline: 1. Year One - Simulation and Design ​ ​ ​​ Objective: Design and simulate a highly efficient regenerative shock absorber with ​​ semi-active and active controls Methodology: Literature review and optimization of existing ARSA topology. Use ​​ dSPACE simulator to implement active and semi-active controls to prototype which will be integrated in future multi-layer control. Simulation using stochastic road models will be used to guide design. 2. Year Two - Active Regenerative Shock Absorber Experimentation ​​ ​​ ​​ Objective: Build ARSA and experiment with control parameters to optimize both active ​​ and semi-active logic. Methodology: Integrate the ARSA into the LabRAT to iterate the control parameters in ​​ each control strategy. 3. Year Three - ARSA Integration, Controls Assimilation and Testing ​ ​ ​​​ Objective: To evaluate and improve robustness of control scheme which will result in an ​​ ​ ​ ​ ​ ​ ​ ARSA that is ready for integration to the SEV. Methodology: The ARSA and multi-layer control will be integrated to the SEV for ​​ experimentation. Accurate road experiments will be done to iterate to robust control. Robustness will be assessed by designing to International Organization for Standardization requirements. Broader Impact: With the anticipation that societal and economic growth will require increased ​ movement of people and things, we must ensure that our methods of transportation are clean and efficient. ARSAs are a step towards a better future where transportation is not only sustainable, but more intelligent. The SEV represents the most efficient vehicle topology and will benefit from the development of the ARSA because it will enable the in-wheel motor. Using the in-wheel motor will result in improved vehicle range by decreasing energy usage of the drivetrain. The proposed control strategy is not constrained to passenger vehicles and can be adapted to all ground vehicle applications. The findings will be communicated in IEEE and SAE conferences and journals, and the technology will be demonstrated in solar car outreach activities, such as the EV Challenge, the Science Olympiad, and minority engineering summer transition programs to engage early college and high school students in automotive engineering research. SolarPack has already arranged to show our SEV at future North Carolina Science & Engineering Fairs, which will be a great way to inspire K-12 students to break barriers in the automotive industry through research. The NSF Graduate Research Fellowship will allow me to engage sustainable transportation research on a deeper level and focus on contributing unique insights to the automotive engineering field. References: 1 ​Watts. A et al., SAE International, 9-10, 2010 ​ ​​​ 2 ​Anderson M. et al., Advanced Vehicle Control, 2-3, 2010 ​ ​ 3 ​Rojas. A.E. et al., SAE International, 14, 2010 ​ ​ 4 ​Shi D. et al., Smart Materials and Structures, 8-9, 2015 ​ ​ 5 ​Zuo L. et al., Smart Materials and Structures, 2010 ​ ​ 6 ​Sabzehgar R. et al., IEEE/ASME Transactions on Mechatronics, 2013, ​ ​ 7 ​Huang K. et al., International Journey of Automotive Technology, 2011 ​ ​	Winner!
128	"Radiolytic Production of Gadolinium Nanoparticles for Cancer Therapy Hypothesis: The average particle size of the gadolinium will decrease as total absorbed dose increases within a nuclear reactor. Background and Introduction: In the United States, cancer is the second leading cause of death and is a major public health problem worldwide [1], [2]. There were estimated to be 1,685,210 new cancer cases and 595,690 cancer deaths in the U.S. alone in 2016 [2]. Current treatment for various cancers include chemotherapy, radiation therapy, hormone therapy, and surgery. Although survival rates have improved from these treatments, each one has its drawbacks and limitations. Chemotherapy, for example, distributes the toxic therapeutic agents throughout the entire body, thus, damaging both cancerous and normal cells. This limits the amount of dose to the cancer cells while causing adverse side effects to the patient including weakness, hair-loss, and organ dysfunction [1]. An interest in nanoparticles (NPs) has increased over the last decade for researchers because of their ability to carry both drugs and imaging probes throughout the body [1]. Additionally, they can be uniquely designed to target the molecules of diseased tissues, thus, having the potential to increase the dose to cancerous cells while decreasing the dose to healthy tissues and organs [3]. Gadolinium neutron capture therapy (GdNCT) is another potential method for the treatment of cancer [4]. GdNCT takes advantage of the energy released when stable gadolinium-157 (157Gd) is bombarded by a neutron, producing an excited 158*Gd that decays by gamma emission, conversion electrons, and Auger electrons (Figure 1) [4] [5]. 157Gd is appealing in NCT for its extremely high neutron absorption cross section Figure 1: Gd decay scheme (255000 barn) and short path length in tissue, restricting cell death to the gadolinium-containing regions only [5]. The objective of this research proposal is to fabricate gadolinium nanoparticles for GdNCT applications by irradiation in a nuclear reactor. Average particle sizes of less than 100 nm are desired so that they can pass through the body and localize around tumors. Once the NPs have gathered around the tumor, neutron irradiation can occur using either a nuclear reactor or a neutron accelerator, destroying the tumor with minimal damage to the surrounding healthy tissues and organs. Research Plan: The first step for this research project will be to determine the likely material components. A form of the gadolinium precursor, a reducing agent, a particle capping agent, and a solvent to prevent excess agglomeration will need to be used to create the gadolinium solution [6]. The solution will be distributed into 8 labelled vials, with Sample 1 being used as the un- irradiated reference sample. Samples 2-8 will be irradiated in the High Flux Isotope Reactor at a constant power (200kW) for times ranging from 30 to 1200 seconds (Table 1.) Table 1 – Irradiation Times Sample 2 3 4 5 6 7 8 Irradiation 30 60 180 300 600 900 1200 Time (s) NSF Graduate Research Statement Mikayla Molnar I will complete dose calculations in order to determine the total absorbed dose for each sample irradiated in the reactor. The samples will be subject to both incident neutrons and gammas radiation. Therefore, the dose rate D is given by Equation (1) t 𝐷 = 𝐾 +𝐷 (1) 𝑡 𝑛 𝛾 Where K is the neutron kerma rate and Dγ is the gamma dose rate. These dose rates will be n determined using a Monte Carlo N-Particle (MCNP) code simulation of the reactor. The samples will then be distributed onto a silicon wafer and left to dry. It is important that the radiation levels of the samples are safe and within NRC/reactor limits before removing them from the reactor for analysis. I will then image each sample under a scanning electron microscope (SEM) to determine the particle size distribution. An Energy Dispersive X-ray Spectroscopy (EDS) analysis will also be performed to verify the elemental composition of the particles. Intellectual Merit: My background as a nuclear engineer is essential for the fulfillment of this project. It will combine my understanding of radiochemistry, reactor physics, and electron imaging with an engineering perspective. I completed a similar process for the production of boron nanoparticles in my Reactor Laboratory II class at Missouri University of Science and Technology, so I have first-hand experience in the process that needs to unfold. A much more comprehensive and in-depth analysis will need to be taken, however. I plan to collaborate with members of Oakridge National Laboratory to complete the irradiation procedure and use my knowledge of MCNP and scanning electron microscopy to determine the total absorbed dose for each irradiated sample. Broader Impact: GdNCT is becoming a safe and effective way to destroy cancer cells with minimal damage to surrounding healthy cells. If the results are successful, my research will provide an efficient and valuable mean for creating gadolinium nanoparticles. This will make GdNCT cheaper and more accessible for cancer patients, resulting in the potential savior of countless future lives. [1] K. T. Nguyen, ""Targeted Nanoparticles for cancer therapy: Promises and challenges,"" Journal of Nanomedicine & Nanotechnology, vol. 02, no. 05, 2011. [2] R. L. Siegel, K. D. Miller, and A. Jemal, ""Cancer statistics, 2016,"" CA: A Cancer Journal forClinicians, vol. 66, no. 1, pp. 7–30, Jan. 2016. [3] R. Subbiah, M. Veerapandian, and K. S. Yun, ""Nanoparticles: Functionalization and Multifunctional applications in biomedical sciences,"" Current Medicinal Chemistry, vol. 17, no. 36, pp. 4559–4577, Dec. 2010. [4] G. A. Miller, N. E. Hertel, B. W. Wehring, and J. L. Horton, “Gadolinium Neutron Capture Therapy,” Nuclear Technology, vol. 103, no. 3, pp. 320–331, Sep. 1993. [5] C. Salt, A. J. Lennox, M. Takagaki, J. A. Maguire, and N. S. Hosmane, “Boron and gadolinium neutron capture therapy,” Russian Chemical Bulletin, International Edition, vol. 53, no. 9, pp. 1871–1888, Sep. 2004. [6] A. Abedini et al., “A review on radiation-induced nucleation and growth of colloidal metallic nanoparticles,” Nanoscale Research Letters, vol. 8, 2013."	Winner!
129	Background literature. Introductory chemistry courses are often delivered lecture-style and assessed with multiple-choice exams in classes of hundreds of students, often lacking opportunity for realistic scientific activity.1 Students often develop the notion that scientific knowledge may be acquired from external authorities rather than viewing science as a sense-making endeavor2 and as such they may default to rote learning, as material seems removed from everyday experiences.3 Often, students’ understandings of the nature of scientific knowledge, that is, their epistemological understandings of science, are not explicitly addressed in traditional curriculum, yielding students, some who complete STEM degrees, who do not understand how science progresses.4 Evidence suggests that undergraduate chemistry students possess significantly less sophisticated epistemologies than practicing chemists but that sophistication of undergraduates’ epistemologies are positively correlated with authentic experience. However, processes by which shifts in student epistemologies occur remain unclear.5 For instance, one authentic scientific activity identified by the National Research Council’s A Framework for K-12 Science Education, is that of scientific modeling.6 However, because students often perceive scientific claims as “proven,” they may not consider the nature and purpose of scientific models or a models’ associated assumptions, known as metamodeling knowledge.7 Students may have different metamodeling beliefs in different contexts.8 For example, students have demonstrated particular difficulty reasoning with mathematical models in meaningful ways.9 In contrast, expert scientists create and use models purposefully, to explain and predict phenomena, and generally possess modeling epistemologies that are far more stable and contextually appropriate.8 Evidence suggests that incorporating authentic modeling activities into introductory courses can support students’ development of expert-like epistemological ideas.10 Little is known about the mechanisms by which students’ epistemologies shift in response to engagement with modeling activities, but understanding these mechanisms could inform practices for instruction on scientific modeling epistemology. I intend to identify the moments in which students’ epistemological ideas are challenged, how and why students’ epistemological ideas change, and the factors that contribute to such realizations in a modeling-focused classroom setting. Research questions. 1) How do students rationalize their epistemological decisions while engaging in collaborative modeling activities in introductory chemistry contexts? 2) How do modeling activities and instructor facilitation contribute to shifts in epistemological ideas? Methods. Because students are often unaware of their own epistemologies, it is not reasonable to ask students to describe their epistemological ideas.11 Rather, it is logical to analyze student discourse to deduce the ways in which students think about the nature of scientific knowledge. Furthermore, as I am interested in the process of students’ shifting epistemologies toward those appropriate in chemistry contexts rather than static knowledge states, microgenetic analysis, a detailed, moment-by-moment analysis of the processes which contribute to learning, is appropriate for developing a nuanced understanding of change processes and the factors which contribute to them, such as design features of classroom activities or instructor-student interaction.12 My current graduate research is aimed at developing and testing modeling-focused collaborative learning activities that support students’ understanding of the nature and purpose of models. As part of this project, I plan to examine pre- and post-intervention survey data to gain a sense of the impact of the activities on students’ metamodeling knowledge (see Personal Statement). Support from the GRFP would allow me to expand upon this work to conduct a microgenetic analysis of classroom discourse as collaborative modeling activities take place, which would provide a finer-grained understanding of the mechanisms by which students’ epistemological ideas shift toward more expert-like conceptions. Such a fine-grained account of Katherine Lazenby | NSF GRFP Graduate Research Plan Statement 2 how collaborative learning activities support students’ epistemologies of modeling would provide insight for educators on how to orchestrate learning environments conducive for student development of robust understanding of the nature of scientific inquiry. While common in the mathematics education research community, few discourse analysis studies and fewer still microgenetic studies in particular, exist in chemistry contexts that would provide the kind of detailed mechanistic understanding that would support instructor facilitation efforts. For this study, I will collect multiple sources of data including video recordings of whole class discussion and small group work during collaborative modeling-focused activities, and written work generated during the modeling activities. The collaborative nature of the modeling activities will necessitate that students express their ideas about models to other group members, providing data about students’ thought processes. Since students will complete three modeling activities over the course of a semester, these data will allow me to observe the changes in the ways students discuss epistemological ideas about models over time. As an initial analytical framework for helping me to identify epistemological shifts in classroom discourse, I will adapt Grünkorn et al.’s developmental progression of epistemological views of modeling on five subscales (nature, purpose, and changeability of models, testing models and multiple models for a single phenomenon). Grünkorn et al.’s progression ranges from naïve- realistic interpretations of models as mere copies of phenomena, to constructivist views of models as the scientific products of developing explanations about the natural world, an idea which students should ideally develop as they mature into individuals capable of contributing to science. This framework would allow for the characterization of the sophistication of student modeling epistemologies and for determining whether engaging in modeling activities supports shifts toward more expert-like views of models.13 In completing this project, I will draw on my experience analyzing qualitative data. Additionally, my adviser has experience in classroom discourse analysis and will serve as a valuable resource during the analysis for this study.14 Summary of Intellectual Merit. The proposed research will provide insight about how students reason about the nature of models in chemistry. Since models are key tools used by scientists to communicate and explain the natural world, it is important that introductory courses can provide students with opportunities to develop scientifically appropriate understanding of models’ nature and purpose. Additionally, microgenetic studies of chemistry learners are few; to my knowledge, this analytical technique has never been used in undergraduate chemistry contexts, but these methods are powerful for investigation of agents that contribute to shifts in student ideas. Summary of Broader Impacts. The proposed research will inform instruction on science epistemologies. Students who understand the nature of scientific inquiry are more likely to engage meaningfully in subsequent coursework and experience academic success.15 Evidence-based instruction on epistemologies can support student success. Furthermore, students will be able to apply knowledge of the nature of scientific knowledge beyond classroom chemistry contexts to understand how scientists model globally-relevant phenomena such as climate change. (1) Smith, A. C.; Stewart, R.; Shields, P.; Hayes-Klosteridis, J.; Robinson, P.; Yuan, R. Cell Biol. Educ. 2005, 4 (2), 143–156. (2) Hofer, B. K. Contemp. Educ. Psychol. 2004, 29 (2), 129–163. (3) Songer, N. B.; Linn, M. C. J. Res. Sci. Teach. 1991, 28 (9), 761–784. (4) Clough, M. P. Sci. Educ. 2006, 15 (5), 463–494. (5) Samarapungavan, A.; Westby, E. L.; Bodner, G. M. Sci. Educ. 2006, 90 (3), 468–495. (6) National Research Council. Washington, DC: The National Academies Press 2012. (7) Schwarz, C. 2002. Proceedings of Int. Conf. of Learning Sci. (8) Hammer, D.; Elby, A.; Scherr, R. E.; Redish, E. F. Transf. Learn. Mod. Multidiscip. Perspect. 2005, 89. (9) Brandriet, A.; Rupp, C.; Lazenby, K.; Becker, N. Chem. Educ. Res. Pract. (under review). (10) Schwartz, R. S.; Lederman, N. G.; Crawford, B. A. Sci. Educ. 2004, 88 (4), 610–645. (11) Berland, L.; Crucet, K. Sci. Educ. 2016, 100 (1), 5–29. (12) Siegler, R. S. Handb. Child Psychol. 2006. (13) Grünkorn, J.; zu Belzen, A. U.; Krüger, D. Int. J. Sci. Educ. 2014, 36 (10), 1651–1684. (14) Becker, N.; Stanford, C.; Towns, M.; Cole, R. Chem. Educ. Res. Pract. 2015, 16 (4), 769–785. (15) Tsai, C.; Liu, S. Int. J. Sci. Educ. 2005, 27 (13), 1621–1638.	Winner!
130	One of the most important unknowns in high-z extragalactic astronomy is how reionization occurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominates the intergalactic medium (IGM). HI attenuates radiation from early stellar populations, masking galaxies from detection. Understanding how and when reionization occurs can reveal whether or not these young galaxies provided the necessary ionizing radiation to completely reionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the high redshift-space this implies, spectroscopic observations are limited as these galaxies are very faint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths. My background in NIR spectroscopy and observational astronomy has prepared me to assist in addressing this question. I propose using Lyα and CIII] to investigate the properties and ionization state of young galaxies using ground- and space-based telescopes, the structure and distribution of HI in the IGM and the circumgalac- tic medium (CGM) of certain galaxies, and implications for the evolution of the neutral fraction of the IGM throughout the EoR. The individual points proposed will be summarized as follows: (i) small scale testing and building of an analysis technique, (ii) distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. By understanding more about the IGM during the reionization era and of the galaxies within it, we can further constrain the properties of current galaxy evolution and reionization models. Small scale testing & building of analysis technique: In the search for galaxies during the EoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric 0 surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UV bright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring the escape fraction of Lyα many studies have inferred an increasingly neutral fraction of the IGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emission hundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent method uses a complementary spectroscopic tracer not attenuated by HI, with the UV metal line CIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5]. 0 In my current work, I measure CIII] H-band emission of galaxies found via Lyα emission using Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) and attenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGM of these galaxies and surrounding IGM. From my previous work experience, I have developed a proficiency in coding which enabled me to gain a close familiarity with the MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fix bugs often encountered when working with incredibly faint emission lines and less common dithering patterns for standard star observations. I wrote code to optimally extract my 1D spectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage of IRAF and Python, I developed code that can track the photometric variability of my data from a frame-to-frame basis – important when working with faint emission lines. Distribution of galaxies and evolution of neutral fraction: Using the foundation built from my previous work, I will build a statistical sample of galaxies during the last half of the reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escape fraction as a function of redshift. Using my optimized extraction technique to improve Taylor A. Hutchison 2 measurements, I will use this dataset to constrain the offset between these galaxies’ systemic and attenuated redshifts. This work will significantly increase the sample of high-z galaxies with both Lyα and CIII] measurements. In addition, it will provide a more significant comparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses. A current complication for this project is the lack of a complete spectroscopic sample of LAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines or the [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to 0 NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes – it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in the range of NIRCam on JWST. As a first approach to resolving this, I will take the current sample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using my optimized extraction technique to improve measurements. This has already been attempted for some galaxies [4], providing useful lower limits for determining exposure times and potential telescopes for future observations, including JWST. We are planning proposals for the first JWST cycles for this work. I will then take advantage of the deep multi-wavelength imaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I am part of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with early indications that the proposal has so far been successful. Finally, as a scientific collaborator on an ERS JWST proposal, I will prepare for access to that data – understanding what spectra I will be looking for from running binary stellar population models (eg. BPASS), scaled to match expected bandpass magnitudes, through the JWST exposure time calculator. With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will be able to further constrain the amount of hard radiation emitted from these galaxies; as shown with mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5]. By tracking its evolution through the last half of the EoR, I can inform current reionization models. Lastly, through gathering my sample I will map out the distribution of these galaxies, identifying whether galaxies with large escape fractions are tracing over-dense luminous regions, located within large ionized bubbles [3,8]. Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space- based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of the fluxes of the CIII] doublet, when measurable, I can infer an estimate of the electron density of the gas in the CGM. This is closely linked to the metallicity of the CGM, which directly affects the velocity offset of Lyα emission. Not only do recent studies indicate that a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, they also suggest (from mid-z analogs) a strong link between the profile of Lyα emission and the properties of the gas within the CGM [5]. This can be incredibly important as some high-z galaxies have been found to have more symmetric Lyα profiles, contrary to the archetypal asymmetric shape, thought to be indicative of high star formation and galactic winds [6]. Understandingthe rateand distributionof reionization, including thefactors andprocesses responsible for it, remains one of the most important unknowns in extragalactic astronomy. My work will aim to shed more light on this question, enabling more precise modeling of thiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies. References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erb et al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004	Winner!
131	One of the most important unknowns in high-z extragalactic astronomy is how reionization occurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominates the intergalactic medium (IGM). HI attenuates radiation from early stellar populations, masking galaxies from detection. Understanding how and when reionization occurs can reveal whether or not these young galaxies provided the necessary ionizing radiation to completely reionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the high redshift-space this implies, spectroscopic observations are limited as these galaxies are very faint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths. My background in NIR spectroscopy and observational astronomy has prepared me to assist in addressing this question. I propose using Lyα and CIII] to investigate the properties and ionization state of young galaxies using ground- and space-based telescopes, the structure and distribution of HI in the IGM and the circumgalac- tic medium (CGM) of certain galaxies, and implications for the evolution of the neutral fraction of the IGM throughout the EoR. The individual points proposed will be summarized as follows: (i) small scale testing and building of an analysis technique, (ii) distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. By understanding more about the IGM during the reionization era and of the galaxies within it, we can further constrain the properties of current galaxy evolution and reionization models. Small scale testing & building of analysis technique: In the search for galaxies during the EoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric 0 surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UV bright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring the escape fraction of Lyα many studies have inferred an increasingly neutral fraction of the IGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emission hundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent method uses a complementary spectroscopic tracer not attenuated by HI, with the UV metal line CIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5]. 0 In my current work, I measure CIII] H-band emission of galaxies found via Lyα emission using Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) and attenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGM of these galaxies and surrounding IGM. From my previous work experience, I have developed a proficiency in coding which enabled me to gain a close familiarity with the MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fix bugs often encountered when working with incredibly faint emission lines and less common dithering patterns for standard star observations. I wrote code to optimally extract my 1D spectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage of IRAF and Python, I developed code that can track the photometric variability of my data from a frame-to-frame basis – important when working with faint emission lines. Distribution of galaxies and evolution of neutral fraction: Using the foundation built from my previous work, I will build a statistical sample of galaxies during the last half of the reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escape fraction as a function of redshift. Using my optimized extraction technique to improve Taylor A. Hutchison 2 measurements, I will use this dataset to constrain the offset between these galaxies’ systemic and attenuated redshifts. This work will significantly increase the sample of high-z galaxies with both Lyα and CIII] measurements. In addition, it will provide a more significant comparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses. A current complication for this project is the lack of a complete spectroscopic sample of LAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines or the [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to 0 NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes – it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in the range of NIRCam on JWST. As a first approach to resolving this, I will take the current sample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using my optimized extraction technique to improve measurements. This has already been attempted for some galaxies [4], providing useful lower limits for determining exposure times and potential telescopes for future observations, including JWST. We are planning proposals for the first JWST cycles for this work. I will then take advantage of the deep multi-wavelength imaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I am part of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with early indications that the proposal has so far been successful. Finally, as a scientific collaborator on an ERS JWST proposal, I will prepare for access to that data – understanding what spectra I will be looking for from running binary stellar population models (eg. BPASS), scaled to match expected bandpass magnitudes, through the JWST exposure time calculator. With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will be able to further constrain the amount of hard radiation emitted from these galaxies; as shown with mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5]. By tracking its evolution through the last half of the EoR, I can inform current reionization models. Lastly, through gathering my sample I will map out the distribution of these galaxies, identifying whether galaxies with large escape fractions are tracing over-dense luminous regions, located within large ionized bubbles [3,8]. Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space- based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of the fluxes of the CIII] doublet, when measurable, I can infer an estimate of the electron density of the gas in the CGM. This is closely linked to the metallicity of the CGM, which directly affects the velocity offset of Lyα emission. Not only do recent studies indicate that a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, they also suggest (from mid-z analogs) a strong link between the profile of Lyα emission and the properties of the gas within the CGM [5]. This can be incredibly important as some high-z galaxies have been found to have more symmetric Lyα profiles, contrary to the archetypal asymmetric shape, thought to be indicative of high star formation and galactic winds [6]. Understandingthe rateand distributionof reionization, including thefactors andprocesses responsible for it, remains one of the most important unknowns in extragalactic astronomy. My work will aim to shed more light on this question, enabling more precise modeling of thiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies. References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erb et al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004	Winner!
132	vectors such as mosquitoes and ticks. Discovered in a screen of over 7,000 molecules, DEET was developed for the U.S. Army for application on human skin in 1946. Although DEET is the world’s most widely used insect repellent, the neurobiological mechanisms of how DEET mediates avoidance remain controversial. Better understanding of the processes underlying DEET’s effectiveness would lead to the development of safer and more effective insect repellents. Background & preliminary data: Previous research shows that DEET’s mechanism of action is multimodal1,2,3. It acts through mosquitoes’ sense of smell, evidenced by animals lacking orco2, a necessary subunit of insect olfactory receptors, and through bitter-sensitive receptors in the labellum3 (Fig. A). Through behavioral assays and video analysis, we have shown that mosquitoes sense and are repelled by DEET on contact through their legs (tarsi)4 (Fig. A) Although the olfactory mechanisms of DEET are better understood, little is known about how DEET repels on contact. Our results demonstrate that when mosquito tarsi come into contact with DEET, the mosquitoes will not blood feed from that surface (Fig. C). Previous work suggests that DEET avoidance acts through bitter receptors in the labellum3 (Fig. A). We found that high concentrations of bitters (such as quinine and lobeline), which are sufficient to prevent feeding when contacted by the labellum3, are ineffective at mediating avoidance Figure | A. Female Aedes aegypti mosquito through tarsal contact when applied to human skin or feeding on a human arm. B. DEET-treated arm an artificial surface (Fig. B & C). My preliminary work with 25mm circle of accessible skin. C. Blood- suggests that DEET repels mosquitoes on contact feeding with indicated compounds applied to through an avoidance pathway more strongly than or human arm. independent of bitter taste sensation, and that this deters them from blood-feeding. But nothing is known about the cellular or molecular mechanisms by which the tarsi detect DEET. For my PhD thesis work, I propose to decipher this contact-mediated pathway to elucidate the biological mechanism of action for DEET in mosquito tarsi. Using genetic tools recently developed in the mosquito, I will locate DEET-sensitive cells in the tarsi and identify receptor(s) that mediate DEET avoidance. Aim 1: Which neurons in Aedes aegypti tarsi respond to DEET? To investigate DEET contact repellency further, I will use in vivo calcium imaging to compare neural activity responses to different chemical compounds in sensory cells within the mosquito tarsi. I will use an Ae. aegypti pan-neuronal promoter to express the genetically-encoded calcium sensor, GCaMP6s, in all tarsi sensory neurons. Using a recent protocol from my lab5, I will present chemical solutions over intact tarsi while recording neural activity with a two-photon microscope and analyze the location and response dynamics of activated sensory cells. To determine if neurons in the tarsi are specialized to encode different chemical cues, I will compare responses within these tarsi sensory cells to DEET, sweet compounds, and bitter compounds. In behavioral assays, Aedes aegypti respond very differently to bitter, sweet, and DEET cues, therefore I hypothesize that chemosensory cells in the tarsi have different population response patterns to these different chemical cues. This may be in the form of different cells reacting to different compounds, or discrimination of chemical cues encoded through differences in population-level activity. This work will advance our understanding of the peripheral sensory response that chemical cues elicit in Ae. aegypti tarsi. Aim 2: Is DEET-sensitivity conferred by different RNA expression patterns? In C. elegans, a subset of neurons express a G-protein coupled receptor, str-217, that is necessary for DEET behavioral response6. To determine if sensory cells Ae. aegypti tarsi express a similar specialized receptor, I will perform RNA expression analysis on separate populations of cells responsive to DEET, bitters, and sugars. Depending on the location and pattern of cells identified in Aim 1, I will develop methods for dissociating sensory cells from mosquito tarsi using either laser-capture microdissection (LCM) or fluorescent activated cell sorting (FACS) on photoactivatable GFP to precisely isolate and harvest these separate chemosensitive cell populations. Collaborating with the Rockefeller University genomics core, I will use single-cell RNA sequencing (sc-RNAseq) to compare RNA expression patterns of DEET-responsive cell populations to tarsal cells responsive to bitters and sweet compounds. I expect cell populations that respond to different tastants to differentially express a number of proteins and receptors. Through this Aim, I will develop a method for in-depth investigation of mosquito tarsal RNA expression while creating a list of candidate molecules that may be sufficient or necessary for DEET sensitivity. Aim 3: Can candidate genes for DEET-sensitivity be validated through genetic knock-out? To identify the functional relevance of genes identified in Aim 2, I will use CRISPR-Cas9-based gene editing2 to create knock-out animals for candidate genes that may confer DEET- responsiveness. I will then use the behavioral screen in Figure B to determine if the mutant animal has become DEET-insensitive, or partially insensitive. Ultimately, this work would result in the first identification of a receptor in mosquito tarsi that mediates avoidance behavior upon contact. Intellectual merit: This work has the potential to advance the field of chemosensation and solve a long-standing mystery in neurobiology. Although DEET is highly effective in repelling a wide range of evolutionarily divergent invertebrates, the mechanism of DEET avoidance is still controversial 70 years after its discovery. Uncovering the mechanism of DEET avoidance promises to elucidate new principles underlying how chemosensation is encoded and subsequently translated into behavior. Broader impacts: Mosquitoes and ticks that blood-feed on human hosts can transmit pathogens that cause a number of devastating diseases, threatening hundreds of millions of lives yearly. Identifying biological processes that mediate avoidance of blood-feeding, such as those underlying the mechanism of DEET, may lead to the development of more effective insect repellents that could last longer at lower doses and reduce the exposure of human populations to dangerous vector-borne diseases. In addition, because the general public has experience with insect repellents, DEET presents itself as a very tractable example for public and youth engagement with the sciences. I am excited to continue my efforts in science communication to inspire young potential scientists with such an accessible topic. I will participate again this year in Rockefeller University’s Science Saturday, an annual science festival for over 1000 children in grades K–8 and their families, where I will host a demonstration around DEET to illustrate basic principles of chemosensation. I will also teach a class on chemosensation at the Rockefeller Summer Neuroscience Program, a graduate student-led course for disadvantaged high school students from New York City public schools. I am eager to share my passion for sensory perception and chemosensation, through a topic that children will already be familiar with. Using my research project on DEET, I hope to help students realize that their personal observations can be of scientific value, potentially inspiring them to pursue their own interests in science. References: 1. M. Ditzen, et al., Science 319, 1838-1842 (2008). 2. M. DeGennaro, et al., Nature 498, 487-491 (2013). 3. Y. Lee, et al., Neuron 67, 555-561 (2010). ). 4. E.J. Dennis, O.V. Goldman, L.B. Vosshall, in revision at Current Biology. 5. B.J. Matthews, M.A. Younger, et al., bioRxiv (2018). 6. E.J. Dennis, et al., Nature 562, 119–123 (2018).	Winner!
133	Introduction: Large carnivores are re-colonizing North America1,2 and parts of Europe3, following decades of systematic eradication4. The expansion of large carnivore populations is creating novel and complex predator-prey interactions5. One well-known example is trophic cascades and associated declines in herbivore abundance6. Predator-prey interactions are among the most fundamental ecological processes and have been the focus of ecology since its origin6. They are integral processes that shape biological communities, affect coupled human-wildlife systems, and drive conservation and management ecology7. Despite this, our understanding of the effect of predators on prey populations, especially in complex food-webs, is in its infancy5,7. Predator-prey theoretical and empirical research is dominated by single predator-single prey systems like the Isle Royale wolf-moose system8. While useful, it is unclear if these simplified models are capable of predicting dynamics where multiple predators are interacting with multiple prey species6,7. For example, the recent recovery of wolves, expansion of grizzly bear populations, and expanding range of mountain lions across the Western United States are increasing the number and complexity of interactions between predator and prey species1,2,4. Mounting evidence that the growing number of interactions can cause previously unknown ecological effects suggests that there is much left to be understood in multi-predator, multi-prey food webs6,9,10. For example, these complex dynamics can spur changes in direct ecological interactions, such as prey switching by predators in response to prey abundance9. A key conceptual way in which single predator-prey interactions differ from more realistic, complex, multiple predator-prey food webs is the inclusion of competition in addition to direct predator-prey dynamics. For example, predator-prey dynamics can lead to indirect ecological interactions, such as apparent competition, where one prey species supports predator populations, thereby reducing alternative prey populations10. With various competitive interactions within a trophic level occurring, the complexity of competition must also be considered11. Ecology has long studied the tension between how the forces of predation and competition structure communities and population dynamics11. Unfortunately, the inherent complexity of such systems has often rendered purely statistical/empirical approaches limited in their utility. Compared to laboratory studies and field experiments, mathematical models, such as multiple predator-prey models (MPPMs) allow ecologists to study these dynamics12. Complex food webs cannot be easily resolved with statistical/empirical approaches because of the large number of parameters to estimate and the scant data to do so with, as well as the challenges presented by some parameters and mechanisms that are impossible to estimate (e.g., carrying capacity). MPPMs are also very powerful in evaluating the consequences of management decisions12. Commonly, natural resource agencies manage populations using independent management strategies for each species; therefore they do not reflect the complexity of predator- prey population dynamics. By failing to incorporate food web interactions into species management strategies and ignoring the role of multi-species predation and competition, agencies may be sub-optimally preserving and managing wildlife populations. I hypothesize that MPPMs which consider alternative interactions will explain empirical systems better than single-predator, single-prey models (SPPMs). I will address these major questions: a) Are MPPMs better at predicting population dynamics in real-world systems than SPPMs? b) If so, are the main advantages of MPPMs in terms of predictive performance driven by predation or inclusion of competitive interactions? c) What are the conditions (e.g., environmental, stochastic) in which predation vs. competition drive food webs? With these ecological questions answered, I will finally address: d) How does management of one species affect populations of other predators and prey within a food web? Research Approach: I will use wildlife agency-collected datasets from the Idaho Department of Fish and Game for predator and prey populations. Then, through funding from my NSF GRFP proposal, I will generalize my results to other high-profile multi-species predator-prey datasets from Banff National Park, Yellowstone National Park, and Serengeti National Park, with the help of my Ph.D. supervisors who have connections to these 3 systems. First, I will gather information about predator or prey population dynamics from previous studies to inform the structure of my models12. Then, I will estimate functional and numerical responses for each predator-prey pair from across systems. I can then incorporate each predator and prey species into a set of coupled equations, one for each species in the food web. If there are i predator species and j prey species, the corresponding predator-prey equations can be written as such: 𝑑𝑉 𝑑𝑃 𝑗 𝑖 [𝟏] = 𝑓(𝑉)−∑𝑓(𝑉,𝑃)−∑𝑓(𝑉) [𝟐] = ∑𝑓(𝑉,𝑃)−𝑓(𝑃)−∑𝑓(𝑃) 𝑑𝑡 𝑗 𝑗 𝑖 𝑗 𝑑𝑡 𝑗 𝑖 𝑖 𝑖 𝑖 𝑗 𝑗 𝑖 where [1] describes the population growth rate for prey (1st term), reduced by the effects of predation (2nd term) and competitive interactions with other prey j (3rd term) and [2] represents the population growth rate for predators (1st term), decremented by predator mortality (2nd term) and, when present, competition from other predators (3rd term). For example, V could represent 1 white-tailed deer, V elk, whereas P could represent wolves, P mountain lions, and so on. The 2 1 2 shape and dynamics of these functions, f (.), will be determined from field data. Intellectual Merit: I will address questions fundamental to predator-prey theory, and also more broadly, the ecological theory about the role of competition vs. predation in driving population dynamics. For example, I will investigate if functional and numerical responses, thought to be integral to predator-prey theory7,9,13, are sufficient or even necessary to understand predator-prey population dynamics. By applying these models to a broad variety of ecosystems, I will identify general properties that drive not only predator-prey systems, but other consumer-resource relationships14. Moreover, I will help natural resource agencies avoid mistakes stemming from un-integrated management, which can be economically and ecologically costly12. Broader Impacts: Through an increased understanding of how management controls predator- prey population dynamics, wildlife agencies will be able to determine how human harvest strategies of one species will affect others in a food web. Additionally, I will work to establish an accurate public image of large carnivores throughout local communities, and bridge the gap between ecologists-wildlife agencies-citizens. I will do so by giving talks at local high schools, writing articles for newspapers and online blogs, and partnering with local radio/TV programs, much of which I have done in the Falkland Islands (see Personal Statement). In sum, I envision that my work will develop ecological principles general enough to transcend ecosystems, but also specific enough to assist management of the natural resources of local communities. Literature Cited: 1) Mech, L. (1995). Cons. Bio. 9(2):270-278. 2) LaRue, M. et al. (2012). J. Wild. Mgmt. 76(6):1364-1369. 3) Chapron, G. (2014). Science. 346: 1517-1519. 4) Ripple, W. et al. (2011). Science. 343:151- 162. 5) Berger, J. et al. (2001). Science. 291:1036-1039. 6) Shurin, J. et al. (2002). Ecol. Lett. 5(6): 785-791. 7) Abrams, P. & Ginzburg, L. (2000). TREE. 5(278): 535-541. 8) Messier, F. (1994). Ecol. 75(2):478-488. 9) Hebblewhite, M. (2013). Pop. Ecol. 55(4):511-522. 10) Holt, R. (1977). Theor.Pop.Bio. 12(2): 197-229. 11) Chesson & Kuang. (2008). Nature. 456: 235-238. 12) Serrouya, R. et al. (2015). Am. Natl. 185(5): 665-679. 13) Berryman, A. (1992). Ecol. 73(5):1530-1535. 14) Vucetich, J. et al. (2011). J. Anim. Ecol. 80(6):1236-1245.	Winner!
134	Keywords: coral reef; resilience; bleaching; climate change; ecosystem based management Introduction: Coral reefs shelter 25% of all marine species1 and provide food, protect coastlines, and support economic opportunities for over 1 billion people worldwide.2 However, coral reefs face multiple threats. Chronic stressors, including overfishing, coastal development, pollution, and ocean acidification, slowly degrade coral reefs by undermining vital ecological processes, such as grazing by reef fish and coral growth.3 Coral reefs are also threatened by acute stressors, such as cyclones and coral bleaching events, that may severely damage or restructure coral reef ecosystems.2,3 Climate change is predicted to compound these stressors and has already increased average ocean temperatures by 0.9°C globally.4 This increase has triggered three global bleaching events to date, and most coral reefs are projected to face annual bleaching by the 2050s.4 Adaptive and innovative management approaches are needed to ensure the longevity of coral reef communities in an era of global change.5 One potential approach supported by an emerging body of research3, 5-13 is resilience-based management (RBM).7 Under RBM, scientists and coral reef managers use a variety of biotic and abiotic indicators to assess coral reef resilience, where resilience is defined as the capacity of an ecosystem to resist and recover from stress without shifting to a less desirable ecosystem state.6 However, several knowledge gaps related to coral reef resilience hinder application of RBM theory.5-12 Scientists have not reached consensus on which biotic and abiotic indicators are the strongest drivers of coral reef resilience.6, 8 Most assessments of coral reef resilience are predictive, and few studies exist that test the accuracy of these predictions following major stress events.6, 13 Furthermore, these assessments are conducted across a range of geographies using varying methodologies, 5-12 which makes it difficult determine if observed variability in resilience is due to context-specific factors or broad biogeographic patterns. In addition, assessments of coral reef resilience are still evolving to incorporate emerging science on ecosystem thresholds and phase shifts between coral- and algal-dominated states.3, 11, 13 Proposed Research: The destructive 2014–2017 global bleaching event (Event), which impacted 51% of the world’s coral reefs,14 presents a unique opportunity to examine the impacts of major stress events on coral reef resilience. By evaluating changes in coral reef condition across an array of sites before, during, and after the Event, I will (1) assess the relative importance of select biotic and abiotic factors in determining coral reef resilience, (2) examine how the importance of these factors varies spatially among reefs in the central Pacific, and (3) ground-truth existing methods used to predict coral reef resilience. Experimental Design: An effective analysis of coral reef resistance to, and recovery from the Event will require extensive monitoring data that documents biotic and abiotic conditions on coral reefs before, during, and after the Event. As a graduate researcher at the Scripps Institute of Oceanography (SIO), I will be well positioned to access comprehensive monitoring data from past studies and collect data through current monitoring programs. To evaluate coral reef condition before the Event, I will analyze benthic and reef fish monitoring data collected as part of an extensive SIO study that monitored coral reefs across 56 islands in the central Pacific from 2002–2009.15 To assess coral reef resilience during and after the Event, I will analyze photo surveys and fish transect data collected as part of the 100 Island Challenge (Challenge), which is currently surveying 100 islands across a range of environmental and anthropogenic gradients in the Pacific and Caribbean.16 Photo surveys collected as part of the Challenge are orthoprojected to generate comprehensive 3D models of the coral reef benthos that document species composition and spatial arrangement to a resolution of 1cm2. The Challenge surveys sites (many of which were affected by the Event)14, 16 at regular intervals to document ecological changes, and has been monitoring numerous islands since 2013. I will select study sites by cross-referencing monitoring locations from the 2002–2009 study15 with locations currently being monitored by the Challenge (I anticipate this will reveal several dozen sites with sufficient monitoring data). I will evaluate the historic exposure of study sites to chronic stressors using the World Resources Institute’s Reefs at Risk spatial dataset of anthropogenic impacts.9 Similarly, I will evaluate historic exposure to acute stressors before the Event, and bleaching alert levels during the Event, using spatial data produced by NOAA’s Coral Reef Watch. Based on this evaluation, I will classify sites into eight experimental groups (Fig. 1). I will build off existing meta-analyses of RBM studies6, 12 to identify priority resilience indicators to measure, such as coral cover/diversity, herbivore biomass/diversity, coral recruitment, coral disease, macroalgae cover, bioerosion, substrate availability, and topographic complexity.3, 5-12 I will monitor these indicators as a member of the Challenge research team and extract relevant data from the 2002-2009 SIO monitoring dataset.15 Using this data, I will generate site-level averages for each indicator before the Event to determine baseline conditions and analyze indicator variance to assess pre-bleaching trends in ecosystem health. I will compare these values to indicator averages and variance after the Event to identify indicators with a statistically significant impact on coral reef resistance to and recovery from bleaching. I will also analyze spatial variability in site resilience to detect regional or context specific patterns (i.e., which indicators determine resilience and where), a knowledge gap prior studies have not addressed. Lastly, I will examine areas of overlap and divergence between my findings and those of prior resilience assessments. Fig. 1: Sites will be Anticipated Results: I hypothesize that reef resilience will vary classed into 8 groups based on past exposure. depending on each site’s history of chronic and acute stress exposure. In addition, I anticipate that the accuracy of resilience predictions and the relative importance of biotic and abiotic drivers of resilience will exhibit spatial variability at regional and local scales. Intellectual Merit and Broader Impacts: This study will ground-truth RBM theory using empirical data to analyze patterns of coral reef resilience before, during, and after a major environmental disturbance. As a co-author of an RBM study,9 I recognize the potential of this research to address knowledge gaps, and thereby enable scientists to further refine RBM methods to reflect observed patterns of coral reef resilience. This continued refinement will be critical6 as reef managers, communities, and governments move to adopt RBM to inform marine protected area design, fisheries regulations, and other conservation measures7-10 that would have broad impacts for millions of people who depend on coral reef ecosystems.2 In addition, the methods outlined in this study could be modified to test RBM assumptions for other ecosystems threatened by climate change.7 As I conduct this research, I will use connections I have cultivated in the marine non-profit community by collaborating with researchers and practitioners, and draw from my communications experience as an environmental blogger to widely disseminate my findings. References [1] Plaisance et al. 2011. PLoS one, 6(10): e25026. [2] Hoegh-Guldberg et al. 2007. Science, 318.5857: 1737-1742. [3] Anthony et al. 2014. Glob Change Biol, 21: 48–61. [4] Van Hooidonk et al. 2016. Scientific Reports, 6: 39666. [5] Nyström et al. 2008. Coral Reefs, 27: 795–809. [6] McClanahan et al. 2012. PloS one, 7.8: e42884. [7] Maynard et al. 2015. Biological Conserv, 192: 109-119. [8] Maynard et al. 2010. Coral Reefs, 29.2: 381- 391. [9] Harris et al. 2017. Aquatic Conserv, 27.S1: 65-77. [10] Weeks & Jupiter. 2013. Conserv Biol, 27.6: 1234- 1244. [11] Mumby et al. 2013. Conserv Letters, 7.3: 176-187. [12] Lam et al. 2017. PloS one, 12.2: e0172064. [13] Jouffray et al. 2014. Philosph Trans B, 370.1659: 20130268. [14] Eakin et al. 2017. Reef Encounter, 32(1): 33-38. [15] Smith et al. 2016. Proc R Soc B, 283(1822): 20151985. [17] 100islandchallenge.org/study-design/	Winner!
135	One of the most important unknowns in high-z extragalactic astronomy is how reionization occurred; during the epoch of reionization (z≈10–6; EoR), neutral hydrogen (HI) dominates the intergalactic medium (IGM). HI attenuates radiation from early stellar populations, masking galaxies from detection. Understanding how and when reionization occurs can reveal whether or not these young galaxies provided the necessary ionizing radiation to completely reionize the IGM by z(cid:39)6, one billion years after the Big Bang. However, due to the high redshift-space this implies, spectroscopic observations are limited as these galaxies are very faint, with their UV spectral features pushed out to near-infrared (NIR) wavelengths. My background in NIR spectroscopy and observational astronomy has prepared me to assist in addressing this question. I propose using Lyα and CIII] to investigate the properties and ionization state of young galaxies using ground- and space-based telescopes, the structure and distribution of HI in the IGM and the circumgalac- tic medium (CGM) of certain galaxies, and implications for the evolution of the neutral fraction of the IGM throughout the EoR. The individual points proposed will be summarized as follows: (i) small scale testing and building of an analysis technique, (ii) distribution of galaxies and evolution of neutral fraction, and (iii) metallicities of galaxies. By understanding more about the IGM during the reionization era and of the galaxies within it, we can further constrain the properties of current galaxy evolution and reionization models. Small scale testing & building of analysis technique: In the search for galaxies during the EoR, Lyman-α (λ =1216˚A; Lyα) traditionally has been the best tracer both in photometric 0 surveys and spectroscopic follow-up. This is in part due to the increasing fraction of UV bright galaxies (with strong Lyα emission) with increasing redshift [1]. By measuring the escape fraction of Lyα many studies have inferred an increasingly neutral fraction of the IGM at z>6.5 [2]. One complication of Lyα is its attenuation due to HI, pushing emission hundreds of km/s redwards of the galaxy’s systemic (or true) redshift. A recent method uses a complementary spectroscopic tracer not attenuated by HI, with the UV metal line CIII] (λ =1907,1909˚A) as the most robust according to mid-z (z(cid:39)2–3) analog surveys [1,3,4,5]. 0 In my current work, I measure CIII] H-band emission of galaxies found via Lyα emission using Keck+MOSFIRE [6]. With both measurements, I compare the systemic (CIII]) and attenuated (Lyα) redshifts, shedding light on the structure and ionization of the CGM of these galaxies and surrounding IGM. From my previous work experience, I have developed a proficiency in coding which enabled me to gain a close familiarity with the MOSFIRE data reduction pipeline (DRP), having to dive into the sourcecode to fix bugs often encountered when working with incredibly faint emission lines and less common dithering patterns for standard star observations. I wrote code to optimally extract my 1D spectra, adapted from Horne (1986), boosting the S/N of my detection. Using a marriage of IRAF and Python, I developed code that can track the photometric variability of my data from a frame-to-frame basis – important when working with faint emission lines. Distribution of galaxies and evolution of neutral fraction: Using the foundation built from my previous work, I will build a statistical sample of galaxies during the last half of the reionization era (z(cid:39)8–6.5) in order to track the evolution of the Lyα escape fraction as a function of redshift. Using my optimized extraction technique to improve Taylor A. Hutchison 2 measurements, I will use this dataset to constrain the offset between these galaxies’ systemic and attenuated redshifts. This work will significantly increase the sample of high-z galaxies with both Lyα and CIII] measurements. In addition, it will provide a more significant comparison with z(cid:39)2–3 LAEs and LBGs, mid-z analogs commonly used in these analyses. A current complication for this project is the lack of a complete spectroscopic sample of LAEs at z≥6.5 with even fewer galaxies with systemic measurements (via UV metal lines or the [OIII] doublet; λ = 4959,5007˚A). This is partly due to these lines being redshifted to 0 NIR and mid-IR wavelengths; the latter is impossible to detect with ground-based telescopes – it is useful to note that during the EoR, both [OIII] and the UV metal lines fall in the range of NIRCam on JWST. As a first approach to resolving this, I will take the current sample of confirmed LAEs at z(cid:39)6.5–8 and measure their CIII] emission, using my optimized extraction technique to improve measurements. This has already been attempted for some galaxies [4], providing useful lower limits for determining exposure times and potential telescopes for future observations, including JWST. We are planning proposals for the first JWST cycles for this work. I will then take advantage of the deep multi-wavelength imaging campaigns available to me, including the CANDELS datasets [7]. Moreover, I am part of a proposal to increase the sample of z∼7 galaxies with Keck+MOSFIRE, with early indications that the proposal has so far been successful. Finally, as a scientific collaborator on an ERS JWST proposal, I will prepare for access to that data – understanding what spectra I will be looking for from running binary stellar population models (eg. BPASS), scaled to match expected bandpass magnitudes, through the JWST exposure time calculator. With a large, statistical sample of LAEs at z(cid:39)6.5–8 with both Lyα and CIII], I will be able to further constrain the amount of hard radiation emitted from these galaxies; as shown with mid-z analogs, this is closely linked to the transmission of Lyα through the CGM [5]. By tracking its evolution through the last half of the EoR, I can inform current reionization models. Lastly, through gathering my sample I will map out the distribution of these galaxies, identifying whether galaxies with large escape fractions are tracing over-dense luminous regions, located within large ionized bubbles [3,8]. Metallicities of galaxies: Using my high-z sample of galaxies with deep ground- and space- based spectroscopy, I will study the metallicity of galaxies in the EoR. From a ratio of the fluxes of the CIII] doublet, when measurable, I can infer an estimate of the electron density of the gas in the CGM. This is closely linked to the metallicity of the CGM, which directly affects the velocity offset of Lyα emission. Not only do recent studies indicate that a neutral CGM attenuates Lyα photons, diminishing the effect the IGM will have, they also suggest (from mid-z analogs) a strong link between the profile of Lyα emission and the properties of the gas within the CGM [5]. This can be incredibly important as some high-z galaxies have been found to have more symmetric Lyα profiles, contrary to the archetypal asymmetric shape, thought to be indicative of high star formation and galactic winds [6]. Understandingthe rateand distributionof reionization, including thefactors andprocesses responsible for it, remains one of the most important unknowns in extragalactic astronomy. My work will aim to shed more light on this question, enabling more precise modeling of thiserawiththeintentionofprobingeverfurtherbackintimetowardstheyoungestofgalaxies. References: [1] Stark et al. 2017 [2] Dijkstra 2014 [3] Stark et al. 2015ab,2016 [4] Matthee et al. 2017 [5] Erb et al. 2014 [6] Finkelstein et al. 2013 [7] Grogin et al. 2011 [8] Furlanetto et al. 2004	Winner!
136	The Eccentricity Distribution of Long-Period Brown Dwarf Companions Keywords: brown dwarfs, direct imaging, radial velocity Abstract: I propose to measure the eccentricity distribution of long-period (1-500 yr) brown dwarfs. I will accomplish this measurement by modifying the Orbits for the Impatient algorithm and applying it to calculating posterior eccentricity probability density functions (PDFs) for a statistically large sample of long-period brown dwarfs. I will then aggregate these PDFs to determine the underlying eccentricity distribution of long-period brown dwarf companions. These results will allow direct comparison with the empirical distribution of exoplanets discovered by radial velocity and direct imaging, allowing insights about planetary formation, and will provide opportunities for statistical inferences about individual systems. Background and Motivation: Whether brown dwarfs form like large planets or small stars is key to understanding the process of exoplanet formation. Brown dwarf eccentricities can provide insight into formation scenarios, since brown dwarfs with high eccentricities are likely to have experienced close encounters with other orbiting objects early in their lifetimes (Morbidelli 2014), but in spite of this, little analysis has been done to characterize the eccentricities of individual brown dwarfs, let alone the population of brown dwarfs at a wide range of orbital separations. In order to study the formation scenarios of brown dwarfs and make statistical inferences about the population of brown dwarfs with respect to the population of giant exoplanets, it is therefore necessary to have accurate eccentricity measurements from a large sample of brown dwarfs orbiting stars. Fortunately, several long-period brown dwarfs have already been observed with both the radial velocity and direct imaging methods, but few orbital fits intended to characterize their orbital eccentricities have been attempted, in many cases due to their long orbital periods, for which only short orbital arcs have been observed. In order to provide these empirical constraints, I propose to use the NSF Graduate Research Fellowship to compute eccentricity PDFs for each of a statistically large sample of long- period brown dwarfs using our novel Bayesian technique, Orbits for the Impatient (OFTI; Blunt et al 2016). Since many systems will only have observations covering a fraction of a full orbital period, the uncertainty on the eccentricity of any given system will be relatively large, but with a large sample of systems, the underlying eccentricity distribution of the population can be measured to much higher precision than for any one object. I will combine the eccentricity PDFs to compute the physical distribution of the eccentricities of brown dwarfs, and determine whether this distribution is consistent with or qualitatively different from that for long-period giant planets. Methods: Combining data from radial velocity and direct imaging measurements can result in precise determinations of orbital eccentricities (Nielsen et al 2016), so the first step in my analysis will be to modify the OFTI algorithm, which produces orbital element PDFs from direct imaging data, to fit orbits to data sets composed of both radial velocity and direct imaging measurements. Once I’m prepared with this necessary tool, I will compile NSF Research Statement Sarah Blunt radial velocity and direct imaging data on >40 brown dwarf companions from the literature, and supplement these by reducing and analyzing unpublished data. Stanford is ideally suited for accomplishing this work, since there are many individuals at Stanford and in the Gemini Planet Imager Collaboration (including GPI PI Bruce Macintosh, and collaboration members at Stanford and nearby institutions Eric Nielsen, James Graham, Robert De Rosa, and Jason Wang) who are experienced with reduction of both radial velocity and direct imaging data, and since Stanford has access to the high-performance computing resources necessary for robust and efficient data reduction. Once I have compiled reduced direct imaging and radial velocity data for a sufficient number of brown dwarfs, I will run the modified version of OFTI on each data set to produce a posterior PDF in eccentricity for each brown dwarf. I will then aggregate these posterior PDFs to obtain the physical probability distribution of long-period brown dwarfs, testing the effectiveness of several functional models at reproducing this distribution, and so determine for the first time the functional form of the long-period brown dwarf eccentricity distribution. Anticipated Results: The most direct application of the calculation of the physical eccentricity distribution of long-period brown dwarfs will be a comparison with the eccentricity distribution of exoplanets detected by the radial velocity method (Nielsen et al 2010), and corresponding analysis to determine the probability that the two empirically determined distributions are the same. Additionally, the orbit fits to the selected brown dwarfs will in many cases be the first measurements of eccentricity, semi-major axis, and relative inclinations of these systems, which will be useful for understanding interactions between the substellar companions and circumstellar disks (e.g. Rameau et al. 2016). Orbit fits can also be used to calculate the probabilities of the existence or absence of additional bound exoplanets or brown dwarfs orbiting a given star (e.g. Bryan et al 2016), and guide future observations of these systems. Proposed Timeline: Year 1: Modify OFTI and compile brown dwarf astrometric and RV data. Year 2: Publish early results for individual systems and finish fitting orbits to remaining systems. Year 3: Produce eccentricity distribution of long-period brown dwarfs, publish comparison to planet population. References: Blunt, S., Nielsen, E. L., De Rosa, R. J., et al. 2016, ApJ (submitted) Bryan, Bowler, Knutson, Kraus, Hinkley, Mawet, Nielsen, Blunt (2016). ApJ, 827, 100 Morbidelli, 2014, Phil. Trans., DOI: http://dx.doi.org/10.1098/rsta.2013.0072 Nielsen, E. L. & Close, L. M. 2010, ApJ, 717, 878 Nielsen, E. L., De Rosa, R. J., Wang, J., et al. 2016, AJ, eprint arXiv: 1609.09095 Rameau, J., Nielsen, E. L., De Rosa, R. J., et al. 2016, ApJL, 822, 2	Winner!
137	Keywords: Chlamydomonas reinhardtii, flagella assembly, kinase, length regulation Introduction: Recent discoveries concerning cilia assembly suggest complex signaling pathways play a prominent role in cilia length regulation and function.1 Far less is known about the about the kinases that regulate these pathways. The proposed research will attempt to uncover the signaling pathways responsible for growth and regulation by establishing which kinases and mechanisms are responsible for the regulation of cilia length. Background and Rationale: Cilia and flagella are found on almost every cell in the human body and consist of microtubules that extend from the cell surface. Cilia are typically divided into two types, primary and motile, which both sense extracellular signals. Primary cilia, found on the majority of cells in the human body are immobile. Motile cilia, found on the majority of epithelial cell surfaces, create wave-like patterns to propagate fluid. As flagella and motile cilia have identical structures, the words cilia and flagella are used interchangeably. The process of assembling cilia, ciliogenesis, is highly regulated as the centrioles that nucleate cilia are also required for cell division. The mechanisms regulating ciliogenesis, including initiation, assembly and resorption, are poorly understood. Learning more about these mechanisms will facilitate the study and treatment of diseases involving ciliary dysfunction. Flagella of the unicellular green alga Chlamydomonas reihardtii are essentially identical to cilia of vertebrate cells and provide an excellent model to study ciliogenesis. Chemical studies in Chlamydomonas have demonstrated length-regulating roles for G-protein coupled receptors2. Similarly, flagella assemble in a length-dependent manner, with rapid early assembly and very slow assembly as they approach their steady-state length. The rate of disassembly is length independent and the length at which assembly and disassembly are in equilibrium is Figure 1. Preliminary kinase inhibitors with known as the balance point3. To identify kinase increased rate of assembly during regeneration. pathways that affect flagellar assembly, we performed a small-molecule screen using a kinase inhibitor library. Preliminary data show inhibiting Protein Kinase A and G causes an increased rate of flagellar assembly during regeneration over of 2 hours as compared to wild type. Also, inhibiting Protein Kinase C causes a slower rate of assembly during regeneration at 1 hour as compared to wild type (Figs. 1,2). We hypothesize these inhibitors target regulators that control the switch from rapid to slow assembly rates. Reversing this switch has the potential to rescue defects caused by slow or impaired assembly. Aim 1: Validate Targets and Phenotypes with Novel Inhibitors and Activators: To confirm phenotypes and identify the kinases responsible for the observed phenotypes, we will use different inhibitors of the same targets. In contrast to the kinase inhibitors, the activation of these kinases should show us the opposite effects, confirming the targets and phenotypes previously identified (Figs. 1,2). Following pH shock to induce flagellar loss and regrowth, we will treat Chlamydomonas cells (CC125) with inhibitors and activators of protein kinase A, G, C and Figure 2. Preliminary kinase inhibitors tyrosine kinases (Table 1). Flagella will be imaged using with decreased rate of assembly during regeneration. automated phase contrast microscopy and flagellar length Table 1. Inhibitors and Activators of measurements will be performed using ImageJ software. With proposed targets to be used in Aim 1 these experiments, we expect to confirm the data seen in Figures 1 and 2 while helping us to further confirm these kinases as regulators of the switch from rapid to slow assembly. Aim 2: Identify Regulatory Pathways for Flagellar Assembly: As many of the kinases identified in the preliminary screen have both cytoplasmic and nuclear downstream targets, we will identify which subset of targets are responsible for the flagellar assembly phenotype. To discriminate between the targets, we will use inhibitors from the preliminary screen simultaneously with cyclohexamide, a protein synthesis inhibitor that will prevent gene expression in downstream transcription factors (Table 2). Next, we will perform an epistasis experiment by inhibiting or activating a preliminary target as well as a potential downstream targets to see if they are in the same pathway. The process of pH shock and flagellar length measurement will be followed according to the steps described in Aim 1. These experiments will narrow down the pathway components regulating the switch from rapid to slower assembly rates. Aim 3: Determine the Role of Identified Kinases in Trafficking of Flagellar Cargo: We will use total internal reflection fluorescence (TIRF) microscopy to Table 2. Cytoplasmic and determine the role of kinases in trafficking of flagellar cargo by Nuclear compounds to be used assessing the preliminary targets’ effect on transportation of tagged for experiments in Aim 2. proteins in regenerating flagella. We will treat the cells with the kinase inhibitors during flagellar regeneration and use TIRF imaging to compare the amount of cargo traveling from the base to the tip of the regenerating flagella by quantifying fluorescence intensity of tagged cargo.4,5 Results from this visualization and quantification of tagged cargo will identify the mechanism with which identified pathways regulate flagellar assembly. Intellectual Merit/Broader Impacts: My familiarity with the culture conditions and flagellar phenotyping of the model organism Chlamydomonas reinhardtii will facilitate the proposed experiments. I will gain the necessary skills to perform TIRF microscopy through future mentoring from Dr. Avasthi. The initial microscopy work in the outlined project, allows Rockhust University undergraduate students participating in research at the University of Kansas Medical Center to be trained on microscopy. The findings from these experiments impact the science community through the identification of fundamental principles of ciliary regulation. Society is influenced by these findings as they will provide the foundation for the treatment of diseases of ciliary dysfunction. Results will be shared in relevant conferences, preprints and publications. Also, the proposed experiments and results, will be shared with undergraduate students at Rockhurst University with the intention of using relevant basic science research to engage future students. This will capture their attention and spark their interest for research opportunities at the University of Kansas Medical Center. Support from the NSF through the Graduate Fellowship Research Program will promote my success as a future scientist by facilitating research during my graduate career, but will also benefit society by providing a more approachable path to science careers for women. [1] Nachury,Maxence V.(2014) Philosophical Transactions of the Royal Society B: Biological Sciences 369.1650 [2] Avasthi, Prachee et al. (2012). ACS Chemical Biology 7.5 [3] Marshall, Wallace F. et al. 2005.Molecular Biology of the Cell 16.1 [4] Engel, Benjamin D. (2009) Method Cell Biol.93 [5] Avasthi, Prachee et al. (2014).Curr Biol. 24.	Winner!
138	Understanding the Role of the Cytoskeleton on Intracellular Particle Dynamics Introduction – Intellectual Merit: The dynamics of the cytoplasm, which includes the cell chromosome and other intracellular particles is relevant to many biological processes, including cell replication and genomic control. However, these systems exist out of equilibrium and show complex dynamic behavior, not easily explained by classic dynamic theory. Recently, new fluorescent labeling techniques have allowed intracellular particles to be tracked as they move though the cell1,2. These experiments, done on E. coli, have shown that that these particles show sub-diffusive behavior. It seems that the cell environment confines the movement of these particles, and blocks them from fully exploring their surroundings. Various theories have been proposed to explain this behavior.3,4. These depend mostly on studying polymer models for the chromosomes and treating the cytoplasm as a viscoelastic medium with a simple memory kernel. This continuum approach has shown some success in reproducing the sub-diffusive behavior of particles. However, there is no need to introduce an artificial memory kernel, the cytoplasm can be modeled explicitly using colloidal models. Then, the particles are treated as colloids moving in a Newtonian medium (water). Then the constraints that induce the caging on these particles and lead to sub-diffusive behavior can be inserted explicitly. There are two factors that could lead to this diffusive behavior. The first is the fact that the particles are confined in the cell. This confinement could lead to a reduced ability to explore certain parts of the cell. Additionally, the cell cytoskeleton would induce an additional major hindrance to their movement. The existence of the bacterial cytoskeleton has only recently been recognized5. However, by now it is well understood that prokaryotes have analogues to most components of the eukaryotic cytoskeleton. These help give the cell it’s structure, and connect different parts of the cell with each other. However, they are also major sources of hindrance in the movement of the cell particles. It is appropriate to use a colloidal model of the cytoplasm, since many large biomolecules easily fall in the colloidal size regime. The hindered diffusive behavior is analogous to that seen in classic colloidal glassing6, and could be explored using the same fundamental theory. Goal: We wish to develop a simple colloidal model to study particle dynamics within a cell, including the role of confinement and the cytoskeleton. This model will demonstrate that this confinement, along with the additional interference of the cell cytoskeleton leads to the sub- diffusive behavior shown in experiments. To achieve this, we will first use existing polymer models to properly study the behavior of the bacterial cytoskeleton. Then Brownian Dynamics studies of a simple cell model would be realized, and the confined movement of the particles in the model cell would be studied. Finally, the role of hydrodynamics would be explored, using newly developed theory and computational methods. Objective 1: Match Cytoskeleton Dynamics to Polymer Models: The first step is to develop a good model for the cytoskeleton, which is a complex network of polymer chains that connect different parts of the cell. A convenient way to model the behavior of these polymers is to use the shearable stretchable Worm Like Chain (ssWLC) model developed by the Spakowitz Group to study general semi-flexible polymers7. This general model allows one to study a wide range of polymers at many timescales. Using their methodology7 to match the known chemical structure of the components of the bacterial cytoplasm5, we can obtain a simple polymer model applicable for Brownian Dynamics, which we will use in the following simulations. Objective 2: Study Particle Diffusion in Cellular Environment: The cell can be modeled as a sphere, and the cytoplasm can be modeled as a colloidal solution inside this sphere. The sphere would have polymers, whose dynamics follow the ssWLC model in a network analogous to the bacterial cytoskeleton5. Then the dynamics of the colloidal solution would be explored using Brownian Dynamics, a classic simulation methodology in colloidal physics. Various factors can affect the overall dynamics of the solution. The first is the concentration, which would be kept neat to cellular concentrations. Additionally, the exact structure of the cytoskeleton is likely to be very relevant. Various randomized structures would be used to study this effect. Finally, a single particle would be used as a probe, and its movement though the cell would be studied to determine if it shows sub-diffusive behavior. Objective 3: Explore the Role of Hydrodynamics: An important factor in the dynamics of a colloidal system is hydrodynamics. These can be included in the Brownian Dynamics simulation through the use of the Accelerated Stokesian Dynamic methodology8. This methodology includes the full effects of hydrodynamics. A challenge is exploring the role of the confinement in the hydrodynamics. Fortunately, the relevant mobility functions have recently been published9.. These would be used along the classic particle-particle mobility, which are well known8, and can be extended to the polymer model10. Since these computations are likely to require significant computational power, they would be parallelized using newly developed methods11. Broader Impacts: Understanding the dynamics of the intracellular environment could lead to increased understanding of genome expression. This in turn could lead to new understanding of many genetic diseases and their mechanism of action. Every effort would be undertaken to undergraduate students in this project. Several parts, including the managing of simulations, can be easily performed by a student new to the field, and could serve as a great learning opportunity. This would be done through REU and other programs for underrepresented students. All papers published from this project will be made available to the wider public using Open-Access publication models. References: 1. Weber, S. C. et al. 2010. Physical Review Letters 104, 238102. 2. Kuwada, N. J. et al. 2013. Nucleic Acids Research 41 (15). 3. Lampo, T. J. et al 2015. Biophysical Journal. 108. 4. Tampo, T. J. et al 2016. Biophysical Journal 110. 5. Cabeen, M. T. et al. 2010. Annual Reviews of Genetics 44. 6. Parry, B. R. et al. 2014. Cell 156 (1-2) 7. Koslover, E. F. 2013. Soft Matter. 9, 7016 8. Banchio, A. J. et al. 2003. The Journal of Chemical Physics. 118 (10323) 9. Aponte-Rivera, C. et al. 2016. Physical Review Fluids 1 (2). 10. Nieves-Rosado, L. et al 2016. Unpublished Work 11. Bülow, F. et al. 2016. Computer Physics Communitcation. 204.	Winner!
141	Deriving Language Signatures for Bilingual Code-Switching Keywords: Code-Switching, Probabilistic Language Models, Sociolinguistics ​ Research Question: How do bilingual speakers of the same language pairings code-switch between them differently? More specifically, what components can be extracted from bilingual data to differentiate speakers of the same languages? Background: Linguistic scholars have observed that there is wide variation in code-switching (CS) due to social differences (Gardner-Chloros, 2009). For example, Post (2015) observed variations in frequency and type of CS as a function of gender among Arabic-French speakers in Morocco. Unfortunately, findings like these have been restricted to specific languages and small datasets and until recently, there have been no tools to classify CS at the level of large corpora (Gambäck & Das, 2016). Furthermore, there have been no attempts to distinguish the unique CS patterns presented by speakers, i.e., individual language signatures. The key problem is that CS can include small word-level insertions of single lexical items or long stretches of dialogue across several speakers, which make its study difficult as speakers can vary their patterns of speech considerably from one utterance to another. I propose to address these gaps in the study of CS by applying statistical models to extract and extrapolate patterns from bilingual corpora. The crux of my approach is to look at CS as a sequence of language spans, in which a speaker remains in one language before switching to another. Solorio and Liu (2008) have previously exploited this idea to predict switch points and to tag for Part-Of-Speech, yet their approach made no attempt to distinguish or identify different patterns. Outside of current analyses at the level of corpora I do not know of any statistical approach to studying bilingual CS across speakers that exploits this idea of language spans. By adapting this concept to the alternation of language at the individual level and not corpora, I believe that it is possible to distinguish the CS of one bilingual speaker from others to produce a distinctive language signature, regardless of small changes in speech. Hypothesis/expected findings: Based on my previous work with the Killer Crónicas, Yo-Yo ​ ​ ​ Boing!, and Bon Cop, Bad Cop datasets, I hypothesize that CS can uniquely characterize ​ ​ individual speakers and that the relative frequencies of language spans across speakers are distributed differently, with some speakers preferring spans of one length to another. I expect that speakers of the same language pairings vary enough in the length of languages spans that there are statistically significant differences in the speech of two bilingual individuals. I anticipate that there are also several variables contributing to these differences such as region, attention paid to speech, and social factors. My methods to extract different features of CS and provide unique signatures of CS across bilingual speakers will be language-neutral and applicable across different language pairings. Approach and Methods: The first component of this project will be the gathering of a large number of bilingual corpora involving CS in order to introduce as much variation as possible. Given my previous work in language annotation, I am free to work with larger, untagged datasets so long as enough training data exists. By far the largest collection of such datasets is the Linguistic Data Consortium (LDC), which charges a fee for unlimited use and access. The remaining resources are access to powerful computing resources (or XSEDE access) and the expertise of faculty members working on CS and statistical language models. After preliminary analysis of the datasets, I expect distinct patterns of CS to emerge across speakers such as switching differently around breaks in speech, which will lead to differing patterns in the corresponding language signatures. At this point, I will work to examine Gualberto A. Guzman Graduate Research Plan Statement NSFGRF 2016 bilingual CS with statistical models by looking at the distribution of language spans per speaker and by examining the probability of switching in a discourse as a function of time, both of which necessitate large datasets. Assuming the simplest case, the distribution of language spans of a speaker can be modeled as a stochastic exponential decay after normalization for length of text. I expect that different speakers will present different rates of decay in their language, resulting from varied use of language spans. In addition, by looking at bilingual text as a series of switches between monolingual spans, I will model CS as a Poisson process and find the most apt parameters for individual speakers by working backwards from their speech. I will tune different stochastic models to speakers in order to find statistically significant differences in speech patterns and I plan to subject the data to a rigorous analysis of different stochastic models to test my hypotheses. I will also perform a regression analysis to find correlations between the social and environmental factors mentioned above and any differences from the models. Intellectual Merit/Broader impacts: A Graduate Research Fellowship will allow me to ​ promote further research between the fields of Linguistics and Mathematics. My work will inspire the development of a general framework with which to examine cases of switching phenomena within Linguistics. It will have broad impacts in computational linguistics, sociolinguistics, and bilingualism both as a tool and as a theoretical construct. My model will contribute a new, language-neutral approach to examining bilingual CS, freeing researchers from being constrained by the availability of data in dominant languages like English. In addition, my proposal has potential contributions to the fields of linguistic methodology and linguistic anthropology. Its development may lead to the possibility of deconstructing seemingly homogenous language or CS use into discrete subgroups by geography, ancestry, or culture. Finally, it must be noted that the development of my model need not be restricted to the study of switching between two languages. My ambition is to generalize my model to work with as many languages as needed. In addition, a refined version of my proposed model would be able to uniquely identify changes in style, dialect, or register given enough training data. As an example, learners of a second language could apply the principles of my model to pinpoint exactly where their usage differs from that of a native speaker, which provides a new possibility for accelerated language learning and for the study of second-language acquisition. Björn Gambäck and Amitava Das. 2016. Comparing the level of code-switching in corpora. Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), pages 1850–1855. Penelope Gardner-Chloros. 2009. Sociolinguistic factors in code-switching. In Barbara E. Bullock and Almeida Jacqueline Toribio, editors, The Cambridge handbook of linguistic code-switching , pages 97–113. Cambridge University Press, Cambridge, UK. Thamar Solorio and Yang Liu. 2008. Learning to predict code-switching points. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 973–981. Association for Computational Linguistics. Thamar Solorio and Yang Liu. 2008. Part-of-speech tagging for English-Spanish code-switched text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1051-1060. Association for Computational Linguistics. Rebekah Elizabeth Post. 2015. The impact of social factors on the use of Arabic-French code-switching in speech and IM in Morocco. Ph.D. thesis, University of Texas at Austin.	Winner!
142	Interferometric Reflectance-Based Nanoparticle Imaging with Patterned Illumination Introduction: A significant issue in current medical standard-of-care is the accurate detection of infectious diseases. Viruses, bacteria, parasites, and other microorganisms causing these diseases are difficult to detect directly due to their micro- and nanometer length scales. Existing diagnostic techniques typically rely on indirect detection through monitoring bulk tissue changes in a patient, analyzing biological samples in vitro, or determining an infection based on the patient’s symptoms and immune response. While these techniques are effective in certain cases, indirect detection methods increase the difficulty of achieving a proper diagnosis which can lead to harmful consequences for patients [1]. One such primary diagnostic tool experiencing this limitation is the optical microscope. New optical technology has improved microscopy’s capabilities in imaging small-scale objects, but many modern systems have become diffraction limited. Diffraction limits occur when the particles of interest are smaller than the imaging wavelength of light. This sizing issue results in light scattering that prevents nanoparticles from being resolved with conventional microscopy techniques. This limit has been bypassed previously using methods such as fluorescence microscopy, where the particle of interest is indirectly detected by imaging a fluorescent dye that has been bound to the particle. Such techniques are successful, but they have significant drawbacks including the need for extensive sample preparation, augmentations to the sample prior to analysis, and expensive imaging hardware [2]. These factors create significant barriers of entry for these modalities from becoming common disease diagnosis platforms in developing and developed countries. Thus, a substantial need exists for an affordable diagnostic platform capable of nonspecifically detecting nanoscale biological particles. Proposal: I propose a new microscope design combining the imaging modalities of Single- Particle Interferometric Reflectance Imaging Sensors (SP-IRIS) and Fourier Ptychography (FP) Microscopy for high resolution, high throughput imaging of biological nanoparticles. SP-IRIS, developed in Dr. Selim Unlu’s lab at Boston University, utilizes wide-field interferometric imaging techniques to acquire weak scattered light signals from nanoparticles over a large sample region. These signals provide information regarding nanoparticle geometry and have been used for label-free detection of viruses at attomolar concentrations (Figure 1). These factors make SP-IRIS a desirable option for both large sample virus diagnostics and biological nanoparticle characterization applications. However, drawbacks including the requirement of mechanical sample scanning and device limitations in detecting differences between floating and Figure 1: Label-Free Figure 2: Standard adhered nanoparticles limit the system’s current Virus Particle Microscope (Top) and abilities as a diagnostic tool [1]. Visualization with SP- FP-Reconstructed IRIS Microscope [1] (Bottom) Image [3] Fourier Ptychography techniques could remove these existing issues in SP-IRIS technology. FP is a computational microscopy approach wherein different angled illumination patterns are projected on the sample via an LED array to obtain low-resolution image sets. These images can be recombined to create images with higher resolution and wider field-of-view than standard microscope techniques (Figure 2). These angled NSF Research Statement Alex Matlock illumination measurements also enable tomographic and 3-D reconstruction of the imaged sample. With the capabilities of FP in achieving near real-time imaging while providing high- resolution images, the synthesis of FP with SP-IRIS could create a highly sensitive and specific nanoparticle detection platform with volumetric information regarding each particle [3]. These additions would remove the need for depth sectioning in the SP-IRIS system and would allow the user to differentiate between floating and static particles as well as provide additional information for nanoparticle characterization. Year 1: Proof-Of-Concept Prototype The first year will focus on proof-of-concept research illustrating the successful combination of SP-IRIS and FP. I have already constructed an SP-IRIS bench-top microscope and will be validating the instrument’s operation prior to adding FP. This modification will require the addition of a programmable LED array for angled illumination, adapting FP algorithms for reflection microscope geometries, changing SP-IRIS forward modeling to use FP images, and determining whether volumetric FP results are viable with SP- IRIS imaging methods. This year’s goals will be achieved when floating and static customized carbon nanotubes can be identified with the system and an improvement in particle visualization is achieved with the combined system over SP-IRIS alone. Year 2: System Design and Speed Improvement: The primary work in this phase will focus on achieving real-time imaging using the combined software platforms from both modalities. Additional hardware and software modifications will likely be necessary to determine whether different illumination patterns, LED arrays, lens setups, or other aspects of the system can improve the imaging quality or speed. This year’s success criteria will be satisfied once real-time imaging of floating carbon nanotubes in a microfluidic channel is achieved. This phase can be extended into Year 3 if additional time is required for real-time imaging with the system. Year 3: System Validation in Biological Particles: The third year will investigate the device’s applications in biological imaging. The system will be tested for its sensitivity to biological particle detection and characterization of different nanoparticles. The throughput and speed of this system will also be tested by analyzing samples with increasing particle counts under different fluid flow conditions. Should this device exhibit reliable results in identifying and characterizing biological samples, the use of this device in clinical trials at Boston University’s medical hospital will be explored. Intellectual Merit: Achieving high-resolution, high throughput imaging of nanoparticles would open opportunities for nanoparticle imaging in many other scientific fields including the semiconductor industry. This technology also uses relatively low-cost optical components allowing other research facilities to build their own systems. This research will be published in research journals and presented at conferences. Broader Impacts: This technology would be viable as a low-cost, high sensitivity and specificity diagnostic platform for infectious diseases. The high throughput capabilities of this proposed device would be significant for detecting diseases with low concentrations of biological markers in the body. The results from this project will also be published in multiple journal articles and presented at optics-focused and biological research-related conferences. [1] Avci, O., Ünlü, N. L., Özkumur, A. Y., & Ünlü, M. S. (2015). Interferometric Reflectance Imaging Sensor (IRIS)--A Platform Technology for Multiplexed Diagnostics and Digital Detection. Sensors (Basel, Switzerland), 15(7), 17649–65. http://doi.org/10.3390/s150717649 [2] Jesus, D. M.; Moussatche, N.; McFadden, B. B. D.; Nielsen, C. P.; D’Costa, S. M.; Condit, R. C. Vaccinia Virus Protein A3 Is Required for the Production of Normal Immature Virions and for the Encapsidation of the Nucleocapsid Protein L4. Virology 2015, 481, 1−12. [3] Tian, L., Liu, Z., Yeh, L.-H., Chen, M., Zhong, J., & Waller, L. (2015). Computational illumination for high- speed in vitro Fourier ptychographic microscopy. Optica, 2(10), 904. http://doi.org/10.1364/OPTICA.2.000904	Winner!
143	Introduction: Understanding the atmospheric questions about the fate of ISOPO in differing 2 oxidation of organic compounds is important NO regimes. x for predicting the production of tropospheric To combat this, I aided in the development ozone and organic aerosols, both of which im- of a field-hardened high resolution time-of- pact our health and climate. Though the for- flight CF O- CIMS coupled with a low pressure 3 mation of these two constituents involves many gas chromatograph (GC-HR-ToF) that is capa- compounds, isoprene plays a dominant role due ble of observing isomer distributions of various to its large biogenic emissions and rich oxida- oxidation products in ambient air. An initial tive chemistry. However, it is the complexity of field test has provided promising preliminary its oxidation scheme that serves as a major chromatograms of several isoprene products source of uncertainty when making predictions, and my plans to continue the development of generating chemical models that fall short of this instrument will allow it to serve as an inval- replicating ambient observations especially near uable analytical tool, allowing for an increased isoprene-dominated environments1. understanding of ISOPO chemistry and its ef- 2 One reason for this uncertainty is due to its fects on global air quality. peroxy radical (ISOPO ), formed predomi- 2 nantly through the oxidation of isoprene with OH. ISOPO is known to undergo three individ- 2 ual reaction pathways, with the likelihood of each strongly dependent on the availability of NO . In areas where NO levels are decreasing x x (e.g., the United States), the isoprene chemistry is being shifted towards a system dominant in autoxidation and HO reactions, resulting in the 2 production of species such as isoprene epoxides Figure 1: Instrument schematic of GC-HR-ToF. Green boxes represent mass flow controllers; blue circles repre- (IEPOX), which contribute to the formation of sent valves; arrows indicate direction of analytical flow. secondary organic aerosols2. Analogously, as (A) GC column; (B) Heating/cooling unit; (C) Radioac- NO x emissions increase in more pristine areas tive ionizer; (D) Glass flowtube; (E) ToF mass analyzer. (e.g., the tropics) ISOPO 2 begins to favor its re- Instrument Description: The GC-HR-ToF action with NO, either increasing the ozone pro- uses a 1-meter column with its resulting effluent duction potential of the area or forming reser- sampled directly into the CIMS. During collec- voirs, like isoprene nitrates (ISOPN), that can tion, the sample is cryofocused near the en- transport NO x elsewhere with unknown effects. trance of the column via a custom-built heat- Observations of ISOPO 2 products in the at- ing/cooling unit and the temperature of the cold mosphere can shed light on the relative im- trap is controlled through alternating CO cool- 2 portance of the reaction pathways that produce ing and resistive heating. Furthermore, during them. The utilization of clustering ion chemistry the trapping phase, ambient air is also simulta- in conjunction with chemical ionization mass neously analyzed by the CIMS, allowing for di- spectrometry (e.g., CF 3O- CIMS) has allowed rect comparison between the GC sample and low fragmentation sampling of important multi- traditional measurements. The overall instru- functional isoprene products considered too ment schematic is shown in Figure 1, with a fo- fragile to be detected otherwise3. However, be- cus on the GC components I constructed. cause isoprene oxidation results in eight Preliminary Results: Our first field test oc- ISOPO 2 isomers, CIMS cannot distinguish be- curred during PROPHET 2016, a campaign held tween the isomeric products, causing ambiguity at the University of Michigan Biological Sta- in the data provided and leaving unanswered tion. Sitting on the PROPHET tower 30 meters NSF GRFP 2017 1 Krystal Vasquez Graduate Research Plan Statement 6 10 14 Retention Time [min] Figure 2: (A) Peak assignment of ISOPOOH/IEPOX (blue) and ISOPN (red, signal x3) for data collected on 23 July, 10:00 EDT; (B) Sample of peak identification of ISOPOOH/IEPOX (black) using known product ions (red & blue, signal x2) from data collected on 23 July, 14:00 EDT above ground, the GC-HR-ToF sampled above be able to obtain isomer-specific measurements the tree canopy obtaining flux measurements of of isoprene products in areas with a spectrum of various compounds. Even more, in situ isomer NO concentrations. This will be key in under- x distributions of isoprene products (e.g. isoprene standing both the favorability of ISOPO path- 2 hydroxy hydroperoxides (ISOPOOH), IEPOX ways and its subsequent effects on air quality, and ISOPN) were observed for the first time in particularly in areas where VOC/NO ratios can x ambient data. I have performed preliminary be effected by increasing emissions, the peak assignment using known product ions2 to transport of NO or air quality regulations. x identify the compounds (Figure 2). The isomer Conclusion & Broader Impacts: The data I distributions determined by this data set will will obtain using the GC-HR-ToF will provide provide information regarding the fate of a critical test of our current grasp of isoprene ISOPO in a Northern Michigan forest influ- chemistry, adding to the current kinetics used in 2 enced by both pristine air from the north and models to improve predictions. In addition, its high-NO pollution from nearby urban centers. use by several atmospheric groups at Caltech, as x Research Plan: Though overall successful in well as collaborators, will extend the use of this the field, I plan to further improve the GC cry- new technique beyond our initial focus, demon- otrapping system. Despite the fact that the col- strating both its versatility and benefit to the sci- umn separated the product isomers, the high hu- entific community. midity levels seen in Michigan served as a ma- Lastly, as this instrument aims to better un- jor obstacle during sample collection; trapped derstand some of the fundamental science be- water easily degrades the chromatography by hind isoprene chemistry, it provides an oppor- hydrolyzing isomers and overshadowing the tunity for me to enhance my science communi- visibility of early eluting peaks. Though I per- cation skills. By using my blogging platform, I formed constant on-site adjustment to minimize plan to discuss the various aspects of atmos- this effect in the field, improvement of the tem- pheric chemistry and atmosphere-biosphere in- perature control and automating its adjustment teractions in order to teach a demographic com- with ambient humidity will make the GC system posed of young students and future scientists more robust in the field. about a field they may have otherwise never Afterwards, I propose to commission the in- been introduced to. Additionally, social media strument to continuously sample ambient air on has been and will continue to be utilized when Caltech campus (an urban high NO environ- out in the field to give my readers a peak into x ment) while also preparing for a summer collab- the day to day workings of an atmospheric oration at Indiana University (a rural low NO x chemist. References: (1) Horowitz, L. W. et al. JGR: regime). During both field experiments, as well Atmos. 112, 13 (2007). (2) Bates, K. H. et al. JPCA 118, as with future campaigns, the GC-HR-ToF will 1237-1246 (2014). (3) Crounse, J. D. et al. Anal. Chem. 78, 6726-6732 (2006). NSF GRFP 2017 2 langiS yrartibrA 1 0.5 0	Winner!
144	Introduction: The Channeled Scablands is a striking landscape that captures a remarkable moment in Earth’s history when enormous quantities of glacial meltwater poured across the region. These glacial floods carved deep canyons, referred to as coulees, into basalt bedrock within the otherwise subdued topography of the Columbia Plateau. The most impressive of these, Grand Coulee, is the largest flood-carved canyon on Earth at 200 m deep and nearly 100 km long. Once thought to be glacially-carved, the recognition that Grand Coulee formed due to the upstream erosion of what must have been one of the largest known waterfalls on Earth [1] brought about a revolution in geological thinking by proposing that catastrophic events—rather than slow, uniformitarian processes—can dominate the evolution of Earth’s surface. Though a catastrophic flood origin of the Grand Coulee is now accepted, many questions still remain regarding the size and number of floods that carved it [1]. I propose to use cosmogenic nuclide exposure dating to measure the retreat rate of the Grand Coulee waterfall, and to combine field evidence of sediment transport with numerical flood models to constrain the discharge of the outburst floods that carved Grand Coulee. This approach will address longstanding questions concerning the role of catastrophic events in shaping Earth’s surface, make inferences about the hydrology of early Mars (which contains canyons similar in form to Grand Coulee). Additionally, my project will highlight the process of scientific discovery to the public via Grand Coulee’s status as a National Natural Landmark and a key feature on the Ice Age Floods National Geologic Trail. Questions: The project will address two fundamental questions regarding the role of catastrophic floods in eroding Grand Coulee. Question 1: Did Grand Coulee form geologically instantaneously during a single flood, or by flooding throughout the last ice age or even earlier glaciations? Question 2: What was the magnitude of the flood(s) that carved Grand Coulee and how did flood volume change as the landscape evolved via canyon incision? Research Plan: I propose to integrate field, geochronological, and numerical modeling methods to unravel the geomorphic history of the Upper Grand Coulee, under the advisement of Dr. Isaac Larsen at the University of Massachusetts. Question 1 will be addressed using primarily geochronological methods. Determining exposure ages along the length of the canyon rim of Upper Grand Coulee will constrain the location of the waterfall as it retreated upstream. I have collected samples of fluvially-transported granite boulders and flood-carved basalt surfaces from the study area for exposure dating [2]. Granites will be processed for 10Be dating in the UMass Cosmogenic Nuclide Laboratory, and basalts will be dated using 3He in labs of collaborators. If the waterfall experienced gradual retreat in response to multiple floods, we expect to find a decrease in exposure ages of flooded basalt surfaces with distance upstream. Alternatively, similar ages of flooded basalt along the rim of Upper Grand Coulee would support a more rapid landscape response driven by a single flood or several floods occurring in rapid succession. Question 2 will be addressed by field and numerical methods. I will use a 2D, depth-averaged shallow water hydraulic script to numerically simulate floods of varying magnitudes in Grand Coulee, with discharges ranging from the minimum required to barely inundate the canyon floor, to that which fills the canyon to the brim. This script will be run on the Massachusetts Green High Performance Computing Center, a supercomputer accessible from UMass. Field measurements will be used to determine which of these modeled discharges is most consistent with the geochronological evidence. I measured boulder dimensions on depositional bars in Grand Coulee, and will use these to constrain the threshold bed stresses and flood discharges required for their transport. Similarly, field measurements of basalt columns and physics-based estimates of bed stresses required to erode the bedrock channel floor will be used to constrain the canyon-forming discharge [3]. These discharge constraints will allow me to assess whether Grand Coulee was filled to the brim by floods, as is often assumed in flood reconstructions, or whether smaller, but still exceptional, floods carved the canyon. Moreover, the geochronology will allow me to independently assess the predictions of my modeling, as the dating will indicate whether the appropriate paradigm of incision requires huge, brim-full floods or smaller floods. Intellectual Merit: The evidence to support extensive flooding in the Channeled Scablands is overwhelming, but it remains a challenge to quantitatively constrain the pace and timing of the evolution of the bedrock landscape and the coevolution of flooding and canyon incision [3]. Our understanding and interpretation of the roles that floods of varying magnitudes may have played in generating the topography of the Channeled Scablands therefore remains far from complete. By addressing the magnitude of the floods that carved Grand Coulee, my work will address long- standing questions regarding the balance between catastrophic and gradual processes in shaping topography. There is great value in constraining the discharge of megafloods: large freshwater releases can alter ocean circulation and trigger abrupt climate change [4], so understanding the magnitude of paleo-floods is key to understanding Earth’s past climate and sensitivity for future climate change, given current ice melting in polar regions. Additionally, understanding the processes and formation rates of Grand Coulee can yield insight into the evolution of the much larger Martian Outflow Channels and contribute to a clearer picture of the volume of water that flowed on the surface of early Mars, where direct dating is not yet possible. Broader Impacts: The story of the megafloods is exciting, coherent, and illustrative of the nature of scientific research, and outreach on this topic can motivate the next generation of scientists. In fact, watching a documentary on the Channeled Scablands played a major role in my own decision to pursue graduate research. Outreach will therefore be a significant broader impact of this work. I participate in UMass’ Graduate Women In STEM’s Science Café, and already have multiple Channeled Scablands presentations scheduled at a local middle school, through which I hope to ignite interest in science and make research relatable. I will also work with Eureka!, a branch of Girls, Inc., which runs summer programs at UMass for pre-college girls from an underserved community. I will play a leading role in developing a series of local discovery-based field trips to introduce girls to earth science, which is largely absent in the standardized state curriculum. Floods from ice and landslide dam failures are hazards worldwide, and the quantitative methods for estimating flood discharge developed in my work can be directly transferred to smaller floods, and thereby used to assess geohazards using paleoflood evidence and to predict risks from future flooding scenarios. As the usefulness of scientific research is limited until it is communicated to policymakers and members of the public, I will post project updates to my field blog to convey the research process, discuss my findings, and provide a personal perspective of geoscience. I also intend to collaborate with other researchers to develop a field trip to the Scablands at a national conference to showcase current research on the megafloods, and will work with state parks in the Grand Coulee area to develop interpretive displays to communicate the story of the Scablands’ dynamic past to the public. References [1] Bretz, J. 1932. American Geog. Soc. 15. [2] Lal, D. 1991. Earth and Plan. Sci. Letters. 104, 424-439. [3] Larsen, I. and Lamb, M. 2016. Nature. 538, 229-232. [4] Barber, D.C., et al. 1999. Nature. 400, 344-348.	Winner!
147	The evolution of similar traits in distantly related species is one of nature’s great surprises. Convergent evolution of traits has been widely observed throughout the animal kingdom; however, it is often unclear whether this phenotypic convergence results from convergent evolution of genes (Stern, Nat Rev Genet 2013). On a molecular level, convergent evolution occurs when the amino acids of a specific protein preferentially undergo mutations that produce similar or even identical amino acid sequences within distinct evolutionary lineages. A high level of adaptive convergent evolution – that is, convergence due to positive selection of beneficial mutations – would suggest that some genes have “optimal configurations,” which evolution uses and reuses across species. If such genes exist, then evolution is somewhat predictable, proceeding by one of a small number of possible paths (Stern & Orgogozo, Science 2008). In contrast, a complete absence of adaptive convergence would indicate that protein configurations beneficial to one species are seldom optimal within other species, and that evolution may proceed by a much wider set of paths. Thus, two fundamental questions in evolutionary molecular biology are 1) how often and in which genes convergent molecular evolution occurs and 2) whether molecular convergence is a primary driver of phenotypic convergence. Most analyses of convergent evolution have been limited to single genes (Li et al., Curr Biol 2010) or small taxa (Bazykin et al., Biol Direct 2007); to date, no genome-wide analysis involving a wide variety of species has been completed. Recently, the advent of whole- genome sequencing has opened new opportunities for exploring molecular convergence. Published genomes now exist for over 100 animal species, and 177 more are currently underway (Koepfli et al., Annu Rev Anim Biosci 2015). Thus, a genome-wide search for convergent mutations across multiple species is now possible, and might reveal new evidence of adaptive molecular convergence. I plan to develop and apply a novel computational framework to test for convergent evolution among 61 sequenced mammalian species. I hypothesize that convergent molecular evolution occurs at a higher rate than has been previously observed, and that this genetic convergence drives convergence of observable traits. This framework will extend the boundaries of biological knowledge by quantifying the frequency of convergent evolution, and will augment existing phylogenetic methods in a broadly applicable framework that can be extended to other genomes in the future. Aim 1: Develop a novel computational framework for identifying convergent evolution between genomes. My framework will be generalizable to any data set with the following inputs: 1) a phylogenetic tree for a set of species, and 2) the pairwise sequence alignments of all proteins in those species. I will limit my analysis to genes unambiguously alignable to a one-to- one human ortholog in at least 80% of mammalian genomes. I have verified that even after filtering on these criteria, >50% of all human genes are retained. My analysis will integrate these data as follows:  Step 1: Infer ancestral sequences. For every amino acid in every sequence, I will compute amino acid sequences at each ancestral node in the phylogeny using the linear-time maximum parsimony method implemented in PAML 4 (Yang, Mol Biol Evol 2007).  Step 2: Infer patterns of adaptive convergent evolution between species pairs. For every pair of species within our analysis, for each gene, I will identify the set of amino acids that converged in that pair of species with probability >0.9, as well as those that diverged with probability >0.9, based on the ancestral sequences inferred in Step 1. I will report a convergence score for the gene in this pair of species: the ratio of convergent mutations to divergent mutations. Thus, genes with high rates of both divergent and convergent mutations (such as rapidly evolving immune genes) will score lower than those with relatively higher rates of convergent mutations. After computing the full distribution of convergence scores for every gene in every species pair, I will denote the upper outliers as convergence events.  Step 3: Evaluate model performance on simulated data. I will simulate evolution of protein sequences in which a small fraction of the sequences evolve under a non-neutral convergence pattern and the rest evolve neutrally. I will measure my framework’s precision and recall in inferring which sequences evolved under the convergent model. If my framework is able to detect convergence events in the simulated data at a low false discovery rate, but observes no convergence events in the biological data, then these results will cast doubt on the hypothesis that adaptive convergence is a significant driving force in molecular evolution. Aim 2: Quantify the levels of convergent evolution within mammalian genomes, and identify functions enriched within genes evolving in convergence.  Step 1: Identify specific genes that have evolved in convergence across species. Using public alignment tools (Kent et al., PNAS 2003), my collaborators in the Bejerano Lab at Stanford have provided cross-species sequence alignments and a phylogenetic tree for 61 mammalian species. I will apply my framework from Aim 1 to all pairs of sufficiently diverged mammalian species and identify genes within each species-species pairing that exhibit non- neutral convergence. I hypothesize that I will find evidence of adaptive convergent evolution within many genes and within almost all species-species pairings.  Step 2: Identify convergent genotypes potentially responsible for known convergent phenotypes. If evolutionary parallelism is truly adaptive, this would suggest that genes converge when they undergo similar selective pressures within different species (Castoe et al., PNAS 2009). Therefore, I hypothesize that genes facing similar pressures within independent species will be more likely to evolve in convergence within those species. For example, in aquatic mammals such as dolphins and manatees, we might expect high convergence of skin-expressed genes involved in thermoregulation. Indeed, my preliminary analysis has identified 4 convergent mutations in the gene TGM1 in dolphins and manatees, a significantly greater number than expected by chance. Human mutations in this gene cause a skin disease called ichthyosis, characterized by fish-like, scaly skin (Laiho et al., AJHG 1997). In order to find similar cases, I have identified a set of 20 convergent phenotypes that arose independently in mammals, such as adaptation to high altitudes or dry environments. For each of these phenotypes, I will identify genetic convergence events that may be responsible for the phenotype in question. If my hypothesis is correct, this analysis will suggest functions for which similar selective pressures across species have necessitated similar courses of protein evolution across these species. The specific amino acid substitutions that I identify will then be prime targets for further examination in functional assays, as described by Liu et al. (Mol Biol Evol 2014). Broader Impacts: This project will help us to understand whether phenotypic convergence is a direct result of genotypic convergence. I will create a publicly available, user-friendly visualization for the UCSC Genome Browser that highlights hotspots of convergent evolution. This tool will allow biologists to visually explore instances of adaptive molecular convergence and to ask even deeper questions about the specific functional roles and phenotypic effects of these convergent mutations.	HM
152	Catherine Alves | October 2016 Keywords: Coral reefs, conservation, community-based fisheries, sustainability, survey analysis Proposed Research: Does community-based fisheries management restore ecological function and improve the livelihood of fishers in Belize? Background: Overfishing is a significant threat to the world’s ocean ecosystems (1) that has caused an 80-95% reduction in large predatory fish biomass (2). This not only disrupts ecosystem functioning but threatens invaluable commercial and subsistence fisheries that provide livelihoods and fish protein to nearly 3 billion people annually (3). Marine reserves are one tool designed to mitigate these impacts and to meet both biodiversity conservation and fisheries management goals. Marine reserves function by restricting fishing access with the intention of increasing fish abundances and diversity within no-take zones, ideally with fish spilling over into adjacent non-protected areas (4). However, poaching, lack of enforcement, and limited spillover often limit the broader success of marine reserves (5). An emerging approach is to more directly involve and incentivize local communities in the restoration and management of overfished stocks. Such “TURFs” (“Territorial User Rights for Fishing”) assign local fishers the rights to fish in designated areas in exchange for reporting their catch. These initiatives encourage environmental stewardship in coastal communities by providing effective ownership of fish stocks, further incentivizing sustainable fishing practices (6, 7). TURFs have been implemented worldwide by the Environmental Defense Fund (EDF), but little is known about their effectiveness, particularly in the tropics where implementation is only beginning (6). Research to examine the impact of TURFs from ecological and social perspectives is limited (6, 7), despite catch improvements reported by fishers participating in the program. TURFs have been designed to prevent the “race to fish” oftentimes accompanying small-scale fisheries because they assign catch shares to fishers. Furthermore, by assigning fishers locations to fish, poaching decreases in restricted areas, enabling fish populations to recover (6, 7). In 2011, the first TURFs in the Caribbean were established by the Belize Fisheries Department and they incorporated The Port Honduras Marine Reserve (est. 2000) and the Glover’s Reef Marine Reserve (est. 1993) (7). The Belize Fisheries Department is currently in the process of implementing a nation-wide TURF system, adding seven additional TURFs to pre- existing marine reserves (7, www.fisheries.gov/bz/#). The purpose of my study is to quantify the efficacy of Belize’s newly implemented TURFs in restoring overfished stocks, general biodiversity, and ecosystem functioning as well as in improving the livelihood of fishers. Specifically, I will test the following hypotheses: H : Fish species richness, density and biomass will be greatest in locations with TURF 1 implementation and lowest in unmanaged control sites. H : TURF implementation will improve the perception, livelihood, and catch per unit 2 effort (CPUE) of fishers who participate in the TURF program versus those who do not. Study Design: Visual fish surveys will be performed in the nine TURF locations plus nine unmanaged control sites (7, www.fisheries.gov/bz/#). Fish species richness, density and biomass will be quantified via underwater transect surveys using SCUBA. At all sites, I will quantify 1 ecological factors that could influence coral reef community structure and potentially compromise co-management efforts – such as reef structural complexity, temperature, chlorophyll, macroalgal cover and human population density (Cox et al., in review). The quantitative social science surveys will consist of structured interviews of 100 individuals randomly selected from four stakeholder groups: fishers participating in the TURF program, fishers not participating in TURFs, natural resource managers, and scientists (8). Closed-ended questions will be asked of all survey respondents to collect socio-economic, demographic and perceptions data including income, number of years in profession, gender, and perceived goals of the TURF program. Specifically, fishers will be asked to identify fishing locations on a map, provide CPUE, and share the percent of their income that comes from fishing. I will use the multilevel, nested framework of studying social-ecological systems (9) to build Bayesian hierarchical models to quantify the relationship between covariates. It is crucial to include fishers in management decisions because they become resources of change in their communities (5, 6, 7). A key component to the success of this project will be my partnership with the Belize Fisheries Department, the University of Belize, and local non-profits like Belize Healthy Reefs – all of whom are currently collecting CPUE data at locations where fishers sell their catch to vendors. During a research trip to Belize this past summer, I began to make connections with individuals at all of these institutions, with intentions to collaborate in the future. My partnership with local contacts is essential for establishing trust among the community because it will increase the likelihood that the fishers, managers, and scientists will consent to the study (5, 9). This collaboration will also enable the survey questionnaire to be implemented in the local languages of English, Spanish, and Belizean Kriol, therefore reaching different communities in Belize. In addition, I will draw upon data collected by my PhD advisor, Dr. John Bruno, who has conducted long-term monitoring research in Belize across 16 sites with varying levels of marine protection. Broader Impacts: This study will advance the field of community-based fisheries management by providing natural resource managers and fishing associations with insights into the efficacy of the TURF program in Belize from social and ecological perspectives. Information gleaned from this study has the potential to maintain livelihoods of the commercial and subsistence fishers in Belize while preserving coral reef fish biodiversity. I will also incorporate public outreach and education to increase scientific literacy and engagement of the public, both among the public in Belize and in my local community in North Carolina. I will collaborate with local institutions to co-organize public forums, workdays and outreach events for citizens of Belize to educate them about their local marine ecosystems. For outreach within my community in North Carolina, I have already developed a lesson plan for grades 8-12 on marine food webs for the Scientific Research and Education Network (SciREN). I hope to incorporate the findings of this study into a different lesson plan that focuses on marine resource management decision making. Both of these outreach programs will show the public the importance of interdisciplinary conservation science, and encourage environmental stewardship among the next generation. Community- based environmental management techniques are emerging across the globe as some of the most promising ways to combat anthropogenic threats to ecosystems, and I look forward to becoming a part of that endeavor. References: 1. Jackson, J.B.C., et al. Science 293:629-638 (2001). 2. Valdivia, A., et al. PeerJ PrePrints 3:e805v1 (2015). 3. FAO. The State of World Fisheries and Aquaculture: Opportunities and Challenges p. 243 (2014). 4. 2 Gaines, S.D., et al. Proceedings on the National Academy of Sciences 107(43):18286-18293 (2010). 5 Valdés- Pizzini, M., et al. Caribbean Studies 40(2):95-128 (2012). 6. Barner, A.K., et al. Oceanography 28(2):252–263 (2015). 7. Foley, J.R. Proceedings of the 12th International Coral Reef Symposium: Evaluating Management Success (2012). 8. Bernard, H.R. Research Methods in Cultural Anthropology ch. 9 (1988). 9. Ostrom, E. Science 325:419-422 (2009). 3	Winner!
154	"OPTIMIZING THE SYNTHESIS OF METAL ORGANIC FRAMEWORKS USING SEGMENTED FLOW TUBULAR REACTOR TECHNOLOGY Keywords: metal organic framework, scale up, continuous flow processes I. BACKGROUND The segmented flow tubular reactor (SFTR) is an exciting breakthrough in the realm of nano-scale technology. I first encountered this reactor during my research at Sandia National Labs (see personal statement), where it showed much promise in scaling up the synthesis of titania nanowires. Due to the limitations of reactor size, high chemical costs, and uncontrollable side effects of chemical impurities, mass production of nanoparticles remains an ever increasing challenge of engineers today. The expansion of a typical batch process will often lead to low quality synthesis due to the inability to control particle size and morphology under heterogeneous conditions. This same challenge is also paralleled within the metal organic framework (MOF) industry. How can MOFS, having synthesis routes and material properties that depend on nucleation at a reaction surface, be produced at a large scale where quality is often sacrificed for quantity? Fortunately, the SFTR is able to address this issue in much the same way as it did for nanomaterials. The key principle behind the design is the segmentation of the reactants into micro-reactors in a continuous tubular process. Each microvolume is separated by an immiscible fluid or gas as shown in Figure 1. Within this arrangement, borrowing from the concept of ‘plugs’ in plug flow reactors, reagents are perfectly mixed in the radial direction but not in the axial direction, thus removing the possibility of axial Fig. 1. Schematic Representation of SFTR [1]. back-mixing, and ensuring that all reactants are endure a similar history (residence time and heat exchange). This process allows the synthesis of homogeneous products with narrow particle size distributions, enhanced control of particle morphology, polymorph selectivity and better stoichiometry control. II. MOTIVATION Current literature demonstrates that continuous MOF synthesis is possible, even at the scale of several kilograms per day [2]. The largest MOF manufacturing company, BASF, whose pilot plant is located in Germany, can produce MOFs on order of several kilograms per batch using the solvothermal method. What will be done as the demand for these hydrogen-capturing materials increases, especially as the United States is becoming an increasingly hydrogen-based economy [3]? NSF GRFP Research Proposal III. RESEARCH PLAN a. Year I –Determine Optimal Reacting Conditions for Common MOF Compounds It is desirable to know exactly what operating conditions the SFTR is expected to perform under before a prototype reactor can be prepared, and thus its effectivity in optimization tested. The respective MOF compounds under analysis include, Mn3[(Mn4Cl)3(BTT)8]2, Zn4O(BDC)3, Cu3(BTC)2(H2O)3. These should all be explored due to their ability to their differing levels of hydrogen storage capacity and differing size and geometry. The optimal reacting conditions will also be depend on what pore size is desired for the respective MOFs and what substrate is being used in synthesis. b. Year II- Determine Optimal Reactor Conditions for the MOF Synthesis Whereas year I focused exclusively on the chemistry of the MOFs to be prepared and demonstrating that specific reaction conditions generate material with the desired properties, year II will be one of matching those conditions to that of a SFTR. Here, I will be sizing an SFTR that meets all of the requirements given the reaction conditions. c. Year III- Design a Bench Scale Model and Test Performance At this point in research, it is expected that the prototype reactor can be developed as as bench scale model. Necessary parameters such as tube size, material construction, volume, pumping efficiency and the need for a cooling or heating bath have been determined based on analysis in years I and II. The performance will have to become compared against traditional MOF synthesis routes such as lab scale layer-by-layer deposition, and microwave synthesis. IV. ANTICIPATED RESULTS Just as the segmented flow tubular reactor has shown much promise in optimization of calcium carbonate nanomaterial production [1], it is expected that, it will also be successful regarding synthesis of the three chosen MOF compounds. The many similarities between MOF synthesis and nanoparticle synthesis is what will be exploited in this study, to hopefully achieve similar results. Solvothermal synthesis is useful for growing crystals suitable to structure determination, because crystals grow over the course of hours to days. It is expected that by optimizing the SFTRs models to each respective MOF mechanism, a continuous production of uniform and low–defect material can be achieved over this same length of time or even a shorter duration. V. INTELLECTUAL MERIT & BROADER IMPACTS The implications of this research project are far reaching, even beyond its potential to mass produce MOFs and thus meet the demand of our growing hydrogen economy. It has the potential to introduce an energy efficient way of producing compounds designed for energy efficient applications to begin with. Double threat! This projects represents the cross between the chemistry and chemical engineering discipline to address the issues in sustainability that plague our nation. The scale-up of MOFs is not an area that has not been widely studied, and thus this research study represents a much needed contribution to the world of sustainable chemistry. NSF GRFP Research Proposal References: [1] “Precipitation of nanosized and nanostructured powders: process intensification using SFTR”, applied to BaTiO3, CaCO3 and ZnO - Chem. Eng. & Techn., 34(3) 344-352 (2011). [2] “BASF Develops Method for Industrial-Scale MOF Synthesis; Trials Underway in Natural Gas Vehicle Tanks.” Green Car Congress. 5 October 2010. [3] ""Global Hydrogen Fuel Cell Electric Vehicle Market Buoyed as OEMs Will Launch 17 Vehicle Models by 2027, IHS Says"". IHS Inc. 4 May 2016. Retrieved 13 May 2016."	Winner!
155	cues on relationship perception and intent biases Men are more likely to perceive a woman’s friendliness as sexual interest, and this pattern holds up across surveys, actual behaviors, and beyond lab conditions [1-3]. Although communication about sexual interest have always been complicated, they recently have become legal and societal issues. A more complete understanding of how individuals communicate about sex is necessary, especially when 23.1% of college women experience sexual assault [4]. Intellectual Merit Error management theory (EMT) explains this “sexual overperception” effect in men as a strategic bias favoring specific types of judgment errors over other types of errors. Differential parental investment theory [5] states that male mammals are less physically obligated to invest in offspring, so they tend to be more willing to engage in sexual activity, whereas females are more selective about potentially costly sexual activity. For males, the error of “missing” an interested female is costlier than the “false alarm” error of judging an uninterested female as interested, resulting in a pattern of decisions that adaptively reduces costs and increases benefits, even as it fails to minimize errors overall. EMT is, at its core, Signal Detection Theory (SDT) applied to intersexual relationships [6], using differential parental investment to model the costs and benefits of relationship decisions. SDT is a way to describe how observers judge the presence or absence of a “signal” when the given stimuli have some level of ambiguity (“signal + noise”) [7]. The division of EMT from SDT has resulted in an unnecessarily restricted analysis of data that could present a fuller explanation of behavior if analyzed using signal detection models. Although EMT, like SDT, considers different judgments and possible outcomes, it ignores several extensions and implications which a full SDT analysis can provide. For instance, EMT does not consider the base rates of signals compared to noise (that is, the frequencies with which signals and noise occur in the environment), and how that influences judgments. Very common true signals, with rare non-signals, will encourage signal-present judgments in ambiguous situations (known as a liberal bias). Conversely, a low signal rate and common non-signals will encourage no-signal judgments (known as a conservative bias). Additionally, SDT provides a measure (sensitivity) of how well people distinguish signals from noise. One benefit of this approach is that concepts already developed within SDT can transfer to EMT contexts. At the theoretical level, SDT specifies situations in which both men and women should have systematically different signal detection strategy profiles. Individuals with faster (v. slower) life history strategies, more unrestricted (v. restricted) sociosexuality, and more short-term (v. long-term) mating orientation should show more liberal biases (Hypotheses 1-3). Similarly, people high in mate value should show a liberal bias because their experience is of a higher signal base rate (H 4), which EMT cannot predict as it does not take signal/noise ratio into account. It also is possible to manipulate aspects of the social situation, and thus the value of decision outcomes, by manipulating the attractiveness of the stimuli used as signals (H 5) and by changing the signals-to-noise base rates through exposure to different sex ratios of stimuli (H 6). Additionally, methods and analyses from SDT research can be used to more fully understand and analyze existing EMT results. For preliminary results, I analyzed the data from Perilloux, et al. [8] using SDT. This confirmed that men are more liberally biased in perceiving sexual interest, but also yielded unanticipated insights: Women are more sensitive to the difference between sexual interest versus non-interest (d’ in Figure 1), and -surprisingly- both men and women in this study are conservatively biased in perceptions of sexual interest (c in Figure 1). Differential parental investment theory predicts why men have a lower sensitivity than women, as females may conceal their signals of sexual interest, making it more difficult for men to differentiate signal from noise. This also may explain why men are more liberally biased than women, since they need to compensate for their lower sensitivity to maintain the same level of optimality at detecting sexual interest. This difference in sensitivity led to an additional hypoth- esis that men’s sensitivity will increase as the woman’s sexual cues become more overt (H 7). Methodological Approach – My stimuli will include 96 video clips showing heterosexual pairs engaging in conver- sations. Each videotaped person will rate their sexual interest in their conversation partner, then complete questionnaires to evaluate individual differences (described above). Method: Studies will involve participants watching the muted clips and rating each actor regarding their levels of sexual interest in their conversation partner. Multiple study variations will look at influences of the observers’ life history strategy (H 1), socio- sexual orientation (H 2), mating strategy (H 3), and mate value (H 4). Additionally, experimentally manipulated sets of clips will be shown to evaluate the causal effects of skewed ratios of attractiveness of each conversant (H 5) in the video clips, prior exposure of participants to skewed sex ratios (H 6), and exposure to overt sexual cueing (H 7). Analysis will use both EMT and SDT methods, utilizing multilevel probit regression to determine c and d’ for this repeated measures design. [9] Broader Impacts Underrepresented Minorities in STEM: Efforts will be made to recruit underrepresented and first-generation undergraduates as research assistants, who will be encouraged to learn about the research process, present results at conferences, and participate in authorship of publications. Increasing Scientific Literacy and Public Engagement with STEM: Research about romantic relationships often gets public media attention, which will be used to broadly communicate the results of this research and bring attention to current directions in psychological science. This research will also be presented at regional and national conferences. Improving Individual Well-Being: This SDT approach will increase knowledge about the abilities and biases different people have about sexual communication, empowering individuals to make informed, healthy decisions about their sexual and relationship behaviors. Identification of individuals and situations where sexual interest and intents are often misinterpreted will aid in locating at-risk populations, improving sexual assault prevention policies, and inhibiting interference with the right to receive an education free from discrimination through sexual harassment and sexual violence (per Title IX of the Education Amendments of 1972). References: [1] A. Abbey, J. Pers. Soc. Psychol.42, 830-838 (1982). [2] A. Abbey, Psychol. Women Quat 11, 173- 194 (1987). [3] M. G. Haselton, J. Res. Pers. 37, 34-47 (2003). [4] The Association of American Universities, Report on the AAU Campus Climate Survey on Sexual Assault and Sexual Misconduct (Westat, Rockville, MD, ed. 2, 2015). [5] R. L. Trivers in Sexual Selection and the Descent of Man, B. Campbell Ed. (Aldine, Chicago, IL. 1972), 136–179. [6] D. Nettle in Evolution and the Mechanisms of Decision Making, P. Hammerstein, J. R. Stevens Eds. (MIT Press, Cambridge, MA, 2012), 69-79. [7] D. M. Green, J. A. Swets, Signal Detection Theory and Psychophysics, (Wiley, New York, NY, 1966. [8] C. Perilloux, J. A. Easton, D. M. Buss, Psychol. Sci. 23, 146-151 (2012). [9] L. T. DeCarlo, Psychol. Methods 3, 186-205 (1998).	Winner!
156	studies of the RAG complex led to my interest in the evolutionary origins of adaptive immunity. V(D)J recombination is the process responsible for generating the massive diversity of antigen receptors that characterizes the vertebrate immune system. RAG1 and RAG2, the protein prod- ucts of recombination activating genes 1 and 2, cooperate to initiate V(D)J recombination in lymphoid cells by making double-stranded breaks at recombination signal sequences (RSSs)(1). The recombinational DNA rearrangements catalyzed by RAG have long been biochemically lik- ened to the cleavage reactions effected by transposases (TPs)(1). In 1998, the demonstration that RAG displays transposition activity in vitro strongly suggested that this likeness can be ex- plained by homology and that RAG is a descendant of an ancient transposable element(1). Due to extensive sequence divergence, a close homolog of RAG within the modern diversity of TPs evaded detection until targeted PSI-BLAST searches linked the RAG1 core to the Transib family of TPs(2). Subsequent biochemical analysis of Hztransib, a Transib transposon active in the ge- nome of the corn earworm, revealed that, like RAG, Hztransib TP cleaves DNA through nicking and hairpinning steps that produce blunt transposon ends and hairpinned flanking ends(3) (Fig. 1). Additionally, insertion events create CG-rich 5-bp target-site duplications, as is typical for RAG(3). These results conform to expectations of a RAG-like TP, but it is reasonable to suppose that some of RAG’s properties are specific to V(D)J recombination and do not describe an ances- tral TP. I propose to conduct an exhaustive biochemical analysis of the Transib transposon in the Yale Department of Molecular Biophysics & Biochemistry, in the laboratory of David Schatz, who discovered and biochemically characterized RAG1 and RAG2. Biochemical similarities be- tween RAG and Transib can lend further support to their homology. Biochemical differences can suggest which functional aspects of RAG are evolutionarily recent recombinase-specific innova- tions, perhaps due to association of RAG1 with other factors (e.g. RAG2) or perhaps due to structural changes within the endonuclease itself. Aim 1: Determine the substrate requirements for Hztransib TP activity. Each RSS comprises a conserved heptamer and nonamer separated by a nonconserved spacer of either 12 or 23 bp(1). RAG’s activity is governed by the 12/23 rule: cleavage can only occur if both a 12- and a 23-RSS are present(1). Hztransib TP has already been demonstrated to cleave at paired 12/23 RSSs (unpublished data in the Schatz lab), but other RSS combinations have not been tested. To evaluate Hztransib TP’s adherence to the 12/23 rule, I will incubate purified Hztransib TP pro- tein with DNA substrates containing various combinations of 12- and 23-RSSs, and I will deter- mine the efficiency of cleavage by visualizing and characterizing radiolabeled DNA products on a denaturing polyacrylamide gel. In this and all other described experiments, a negative control reaction will contain no endonuclease, and a positive control reaction will use RAG as the endo- nuclease. RAG activity is highly dependent on conservation of the first 3 bp of the heptamer (CAC), while flanking sequences have little effect on cleavage efficiency(1). I will investigate the precise sequence requirements of Hztransib TP by quantifying cleavage of DNA substrates with various point mutations in the RSSs and their flanking DNA. The sequences with greatest cleav- age efficiency will likely approximate Hztransib’s own terminal inverted repeats (TIRs), which resemble RSSs and begin with the same CAC sequence. Accordingly, for all described experiments, I will compare reactions that use RSS-containing substrates to reactions using TIR- containing substrates to determine the sequence dependence of any effects I observe. Aim 2: Determine structural characteristics of Hztransib TP’s catalytic state. RAG can nick individual RSSs, but completion of cleavage via hairpin formation can only occur in a synaptic complex containing a 12- and a 23-RSS(1). To assay Hztransib TP for nicking and hairpinning activity in the absence of synapsis, I will immobilize low concentrations of biotinylated DNA substrates containing a single 12- or 23-RSS on streptavidin agarose beads, and I will character- ize products after addition of TP. I will then add free DNA substrates to the slurry to assay for cleavage activity with specific synaptic pairings. RAG’s cleavage efficiency is greatly enhanced by the DNA-bending high-mobility-group protein HMGB1 because cleavage requires DNA dis- tortion(1). I will add HMGB1 to standard Hztransib TP cleavage reactions and observe its effect on cleavage efficiency. Following RAG cleavage, the four newly created DNA ends remain syn- apsed in a postcleavage complex(1). To probe for an Hztransib postcleavage complex, I will bi- otinylate specific DNA ends, pull down biotinylated cleavage products with streptavidin agarose beads, and characterize any unbiotinylated DNA species that are also pulled down. Aim 3: Determine secondary nuclease activities of Hztransib TP. In vitro, RAG exhibits vari- ous nuclease activities besides cleavage at RSSs: it cleaves single-stranded heptamers, it cuts off 5’-ended overhangs on duplex DNA, and it removes 3’-terminated single-stranded flaps(1). By incubating Hztransib TP with representative radiolabeled substrates and characterizing products, I can determine whether Hztransib TP also exhibits these activities. Aim 4: Suggest catalytic and regulatory roles for RAG2. While RAG1 requires RAG2 for activity(1), Hztransib TP bears sequence similarity only to RAG1(2) and is able to effect cleavage without supplementary protein factors(3). To elucidate RAG2’s role in V(D)J recombination, I will include in each of the previously described experiments an additional reaction containing both Hztransib TP and RAG2. If RAG2 enhances a RAG-like biochemical property of Hztransib TP, that property may have evolved due to recombinase-specific selection pressures. Challenges: As of yet, cleavage activity in low-purity Hztransib TP preparations from another lab has been observed only after addition of Mn2+ (2), which deregulates RAG endonuclease ac- tivity when substituted for the physiological electrophile Mg2+ (1). The Schatz lab has ample ex- perience developing expression constructs and purification/reaction protocols for RAG, expertise that can now be applied to Hztransib TP to increase purity and, I predict, allow cleavage with Mg2+. Hztransib may not represent the entire Transib family in all details; whereas several other Transib transposons contain V(D)J-like asymmetric TIRs(2), Hztransib has symmetric TIRs. I will use my background in computational sequence analysis to identify and conduct experiments with an active Transib transposon bearing asymmetric TIRs, allowing requirements of asymmet- ric synapsis to be evaluated both with RSSs and with the transposon’s own TIRs. Broader Impacts: I will recruit undergraduate mentees from my classes and from oSTEM to get involved in this work, capitalizing on the multifaceted nature of the project to teach them to ap- proach problems from various angles. Additionally, because the USA currently falls far behind other scientifically advanced nations in popular acceptance of evolutionary theory, I will present the exciting history of this tamed transposon at high school teacher conferences to encourage DNA-level approaches to evolution pedagogy, obviating higher-order misinterpretations. Intellectual Merit: Because V(D)J recombination is an essential step in the development of an- tigen-specific lymphocytes, a complete functional comparison between Transib TP and the RAG complex would strengthen the current model for adaptive immune system development. It would also offer clues as to how early organisms acquired pathogen defense capabilities, a significant evolutionary hurdle that, once cleared, initiated a dramatic increase in organismal complexity. References: (1) Gellert M. 2002. Annu Rev Biochem 71: 101-32. (2) Kapitonov VV, Jurka J. 2005. PloS Biol 3: e181. (3) Hencken CG, Li X, Craig NL. 2012. Nat Struct Mol Biol 19: 834-6.	Winner!
157	Motivation and Background: Mammalian white adipose tissue (WAT) distribution and expansion is sex-dependent, with males preferentially accumulating visceral WAT (VWAT) and females exhibiting a subcutaneous WAT (SWAT) accumulation bias. Interestingly, females switch to a male-like pattern of WAT distribution after menopause when estrogen levels decline, indicating sex hormones play a role in the distribution of subcutaneous and visceral WAT mass, yet the molecular mechanisms governing these processes in vivo are not well understood. WAT distribution is strongly correlated with the development of pathologies related to obesity, with accumulation of VWAT being more detrimental for metabolic health than accumulation of SWAT, which may confer protection against these pathologies. Our lab has shown that there is a sexually dimorphic pattern of adipocyte precursor (AP) activation in mice in response to high fat diet, with males having robust AP activation in the VWAT but not SWAT and females having activation in both VWAT and SWAT.1 Once APs are activated they commit to differentiating into mature adipocytes and thus contribute to WAT mass. Interestingly, the sex-specific AP activation pattern observed occurs in an estrogen-dependent manner. Therefore, estrogen levels appear to be crucial for AP activation and expansion of SWAT but not VWAT. Herein I propose to identify the role of estrogen signaling in sexually dimorphic WAT expansion and elucidate the mechanisms controlling differential AP activation in male and female mice. Hypothesis: Estrogen receptor alpha (ERα) is required for AP activation and expansion of SWAT and there are distinct molecular mechanisms driving WAT expansion in VWAT and SWAT, with VWAT expansion being independent of ERα activity. Aim1: Characterize the requirement of ERα in the activation of adipocyte precursors in SWAT. For this aim, we will knockout the Esr1 gene in APs using an inducible Cre-recombinase system driven by the AP-specific promoter PdgfRα (Figure 1).2 This will enable us to ablate ERα expression postnatally to avoid any developmental phenotypes. We will then test the proliferation of APs via incorporation of BrdU, a nucleoside analog, in these ERα-APKO mice upon high-fat diet (HFD) or standard diet feeding (SD). After the HFD-induced AP proliferation phase, incorporation of BrdU will be assessed in APs via flow cytometry. If ERα is required for AP proliferation in female SWAT, we expect to see a decrease in BrdU positive cells when challenged with HFD only in this depot. If ERα is also important for AP proliferation in male SWAT, we expect to see an even lower percentage of BrdU+ cells than wildtype (WT) littermates. We do not expect to see an impairment in AP proliferation in visceral fat in males or females. Aim2. Identify differences in WAT depot estrogen Figure 1. Adipocytes are derived from PdgfRa+ levels. Even though WAT can produce estrogen locally, our precursor cells. Fat from mouse strain with findings suggest that circulating levels of estrogen are fluorescent-membrane dTomato/ membrane eGFP required for SWAT AP activation but not VWAT AP (mT/mG) Cre reporter. Cre excision is marked by a switch from tdTomato expression to eGFP activation in both males and females.1 Therefore, I expression. PdgfRa-Cre labels all mature hypothesize that circulating levels of estrogen influence adipocytes in WAT but PdgfRa is not expressed in estrogen levels in SWAT but not VWAT to drive mature adipocytes, thus the GFP expression observed in adipocytes of PdgfRa-Cre:mT/mG adipogenesis upon periods of HFD. To test this, I will mice is due to lineage tracing.2 measure estradiol levels in the WAT depots of WT female and male mice on days 1, 3, and 5 of HFD or SD. Our lab has shown that activation of APs initiates on day 1 of HFD, with a peak on day 3, and returns to SD levels by day 5.3 Hormone extraction from WAT will be performed and levels of estradiol and estrone will be quantified by liquid chromatography tandem-mass spectrometry. I expect to see increased levels of estrogen in SWAT of WT female mice on day 3 of HFD compared to VWAT. Because WT males do not have significant circulating levels of estrogen, I do not expect to see a difference in VWAT and SWAT levels. I can also perform the same experiment in ovariectomized (Ovx) females and estrogen-treated males, where circulating levels of estrogen are diminished/increased respectively as compared to WT mice. If I see a decrease in estrogen levels in SWAT of Ovx females and an increase in SWAT of estrogen-treated males on day 3 of HFD as compared to WT, then circulating levels of estrogen influence SWAT levels of estrogen upon HFD and promote WAT expansion through estrogen signaling in this depot. Aim3: Elucidate distinct molecular mechanisms of adipogenesis in VWAT and SWAT. Our lab recently identified FOXM1, a nuclear fork box protein, as an important gene in male visceral AP activation (unpublished). Interestingly, FOXM1 has been shown to work with ERα to promote gene expression in a breast cancer model.5 Furthermore, when in the presence of activated ERα, FOXM1 drives the expression of a different gene program than when in absence of ERα.5 Therefore, we hypothesize that FOXM1 is important in adipocyte hyperplasia in both males and females but it acts through distinct molecular mechanisms depending on the presence of estrogen. To test this, we will perform RNAseq on isolated APs from both fat depots under HFD and SD conditions on WT and Ovx female mice. If FOXM1 is working with ERα to promote AP activation in SWAT, we expect to see an increase in gene expression in FOXM1-ERα targets only in subcutaneous fat of WT females. If we do not see this same pattern in the subcutaneous fat of Ovx females, but we do find increased gene expression of FOXM1 targets in the visceral fat, we can conclude that in the presence of estrogen, FOXM1 and ERα promote the activation of APs and expansion of subcutaneous WAT and in the absence of estrogen, FOXM1 alone promotes activation of APs and expansion of visceral WAT. To further confirm this, I will perform co- immunoprecipitation (co-IP) on isolated APs from both depots from SD and HFD-fed mice to assess if FOXM1 partners with ERα in SWAT but not VWAT of WT females. Intellectual Merit and Broader Impact: This study will clarify for the first time the mechanistic role of estrogen signaling in WAT and will significantly impact the field of adipose tissue biology. We will also set precedent on elucidating distinct sex-dependent molecular pathways governing WAT mass expansion. As a hispanic woman in science, my goal is to inspire others to pursue careers in science and become advocates for minorities in STEM. By sharing my research findings in activities coordinated by Yale organizations and minority-focused science conferences I plan to motivate not only undergraduate women and minorities to pursue careers in science, but I will also educate the greater community and general public about the importance of science education in order to advance knowledge beyond an academic environment. IACUC Approval: We have clearances and training for all handling and proposed mouse procedure (Yale IACUC protocol 2012-11249). The University’s Assurance number with the Office of Laboratory Animal Welfare is #A3230-01, approval through 5/31/19. IACUC oversees the University’s centralized, AAALAC-accredited animal resource, the Yale Animal Resources Center (YARC). References: 1Jeffery, E., et. al. (2016). Cell Metabolism, 24(1):142-50. 2Berry, R., Rodeheffer, M. (2013). Nature Cell Biology, 15(3): 302-308. 3Jeffery, E., et. al. (2015). Nature Cell Biology, 17(4): 376-385. 4Falk, R. T., et. al. (2008). Cancer Epidemiology, Biomarkers, and Prevention, 17(8): 1891–1895. 5Sanders, D., et. al. (2013). Genome Biology, 14:R6.	Winner!
158	Assessing Heterogeneity in Organic Municipal Solid Waste Across City-Scales for Optimized Urban Biogas Production Keywords: biogas, sustainable development, urbanization, waste management, OFMSW Hypothesis: Biogas projects have found success in supplying renewable energy for niche markets with homogenous waste streams, but are limited by heterogeneous waste streams at the urban scale. Disaggregating waste streams to homogenize anaerobic digestion feedstocks will aid stability of biogas production at the urban scale. Introduction: Bangkok currently produces one of the highest municipal solid waste generation rates of megacities within the developing world, at over 11,000 tons per day1. The majority of Bangkok’s organic fraction of municipal solid waste (OFMSW) is landfilled, with adverse impacts on both public health and the environment through degradation of water resources and large releases of the potent greenhouse gas methane. One commonly accepted method for management of the OFMSW is anaerobic digestion (AD). Anaerobic digestion of OFMSW has numerous benefits: voluminous production of biogas (a biogenic gas that may be combusted for electricity and heat production), reduction of landfilled waste volume, reduction of methane emissions, and production of a high-quality organic fertilizer by- product2. However, variations in biochemical composition of organic waste streams largely dictate the stability of biogas production, as heterogeneities in feedstocks can cause inhibition of the microbiological processes that produce biogas3,4. For example, significant differences in moisture content between two areas may necessitate the implementation of different AD technologies, such as wet, dry, or a wet-dry combination of AD. Additionally, OFMSW collection infrastructures can be complicated and expensive due to waste originating from numerous sources over large spatial areas. Understanding how generation of OFMSW varies over urban to exurban spatial scales will better inform strategic homogenization of AD feedstock waste streams, more effective collection infrastructure, and appropriate siting of future biogas production plants for the sustainable management of OFMSW. Research Plan: My intended research will fundamentally address the following questions: 1. Do urban “pockets” exist in which municipal solid waste is predominantly composed of organic, digestible waste? 2. Can waste collection infrastructure and waste facility siting be restructured to better reflect spatial variations in OFMSW generation? Methods: I will combine spatial mapping, waste-transport charts, and waste sampling to assess how generation of organic waste is distributed over Bangkok’s urban environment. 1. Delineate urban, suburban, and rural-urban fringe (RUF) zones. I will use Geographic Information Systems (GIS) to demarcate urban, suburban, and RUF zones of Bangkok based on census data, land use maps, and aerial photography5. The demarcation of the three urban zones will inform my in-field waste sampling during the summer of 2015. 2. Identify waste management plants/landfills that collect waste within each of the above-listed zones, and sample waste from May – August, 2015. Municipal solid waste in Bangkok is not source separated, thus I will conduct a waste composition study under ISO 14001 standard6. Depending on the waste center, I will either collect samples from waste screens and grinding operations, or will hand sort the waste to collect samples of the organic fraction. I will then assess the waste sample for moisture content and biochemical composition through local university facilities7. I Graduate Research Proposal NSF GRFP will collect waste samples two times per day from ten replicates within each urban zone to best account for expected high variability in waste structure. 3. Identify appropriate sites for siting of future AD facilities. By comparing samples of organic waste with existing waste collection routes, inferences can be made as to whether or not organization of waste management facilities are appropriate for the waste composition originating from Bangkok. I expect that city areas with high densities of malls (often with large food courts) and food markets will have disproportionately high food waste suitable for AD. 4. Model the potential biogas production based on computer simulation of biogas production, and scale the waste streams for their representative urban areas to the city scale. I will input the elemental compositions of organic wastes into the Anaerobic Digestion Model 1 (ADM1) computer simulation to gain rough estimates of biogas productions8. I will then scale the production rates from each urban area type to its full city-wide extent to estimate Bangkok’s biogas production potential. 5. Model future urban development and its implications for organic waste based on the Bangkok Development Plan released in 2013. Similar to step 4, I will use projections of future shifts in urban environment (e.g., from suburban to urban) to model future growth in urban waste/biogas. Research collaborations: Through Yale University’s Urban Resources Initiative (URI), I will conduct a pilot project to assess neighborhood-wide waste streams during the spring semester of 2015. Working with URI will give me valuable experience and insight into potential pitfalls that may arise during my summer data collection, and will allow me to extend my research to New Haven’s local community. Additionally, my previous research collaborations through the Joint Graduate School of Energy and Environment in Bangkok will afford me access to university facilities for biochemical analyses and support from Thai professors currently involved with waste-to-energy projects. Intellectual merit: My proposed research aims to bridge the gap between the use of AD technology in the developed and the developing world. Understanding how waste-streams respond to urban growth will allow city planners to best implement future waste management plans for developing urban environments in both the developing and developed world – such as the U.S. My intended research will contribute to the broader knowledge on waste management through the goal of a peer-reviewed publication by the end of my second year, as well as presentations of findings at university-based and international conferences. Broader impacts: Capitalizing on OFMSW for production of biogas is a comprehensive sustainable development strategy that tackles the increasing challenges of managing waste, providing stable electricity, and mitigating greenhouse gas emissions. The findings of the research will be particularly valuable in developing urban environments, for example those in India or sub-Saharan Africa, in which putrefying organic waste directly contributes to public health concerns and ecological damage. Furthermore, generation of electricity from biogas may become significant for assisting intermittent renewable energies, such as wind and solar photovoltaics, in future provisioning of base load electricity supply. This work will directly aid local professors, students and urban planners, as few comprehensive waste-structure studies of Bangkok currently exist in the literature. References: 1 Udomsri et al. 2011. Energy for Sustainable Development 15: 355-364. 2 Wellinger, A. et al. 2013. The Biogas Handbook. 3 Curry and Pillay. 2012. Renewable Energy. 41: 200-209. 4 Browne, J.D. et al. 2013. Applied Energy 128: 307-314. 5 Pryor, R.J. 1969. Geografiska Annaler. Series B, Human Geography. 51:33-38. 6 ISO 14001. 2004. Environmental Management. 7 Zhang, R. et al. 2007. Bioresource Technology 98: 929-935. 8 Batstone, D.J. et al. 2002. Water Science Technology 45: 65-73.	Winner!
159	global coral reef mortality. These stressors may reduce reef resilience by stimulating macroalgal growth and competition [1]. Parrotfish control macroalgae through herbivory but often predate corals to supplement their diets. This causes some coral tissue damage in the form of individual lesions, but rarely causes total colony mortality [2]. However, Zaneveld [1] surprisingly found that in waters enriched with nitrogen and phosphorous, colony mortality increased from zero to ~65% in Porites colonies after parrotfish predation, but why this occurred was unknown. For my dissertation project I aim to study if and how the combined stressors of predation and nutrient enrichment disrupt coral physiology and/or their microbiomes to cause this increase in mortality. The coral holobiont is a dynamic assemblage of the coral animal and its associated microorganisms such as bacteria and algae which collectively make up the microbiome. Elevated nutrients can alter the abundance and types of coral-associated mutualistic algae in the genus Symbiodinium [3]. Environmental stressors cause shifts in healthy coral-associated bacteria [4], that may provide antibiotic activity against invasive microbes and pathogens [5]. Zaneveld [1] found that the combination of predation and nutrient enrichment increased the amount of potentially opportunistic bacteria when compared to proposed coral mutualists. My project will determine if parrotfish are a vector for bacterial opportunism and/or if nutrients drive an increase in host susceptibility to infection following wounding by predation. Hypothesis: I hypothesize that nutrient enrichment and predation interact to cause two major changes to the coral holobiont that result in coral death: reduced host immunity and the proliferation of pathogens. My work will focus on the following questions: Q1. How do bacteria in the coral mucus protect against predation-mediated mortality in water with ambient nutrient levels? I hypothesize that coral mucus already possess specialized microbiota that protect corals from pathogens via several testable mechanisms such as antibiotic production, competitive exclusion, or predation. Q2. How do nitrogen and phosphorous alter coral and algal symbiont physiology and the microbiome? I hypothesize that a decrease in host immunity and destabilization of the Symbiodinium community will combine to reduce the holobiont’s ability to regulate its microbiota. I also predict that the microbiota with anti-pathogen activity identified in Q1 will be reduced and opportunistic bacteria will increase in nutrient enriched treatments. Q3. Is the microbiome-dependent route to coral death driven by physical wounding or predator specific corallivory in nutrient enriched waters? I hypothesize that parrotfish serve as vectors for the proliferation of pathogens in the mucus around the wound site. Alternatively, I hypothesize that any wounding in the presence of elevated nitrogen and phosphorus provides a route to enhanced bacterial infection. Research Plan: These questions will be addressed at the Gump South Pacific Research Station on Moorea, French Polynesia, through two complementary experiments: on the reef and in controlled tanks. While I expect to see similar changes in microbial communities and host health between the two experiments, each will provide a specific component to my investigation. Using SCUBA, individual Pocillopora colonies will be transplanted to saltwater mesocosm tanks. In the tanks, I will pre-expose corals to an antibiotic mix [6] to deplete the bacterial community. The types, concentrations, and exposure length will be determined the year prior to the experiment. Then I will move the treated and untreated corals to new tanks with natural seawater or to the field for monitoring. I will simulate predation in the tanks by physically wounding the coral and track host immunity to determine if the host alone is capable of preventing mortality or if associated microbiota are necessary for defense and recovery (Q1). On the reef, a subset of corals will be exposed to parrotfish predation while others will be shielded from predation with herbivore exclosures (Q3). A subset of the colonies in both experiments will be maintained at ambient nutrient levels while others will be enriched using slow-release fertilizer diffusers (Q2). N and P concentrations will be comparable to those on reefs impacted by nutrient pollution [1]. Phase 1. Simulate the effects of predation, nutrient loading, or a combination of these stressors with manipulative experiments on the reef and in tanks. At regular intervals, 1) photographically monitor coral tissue growth/loss and coral mortality, 2) record dissolved organic nitrogen and soluble reactive phosphorus concentrations via autoanalyzer, 3) measure Symbiodinium density with Pulse Amplitude Modulation, 4) measure bacterial respiration with oxygen probes, 5) count mucus associated bacteria with epifluorescence microscopy, and 6) sample coral tissues for DNA/RNA, taking care to minimize any serious damage to the coral. Phase 2. Track changes in the holobiont. For bacterial community dynamics, extract DNA from mucus to generate microbial 16S amplicon libraries [1] and metagenomes [4] for bacterial functional analysis. For Symbiodinium and host gene expression changes, extract RNA and DNA from tissue for RNAseq as well as for ITS-2 amplicon libraries [3]. Phase 3. Sequence the libraries on Illumina platforms at OSU’s Center for Genome Research. Phase 4. Use bioinformatics (e.g. QIIME [7], Shotmap [8]) and statistical pipelines (e.g. STAMP [9]) to analyze changes in structure and function of microbial communities and in gene expression patterns of innate holobiont immune responses. Predictions: Antibiotic producing bacteria, not host immunity, will be the primary defense against coral tissue loss or mortality from predation or wounding. Nutrient enrichment in combination with predation or wounding will lead to coral mortality. Coral mucus will exhibit an increase of one or more pathogenic strains, either found in low abundance in the communities of control colonies or absent from control colonies and therefore introduced by parrotfish predation. Coral mucus will also exhibit a decrease of one or more strains with antibiotic capabilities. I will identify the proliferated pathogenic strain(s) and the reduced defensive strain(s) thereby identifying the microbial route to colony mortality. Intellectual Merit: The Zaneveld study [1] is the first to document increases in predator- mediated mortality in the presence of elevated nutrients. Parrotfish herbivory is accepted as beneficial to coral reefs, and parrotfish predation is accepted as normally benign. My study will pin down the mechanism(s) in which these herbivores become agents of mortality and will transform how we approach the conservation of coral reefs, and more specifically, trophic interactions on a reef. Restoration of parrotfish populations may have negative consequences for coral health if efforts are not simultaneously made to combat water quality issues. Broader Impacts: During the field season in Moorea, I will design and conduct interactive teaching workshops for the local community similar to my outreach as an undergraduate. Using resources at the Gump Station and connections with the Atitia Center for outreach, I will print 2- D reef replicas of my nutrient-enriched and ambient level in situ corals over time for use in citizen science training. Local schoolchildren and adults will use the photos along with quadrats, transect tape, identification guides, and whiteboards to ‘become’ a marine biologist for a day. I will guide participants in using quadrats to quantify metrics of reef change, such as percent live coral cover. Children and adults will experientially observe how nutrients such as fertilizers affect the health of marine species. Through this citizen science initiative, I hope to transform how local communities understand and interact with their coral reefs. Citations [1] Zaneveld et al. (2016) Nat Commun, [2] Rotjan & Lewis (2008) Mar Ecol Prog Ser, [3] Correa et al. (2009) Coral Reefs, [4] Vega Thurber et al. (2009) Environ Microbiol, [5] Ritchie (2006) Mar Ecol Prog Ser, [6] Glasl et al. (2016) The ISME Journal, [7] Caporaso et al. (2010) Nat Methods, [8] Nayfatch et al. (2015) PLoS Comput Biol, [9] Parks et al. (2014) Bioinformatics.	Winner!
162	extreme hydrodynamic and aerodynamic loads on offshore wind turbines (OWTs), specifically wave and wind loads during hurricanes. To this end, I propose to numerically simulate OWTs subjected to extreme wind and waves using computational fluid dynamics (CFD). This research aims to advance basic understanding of OWT loads by answering the questions: 1. How do extreme hydrodynamic and aerodynamic loads on OWTs vary for different support structures and hurricane characteristics? 2. How do OWTs (especially floating OWTs) respond to hurricane wind and waves? Motivation: To meet the federal goal of 20% electricity from wind energy by 2030, the U.S. wind industry must expand to include offshore wind development. Offshore wind offers several advantages over onshore wind, including the mitigation of aesthetic and land use issues, as well as the utilization of abundant, high-quality offshore wind resources in proximity to population centers [1]. However, wind farms off the eastern and southern U.S. coast could be destroyed by hurricanes, unless their support structures are designed with such extreme loads in mind [2]. These designs require accurate load data, but experimental data is mostly unavailable due to the lack of OWTs in hurricane-prone areas. So, current OWT simulations find hydrodynamic and aerodynamic loads using simple empirical models, which are much less accurate than CFD [3]. This inaccuracy is catastrophic when designing OWTs to withstand hurricanes, so using CFD to better predict extreme loads will inform more robust OWT designs. Methods: To generate hydrodynamic and aerodynamic loads typical of hurricanes, I will numerically simulate OWTs subjected to extreme, hurricane-like wind and waves. These simulations will be done in the CFD software Converge from Convergent Science. Unlike most CFD software, Converge doesn’t require user-made meshes, which means researchers can complete simulations faster. Converge’s adaptive automatic meshing also improves solution accuracy for moving objects like floating OWTs, since the grid resolution adapts where necessary. Converge could also fix stability issues found when modeling floating OWTs in other CFD software like OpenFOAM [4]. I will first identify hurricane parameters characteristic of the U.S. coast, focusing on areas where OWT development is likely. I will then validate my predicted hydrodynamic and aerodynamic loads against experimental and numerical data: CFD-based numerical data for non-extreme waves is available from Benitz [4], while proprietary experimental data for loads from Hurricane Irene is available from industry collaborators. Aerodynamic loads will be validated against the open literature for onshore wind turbines. Finally, I will create databases of hydrodynamic and aerodynamic loads corresponding to various hurricane parameters for several support structure types. The predicted non-hurricane hydrodynamic loads will be validated for some OWT structures prior to the beginning of the proposed project: the proposed team (detailed below) is currently collaborating on a 1-year project on OWTs in breaking waves, which includes validating the predicted hydrodynamic loads on non-floating structures against numerical data and experimental data from industry partners. Deliverables: The main deliverables of the project and their estimated times of completion are: 1. Identify representative hurricane characteristics for the U.S. coast (2 months), 2. Validate aerodynamic loads against experimental data (3 months), 3. Validate hydrodynamic loads against experimental and numerical data (5 months), 4. Generate hydrodynamic and aerodynamic load databases for various hurricane parameters for non-floating structures (12 months) and floating structures (14 months). Collaborations: This work will involve collaboration between University of Massachusetts faculty from two departments, as well as collaboration with industry partners (Convergent Science and others in development). Dr. David Schmidt and Dr. Matt Lackner (Mechanical Engineering) have previously studied hydrodynamic loads on OWTs [3,4], and have access to the computing resources necessary for CFD. Dr. Schmidt also worked in Converge on other applications, and his long relationship with Convergent Science enables their collaboration and assistance in introducing hydrodynamics and aerodynamics as new applications. Dr. Sanjay Arwade (Civil Engineering) brings expertise in OWT support structures and hurricanes’ impacts on OWTs. Intellectual merit: The proposed project furthers basic scientific understanding of OWT loads, introduces better software for OWT modeling, and provides better data for structural models of OWTs. First, using CFD to study extreme hydrodynamic and aerodynamic loads on OWTs will improve fundamental understanding of how OWT support structures behave during hurricanes, which is currently hindered by overly simple models and a lack of experimental data. Second, this project will validate a faster, more accurate CFD software for OWTs and other ocean engineering applications. Third, this research will provide more accurate loads used in OWT structural analysis, like that done by civil engineers. These results will be distributed at wind energy and CFD conferences, in journal articles, and in my PhD dissertation. Broader impacts: By providing more accurate hydrodynamic and aerodynamic hurricane loads for use in structural OWT models, the proposed project allows for better designs of OWTs that can withstand hurricanes. Hurricane-resistant OWTs lower the risks of offshore wind, encouraging the widespread development of offshore wind energy in the U.S. and other hurricane-prone countries. In this way, the proposed research contributes to the growth of renewable energy on the national and global scale. References 1 Musial, W., and Ram, B. 2010. Large-scale offshore wind power in the United States: Assessment of opportunities and barriers. Technical Report NREL/TP-500-40745, U.S. Dept. of Energy, 1–221. 2 Wei, K., Arwade, S.R., Myers, and A.T. 2014. Incremental wind-wave analysis of the structural capacity of offshore wind turbine support structures under extreme loading. Engineering Structures 79, 58-69. 3 Benitz, M.A., Lackner, M.A., and Schmidt, D.P. 2015. Hydrodynamics of offshore structures with specific focus on wind energy applications. Renewable and Sustainable Energy Reviews 44, 692-716. 4 Benitz, M.A. 2016. Simulating the hydrodynamics of offshore floating wind turbine platforms in a finite volume framework. PhD thesis, University of Massachusetts - Amherst.	Winner!
163	Starsexplode. Supernovae(SNe), orstellarexplosions, canoccurthroughtheignitionofa degenerate white dwarf (WD) star, a star that is supported by quantum electron degeneracy pressure, orbythecorecollapseofamassivestar. Recenttransientsurveys, suchasTheDark Energy Survey have discovered and imaged thousands of supernovae since 2013 including the anomalous SNe DES13S2cmm. The forthcoming Large Synoptic Survey Telescope will further these efforts utilizing a three billion pixel digital camera to cover more than 20,000 deg2 of the night sky. However, even with this wealth of observational data, many aspects of the evolution and subsequent explosion of massive stars remain unknown. My background in theoretical astrophysics has prepared me to aide in the advancement of theseefforts. I propose to investigate the stellar structure and evolution of massive stars, core collapse supernovae explosion (CCSNe) mechanisms, and implications for cosmic chemical evolution and gravitational wave radiation. The confluence of advancements in multiple fields will provide the empirical basis needed to accomplish these goals. The focused efforts proposed are summarized as follows: (i) getting the progenitor right, (ii) supernova explosion mechanisms, and (iii) nucleosynthetic yields and gravitational wave bursts. Advancement in our understanding of massive stars can lead to furthering our knowledge of the cosmic chemical evolution of the Universe and provide direct tests of Einstein’s General Theory of Relativity (GR). Getting the progenitor right. The star that will eventually explode as a CCSNe is often referred to as the progenitor. Computational modeling of the progenitor star can lead to the insight of how a star will end its life, or allow one to infer the initial progenitor of an observed supernova. Recent 3D hydrodynamic simulations of radiation dominated envelopes in massive stars and internal magnetic field strengths of order ∼105 Gauss, suggest the need for further investigation [4]. I will investigate the uncertainties associated with the structure and evolutionary properties of massive stars that will end their lives as CCSNe explosions. Using a state of the art stellar evolution code, Modules for Experiments in Stellar Astrophysics, I will focus on uncertainties due to the nuclear reaction rates, compositional mixing, and the effects of rotation and induced magnetic dynamos. Specific steps include the sampling of new Monte Carlo nuclear reaction rate distributions for key nuclear reactions using the recently constructed rate library, STARLIB [5], and performing a quantitative assessment of the effect of varying strengths of compositional mixing and rotational values. The utilization of new measurements of nuclear reaction rates at astrophysically relevant energies forthcoming from the Facility for Rare Isotopes Beams willalsobeparamountinthiseffort. My background in stellar astrophysics, especially my past published work on modeling super asymptotic giant branch stars [2], will allow me to play a productive role towards modeling more physically accurate stellar models that can address fundamental questions in stellar and galactic evolution. Supernova explosion mechanisms. Collapse of the iron core within a massive star initiates the CCSNe explosion. The inner core is then halted once densities exceed that of nuclear matter, resulting in core bounce launching a shock towards the still collapsing outer core. However,theshockisnotstrongenoughtoblowupthestarandisusuallyhalted. Thisstalled shock has led to the so-called ‘failed supernovae’ problem and has left many scientist trying to determine the mechanism which allows for the efficient explosion of CCSNe observed. Contemporary approaches favor neutrino transport as an efficient means of reheating, or addingenergyto, thestalledshockallowingforasuccessfulexplosion. Recentstudiessuggest a correlation between the local neutrino heating rate and successful explosion [1]. I propose to continue this effort by investigating various explosion mechanisms of CCSNe and addressing uncertainties therein. My primary numerical instrument will be the 3D adaptive mesh hydrodynamic code, FLASH. The computational resources available at my proposed graduate institution, California Institute for Technology, will make theseeffortsfeasible,whiletheexpertiseofthegroupIwishtojoinwillprovidetheneccessary support to successfully address these scientific questions. Nucleosynthetic yields and gravitational wave bursts. Successful CCSNe explosions are also known to produce iron-group elements and experience bursts of gravitational wave radiation. I propose to investigate gravitational wave bursts caused by CCSNe as well as the associated nucleosynthetic yields. Minutes after the Big Bang, the Universe began to synthesize light isotopes such as 1H and 4He. However, uncertainties still lie within the steps taken to arrive at the m´elange of isotopes in our interstellar medium today. The next step towards understanding the cosmic chemical evolution of our Universe is to move towards a deeper understanding of the nucleosynthetic yields of CCSNe. My current NSF-supported work with Dr. Frank Timmes at Arizona State University on nucleosynthetic yields in WDs [3] is preparing me to address aspects of forging the elements during my graduate work. Furthermore, with the recent upgrade of The Laser Interferometer Gravitational-Wave Observatory (LIGO) complete, direct detection of gravitational waves (GWs) is imminent. These ripples in spacetime can occur during asymmetric collapse to a black hole of CCSNe and provide direct tests of GR. A new field of astrophysics is upon us and requires necessary interplay between astronomy and theoretical physics. While participating in the NSF LIGO summer research program I simulated GWs emitted by compact binary systems in an effort to test the strong-field dynamics of General Relativity and this experience has prepared me to play a large role in this effort. Here I present a framework for maintaining successful completion of these efforts. In years 1-2 of my graduate studies, I will work on focused effort, Getting the progenitor right, with successful completion corresponding to a peer-reviewed journal publication. Years 3-4 will focus on Supernova explosion mechanisms, again with successful completion corresponding to a peer-reviewed journal publication. Lastly, I will spend my final year considering Nucle- osynthetic yields and gravitational wave bursts, with the culmination of this project resulting in a publication and successful completion of my Ph.D. The focused efforts presented here would result in the advancement of our understanding of the evolution and subsequent explosion of massive stars, leading to advancements in the fields of cosmology, astronomy, and theoretical physics. These are immense, broad questions that require expertise in multiple backgrounds as well as interdisciplinary collaborative ef- forts. Being supported by the NSF through the GRFP would accelerate my goals by allowing me to begin research my first year and be invaluable in preparing me for a successful career. [1] Couch, S. M., & Ott, C. D. 2015, The Astrophysical Journal, 799, 5 [2] Farmer, R., Fields, C. E., & Timmes, F. X. 2015, The Astrophysical Journal, 807, 184 [3] Fields et al. 2016, The Astrophysical Journal, in prep. [4] Fuller, J., Cantiello, M., Stello, D., Garcia, R. A., & Bildsten, L. 2015, Science, 350, 423 [5] Sallaska, A. L., Iliadis, C., Champange, A. E., et al. 2013, ApJS, 207, 18	Winner!
166	Modeling the FeMoco Cluster of Nitrogenase: Synthesis of a μ -Carbide Metal Cluster 3 Ammonia is the second-largest synthetic chemical product worldwide, with 1.68 × 108 tons produced annually.1 Typically, ammonia is synthesized via the Haber-Bosch process, where a heterogeneous iron oxide catalyzes the Figure 1. (left) Structure of the FeMo cluster of nitro- reaction between hydrogen and nitrogen.1,2 genase. The terminal iron is bound to cysteine; the ter- About 85% of ammonia produced is used to- minal molybdenum is bound to histidine and chelated wards crop fertilization; large-scale ammonia by homocitrate. (right) Proposed synthetic target, corre- production has been credited for the quadru- sponding to the left cubane of FeMoco. pling of the world’s population from 1.8 to 7.4 studies have attempted to calculate stable nitro- billion in the last 100 years.2 The process is typ- gen binding sites of FeMoco and possible inter- ically performed in excess of 400°C at 200 atm, mediates of the reduction.5,6 However, in order and is highly energy-intensive, consuming to determine the true mechanism, experiments about 3–5% of the world’s yearly natural gas on a structural model are necessary. production and 1–2% of the global energy pro- Due to the high complexity of FeMoco, a duced. These are consequences of nitrogen’s synthetic model adequate for detailed struc- high stability: the N–N triple bond of N is one ture-function studies has proven elusive thus 2 of the strongest covalent bonds known, with a far; as of yet, no models incorporate the inter- dissociation energy of 226 kcal/mol.3 stitial carbon atom. I propose to develop a syn- Despite the stability of nitrogen gas, many thesis of a novel cubane cluster structurally rel- classes of bacteria and archaea have evolved evant to FeMoco (Figure 1). This tetrametallic the ability to catalytically fix nitrogen via the cluster contains an unusual bridging carbide enzyme nitrogenase, which couples the reduc- ligand coordinated to three iron centers. tion of nitrogen to the hydrolysis of ATP over Trimetallic bridging carbide clusters are 8 single-electron transfer events (Scheme 1). known for various metals, including Ti,7 Co,8 Remarkably, nitrogenase is capable of fixing Ru,9 and Os.10 However, these isolated com- nitrogen at ambient temperatures and pressures, plexes all contain strong-field ligands such as a feat that humans have yet to mimic. CO and cyclopentadienide; μ 3-carbides are ex- tremely rare for complexes containing low- field ligands such as sulfides, making the syn- thesis of a [4Fe-3S-C] cluster such as this a for- Scheme 1. Balanced half-reaction for nitrogenase- mediated nitrogen fixation. P = inorganic phosphate. midable challenge. i Nitrogenase contains three types of metal Lee and coworkers have previously synthe- cluster cofactors: the most complex of these is sized a [4Fe-3S-N] cluster containing a μ -im- 3 the FeMo cluster (FeMoco), where nitrogen is ido ligand, analogous to the proposed μ -car- 3 bound and reduced (Figure 1). FeMoco is com- bide, from a [4Fe-4S] cluster and a nitride- prised of 8 metal centers encapsulating a small carrying bimetallic species.11 I will use this interstitial atom. The interstitial atom’s identity route as a model synthetic strategy (Scheme 2). has been long debated as either C, N, or O; re- The μ -imido nitrogen in Lee’s system is de- 3 cent research has provided strong evidence for rived from an amine; an isoelectronic reagent the identification of this atom as carbon.4 such as an organolithium reagent could react Much is still unknown about the mechanism similarly. A carbene equivalent such as a diazo through which FeMoco reduces nitrogen, in- species could also serve as a carbon precursor. cluding the precise N binding site, the elec- An alternative synthesis draws from the 2 tronics of the cluster during reduction, and the work of Holm and coworkers, who synthesized role of the interstitial carbon. Computational pentametallic cuboidal clusters containing a NSF Graduate Research Fellowship Program 1 Graduate Research Plan Statement Jeremy C. Tran Scheme 2. Two proposed syntheses of the target cubane clusters, inspired by Lee (top) and Holm (bottom). [4Fe-3S] moiety capped by a fifth metal gating the clusters together, a true analog of through sulfide linkages.12 By using a small FeMoco could be synthesized. This full model phosphine such as PMe as L and a bulky phos- could be used in structure-function studies and 3 phine such as P(tBu) as L', the sulfides capping would prove invaluable in determining the pre- 3 the cuboidal face could potentially be more ac- cise mechanism of nitrogenase. cessible. This would allow for selective re- INTELLECTUAL MERIT, BROADER IMPACTS moval of the capping sulfides with a thiophilic The proposed research would provide a syn- reagent such as mercury, opening up a vertex thetic route to low-field μ -carbide metal clus- 3 for carbide insertion. ters, which are very uncommon and not well Although 13C NMR spectroscopy could po- understood. This research would also fill a tentially be used to check for incorporation of wide gap in our knowledge about the mecha- the carbide, the paramagnetism of iron clusters nism of nitrogen reduction via FeMoco. Know- diminishes the usefulness of NMR spectros- ing the mechanism through which biological copy as a characterization tool. Instead, product systems fix nitrogen could lead to the develop- characterization will focus heavily on mass ment of new manmade nitrogen fixation meth- spectrometry and X-ray crystallography, to de- ods, boosting the production of ammonia while termine whether the synthesis was successful. I simultaneously reducing energy inputs. will also use Mössbauer spectroscopy to deter- The primary use of ammonia is as a fertilizer; mine the oxidation states of the iron centers. a more efficient means of ammonia production Upon successful synthesis of the target, fur- would correspond to a potential increase in ag- ther experiments will be performed to fully ricultural productivity, helping to sustain the characterize the cluster. Cyclic voltammetry ever-growing global population. Although this will be utilized to test the redox properties of is not likely to outcompete the Haber-Bosch the clusters. The ability of these clusters to bind process, a small-scale source of readily obtain- N will also be examined, both under redox- able ammonia would be incredibly useful for 2 neutral and reducing conditions. In the event remote regions where fertilizer is scarce. that N successfully binds, characterization to References: (1) Ullmann’s Encyclopedia of 2 determine the binding site(s) will be carried out Industrial Chemistry; 7th ed. 647–698. (2) Nature 1999, 400, 415. (3) Comprehensive Handbook of Chemical via X-ray crystallography. The reactivity of a Bond Energies; 1st ed. (4) Science 2011, 334, 940–940. nitrogen-binding species towards various nu- (5) J. Am. Chem. Soc. 2003, 125, 15772–15778. (6) Annu. cleophiles and reductants will also be explored. Rev. Biochem. 2009, 78, 701. (7) Organometallics 1994, Future directions include using this carbide- 13, 2159–2163. (8) J. Am. Chem. Soc. 1958, 80, 6529– 6533. (9) J. Organomet. Chem. 2001, 633, 51–65. (10) containing cluster as a stepping stone towards Inorg. Chem. 1996, 35, 1405–1407. (11) Inorg. Chem. the full synthesis of a FeMoco-like cluster. By 2012, 51, 12891–12904. (12) J. Am. Chem. Soc. 1993, constructing the other half of the cluster and li- 115, 5549–5558. NSF Graduate Research Fellowship Program 2	Winner!
167	Motivation: Functional magnetic resonance imaging (fMRI) allows non-invasive measurement of real-time brain activity in humans. The success of this technology has been evidenced by its rapid growth in popularity in the 25 years of its existence, resulting in nearly 40,000 research papers1. A large portion of these studies investigates the correlational structure of the brain signal, known as functional connectivity (FC). FC studies are most often implemented in resting state fMRI (RS-fMRI). Historically RS-fMRI has been especially useful in clinical and developmental imaging because it requires no task demands, avoids performance confounds and measures network connectivity in largely the same way that task fMRI does2. These methods have led to many groundbreaking findings in brain science that were previously inaccessible. Recent evidence3 has shown that many of these findings may be spurious and insidiously riddled with artifactual patterns of connectivity created by head motion. This is most often evident in clinical and developmental populations because head motion is confounded with the group effect of interest. Even small movements, on the scale of .1 mm, have been shown to cause structured patterns of spurious variance, enhancing short-range connectivity and decreasing long- range connectivity3. These findings have caused many groups to entirely reevaluate previous FC findings3 and attempt to develop ways of overcoming this major problem. Current motion correction methods summarize head motion as a rigid body transform with 6 parameters (motion in the 3 spatial dimensions as well as rotations along each of these axes). The amount of motion at any single time point can be estimated by the change in each of these 6 motion parameters from the previous time point. Many methods have been developed to characterize and correct for motion-related signal. Common motion correction pipelines model the relationship between each direction of motion and fMRI signal changes linearly in confound regression. However, this linear assumption may be inadequate4. Higher order expansions of this model4 that allow for temporal offset and nonlinear relationships between motion and fMRI signals have been shown to perform better than standard methods in high motion populations, however, these models still fail to entirely remove motion related signal. To remove the remaining nuisance signal it is common practice to censor the problematic timepoints5. While this method works, it requires removal of valuable time points and often removal of entire subjects from an analysis. Regularly these removed subjects are patients whose data collection cost thousands of dollars and many person-hours. This leaves the field in a tenuous position in which previous findings require reevaluation and future studies must employ burdensome censoring techniques. If lasting, valid progress will be made with FC-fMRI a thorough understanding of motion and better motion correction techniques are required. The proposed research will apply established methodologies to a unique dataset, which will shine light on this important issue in fMRI. Aim 1: Characterize and model head motion artifacts in a single highly sampled subject. The proposed project will begin by thoroughly characterizing motion related signal changes in the MyConnectome dataset6. This publicly available dataset consists of 88 ten minute RS-fMRI scans of a single healthy adult male, resulting in over 45,000 whole brain images. Originally collected to establish the reliability of FC-fMRI methods, this dataset provides a unique opportunity to understand fMRI signals related to motion, an endeavor previously overlooked. Compared to a typical RS-fMRI dataset this sample has no variance related to individual differences or sex effects, and minimal variance related to age, brain size, or vasculature. From the wealth of time points in this dataset, I will construct smaller datasets out of time points that have motion primarily in a single direction and/or magnitude. Each will be the size of a typical RS-fMRI analysis. I will then apply previously described methods5 to characterize the influence of this highly controlled motion on fMRI signal change and FC. The unique flexibility of the MyConnectome dataset will allow me to further describe the nonlinearity and heterogeneity of motion related signal changes. To do this I will use established multiple regression methods 4, testing models of the linear and nonlinear effects of motion on the fMRI signal and time-course while penalizing for model complexity to avoid overfitting. I hypothesize that this approach will allow a precise description of the influence of directionality, magnitude and the time-course of head motion on fMRI signal and connectivity that will surpass current models in the amount of motion related variance explained. Aim 2: Determine generalizability of head-motion model. A major issue with this model may be that its utility is specific to a single subject’s brain and lacks generalizability to developmental or clinical populations with a larger amount of movement. I will use another unique and publicly available dataset, the Philadelphia Neurodevelopmental Cohort (PNC), to overcome this limitation. Since the PNC consists of a pediatric, demographically diverse, developing population, it is well suited to test the performance of the model defined in Aim 1 with a dataset most prone to the previously identified motion confounds of FC-fMRI. I hypothesize that the model identified will significantly outperform current state-of-the art processing methods, significantly reducing measurable motion related confounds5 and the reliance on censoring. Broader Impacts: This research has the potential to contribute crucial information to the growing discussion of motion in FC-fMRI. In depth investigation and rigorous control of motion in a single highly sampled subject has not previously been achieved and will demonstrate the upper limit of our ability to describe and correct motion artifact. With this information, generalizable gains in fMRI processing will follow, especially in mental illness and developmental research where human fMRI is especially important. Following the global push for openness in research and collaborative science, I will use data that is publically available and openly share all analytic programming code necessary to complete these analyses on GitHub so that the entire neuroimaging community may use and expand upon this work. This type of detailed fMRI artifact investigation is crucial for its validity and without it, progress may be dampened and slowed by confounds that are not adequately managed by current processing methods. Receiving support from NSF would allow me to develop these important motion correction methods during graduate school. Feasibility and Support: The proposed research is to be completed with Dr. Satterthwaite, a leader in the fMRI literature relating to motion artifacts and an investigator for the PNC4 .This will streamline access to the PNC data and its growing longitudinal child dataset. My experience with motion correction in a high motion population (see personal statement) and the tools I have previously developed to do this will immediately lend itself to this project. Access to NSF’s XSEDE computing resources, made possible by this fellowship, will be indispensable for parallelizing the computationally-intensive analysis of the large datasets in this proposal. References: (1) PubMed Search “fMRI OR functional MRI OR functional magnetic resonance imaging”. (2) Cole MW, Bassett DS, Power JD, Braver TS, Petersen SE (2014) Intrinsic and task-evoked network architectures of the human brain. Neuron 83: 238 –251. (3) Power, J.D., Barnes, K.A., Snyder, A.Z., Schlaggar, B.L., Petersen, S.E., 2012. Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. NeuroImage 59, 2142–2154. (4) Satterthwaite, T.D., Elliott, M.A., Gerraty, R.T., Ruparel, K., Loughead, J., Calkins, M.E., Eickhoff, S.B., Hakonarson, H., Gur, R.C., Gur, R.E., Wolf, D.H., 2013. An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data. NeuroImage 64, 240–256. (5) Power, J.D., Mitra, A., Laumann, T.O., Snyder, A.Z., Schlaggar, B.L., Petersen, S.E., 2014. Methods to detect, characterize, and remove motion artifact in resting state fMRI. NeuroImage 84, 320–341. (6) http://myconnectome.org/	Winner!
169	Hypotheses: Molecular dynamics simulations can be used to quantify proton transport capabilities of amphiprotic materials for use in hydrogen fuel cells. Introduction: Vehicular internal combustion engines are responsible for 28% of greenhouse gas emissions in the United States and are the second biggest source of these emissions (1). Proton exchange membrane (PEM) hydrogen fuel cells are a cleaner alternative to the internal combustion engine, emitting only water (2). The high cost of hydrogen fuel cells, however, impedes their success. Current fuel cells operate at low temperatures (85OC) to maintain loading of the membrane with water. Water serves as the proton exchange fluid. Operating at these temperatures requires an expensive platinum catalyst (3). To overcome this problem I will implement cutting- edge modeling in the Scott Auerbach laboratory at UMass Amherst to develop anhydrous proton exchange materials and improve the viability of PEM fuel cells. Among the most promising proton conducting materials are azoles – five-membered carbon and nitrogen rings that both accept and donate protons (4). To increase molecule stability, we tether them to oligomer chains. Oligomers offer a compromise between material stability and liquid-like flexibility, allowing for faster and more efficient proton motion (5). Molecular dynamics (MD) simulations enable the study of hydrogen bonding interactions of these systems. I propose investigating the hydrogen bond lifetime and reorientation rate of amphiprotes tethered to oligomers using MD simulations. I will accomplish the following goals: 1) Run atomistic MD simulations on tethered imidazole; 2) Run coarse-grain MD simulations on tethered imidazole; 3) Benchmark across length scales; 4) Use both models to investigate other azoles. The project focus is on hydrogen bonding networks that govern proton diffusivity. Through modeling we relate microscopic properties to macroscopic performance to design new PEMs. Background: To study hydrogen bonding networks formed by tethered amphiprotes, we run molecular dynamics (MD) simulations, which allow the observation of atomic-level changes. Efficient proton transport takes place via the Grotthuss mechanism, which involves the transport of a proton by the collective motion of many hydrogen bonds (Figure 1). This is reminiscent of bucket brigades to put out fires. The Grotthuss mechanism requires hydrogen bond networks followed by functional group rotation before transport of the next proton (6). Herein lies the challenge of designing efficient proton conductors: extended hydrogen bonding arises in solid-like systems, Figure 1: Grotthus mechanism of while rapid functional group rotation occurs in liquid-like proton transfer in imidazole (6) systems. MD allows us to compare the atomic level trade- offs between extended hydrogen bond clusters and functional group dynamics. Balancing these parameters is vital for designing next-generation PEMs. 1) MD of tethered imidazole: Imidazole is promising because it offers long H-bond lifetime and fast reorientation compared to other azoles (5). I will use MD software DL-POLY to run atomistic simulations on imidazole oligomers. With NPT constant temperature and pressure simulations we extract volume parameters that we input into NVE constant volume and energy models. These simulations enable calculations of hydrogen bond cluster size, lifetime, and reorientation rate. These simulations cover extremely short times – on the order of 10-9 seconds. To study proton transfer over more realistic scales, the next step is coarse-grain modeling. 2) Coarse-grain modeling of tethered imidazole: In MD, the formation and reorientation of hydrogen bonds occur on the order of picoseconds (10–12 s) to nanoseconds (10–9 s), while the MD time step is femtoseconds (10–15 s), thus requiring prohibitively long simulations. Coarse-grained MD allow us to increase the time step by restricting atomistic degrees of freedom (such as vibrational modes between atoms.) Coarse-graining also allows us to consider systems approaching macroscopic dimensions of real membranes. I will build a coarse-grained model in Gromacs software using the Martini force field (7). Martini maps four functional groups (such as CH ) to one coarse-grain bead. These beads imitate the behavior of the functional groups they 2 represent by replicating their dipole moments. Atomistic simulations consist of 300 oligomers, but through coarse-graining we can model up to 4500 oligomers, an order of magnitude increase. This will enhance our understanding of the behavior of proton transfer fluids in an actual PEM. 3) Model benchmarking: We will benchmark coarse-grain models against atomic-level simulations to ensure that hydrogen bond properties agree across length scales. To do so I will work with Qinfang Sun, a graduate student in the Auerbach lab. She has built atomic-level simulations of azole liquids and oligomers, and her expertise will enable me to be successful. I will use her results to build a coarse-grained system to study long range interactions between molecules. By combining the results from our studies, we will have a much more complete picture of proton transfer and potential for use of tethered amphiprotic materials in fuel cells. 4) Expansion of model to other amphiprotes: Once the coarse-grain model is benchmarked, it becomes an effective tool for studying PEM materials. I will investigate how other azoles, including triazole, tetrazole, and pyrazole, change the nature of hydrogen bonding cluster size and lifetime. I will also study how oligomer backbone length affects these parameters, searching for balance between percolating hydrogen bonds and rapid reorientation dynamics. Intellectual merit: My background as a chemical engineer is crucial for this project, combining my knowledge of chemistry with my engineering, problem-solving perspective. I will use my knowledge to build models that accurately represent material behavior. I will implement the models to study hydrogen bond capabilities of proton exchange materials. However, this project will not be as simple as determining which material offers the highest performance. I must find the optimal trade-off between hydrogen bond lifetime and reorientation rate. It is my responsibility to decide which materials are most promising for next generation PEMs. Broader Impacts: Hydrogen fuel cells offer environmentally sustainable transportation. My research will further PEM development by identifying the most promising materials to focus on in lab testing. These materials have the potential to revolutionize PEM fuel cells, reducing cost and increasing cell lifetime. The goal of this research is to make fuel cells competitive with the internal combustion engine, offering an environmentally friendly and cost effective alternative. If successful, my research will give everyone access to affordable, green transportation. References 1. EPA. Sources of Greenhouse Gas Emissions. 2014.http://www.epa.gov/climatechange/ghgemissions/sources.html 2. EPA. Fuel Cells & Vehicles. 2012. http://www.epa.gov/fuelcell/basicinfo.htm#performance 3. Baschuk, JJ, Li, X. Carbon monoxide poisoning of proton exchange membrane fuel cells. International Journal of Energy Research. 2001; 25(8): 695-713. 4. Viswanathan, U, Basak, D, Venkataraman, D, Fermann, JT, Auerbach, SM. Modeling Energy Landscapes of Proton Motion in Nonaqueous Tethered Proton Wires. J. Phys. Chem. A. 2011; 115: 54325-5434. 5. Harvey, JA., Auerbach, SM. Simulating Hydrogen-Bond Structure and Dynamics in Glassy Solids Composed of Imidazole Oligomers. J. Phys. Chem. B. 2014; 118: 7609-7617. 6. Mangiatordi, GF, Laage, D, Adamo, C. Backbone effects on the charge transport in poly-imidazole membranes: a theoretical study. J. Mat. Chem. A. 2013; 1: 7751–7759. 7. MARTINI Coarse Grain Force Field for Biomolecular Simulations. http://cgmartini.nl/	Winner!
170	Sedge-Dominated Sites in Discontinuous Permafrost Peatlands Introduction: Rising temperatures in the subarctic are accelerating thaw of organic-rich permafrost peatlands, liberating organic carbon (C) from long term storage through microbial decomposition, and increasing methane (CH ) emissions1,2. Methanotrophic bacteria can oxidize 4 (consume) CH in thawing peatlands, producing carbon dioxide (CO )3,4. Recent biogeochemical 4 2 research has elucidated the controls of CH production and flux in thawing peatlands; however, 4 CH oxidation and its controls remain significantly less understood than CH flux. 4 4 Both CH flux and CH oxidation vary along the gradient of permafrost thaw. As thaw 4 4 occurs, hydrology, plant communities, and geochemical characteristics will all vary spatially and exert controls on the carbon dynamics of thaw5. MethaneandCO are both greenhouse gases, but 2 CH has a >30x larger warming potential than CO . Thus, it is critical to gain further insight into 4 2 the factors determining the ratio of CH flux and CH oxidation in thawing peatlands to produce 4 4 accurate emissions projections for climate change models. Preliminary data from July 2015 from Stordalen Mire in Abisko, Sweden (68°21'N, 18°49'E) provides evidence for permafrost thaw- induced methane oxidation at open-water sedge sites adjacent to collapsing permafrost palsas (Fig. 1). Sedges contain aerenchymous tissues that enable gas transport in and out of the water column4. Sedges act as a conduit for CH out of 4 the water column and transport oxygen (O ) into 2 the rooting zone (rhizosphere), enabling CH 4 oxidation below the water table4. One major gap in validation of biogeochemical models of wetland CH emissions 4 Fig. 1. Cores from above (surface) and below is the lack of Eh or redox potential measurements. (depth) the water table were extracted from 10 sites Field measured Eh of pore water in sedge rooting over a permafrost thaw gradient. Bars represent zones in thawing peatlands should show the extent mean oxidation rate across replicates ± standard error; n=4. By thaw stage: F = 19.75, p < 0.001, by of O diffusion into the water column and indicate 2 depth and thaw stage*depth: p > 0.05. whether aerobic (CH oxidation) or anaerobic (CH 4 4 production) processes are the dominant microbial metabolic pathways in thawing permafrost6,7. Little data is available on the redox state of peatland pore waters because reliable field Eh measurements have previously been difficult to attain and reproduce. Carefully calibrated field Eh electrodes will enable more accurate in situ redox potential measurements8. In addition to field Eh measurements, laboratory incubations of peat from these locations may link redox potential and areas of CH oxidation, particularly in the rhizosphere. 4 Measuring changes in redox potential through the water column of open-water sedge sites will enable more accurate modeling of carbon dynamics in thawing subarctic peatlands, as it will indicate the zones in which CH production or CH oxidation dominate. Current research at 4 4 Stordalen Mire uses the DeNitrification-DeComposition Model (DNDC) to test CH production 4 pathways and flux9. Further monitoring of CH oxidation and its relationship to redox potential 4 will enable DNDC to better integrate CH oxidation and validate wetland CH emissions 4 4 predictions. Understanding the connection between pore water redox potential and oxidation rates in open-water sites is critical to understanding how permafrost thaw will influence carbon dynamics and geochemical characteristics in transitional thaw stages in peatlands as thaw progression advances. Hypotheses: By pairing in situ Eh measurements in open-water sedge sites and laboratory incubations of biomass from sedge sites at each Eh measurement depth, I will test the following hypotheses. 1. A positive correlation exists between redox potential and potential oxidation rate. 2. Redox potential and potential oxidation rate will be highest in the rhizosphere relative to the rest of the water column in open-water sedge sites. 3. Incorporation of redox potential and CH 4 oxidation rates to DNDC will improve modeling of wetland CH emissions. 4 Methods: My research will focus on open-water sedge sites in Stordalen Mire, a thawing subarctic permafrost peatland complex in northernmost Sweden containing palsas, semi-wet Sphagnum sites, wet sedge-dominated sites, shallow lakes, and thaw ponds. As part of my REU experience, I collected preliminary oxidation data using incubations of peat from across a permafrost thaw gradient in July 2015, which revealed high potential oxidation rates in open- water sedge sites proximal to thawing palsas. I will measure redox conditions (Eh) with a platinum electrode8 through the water column in wet sedge areas in Stordalen Mire throughout the course of the snow-free season (June-September). I will also measure environmental correlates including CH flux, water table 4 depth, thaw depth, pH, and plant community composition to examine potential relationships between redox potential and other environmental variables. I will couple Eh measurements with aerobic incubations10 of sedge biomass to determine the relationship between redox potential and potential oxidation rates in sedge areas. Incubations will occur at in situ temperatures and CH 4 concentrations, as determined by field measurements from the preceding field season. Incubation protocol will be held constant across all replicates. After collection, redox potentials, oxidation rates, and environmental data can be incorporated into DNDC to test their effect on emissions scenarios from Stordalen Mire and other similar permafrost peatland complexes. I will work with my advisor’s (R. Varner) collaborators at UNH to integrate these data to DNDC. Intellectual Merit: This project will provide some of the first empirical data on in situ O 2 diffusion through the water column in thawing peatland complexes and its effects on carbon dynamics. As previous models rely on hypothesized values for Eh and O diffusion, these data 2 are essential to developing more accurate biogeochemical models of thawing peatlands to yield more reliable estimates of CH emissions from wetlands as climate changes. 4 Broader Impacts: I am committed to better understanding climate change and biogeochemical systems to both further scientific understanding and to guide mitigation of future carbon emissions and climate change effects. Elucidating less-understood aspects of carbon dynamics, like CH oxidation, and their effect on future emissions is key to creating effective mitigation. 4 I will participate in programs facilitated by the Joan and James Leitzel Center for Mathematics, Science, and Engineering Education (R. Varner, Director) to educate the wider community about global environmental change. Programming from the Leitzel Center aims to engage teachers, high school students, and undergraduates in STEM activities. I will implement educational outreach in regional schools and mentor undergraduate students on research projects, with particular interest in encouraging female and low-income students to pursue their interests in the STEM field. References: 1Turetsky, M.R., et al. (2014), Global Change Biol., 2183, 2Callaghan, T.V. et al. (2010), Geophys. Res. Letts., 3Kip, N., et al. (2010), Nature Geosci., 617, 4Ström, L. et al. (2005), Biogeochem., 65, 5Malhotra, A. and Roulet, N.T. (2015), Biogeosci., 3119, 6de Mars, H. and Wassen, M.J. (1999), Plant Ecol., 41, 7Popp, T.J., et al., (2000), Biogeochem., 259, 8Hagris, T.J. and Twilley, R.R. (1994), Res. Methods Papers, 684., 9Deng, J., et al., (2014), Biogeosci, 4753., 10Larmola, T., et al., (2013), Eco. Soc. of America, 2356.	Winner!
171	The evolution of similar traits in distantly related species is one of nature’s great surprises. Convergent evolution of traits has been widely observed throughout the animal kingdom; however, it remains unclear whether this phenotypic convergence results from convergent evolution of genes (Stern, Nat Rev Genet 2013). On a molecular level, convergent evolution occurs when the amino acids of a specific protein preferentially undergo mutations that produce similar or even identical amino acid sequences within distinct evolutionary lineages. A high level of adaptive convergent evolution – that is, convergence due to positive selection of beneficial mutations – would suggest that some genes have “optimal configurations,” which evolution uses and reuses across species. If such genes exist, then evolution is somewhat predictable, proceeding by one of a small number of possible paths (Stern & Orgogozo, Science 2009). In contrast, the complete absence of adaptive convergence would indicate that protein configurations beneficial to one species are seldom optimal within other species, and that evolution may proceed by a much wider set of paths. Thus, two fundamental questions in evolutionary molecular biology are 1) how often convergent molecular evolution occurs and 2) whether molecular convergence is a primary driver of phenotypic convergence. To adequately answer these questions, genome-wide analyses are needed; however, most previous analyses of convergent evolution have been limited to single genes (Li et al., Curr Biol 2010) or small taxa (Bazykin et al., Biol Direct 2007). Recently, the advent of whole-genome sequencing has opened new opportunities for exploring molecular convergence. Published genomes now exist for over 100 animal species, and 177 more are currently underway (Koepfli et al., Annu Rev Anim Biosci 2015). Additionally, phenotypes for 4,541 mammalian traits have been documented in a public database (O’Leary et al., Cladistics 2011). Together, these data enable a genome- wide search for convergent mutations throughout mammalian species followed by a test for association between convergent genes and specific convergent phenotypes, which might achieve strong statistical power for detecting adaptive convergence. I plan to develop and apply a statistical model to test for convergent evolution among 61 sequenced mammalian species. I hypothesize that convergent molecular evolution occurs at a higher rate than has been previously observed, and that this genetic convergence drives convergence of observable traits. This model will extend the boundaries of biological knowledge by measuring the frequency of adaptive convergence, and will augment existing statistical methods in a broadly applicable framework that can be extended to other genomes in the future. Aim 1: Develop a statistical framework for inferring convergent evolution between genomes. I will make my framework broadly generalizable to any data set with the following inputs: 1) a phylogeny containing the accepted evolutionary relationships within a set of species, and 2) the pairwise sequence alignments of all proteins in those species for which unique human orthologs exist. My analysis will integrate these data as follows:  Step 1: Infer ancestral sequences. For every amino acid position in every sequence, I will employ a linear-time approach to compute probability distributions of DNA and amino acid sequences at each ancestral node in the phylogeny (Yang, Mol Biol Evol 2007).  Step 2: Infer patterns of adaptive convergent evolution between species pairs. At the protein sequence level, I will apply a statistical analysis adapted from the CONVERGE algorithm described by Zhang & Kumar (Mol Biol Evol 1997). For every pair of species within our analysis, I will examine each amino acid and compute the probability that this amino acid converged between the two species, using the ancestral sequences from the previous step as reference. I will then determine the likelihood of convergence within each exon and within the gene. Because rapidly evolving genes often converge by random chance, called neutral convergence, I will control for gene-specific mutation frequencies in order to distinguish adaptive convergence from neutral convergence (Thomas & Hahn, Mol Biol Evol 2015).  Step 3: Evaluate performance on simulated data. I will simulate evolution of protein sequences in which a small pre-selected subset of sequences evolve under a non-neutral convergence pattern and the rest evolve neutrally. I will then test my model’s precision and recall in inferring which of the sequences evolved under the convergent model, correcting for false discovery. If my model is able to detect evidence of convergent evolution in the simulated data at a low false discovery rate, but I observe no signs of convergent evolution in my analysis of the biological data, then these results will cast doubt on the hypothesis that non-neutral convergence is a significant driving force in molecular evolution. Aim 2: Quantify the levels of convergent evolution within mammalian genomes, and identify functions enriched within convergent genes.  Step 1: Identify specific genes that have evolved in convergence across species. Using public alignment tools (Kent et al., PNAS 2003), the Bejerano Lab has obtained cross-species sequence alignments and a phylogenetic tree for 61 mammalian species. I will apply my statistical framework to all pairs of sufficiently diverged mammalian species and identify particular genes within each species-species pairing that exhibit non-neutral convergence. I hypothesize that I will find widespread evidence of adaptive convergent evolution.  Step 2: Identify potential phenotypic effects of convergent genotypes. If evolutionary convergence is truly adaptive, this would suggest that genes converge when they undergo similar selective pressures within different species (Castoe et al., PNAS 2009). Therefore, I hypothesize that genes facing similar pressures within independent species will be more likely to evolve in convergence within those species. For example, in aquatic mammals such as dolphins, manatees, and walruses, we might expect high convergence of skin- expressed genes involved in thermoregulation. I have identified a set of 20 convergent phenotypes that arose independently in mammals. For each of these phenotypes, I will conduct Gene Ontology enrichment analysis on genes that show elevated levels of convergence among the animals possessing the phenotype in question. As a null model, I will apply the same enrichment tests within groups of species that do not exhibit phenotypic convergence. If my hypothesis is correct, this analysis will suggest functions for which similar selective pressures across species have necessitated similar courses of protein evolution across these species. Importantly, in order to confirm these putative links between convergent genotypes and convergent phenotypes, future experiments will be necessary. The specific amino acid substitutions that I identify will be prime targets for further examination in functional assays, as described by Liu et al. (Mol Biol Evol 2014). My experience developing statistical models to quantify bias in genome-wide DNase-seq experiments (Gloudemans, 2015 Honors Thesis) makes me well-qualified to develop a genome- wide statistical model of evolution. This project will help us to understand whether phenotypic convergence is a direct result of genotypic convergence. To further facilitate broader impacts, I will create a user-friendly visualization that highlights hotspots of convergent evolution, and I will make this tool publicly available in the UCSC genome browser. This tool will allow biologists to visually explore instances of molecular convergence and to ask even deeper questions about the specific functional roles of these convergent mutations, ultimately increasing our understanding of how these proteins shape the lives of humans and all other species.	HM
173	first confirmed physics beyond the standard model, but has proven impossible to measure in the lab. However, neutrinos make up a significant fraction of the mass in the universe (up to 1%). Due to their very low, but non-zero, mass, they have a distinct effect on the growth of large scale structure. At early times, neutrinos behaved as “hot” dark matter but became “cold” dark matter as the universe expanded. Hot dark matter is able to stream freely through potential wells created by massive objects, pulling them apart while cold dark matter becomes trapped and enhances the structure. The formation of galaxy clusters, which are the most massive gravitationally bound objectsintheuniverse,ishighlysensitivetotheneutrinomass. Moremassiveneutrinosbecomecold earlier, leading to a larger number of high-mass clusters. Through the Sunyaev-Zel’dovich Effect, high-resolution microwave telescopes are able measure the abundance of clusters over cosmic time. The SZE is the result of photons in the Cosmic Microwave Background (CMB) interacting with high-energy electrons in a galaxy cluster. When CMB photons scatter off the electrons, they are shifted in frequency. At low frequencies (below ∼220 GHz), this results in a reduction of the CMB intensity. These relatively small “shadows” are used to detect galaxy clusters in high resolution CMB surveys. The amplitude of the effect can be used to estimate the cluster mass. Current cluster cosmology studies are limited by systematic biases of the cluster masses, some of which comes from emissive sources in the clusters themselves. Dust emission due to start formation in the member galaxies is a potentially important bias of cluster mass. In 2015, the Planck Collaboration released a paper exploring the correlation between the SZE and the Cosmic Infrared Background (CIB) [1]. The CIB is emitted by hot gas in star forming galaxies. Unfortunately, the Planck cluster catalog only extends to redshift of ∼1.0, and the CIB primarily comes from redshift >1.0. For cluster surveys with higher angular resolution (such as those produced by data collected from the South Pole Telescope and the Atacama Cosmology Telescope), this effect has not been quantified for most of their clusters. I will probe the emission from dust in galaxy clusters out to redshift ∼1.5, using clustersfoundindatafromtheSouthPoleTelescope(SPT)andCIBmapsfromPlanck. Following [1], there are two methods for performing this correlation. The first is a stacking analysis. I will make small cutouts of the Planck CIB maps and SPT CMB maps at the locations of SPT-selected clusters. Stacking the cutouts and using aperture photometry to extract the signal strength will reveal the average correlation between the SZE and CIB in each frequency band. The second method uses Fourier techniques to account for both the clusters detected at high signal to noise and lower significance clusters that are below the detection threshold. I will create an angular power spectrum of the correlation of each band with the SZE signal. These two methods are complementary. The first is a very direct probe of known clusters. The second includes lower significance sources of SZE signal, which leads to higher signal to noise. Both of them require careful modeling of the SZE and CIB in order to distinguish the two terms. ExtendingthisworkbythePlanckteamwillprobearegionofredshiftthatismoreimportantto the systematic bias of cluster mass. The star formation rate is increasing over the range that I will probe,sotheemissionfromdustismoresignificant. Furthermore,mappingoutthecorrelationasa functionofredshiftwillallowmetoprobehowitchangesoverthehistoryoftheuniverse. Ofcourse, this comes with additional analysis challenges, most of which have to do with the extreme distance of the high redshift clusters in the SPT catalog. Since the CIB is emitted by hot compact objects, its amplitude decreases with distance (unlike the SZE, which is a spectral distortion of the CMB). Pushing to higher redshift will mean some loss in signal, simply from the distance. On the other 1 hand, the surface brightness (amplitude at the source) of the CIB is higher in the redshift range I willprobe. Thisisduetotheincreasedstarformationrate. Theexpansionofspaceeffectivelyshifts thePlanckbandsintoahigherintensityregionoftheemissionspectra,furthercounteringthiseffect. My previous work with SPT has prepared me well for this project. I have been running a search for clusters using data collected by the SPT in 2012 and 2013. Through this work, I have become accustomed to working with CMB maps through both my low level analysis tasks, and the high level work to produce a cluster catalog from SPT data. Furthermore, I have been exposed to many more analysis techniques through collaborative work with my colleagues. Finally, as part of the SPT collaboration, I will have access to all of the resources I need to complete this project: SPT CMB maps filtered appropriately to isolate the SZE, Planck CIB maps (which are publicly available) and minimal computing resources. This work will address a significant unknown in the systematic error budget of cluster masses. Even if I find significant contamination of the SZE by the CIB, that is simply the first step in accounting for the error. Since cluster cosmology is currently limited by systematic errors on cluster mass, caused by effects like this, understanding and eliminating them will have lead to significant improvements in cosmological constraints from cluster surveys. Beyond the impact on future cluster cosmology surveys, the results of this correlation probe several other astrophysical phenomena. The Butcher-Oemler Effect [2] predicts that star formation is suppressed in galaxy clusters, relative to a similar galaxy outside a cluster. Since the CIB traces star formation, this correlation would determine if there is statistical evidence for the Butcher-Oemler Effect. Furthermore, thecorrelationcanbebinnedinredshifttodetermineifthestarformationinclusters is time-dependent. The stacking method also includes spatial information, which allowed Planck to determine that the star formation in low redshift clusters is primarily in the outskirts of the cluster. This analysis would extend our knowledge to older clusters. The results of this work also have appeal outside of the scientific community. They help us to understand star formation in the largest objects in the universe. Using SPT-selected clusters allows us to look back in time, when the star formation rate was highest. I will present this knowledge to the general public through several outlets. There are multiple local astronomical societies (the Eastbay Astronomical Society and the Mount Diablo Astronomical Society, for example) that are very interested in having graduate students speak about their work. I will be giving talks to some of them in the near future on my current work with the SPT, so when this work is done, it will be easy for me to return and present our new findings. My research group is also starting a collaboration with the Chabot Space and Science Center. I will work with then to create a standalone exhibit detailing my work, and arrange talks for the public. References [1] PlanckCollaborationetal. Planck2015results.XXIII.ThethermalSunyaev-Zeldovicheffect–cosmicinfrared backgroundcorrelation. ArXiv e-prints,September2015. [2] H. Butcher and A. Oemler, Jr. The evolution of galaxies in clusters. I - ISIT photometry of C1 0024+1654 and3C295. Astrophysical Journal,219:18–30,January1978. 2	Winner!
174	in condensed matter physics. In particular, topological defects have underlied diverse phenomena fromfractionalelectriccharge1tothesuperfluidtransitioninliquidhelium-42. Evenmorerecently, theimportationofideasfromtopologyinelectronicsystemstophotonicsystemshasledtoaflood ofnewdiscoveriesandpotentialdevices3. MyPh.D.researchwillfocusonthepredictionofnovel propertiesoftopologicaldefectsinphotonicandelectronicsystems,withanemphasisonboththe fundamentalphysicsanddeviceapplicationsofthesedefects. Intellectual Merit: I have past experience researching a particularly striking property of topological defects. Since July I have been doing research full-time under Professor Claudio Cha- monatBostonUniversity,whereinarecentpublication(Science,inreview)wehaveproposed to use topological defects to realize non-Abelian exchange statistics in photonic systems4. The defect occurs as a vortex in the order parameter describing a certain lattice distortion in two- dimensional honeycomb lattices, and leads to the formation of localized states which gain non- Abelian geometric phases upon their exchange. Our proposal is the first to predict non-Abelian statisticsinphotonicsystems,asopposedtothedelicateelectronicsystemstheirsearchwasprevi- ouslyconfinedto,andcouldthusleadtotheirfirstconclusiveexperimentaldetection. MypastresearchexemplifieswhatIfindfascinatingabouttopologicaldefectsinelectronic and photonic systems - exotic phenomena, with the potential for real-world realization and appli- cation. My future work willcontinue to espouse these traits. I propose two projects. In project (1) Iwillextendthedefectstatesstudiedinmypreviousworktoafamilyofstatesobeyingabroader class of non-Abelian exchange statistics, and use these states to propose a novel topological nonreciprocal optical device. In project (2) I will separately extend these defect states to Weyl semimetalsandtheirphotonicanalogues,andshowthattheycansustaindissipationlesstransport in both systems. Further, I will address how these topological defects could naturally arise in Weylsemimetalsusingafieldtheoreticapproach. Idescribeeachprojectinmoredetailbelow. Project (1) will seek to realize novel forms of non-Abelian statistics, which when imple- mented in photonics could lead to a strongly nonreciprocal optical device. While the statistics I previouslystudiedrespectreciprocity(lightpassedthroughanexchangeinonedirectionwillgain thesamephaseshiftasintheoppositedirection),Ihavediscoveredasimple,exactlysolvable,fam- ilyoftopologicaldefectstateswith“nonreciprocal”statistics. Importantly,theHamiltoniangiving rise to these states breaks a particular symmetry present in the previous, “reciprocal”, Hamilto- nian. Unfortunately, these particular defects would be difficult to realize in photonics experiment. My research will thus take clues from this simple model and focus on using symmetry classifi- cation to discover new nonreciprocal extensions of these topological defect states, especially those which are experimentally feasible. Breaking symmetries of the system will bring us into a different symmetry class, in which case I will determine whether nonreciprocity and topological protection of our state can coexist. I will be aided by international collaboration with Professor Chamon’s colleague Doctor Christopher Mudry at the Paul Scherrer Institute in Switzerland, an expert in symmetry classification of topological defects. In determining which photonic systems are feasible, I will benefit from current collaboration with engineering and experimental pho- tonics groups at Boston University and MIT working to realize the experiment proposed in my previous publication. Beyond device applications, providing a physical realization of this class of non-Abelianstatisticswouldconstituteamajoradvancementinfundamentalphysics. In Project (2) I will pursue a separate extension of the same topological defect states into both electronic and photonic systems, in which I propose to use a 3D generalization of these states to achieve topologically-protected dissipationless transport in Weyl semimetals. Weyl semimetals have a low-energy electronic theory similar to the systems I previously researched5, and are thus capable of sustaining 3D analogs of the same defect states. Crucially however, these formerly-2D states will now extend in the third direction, and can carry current in that direction. I have already analytically shown that such states would have a chiral dispersion relation, leadingtoalackofbackscatteringandthusdissipationlesstransport. Tosubstantiatetheseclaims, in the first part of project (2) I will determine the position-space changes in hopping needed to create these topological defects, using common tight-binding models of Weyl semimetals. As in mypreviouswork,Iwillusethesehoppingstoperformexactdiagonalizationsimulationstoquan- titativelyprobetheresultingdefectstates,andverifytheirchiraldispersionrelation. Experimental verification of these properties may be easiest in photonic realizations of Weyl semimetals3, for which I would adapt my model to the photonic setting and find estimates of experimental condi- tionsneededforthestates’realization,similartomypreviouswork. Thesecondpartofmyproject(2)willaddressthefactthat,inelectronics,theconductance ofasingledefectwouldbesmallevenwithdissipationlesstransport. Toremedythis,Iwillsearch for a field theory of the order parameter describing the appropriate hopping distortions, in which the creation of a macroscopic number of topological defects would occur. The starting point must be a nonzero mean field value of this order parameter. To determine the conditions for this to occur, I will introduce the appropriate order parameter couplings into the electronic field theory, and integrate out the electron degrees of freedom to derive an effective action for the or- der parameter. Assuming a nonzero mean field value, one would anticipate a massless Goldstone mode associated with fluctuations in the order parameter phase. It would then be an open prob- lem to determine how topological defects in this Goldstone mode might form, although pursuing connections to vortex formation elsewhere in condensed matter may be fruitful. My extensive graduate-levelcourseworkinfieldtheorywellpreparesmeforthiscomponentoftheproject. BroaderImpacts: Bothofmyproposedprojectsfeaturetopologically-protectedstateswith large potential societal impacts through use in photonic and/or electronic devices. Project (1) proposes to use the geometric phase of a topological defect state to uniquely achieve strongly nonreciprocal photonic devices, usable in essential optical elements such as isolators and circula- tors. This could greatly reduce undesirable optical loss compared to traditional nonreciprocal devices, which rely on weakly (relative to my proposal) nonreciprocal materials such as ferrite or strong permanent magnets. Additionally, my proposal would not require breaking time-reversal symmetry,openingthedoortononreciprocaldevicesusingonlyconventionalmaterials! Project (2) seeks to realize defect states with chiral dispersion in Weyl systems, with po- tential impact in both photonics and electronics. In photonics, these modes could function as losslessopticallines,robustagainstimperfectionsduetotheirtopologicalorigin. Suchtopolog- ical “one-way waveguides” have been realized in photonic quantum hall edge states, and have a plethora of novel device applications3. In electronics, the accumulation of a macroscopic number ofthesemodeswouldleadtomassiveconductance,withclearlytremendousdeviceimplications. Assessing the applications and limitations of such devices would require a theory for how this accumulation of topological defects occurs, highlighting the need for the second component of my project (2). All together, this array of broader impacts makes the investigation of topological defectsinphotonicsandelectronicswellworthpursuing. TheNSFGRFPwillallowmetodoso. [1]R.Jackiw,etal.,Phys. Rev. D(1976). [2]J.M.Kosterlitz,etal., JournalofPhysicsC (1973). [3]L.Lu,etal., NaturePhotonics(2014). [4]T.Iadecola,etal.,(2015). [5]X.Wan,etal.,PhysicalReviewB(2011).	Winner!
175	Dispositional Risk Factors to False Confessions: Personality Traits and Psychopathologies In 2012, Pedro Hernandez was brought in for questioning for the 1979 abduction of six- year-old Etan Patz. Mr. Hernandez, a man with no criminal history, confessed to the abduction and murder of Patz.1 However, his diagnosis of schizotypal personality disorder, extremely low IQ, and prolonged and unrecorded interrogation—all known risk factors to making false confes- sions (FCs)—convinced a lone holdout juror of his innocence, prompting a mistrial in the murder case against Mr. Hernandez. He is scheduled to be retried in February 2016. The potentially ex- culpatory circumstances of this ongoing case share many similarities with other post-conviction DNA exonerations. In fact, research suggests that FCs are present in a significant minority (~27%) of all DNA exoneration cases. The purpose of this study is to empirically examine dis- positional risk factors associated with FCs among a subgroup over-represented within the criminal justice system and among confirmed FC cases: persons with psychopathologies. Background & Rationale. Psycho-legal scholars have proposed compelling theories to explain why innocent suspects admit to criminal acts they did not commit.2 Extensive research indicates that there are two types of risk factors: the use of certain interrogation tactics and dispositional characteristics. Historically, the study of police interrogation tactics has relied on experimental methods, producing a large body of science, while the study of dispositional risk factors has pri- marily employed archival and correlational studies. Thus, less is known about dispositional traits, such as personality. Two such personality traits–suggestibility and compliance–are thought to confer vulnerability to FCs within the context of coercive police interrogations.3,4 Further- more, fewer studies have examined the link between psychopathology and individual differences in these personality traits. Thus this project will investigate whether psychopathology increas- es the risk for suggestibility and/or compliance leading to increased prevalence of FCs. In addition to conferring risk to FCs, suggestibility and compliance may differentially mediate the types of FCs made by innocent suspects. In coerced-internalized confessions, inno- cent, but suggestible, suspects come to believe they are guilty, sometimes even confabulating false memories.2 To date, only one correlational study has assessed the link between individual differences in suggestibility and internalized FCs.5 The Gudjonsson Suggestibility Scale (GSS), a false memory paradigm where participants are presented with misleading suggestive infor- mation, is widely used among forensic psychologists in criminal cases involving disputed con- fessions.6 Yet no experimental research has attempted to use this scale to establish a causal link between suggestibility and internalized FCs—and certainly not among people with diagnosed psychopathologies. Hence, I will evaluate whether psychopathology increases the risk for suggestibility leading to coerced-internalized FCs. In coerced-compliant confessions, the innocent but compliant suspect is induced through interrogation to confess to a crime they did not commit for some immediate instrumental gain. In a naturalistic setting, the Gudjonsson Compliance Scale (GCS), a 20-item, self-report measure of compliance using a true/false format, was shown to discriminate between alleged false confes- sors and defendants who resisted confessing whilst being interrogated.6 Other correlational stud- ies indicate that compliance may be associated with anxiety, low self-esteem, and ADHD symp- toms3,6—factors present in many major psychopathologies. Because compliance is distinct from suggestibility and relevant to the making of FCs 6, I will evaluate whether psychopathology in- creases the risk for compliance leading to coerced-compliant FCs. Proposed Study. I plan to use a well-established experimental paradigm known to elicit FCs among innocent participants to explore these aims (see Kassin & Kiechel, 1996).7 First, partici- pants will be recruited for a ‘typing speed’ task and asked to fill out a demographics Stephanie A. Cardenas Graduate Research Proposal questionnaire. Psychopathologies will be assessed via the Mini-International Neuropsychiatric Interview-PLUS: a 15- minute structured diagnostic interview. Suggestibility and compliance will be measured using the GSS and the GCS, respectively. Next, participants will be assigned to one of four groups: 2 (slow vs. fast task pace) x 2 (presence vs. absence of false incriminating evidence) between-subjects factorial design. During the task, participants’ computers will suddenly ‘crash’ and a distressed experimenter will lay the blame on them. Two forensically relevant components will be manipu- lated: (1) participants’ subjective certainty of their own innocence by varying the pace of the task (i.e., slow-43 or fast-67 letters/min), and (2) the use of false incriminating evidence (i.e., false eyewitness account of alleged participant behavior by a confederate), a common U.S. interroga- tion tactic. To determine whether the paradigm elicited a compliant FC, participants will be asked to sign a handwritten confession admitting their role in the computer crash. To determine whether participants internalized the FC, a ‘curious’ confederate, will ask participants what hap- pened. Independent coders will determine whether participants unambiguously internalized fault based on their description of the event to the confederate (e.g., “I caused the computer to crash.”) Anticipated Results [1] Psychopathologies will predispose individuals to higher levels of sug- gestibility and compliance (Fig. 1). [2] Higher levels of suggestibility will confer increased risk to making coerced-internalized FCs. [3] Higher levels of compliance will confer increased risk to making coerced-compliant FCs. Although different psychopathologies may confer vulnerabil- ity through different pathways (e.g., social anxiety may be associated with increased compliance, and therefore compliant FCs), highly symptomatic individuals are known to have increased rates of alleged FCs regardless of primary diagnoses.8 Therefore, the role played by illness severity (e.g., bipolar disorder vs. hypomania) and time frame (e.g., current/past) will also be evaluated. Broader Impacts. The novel approach of this study integrates findings from criminology, clini- cal-forensic, and social psychology to test a hypothesis with notable implications for identifying and protecting individuals who are vulnerable to persuasion in the interrogation room because of psychopathologies (e.g., providing mandated access to legal advice from individuals sensitive to this population). By disseminating my findings at scientific conferences and in peer-reviewed ar- ticles, I will contribute to the understanding of how dispositional factors interact to influence the wrongful convictions of innocent suspects. This in turn will facilitate further collaboration be- tween attorneys, juries, judges, and social scientists to develop concrete ways to prevent these miscarriages of justice. Moreover, because juvenile status also confers increased risk to making FCs, the proximity at my proposed graduate institution to programs like College Bound and Summer Enrichment Camps and to intercity youths from disadvantaged backgrounds will allow me to provide a positive outlet within the community for academic advancement. Finally, future studies will examine the role of dispositional risk factors in the context of false guilty pleas (FGPs). Even though guilty pleas constitute nearly 95% of convictions in the U.S, FGPs remain grossly understudied. In fact, the same traits that place persons at risk for FCs may also place persons at risk for FGPs.9 Therefore, findings from my proposed project promise to put forward information that will inform this novel and unstudied related area of research Refs: 1 Goldstein & Hager (2015 May). 2 Kassin et al (2010) Law Hum Behav. 3 Gudjonsson et al (2008) Psychol Med. 4 Gudjonsson (1991) Med Sci Law. 5 Sigurdsson & Gudjonsson (1996) Per Indiv Differ. 6 Gudjonsson (2003) Wiley. 7 Kassin & Kiechel (1996) Psychol Sci. 8 Redlich (2010) Law Human Behav. 9 Redlich (in press) APA.	Winner!
176	Imagine an empty room with four walls. Away from the walls and inside the space, a person has the freedom to move 3-dimensionally in any direction and is constrained to move 1-dimensionally with respect to time. However, this person is unable to view the entire room while simultaneously residing in the interior. There are photons, particle interactions, and various physical phenomenon that are unable to transmit their information to the person when these events are out of sight − that is, unless this person is at the wall. At the bound- ary of this room, a person sacrifices a dimension of freedom in exchange for viewing the internal dynamics of their space. This way, a person is able to survey their space and return to the interior with knowledge of the native physical interactions − fully revealing the under- lying dynamics. This duality between an n-dimensional interior and an (n−1)-dimensional boundary is known as the Anti-deSitter Space/Conformal Field Theory Correspondence. This correspondence is powerful. General Relativity asserts that space-time and gravity are fundamentally connected, while a pivotal aspect of Quantum Field Theory is the freedom for symmetries to arise and reduces the number of degrees of freedom. Maldacena[1] was the first to assert that an interior space-time, such as Anti-deSitter Space described by General Relativity, could be connected to a conformal boundary, where a Quantum Field Theory would reside, thereby linking Gravity and QFT. This gauge/gravity duality has a variety of applications ranging from condensed matter experiments to particle physics. With respect to the Standard Model of Particle Physics, Quantum Field Theory asserts that fundamental particles can be described as excitations of quantum fields. These parti- cles, such as quarks and gluons, constitute most of the visible matter in the universe, and are described by Quantum Chromodynamics through the strong force. As a strongly cou- pled gauge theory that lacks a fully theoretical description, the mapping provided by the gauge/gravity duality might reveal a gravitational dual to QCD, which is not only highly desirable but also potentially feasible. Duringtheprevioussummer, Ihadtheopportunitytoconductresearchthroughmysecond NSF REU at the University of Minnesota studying the AdS/CFT conjecture as applied to non-perturbativegaugetheories, specificallyQuantumChromodynamics. Underthetutelage ofDr.JosephKapusta,Istudiedthepropertiesoftheglueball,aparticlecomposedofmultiple gluons predicted by the Standard Model, and a dilaton, a hypothetical particle that arises from the scalar fields accompanying gravity. To incorporate the behavior and structure of these fundamental particles into this duality, one usually embarks on the bottom up approach by assuming the existence of such a dual and, thereby, models QCD as an effective five-dimensional gravitational theory. This approach, known as AdS/QCD, provides the freedom for the computation of physical quantities in QCD that can then be tested at the high energy collisions at particle accelerators. At the REU, I developed the equations of motion from considering an action[2],[3] that connects both gravitational field and the glueball and dilaton fields. This pen and paper work derived an analytic expression for the potential and this result describes the behavior of these particles at the IR and UV energy ranges. Early in the universe’s lifetime and in modern-day particle collisions, a hot QCD quark-gluon plasma exists for short times whose behaviorposesachallengeforQCDphysics. Theresultsofourstudymightshedlightonthis plasma, while furthering a theoretical description for QCD using the AdS/CFT conjecture, thereby connecting theory and experiment. As the lead author, I have attended the DNP 2015 and APS 4C Conferences to present this work and aim to obtain my second publication. 1 Aditya Dhumuntarao Graduate Research Proposal I wish to continue applying the AdS/CFT Correspondence to QCD for my Ph.D dissertation. With my Ph.D research, I aim the further bridge the gap between the plethora of experimental evidence and the developing theoretical considerations for QCD by incorporating additional phenomenological metrics into the AdS/QCD conjecture. A critical question that I wish to pursue is the construction of a model incorporating the dilaton field, glueball field, quark field, and other matter fields with finite temperature field theory. As finite temperature quantum field theory describes the expectation values of physical observables at finite temperatures in (n−1)-dimensions and links them to statistical classical field theories, such as gravity, in n-dimensions, the AdS/QCD conjecture seems to have a natural supplement. Since Dr. Kapusta is a leader in the field of finite temperature theory[4], joining his research group is highly desirable for the formation of this model. Currently, IamworkingwithDr.Kapustatodeterminethepredictedglueballmassspectra from our REU analysis and ultimately compare this value against lattice QCD calculations. In fact, a recent publication[5] has argued that an experimentally detected particle, known as f (1710), is the glueball. Therefore, improving our model to include more realistic glueball 0 dynamics and performing a comparative analysis after determining the mass spectra of the glueball is of principle importance. In addition to tackling fundamental science harmoniously through theoretical and experi- mental considerations, this project contains broader impacts. A significant result from this analysis would lend further credence to the AdS/CFT conjecture. Although the conjecture was founded on the premise of revealing a theory of quantum gravity, a more immediate ap- plication resides in condensed matter experiment where chiral magnetic fields are frequently studied with the conjecture. Also by revealing physics beyond the Standard Model, we may be able to describe or construct new matter from gluons and quarks − one such highlight is the strong evidence for the tetraquark found at CERN. This new matter might impact our searches for dark matter often hypothesized[6] as complex quark matter, additional stable elements that may assist in creating new materials, and efforts in high energy plasma physics which has immediate applications for fusion. The purpose of the AdS/CFT conjecture is to explore and discover a unified description of the universe and the interactions within our universal boundary. Matter is the fundamen- tal link that interacts with and connects gravity with the other forces. In the short term, developing a theoretical framework for QCD will inform and drive further experimental en- deavors, while the long term promises support for a candidate conjecture that currently sees a versatility of applications. With the support from the NSF GRFP, I will have the free- dom to immediately pursue this research and continue my current studies of the AdS/CFT conjecture to develop a unifying framework of quantum matter and gravity. [1] J. M. Maldacena. The Large N limit of superconformal field theories and supergravity. Int. J. Theor. Phys., 38:1113–1133, 1999. [2] S. P. Bartz, J. I. Kapusta. Dynamical three-field ads/qcd model. Phys.Rev.D, 90:074034, 2014. [3] J. I. Kapusta, T. Springer. Potentials for soft-wall ads/qcd. Phys. Rev. D, 81:086009, 2010. [4] Joseph I. Kapusta, Charles Gale. Finite-Temperature Field Theory. Cambridge University Press, second edition, 2006. Cambridge Books Online. [5] F. Bru¨nner, A. Rebhan. Nonchiral enhancement of scalar glueball decay in the witten-sakai- sugimoto model. Phys. Rev. Lett., 115:131601, Sep 2015. [6] E. Witten. Cosmic separation of phases. Phys. Rev. D, 30:272–285, Jul 1984. 2	Winner!
181	In order to understand evolution, we need to understand the complex relationships between biological systems and fitness. This is a difficult task, because there are an enormous number of possible genetic states, and the mutations underlying these states interact non-additively to produce fitness. We can frame this problem by thinking of evolution as a process occurring on a high-dimensional map between this space of genetic possibilities and the fitness of each possibility, a function often referred to as the “fitness landscape.” As a population adapts to a particular environment, it moves between neighboring genotypes, constrained by the force of selection to follow paths of increasing fitness. By understanding the general properties of the fitness landscape, we can answer questions about the functional nature of a biological system - If a mutation knocks out this gene, what effect will that have on fitness? - and ask broad theoretical questions - Is evolution predictable, or does it depend on chance events? The growing field of experimental evolution provides an avenue for addressing these questions by empirically testing important features of the fitness landscapes of microbes. We now know that in the budding yeast Saccharomyces cerevisiae, the effect of a beneficial mutation depends on the fitness of the genetic background where it arises [1], but whether a similar pattern holds for deleterious mutations is an open question. Deleterious mutations may be common in populations due to environmental changes or population bottlenecks, and they provide a novel way to study adaptation and to test the role of contingency in evolution. In my PhD research, I will study the fitness landscape of S. cerevisiae by investigating the interplay between deleterious mutations and adaptation. I will complete my PhD research in Dr. Michael Desai’s lab at Harvard University, where I am uniquely situated to conduct work that combines genetics, experimental evolution, and deep sequencing. Aim 1. Changes in the fitness effects of loss of function mutations over adaptive trajectories Given that most loss of function (LOF) mutations are deleterious, competing models make different predictions about how their effects should change with increasing population fitness. The Desai lab recently found that in S. cerevisiae, the fitness effect of a beneficial mutation in a particular genetic background is primarily predicted by the fitness of the background, creating a pattern of “diminishing returns” during adaptation [1]. If this “global epistasis” model holds for all mutations, deleterious mutations should also become less deleterious as the population becomes more fit. In contrast, Fisher’s geometric model predicts that the fitness effect of some mutations will change from negative to positive at different levels of adaptation [2]. I will use transposon mutagenesis and sequencing fitness assays (Tn-seq) to measure the fitness effects of a large set of loss of function mutations in populations with different initial fitness backgrounds. Hypothesis: I predict that, in accordance with the global epistasis model [1], LOF mutations will be less deleterious in populations with higher fitness. Methods: First, I will evolve 24 S. cerevisiae populations in standard liquid media for 1000 generations (100 days), following a similar protocol to [3]. I will freeze samples every 250 generations to create a “frozen fossil record” of each population as it adapts and gains fitness. At each of the five timepoints in this record, I will unfreeze my populations and use Tn-seq to systematically probe the fitness effects of a large number of LOF mutations. As shown in the figure at right, Tn-seq consists of two steps. In A, I transform a gene disruption library into the population, causing a diverse set of single insertion mutations. In B, I track the frequency of each mutation over 30 generations using deep sequencing of a barcode region in the insertion [4]. Using this method, I can determine the fitness effect of every mutation in parallel by analyzing the change in its frequency [3,4]. I will create my DNA-barcoded transposon (Tn) gene disruption library using genomic DNA from S. cerevisiae [3], and by sequencing this library, I will associate a unique barcode with each gene disruption. These associations will allow me to connect my data to specific genes, yielding additional biologically relevant information about how S. cerevisiae adapts to laboratory conditions. Aim 2. Adaptation after disruption of the genetic system Evolution often involves transient environmental changes that alter selection pressure or population size, both of which can lead to the fixation of mutations that are not beneficial in the organism’s primary environment. Do these events affect long-term outcomes of evolution? The dynamics of adaptation after a population has been “bumped” off of its adaptive trajectory are not well understood, but they have the potential to distinguish between models of adaptation. For fitness landscape models in which mutations interact only additively, any deleterious mutation simply slows adaptation. However, in “rugged” fitness landscape models where mutations interact non-additively, it is possible that deleterious steps can lead to exploration of a previously inaccessible part of genotype space, potentially allowing a population to ultimately reach higher fitness. I will capitalize on the Tn-seq method to distinguish between these models by evolving “disrupted” populations alongside “undisrupted” populations and comparing their fitness trajectories. Hypothesis: I hypothesize that deleterious mutations will be more likely to improve evolutionary outcomes in poorly adapted populations, as predicted by [5]. Therefore, I predict that disruption due to Tn insertions will lead to higher final population fitness relative to the undisrupted populations only when the original disruption occurs at early time points from the frozen fossil record. An alternate prediction is that disruption will slow adaptation in all cases, which would support additive landscape models. Methods: I will propagate “Tn-disrupted” populations from Aim 1 for 500 generations. I will measure mean population fitness every 100 generations in these populations and at the corresponding timepoints in the “undisrupted” populations using standard fluorescence-based competitions [1]. Intellectual Merit My project aims to connect ideas about the dynamics of adaptation on fitness landscapes to a functional understanding of how a model organism changes as it adapts. Using massively parallel, sequencing-based fitness assays, this project will provide unprecedented resolution of the functional changes a population experiences during adaptation, and through evolution of Tn- disrupted clones, this study will test basic questions about the fitness landscape of evolving S. cerevisiae. Broader Impacts We now know that large asexual populations, in the form of pathogens or cancer cells, are involved in over a quarter of deaths worldwide [4]. While my research is centered on basic science questions, these basic principles of asexual adaptation are an important part of building models of how these diseases progress. I will publish my work in peer-reviewed journals aimed at a scientific audience, but I will also use the power of animations and interactivity to make my evolution research come alive on my web site, where it can be shared with the general public. As detailed in my personal statement, I will also use science communication and video to empower young people to pursue STEM careers by showing them the human side of research. [1] Kryazhimskiy S et al. 2014. Science 344: 1519-1522. [2] Fisher RA. 1930. Clarendon Press, Oxford, U.K. [3] Van Opijnen T et al. 2009. Nature Methods 6: 767-772. [4] Levy SF et al. 2015. Nature 519:181–186. [5] Nahum JR, et al. 2015. Proc Natl Acad Sci USA 112:7530–7535.	Winner!
182	Unlocking success: Neurobiological correlates of grit in adolescents. Intellectual Merit: During adolescence, the brain undergoes extensive structural and functional development. Specifically, adolescence is characterized by differential development of reward circuitry and cognitive control systems such that cognitive control regions are relatively underdeveloped compared to reward processing regions.1 Although adolescents are able to reason about risky decision making, they are also vulnerable to social influences. In emotionally salient conditions (e.g., the presence of peers), the maturity of adolescent reward circuitry compared to the less mature prefrontal control system appears to exacerbate risk taking that results in negative outcomes (negative risk taking).2 However, adolescent differential brain development and vulnerability to social influences may also lead to greater recruitment of cognitive control processes used to engage in risk taking that results in positive outcomes (positive risk taking), like “grit”. Grit is defined as the determined pursuit of a superordinate goal in the face of failure.3 Higher levels of grit are associated, over and above IQ, with objectively measured successes (educational attainment, GPA)4 and greater well-being.5 Neurobiological investigations of behavior can corroborate and challenge our assumptions regarding the neural mechanisms underlying motivational, cognitive, and affective components of risk taking. Despite the large body of research investigating negative risk taking, there is a gap in knowledge regarding the neural mechanisms of positive risk taking and whether these mechanisms differ from negative risk taking. Novelty: The brain-based mechanisms of positive risk taking remain unknown, and the only empirical investigations of grit are through self-report. This study will address gaps in our understanding of the association between negative and positive risk taking in adolescence, provide the first ecologically valid experimental manipulation of grit, and will determine how grit behavior relates to external measures of success (e.g., GPA). Experimental Design: Participants will consist of 60 adolescents (14-18 yrs).6 Participants will undergo an MRI desensitization procedure in the Galván Lab mock scanner before completing a novel computer task in an fMRI scanner. The fMRI task, the “Grit Task”, is a money-earning paradigm I created that builds on extensive delay of gratification and delay discounting literature. There are two types of trials, each worth a fixed amount, lower-value trials (LVTs) and higher-value trials (HVTs). Participants must choose to perform either LVTs or HVTs before beginning (path selection). Participants who select the HVT path will be considered “delayers” who have higher grit than those who select the LVT path. If LVTs are selected, money earned will be paid at the end of the session up to $10 max. If HVTs are selected, money earned will be paid in 1 week at a min. of $20, max. $30.7 In addition to the delay in payment, the HVT path will require completion of a mental rotation task (MR task; participants must mentally rotate two 3-D figures and determine whether they are identical) between each money-earning trial. Requiring completion of the MR task will improve ecological validity compared to delay of gratification measures that traditionally do not require completion of an effortful task to achieve higher-value rewards. For example, college success requires continued goal-oriented pursuit, not simply an initial decision to delay the receipt of reward for a greater reward. Prior to path selection, all participants will practice the MR task. Participants will be told they must successfully complete (unlimited attempts) the MR task before each money- earning trial if the HVT path is selected. Traditionally, MR tasks are used as a measure of spatial processing, however here the MR task will facilitate manipulation of “failure”, an essential element of grit. Participants will be told, regardless of performance, that they have failed at some MR task attempts (randomized). This will require that participants sustain their choice of the HVT path and continue to attempt the MR task to receive the higher reward. Between each money-earning trial delayers will decide whether they want to continue with the HVT path or switch to the LVT path (reward decisions). Path selection and subsequent Graduate Research Plan reward decisions are proxies for grit. Those choosing to continue on the HVT path and perform the MR task after they have failed will be considered more “gritty” delayers. Delayers who subsequently switch to the LVT path, and participants who select the LVT path at the outset, will remain on the LVT path and will be capped at the LVT max award. Restricting low-to-high switching and setting min/max awards for each path will minimize strategizing. On the LVT path participants will view the MR stimuli before money-earning trials but will not be required to complete the MR task. MR tasks have been successfully used in adolescent fMRI studies and adapted to eliminate gender differences. Validated survey measures will assess (1) supportiveness of adolescents’ home and peer environments,8 (2) grit and impulsivity,9 (3) academic achievement, optimism, IQ, self- esteem, performance anxiety, and well-being.10 The Stoplight Task (ST), a computerized fMRI task in which participants drive a virtual car, will be administered to determine whether gritty individuals are prone to more negative risk taking. In the ST, participants decide whether to brake as the car approaches a yellow light at an intersection. Not braking results in a higher crash risk but also a potentially higher monetary reward for finishing quicker. Anticipated Findings: On the Grit Task, more gritty individuals will exhibit greater: (1) perseverance on the Grit Task, (2) activation in mesolimbic reward circuitry (ventral striatum) at delayed reward presentation, and (3) activation in regulatory control regions (dorsolateral and ventromedial prefrontal cortices; dlPFC, vmPFC) during reward decisions, compared to less gritty individuals. Ventral striatum activation on the ST and Grit Task are expected to be highly correlated. Gritty individuals are expected to exhibit more PFC activation during both tasks resulting in more gritty behavior and less risky behavior (measured by ST yellow light decisions). Feasibility: I will work with Dr. Adriana Galván, a developmental neuroscientist with expertise in adolescent brain development and my advisor, to implement this program of research. Dr. Galván has a database of over 400 ethnically diverse adolescents from which to recruit participants, and her affiliation with the UCLA Center for Cognitive Neuroscience gives me access to state-of-the-art neuroimaging facilities. Scanning fees will be paid by Dr. Galvan’s unrestricted funds. Broader Impacts: Identifying the neural correlates of grit will advance our understanding of positive risk taking and inform efforts to improve positive goal-oriented pursuits (e.g., academic achievement) for adolescents. For disadvantaged adolescents who lack external encouragement to engage in positive risk taking, this research is critical. As part of UCLA Psychology in Action (PIA), I will share with educators and policy makers at interdisciplinary symposia how positive risk taking is beneficial for adolescents. I will also engage with lay audiences about the implications of my research through PIA’s social media platforms and through community outreach at area schools. I will use my findings to encourage educators and community organizations to provide positive outlets for adolescents. I will advance scientific knowledge by presenting my work in published manuscripts and at conferences, and through transdisciplinary collaboration investigating positive risk taking with the UC Consortium on the Developmental Science of Adolescence. I will directly provide opportunities for adolescents to engage in positive risk taking by conducting leadership workshops at area high schools and will expose underrepresented groups to careers in STEM fields by actively recruiting women and minority research assistants. References: 1Casey, B.J., Getz, S., & Galván, A. (2008). Dev Rev, 28, 62-77. 2Crone, E.A., & Dahl, R.E. (2012). Nat Rev Neurosci, 13, 636-650. 3Duckworth, A., & Gross, J.J. (2014). Curr Dir Psychol Sci, 23(5), 319- 325. 4Duckworth, A.L., Peterson, C., ... (2007). J Pers Soc Psychol, 92, 1087-1101. 5Steger, M.F., Kashdan, T.B., ... (2008). J Res Pers, 42, 22-42. 6Sample size calculated using fmripower.org. 7Amounts based on intertemporal choice heuristic calculation; Ericson, K.M.M, White, J.M., ... (2015). Psychol Sci, 26(6), 826-833. 8e.g., NRI-RQV, NRI-SPV. 9 e.g., Grit Scale, DOSPERT, BIS/BAS. 10e.g., LOT-R , WASI-II, Rosenberg Self- Esteem Scale, LSAS-SR, SWLS.	Winner!
185	More than 120,000 people in the United States are currently on the waiting list for life- saving organ donations; over 6,000 of these people die annually without receiving needed treatment. Tissue engineering may help address issues of donor organ shortage and transplant rejection. However, applications have been limited by the ability to design complicated scaffolds that reproduce the architecture present in the cellular microenvironment. 3D bioprinting (3DBP) holds promise for addressing these shortcomings by producing biologically-inspired structures based upon computer-generated models. Unfortunately, low cell viability due to lack of nutrient transport through thick scaffolds is a current barrier to clinical use. Without blood vessels to facilitate nutrient and oxygen transport, cells at the center of the constructs die. There is an urgent need to improve methods such as 3DBP to develop implants to treat these patients. One method to address this problem involves artificial blood vessels created by physical channels through the scaffolds, seeding with endothelial cells, and adding growth factors. By this method, scaffold design must be further complicated by including vasculature. However, introducing oxygen-releasing polymers directly into the scaffolds may be a simpler way to address the need for oxygen transport.1 My research objective is to investigate the ability of oxygen-releasing microspheres to decrease cell necrosis for adipose-derived stem cells in printed implants. Using oxygen-releasing microspheres to reduce cell death rates for tissue engineering applications has only been referenced in one publication at the time of this application. This system involves a two level approach. First, core-shell microspheres are created with the shell consisting of poly(lactide-co-glycolide) (PLGA) and the core containing H O -modified 2 2 poly(vinyl pyrrolidone) (PVP). This shell system allows for slow release of H O -modified-PVP 2 2 for up to two weeks.3 Second, the microspheres are then encapsulated in a hydrogel with catalase enzyme and cardiosphere-derived cells (CDCs). Catalase reacts with the H O bound to PVP to 2 2 generate oxygen, which is then free for use by the cells. This system has been shown to eliminate significant CDC death.4 This promising result may also be replicable in other cell types. Among the most propitious lineages of adult stem cells are the adipose-derived stem cells (ASCs) due to their ease of acquisition and ability to be chondrogenic, osteogenic, and adipogenic. Despite these advantages, ASC survival rates following in vivo implantation are low. Reduced viability may be due to lack of oxygen at the implant or inject site, which could be addressed by oxygen-releasing polymers.2 The steps involved in making a clinically relevant printed scaffold include material choice, cell type choice, printer type choice, scaffold material characterization, in vitro testing, preclinical animal testing, and clinical testing. Using this approach, I will complete the first six steps to adapt the PVA-H O system for use in 3DBP and test its ability to prevent necrosis in 2 2 adipose-derived stem cells using various hydrogel formulations. I hypothesize that using oxygen-releasing polymers will increase ASC viability in engineered tissue systems. To do this, I must address tissue engineering concerns such as refining 3D printing parameters, determining whether microsphere addition alters the mechanical properties of the gel, and selecting the gel type by printability and cell viability. 2 RESEARCH PLAN 2.1 Gel manufacture: For 3DBP applications, inks must solidify quickly, have appropriate mechanical properties, be non-immunogenic, and promote proliferation. Before manufacturing and encapsulating the microspheres, I will test several gel formulations including poly(ethylene glycol) diacrylate to determine an appropriate method for 3DBP. Printability will be measured by the degree to which the print matched the design specifications by calculating the percent error of the printed scaffold compared to the 1 cm2 design. I will select the weight percent of each hydrogel by testing printability. 2.2 Manufacture of microspheres and gel encapsulation: To create the H O -modified 2 2 polymer, H O will be mixed with PVP in multiple molar ratios. The core-shell microspheres 2 2 will be electrosprayed using coaxial electrohydrodynamic atomization using a protocol described by Nie et al.3 Both the flow rate and voltages will need to be optimized to create particles of uniform size and morphology. I will use the previously determined weight percentage of hydrogel to encapsulate the ASCs, catalase, and microspheres. The optimum concentration of catalase will be determined by studying oxygen release kinetics in a hypoxic, acellular environment and measuring which concentration of enzyme sustains oxygen release for the longest time and at the highest levels. This will be determined over a 21-day period by using Ru(Ph Phen )Cl , a luminescent molecule sensitive to O concentration, while using rhodamine 2 3 2 2 b, a fluorescent molecule insensitive to O to correct for background absorbance. 2 2.3 Testing printed materials: Ensuring that scaffolds are safe for cells in vitro prior to further testing is essential. Cell viability in the scaffolds printed with H O /PVP will be tested in vitro in 2 2 a hypoxic environment using Live/Dead, MTS, DNA content, and IHC assays; they will then be compared to control scaffolds without microspheres and containing microspheres with no incorporated H O . Should cell mortality persist, I will attempt to adjust the gel porosity, printing 2 2 methods, and O release system to increase viability. If viability improves with the microspheres 2 in vitro, I will test their efficacy during a subcutaneous study in mice. 2.4 Timeline and Proposed Laboratory: To conduct this study, I would like to work in Dr. Warren Grayson’s lab at Johns Hopkins. Due to the close alignment of our research interests, when I met him at the BMES conference this year, he expressed considerable enthusiasm in working with me and funding my project should I be awarded the NSF GRFP. I anticipate this project to take five years: two for gel and microsphere manufacture and testing and three for in vitro and in vivo material studies. 3 INTELLECTUAL MERIT AND BROADER IMPACTS Currently, only Li et al. have used microspheres to deliver O to implant sites to prevent 2 cell morbidity. Instead, I will develop and test a rapid, accurate, and programmable method to fabricate these tissues using 3DBP. There have been no published papers that utilize microspheres, oxygen release, hydrogels, and 3DBP in conjunction to print biomaterials; integration of these techniques could greatly increase versatility of 3DBP in tissue engineering applications. As cell death is one of the primary concerns of tissue engineering in general and 3DBP specifically, finding a solution to this problem could advance the field from constructing thin tissue sections, to larger tissues, and eventually organs. Developing a standard method for incorporation of oxygen-releasing microspheres into a hydrogel-based printed scaffold is a novel approach that has potential applications in areas such as cardiac, bone, and cartilage tissue engineering. Osteoarthritis treatment is a particular challenge because of the hypoxic environment, but the H O -polymer complex can be a source 2 2 of oxygen for implanted stem cells while they heal the native tissue. Should this method prove effective, it could be used to encapsulate other materials such as growth factors or nutrients. To eventually reach clinical application, I will collaborate with surgeons at Johns Hopkins to develop materials that have clinical utility. Finally, I will present the results of my work at national and international conferences. [1]Camci-Unal, G., et al. (2013) Polym Int. [2]Tsuji, W., et al. (2014) World J Stem Cells. [3]Nie, H., et al. (2010) J Biomed Mater Res A. [4]Li, Z., et al. (2012) Biomaterials.	Winner!
186	Nonlinear System Identification, Reduced Order Modeling, and Model Updating of the Effects of Mechanical Joints on Structural Dynamics Keywords: mechanical joints, nonlinear system identification, finite element model updating Summary: Mechanical joints are present in nearly every structure, device, or vehicle in operation today. As these become ever more complicated the need for the classification and understanding of the nonlinear effects on structural dynamics grows ever more critical. I propose to apply recently developed nonlinear system identification methods, reduced order modeling and model updating techniques to characterize and model these nonlinear effects. The outcome of this research will be the development of models for use in standard finite element (FE) methods that capture the nonlinear effects of mechanical joints. Literature Review: Several techniques exist for the identification of joint parameters, but these methods require extensive instrumentation and measurements that may not be practical and rely on frequency response functions that are assumed to be linear [1]. Yet the current FE model updating techniques necessitate accurate modal parameters and often produce results that differ greatly from experimental results [1]. A recently developed nonlinear analysis methodology with broad applicability can be applied to alleviate these issues by characterizing the nonlinearities and developing reduced order models for use in standard FE codes. The proposed method relies on the assumption that the application of Empirical Mode Decomposition [2], a time-domain based signal decomposition method, results in nearly orthogonal components, called Intrinsic Modal Functions (IMF), characterized by ‘fast’ oscillations controlled by ‘slow’ changing amplitudes [3-5]. The IMFs result in local nonlinear interaction models [6] that portray the local dynamics through sets of intrinsic modal oscillators (IMO). The IMOs are able to reproduce the measured times series while completely capturing the effects of the nonlinearities. The global dynamics are determined by superimposing the wavelet transform (WT) of the original time series in the energy-frequency domain with the frequency-energy plot (FEP) [7] of the representative Hamiltonian system. By assessing the global dynamics an understanding of the energy dependence of the nonlinear normal modes [7] of the system can be developed. This method was recently applied to a beam with a bolted lap joint to identify the damping nonlinearities and the effects on the structural dynamics [8]. This research aims to continue that study and extend the results into FE model updating. Hypothesis: Through the application of recently developed nonlinear system identification, reduced order modeling, and model updating techniques the nonlinear effects of mechanical joints on structural dynamics can be classified and incorporated into standard FE models. Research Method: In order to study the effects of mechanical joints, steel beams will be constructed that incorporate bolted, riveted, and welded connections. “Monolithic” steel beams without any connections, but with holes, bolts, rivets, etc. will be used as experimental control beams for the analysis. The beams will be constrained in various positions to model the most common structures: cantilever, fixed-fixed, fixed-pinned, and similar configurations. Accelerometers will be attached using adhesives at evenly spaced points across the beams. Two cases will be studied: 1) free vibration characteristics induced by impact forces and 2) forced vibration characteristics induced by an electrodynamic shaker. Keegan James Moore Page 1 of 2 Keegan James Moore Graduate Research Proposal Linear modal analysis in addition to the proposed nonlinear analysis will be applied to both the “monolithic” systems and the systems comprised of mechanical joints. This will allow me to verify that the nonlinear analysis is able to reproduce both the linear and nonlinear effects as well as e the deficiencies of the linear modal analysis. EMD will be applied to the measured time series to decompose them into nearly orthogonal IMFs. The extracted IMFs will be used to develop IMOs that capture the local dynamics. The global dynamics will be characterized by superimposing the Hamiltonian FEP with the WT of the measured time series in the energy- frequency domain. A FE model consisting of two linear beams connected by a nonlinear element will be used to compute the Hamiltonian FEP and will serve as the basis for the model updating. By characterizing both the local and global dynamics, I will be able to develop a reduced order model of the nonlinearity for each particular configuration. These reduced order models will be used to reproduce the dynamics of the measured systems and predict the dynamics of unmeasured systems. Finally, the models will be incorporated into standard FE methods for use in a broad range of applications. Anticipated Results: 1) The application of the proposed nonlinear analysis methodology will fully capture the linear and nonlinear effects of mechanical joints on the structural dynamics. 2) Reduced order modeling techniques will be developed that can be incorporated into standard FE methods that account for the nonlinear modal interactions produced by mechanical joints. Broader Impacts: This research aims to apply recently developed techniques to characterize the nonlinear effects of mechanical joints and to incorporate these effects into standard FE models. These models will made available for use in a broad range of applications in fields such as aerospace, automotive, heavy industrial equipment, turbo machinery and structural engineering. As this research progresses, I will present my findings at technical conferences such as the International Modal Analysis Conference and in technical journals such as the Journal of Sound and Vibration and Mechanical Systems and Signal Processing. Furthermore, this research will lay the foundations for developing further methods aimed at understanding nonlinear effects and incorporating them into standard FE analysis methods. Literature Cited 1. R. Ibrahim, C. Pettit, Uncertainties and dynamic problems of bolted joints and other fasteners, Journal of Sound and Vibration 279 (2005) 857-936. 2. N.E. Huang, Z. Shen, S.R. Long, M.C. Wu, H.H. Shih, Q. Zheng, N.C. Yen, C.C. Tung, H.H. Liu, The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis, Proceedings of the Royal Society A 454 (1998) 903-995. 3. Y.S. Lee, A.F. Vakakis, D.M. McFarland, L.A. Bergman, A global-local approach to nonlinear system identification: a review, Structural Control and Health Monitoring 17 (2010) 742-760. 4. A. Vakakis, L.A. Bergman, D.M. McFarland, Y.S. Lee, M. Kurt, Current efforts towards a non-linear system identification methodology of board applicability, Journal of Mechanical Engineering Science 225 (2011) 2497-2515. 5. Y.S. Lee, S. Tsakirtzis, A.F. Vakakis, L.A. Bergman, D.M. McFarland, Physics-based foundation for empirical mode decomposition, AIAA Journal 47 (2009) 2938-2963. 6. Y.S. Lee, S. Tsakirtzis, A.F. Vakakis, L.A. Bergman, D.M. McFarland, A time-domain nonlinear system identification method based on multiscale dynamic partitions, Meccanica 46 (2010) 625-649. 7. A.F. Vakakis, O. Gendelman, L.A. Bergman, D.M. McFarland, G. Kerschen, Y.S. Lee, Passive Nonlinear Targeted Energy Transfer in Mechanical and Structural Systems, Springer Verlag, Berlin and New York, 2008. 8. M. Eriten, M. Kurt, G. Luo, D.M. McFarland, L.A. Bergman, A.F. Vakakis, Nonlinear system identification of frictional effects in a beam with a bolted joint connection, Mechanical Systems and Signal Processing 39 (2013) 245-264. Keegan James Moore Page 2 of 2	Winner!
188	Keywords climate change; exoskeleton; crustacean; calcification; gene expression Introduction Predicted ocean acidification levels (OA) have been shown to change calcium carbonate structures of taxa ranging from corals to oysters, many of which experience decreases in calcification.1,2 Surprisingly, crustaceans appear to increase calcification, likely because of their osmoregulatory capacity.3 Increased temperature can also hinder internal pH regulation,4 so multivariate experiments are integral to understanding real-world responses of crustaceans to future ocean conditions. The California spiny lobster, Panulirus interruptus, is the fifth most important fishery in Mexico and is fished recreationally by 30,000 people yearly in California alone.5,6 As urchin predators, they play an important role in kelp forest ecology by reducing urchin disease and macroalgae overgrazing.7 P. interruptus rely on their calcified exoskeletons as armor to prevent predation and as a tool for sound production to warn away predators.8 Predicted changes in ocean conditions could alter the integrity of the lobsters’ exoskeleton, ultimately affecting its predation and, given the species’ key role in the ecosystem, producing far-reaching trophic effects. Proposed Research To determine the potential impact of OA and increased temperature (hereafter referred to as multiple stressors) on P. interruptus exoskeletons, I will integrate tools from biomechanics, genetics, and ecology. I aim to understand how multiple stressors potentially affect (1) exoskeleton morphology, (2) gene expression, and (3) defensive sound production. Experimental Design Juvenile P. interruptus are readily collected along the Southern California coast. Sixty-four animals will be maintained at Scripps Institution of Oceanography (SIO) in an existing flow-through, experimental aquarium system for six months. In four header tanks that feed to individual animals, I will maintain four combinations of pH and temperature: ambient conditions, and pH and temperatures that are adjusted to reflect changes predicted by the year 2100 (pH reduced by 0.3; temperature increased by 3°C).1 All equipment is available in my advisor Dr. Jennifer Taylor’s lab or open for use on campus. Aim I: Exoskeleton morphology I will test the hypothesis that lobsters will respond to multiple stressors with increased calcification. I will examine exoskeleton ultrastructure using scanning electron microscopy (SEM) and then quantify elemental composition using both inductively coupled plasma mass spectroscopy and energy dispersive x-ray analysis. Results will establish potential changes in mineralization and thickness of the lobster exoskeleton. Aim II: Regulation of calcification Spiny lobsters will likely respond to multiple stressors by regulating genes like those that concentrate and bind Ca2+ near the calcification site.9 With assistance from Dr. Ron Burton’s lab (SIO), tissue from all treatments will be combined and RNA-seq will be used to assemble a reference transcriptome. After its annotation, I will compare transcriptomes of three animals per treatment against the reference to look for up- or downregulation of epidermal genes. Results will establish the important link between environmental conditions and morphology in P. interruptus. Aim III: Anti-predation strategies Increases in calcification due to multiple stressors tend to confer hardness but also brittleness.10 Attacks may require greater bite forces, yet successful bites may be more catastrophic. I will measure the hardness, stiffness, and brittleness of the exoskeleton using materials testing machines. Results from this study will reveal how multiple stressors affect the integrity of the calcified exoskeleton. These data will be compared to published analyses of the bite mechanics of common predators like horn sharks.11 In addition to armoring, the exoskeleton is critical for other anti-predation strategies. Spiny lobsters rub their antennae across a file near the eye to produce a sound that startles away potential predators.12 Individuals will be stimulated with a predator model while behavior and sounds are recorded using video and acoustic techniques. The file will then be excised and examined using SEM for any ultrastructural changes that may influence ability to produce sound. Alternative hypothesis It is possible that the natural range of temperature and pH experienced by P. interruptus makes them more adaptable to forecasted changes in environmental conditions. Thus, if my hypothesis in Aim I is not supported and there are no morphological responses to multiple stressors, I will expand this research to include multiple spiny lobster species that inhabit different environments. Conducting this experiment across multiple species within a phylogenetic framework will provide robust information about how this important group of animals will respond to climate change. Intellectual merit Studying potential morphological changes and the underlying mechanisms will help us understand how these animals may adjust to predicted ocean conditions and determine if these responses are robust or if they leave them vulnerable to other threats like predation. Examining morphological changes that may impact anti-predation mechanisms elucidates how OA may not only affect individual species, but potentially have much larger community impacts if predation rates change. Spiny lobsters must contend with fishery pressure too, and it is important that managers anticipate any changes in population structure due to a changing environment. This study focuses on juvenile lobster that are not sexually mature until age 4-6, so results will help us understand how potential changes in predation will impact recruitment of mature and legal-size spiny lobster.13 Broader impacts Expanding opportunities: I will involve undergraduate lab volunteers and recruit students in the Scripps Undergraduate Research Fellowship (SURF) program, which provides summer research experience to undergraduates that do not otherwise have access to ocean science opportunities. Promoting education: Given their recognition in Southern California, spiny lobsters create an avenue to communicate climate change impacts with the general public. As such, I will work with SIO’s Birch Aquarium to expand their OA and spiny lobster education programs and develop the results of this research into a SEA day at Birch Aquarium, a half-day event where children learn science through hands-on activities. I will plan a similar event for Expanding Your Horizons San Diego, an annual conference designed to interest young girls in STEM fields. Fishery impacts: I will complete a report for the CA Dept. of Fish and Wildlife’s Spiny Lobster Advisory Committee that clearly details how my results may show changes in juvenile survival, especially via potentially increased predation. I will plan a forum meeting through my interdisciplinary program, which includes graduate students from UC San Diego and El COLEF in Tijuana, Mexico, and invite fishery managers to discuss this regional issue. References [1] Solomon, S. 2007. Cambridge University Press. [2] Ries, J. et al. 2009. Geology 37: 1131-34. [3] Whiteley, N.M. 2011. Marine Ecology Progress Series 430: 257-71. [4] Dove, A.D.M. 2005. Journal of Shellfish Research, 24: 761-65. [5] Castañeda-Fernández-de-Lara, V. et al. 2005. New Zealand Journal of Marine and Freshwater Research 39: 425-35. [6] Neilson, D. 2011. California Department of Fish and Wildlife. [7] Lafferty, K. 2004. Ecological Applications 14: 1566-73. [8] Patek, S.N. et al. 2007. The Journal of Experimental Biology 210: 3538-46. [9] Luquet, Gilles. 2012. ZooKeys 176: 103-21. [10] Wainwright, S.A. 1982. Princeton University Press. [11] Huber, D. et al. 2005. Journal of Experimental Biology 208, 3553-71. [12] Staaterman, E. R. et al. 2010. Behaviour 147, 235-58. [13] Engle, J. M. 1979. PhD Thesis, University of Southern California, 298pp.	Winner!
189	Vehicle-Related Hyperthermia Introduction: In the United States alone, nearly 40 young children die every year due to pediatric vehicle-related hyperthermia1,2,3,4,5,8. Young children are more at risk for vehicle hyperthermia because they cannot regulate their core temperature and sweat less than the average adult1. This health issue was relatively prevalent in the news media during the summer of 2014, with many tragic cases occurring across the United States and abroad. Several researchers have examined pediatric vehicle-related hyperthermia, but a gap in the literature exists with respect to communicating this risk to parents2,3,4. Guard & Gallagher (2005) note that addressing vehicle- related hyperthermia is a multifaceted problem that revolves around education and legislation. My primary focus is education and the need for campaigns that deliver more effective messages to the targeted populations, for example, caregivers and parents2,4. While many public service announcements (PSAs) and state campaigns have been undertaken in recent years, rigorous studies have yet to be completed evaluating these campaigns on their ability to both raise awareness and increase risk perception. To effectively target messages to the most relevant parent audience, a focus needs to be placed on parents of children five years of age and younger. A variety of health communication theoretical frameworks can aid in understanding and modifying parents’ behavior when it comes to perceiving vehicle hyperthermia as a potential risk5. A common heath communication framework to predict health-risk behaviors, is the Health Belief Model. The Health Belief Model is known for its use in the adoption and maintenance of behaviors involving both health prevention and promotion6. In order to encourage pediatric injury prevention among parents with young children, it is essential that education and communication increase parents’ awareness of the risks, dangers, and change their perceptions regarding the likelihood and severity of vehicle-related hyperthermia. Research Questions: 1. What current knowledge do parents, who have children 5 years and younger, possess in regards to the danger vehicle hyperthermia poses to children and safety measures for injury prevention? 2. What is the most effective messaging strategy (e.g. scientific vs. emotional) to communicate the risk, alter the risk perception, and change the behaviors of parents (e.g. following recommendations to prevent pediatric vehicle-related hyperthermia)? Methods: To identify potentially effective messages and communication strategies, when it comes to parents of children five years old and younger, a mental model approach to risk communication will be implemented. The mental model approach consists of interviews with experts and parents, a baseline questionnaire, message construction, and a final message experiment7. (1) Initially, I will interview parents from the Athens-Clarke county community to determine their current knowledge, beliefs, and deficiencies in their understanding of vehicle hyperthermia. (2) After conducting several interviews with parents and experts, I will create a questionnaire using the information received from those interviews to sample a wider audience of parents across the southern United States. The questionnaire will be used as the control group during the message experiment and will generally consist of: parents’ risk perceptions, measures of prevention, motivation and intention of adopting specific preventative measures, and demographics of the sample (Question 1). (3) Two experimental groups will be created in order to test the effects and effectiveness of two different approaches to prevention messages on the knowledge, beliefs, and intentions of parents in relation to vehicle-related hyperthermia. Based on preliminary parent and expert interviews, the nature of the messaging could include a scientific approach or an emotional approach, each of which will demonstrate a separate health communication theoretical framework. Prior to the message experiment, a base-knowledge questionnaire will be given to the participants in the experimental groups in order to understand their initial thoughts on the subject matter. Other questions regarding child vaccination will be provided to mask the intent of the study. (4) During the message experiment, individuals will be assigned to one of the two experimental groups, in which the designated message will appear in brochure format and be mixed in with other messages on child vaccination. (5) After reading the materials, the experimental groups will be given a similar questionnaire in order to assess their understanding of the risks and preventive measures of pediatric vehicle-related hyperthermia. The questionnaire results between the experimental and control groups can then be compared in order to determine which message was the most successful in altering the perception of risk and intention of adopting preventive steps on the subject of vehicle-related hyperthermia (Question 2). Some limitations and weaknesses exist throughout this experimentation process. Although I plan to conduct a survey with a random sample of parents of children five years old and younger in selected southern states, a completely generalizable sample will not be met. A sample of the southern states was chosen due to the high volume of pediatric vehicle-related deaths that have occurred in this region8. A better understanding of the current knowledge involving pediatric hyperthermia would be better achieved using a nationwide sample of parents. Broader Impacts/Intellectual Merit: With my interdisciplinary background, involving the fields of risk communication, psychology, and climatology, I can successfully implement this project and impact society by potentially increasing our understanding of how parents’ perceive vehicle hyperthermia. Using the expertise of my well-rounded committee members at the University of Georgia, we developed a multi-method approach that has the potential to create a new subfield examining risk communication efforts in the atmospheric sciences. Developing a sound communication strategy for vehicle hyperthermia is a positive step toward preventing future pediatric injury2,4. It is my hope that this project can develop and implement an evidence-based method for communicating the risk of vehicle hyperthermia to parents. Using the results of the study, I aspire to use this empirically-based method of communication to further develop messaging strategies for other heat-related and natural hazards. 1 Duzinski, S. V., Barczyk, A. N., Wheeler, T. C., Iyer, S. S., & Lawson, K. A. (2013). Threat of paediatric hyperthermia in an enclosed vehicle: a year-round study. Injury prevention, injuryprev-2013 2 Grundstein, A., Null, J., & Meentemeyer, V. (2011). Weather, geography, and vehicle-related hyperthermia in children. Geographical review, 101(3), 353-370. 3 Grundstein, A., Dowd, J., & Meentemeyer, V. (2010). Quantifying the heat-related hazard for children in motor vehicles. Bulletin of the American Meteorological Society, 91(9), 1183-1191. 4 Guard, A., & Gallagher, S. S. (2005). Heat related deaths to young children in parked cars: an analysis of 171 fatalities in the United States, 1995–2002. Injury Prevention, 11(1), 33-37. 5 Toolo, G., FitzGerald, G., Aitken, P., Verrall, K., & Tong, S. (2013). Are heat warning systems effective?. Environmental Health, 12(27) 6 Richard, L., Kosatsky, T., & Renouf, A. (2011). Correlates of hot day air-conditioning use among middle-aged and older adults with chronic heart and lung diseases: the role of health beliefs and cues to action. Health education research, 26(1). 7 Morgan, M. G. (Ed.). (2002). Risk communication: A mental models approach. Cambridge University Press. 8 Null, J. (Updated: 2014, October 16). Heatstroke Deaths of Children in Vehicles. Retrieved September 8, 2014, from http://www.ggweather.com/heat/	Winner!
190	control gene expression levels. In eukaryotes, a key contributor to transcriptional regulation is the chromatin architecture; that is, the arrangement of nucleosomes, transcription factors (TFs), and other proteins that bind along the genome at any point in time. One way to predict binding sites is by scanning the sequence for motifs commonly associated with TF binding; this method is fairly sensitive but also prone to a large number of false positives. Thus, to ascertain the in vivo chromatin architecture, scientists have developed experimental methods, such as ChIP-seq and DNase-seq, to pinpoint the genomic locations of protein-DNA interactions. ChIP-seq is considered the “gold standard” for locating transcription factor binding sites (TFBSs). This assay identifies the locations at which a particular TF binds to DNA in vivo. However, ChIP-seq requires a separate experiment and antibody for every TF, making the procedure time-consuming and costly. An alternative method, DNase-seq, gauges the accessibility of DNA at specific regions in the genome. In this protocol, the nuclease DNase I makes cuts throughout the genome, and the locations of the cuts are then mapped to a reference genome for analysis. Since regions bound by proteins are less accessible to the DNase enzyme than unbound regions, composite plots of DNase digestion patterns at TFBSs reveal reduced levels of digestion at many TFBSs, creating “footprints” in the data (1). Thus, DNase-seq allows detection of binding sites for multiple TFs using just one experiment. Recent studies showed that unique and dramatic fluctuations exist within the DNase footprints of specific TFs, often at the resolution of a single base pair, distinguishing any one TF’s footprints from those of other TFs (1). Some recent studies have suggested that these patterns mirror the small-scale biochemical interactions between the bound protein and the DNA molecule (1). If true, this might allow identification of a specific TF’s binding sites based on the DNase digestion pattern alone. However, follow-up studies challenged this claim, demonstrating that most high-resolution fluctuations in DNase digestion are an artifact of DNase’s inherent sequence binding preferences (2). Most TFs only bind certain DNA sequence motifs, and some specific positions within these binding site motifs are cut more often than others due to DNase’s own sequence preferences. In fact, some have suggested that most TF footprints are not even footprints at all, but rather false positives where DNase cut counts are depleted due to an inherently DNase-resistant sequence context (2). I have analyzed DNase’s sequence biases using experimental data from DNase digestion of naked (protein-free) DNA. I determined the relative “cuttability” of every possible 6-mer nucleotide sequence and used this information to produce corrected TF footprint plots in the budding yeast S. cerevisiae. My results confirmed that some TFs had almost no visible footprint after bias correction, but almost half of the 102 TFs I tested still showed visible footprints. For those TFs that still had footprints, correcting the sequence bias produced smoother footprints with a lower variance in cuts at individual positions, but for some TFs, such as MCM1, a clear factor-specific footprint shape was still present even after bias correction. Furthermore, different transcription factors have different footprint widths. Thus, although the factor-specific footprint patterns are not as striking as was originally suggested, it is still quite likely that a statistical model could incorporate this information to infer which specific TF binds at a given location. I plan to investigate how the chromatin landscape influences transcriptional regulation in eukaryotes, using yeast as a model organism in order to understand basic regulatory principles that might later be extended to human data. My project will explore how differences in this landscape across time and across cell types are correlated with differences in gene regulation. There are two stages to the project. I will first improve upon existing methods for predicting the locations of transcription factor binding sites and identifying the specific transcription factors that bind at each one. I will then apply my model to existing datasets to investigate to what extent changes in the chromatin landscape correlate with changes in gene expression levels. I have spent over 2 years analyzing data from DNase-seq experiments, with the broad goal of determining transcription factor binding sites. To date, no model exists that explicitly models sequence bias in DNase experiments to fully harness the high resolution of the data. My goal is to develop a novel statistical method that uses DNase data to compute the likelihood that each position is bound by a particular protein. Specifically, I will use a hidden Markov model (HMM) in which the DNase cut counts at each position in the genome are the observed data, and each of the model’s “hidden” states represents a specific bound protein, which may be a TF, a nucleosome, or no protein at all. The HMM will include separate parameters for the individual positions within each TF, in order to take full advantage of the high resolution of DNase data. To avoid confounding the “actual” footprints with the sequence biases of DNase, I will include the sequence at each position as an additional predictor of DNase cut counts, using expectation maximization (EM) to simultaneously infer the parameters that govern the bias and the parameters of the underlying footprint patterns. To validate my results, I will compare my predicted binding patterns with published results of ChIP-seq experiments and nucleosome positioning experiments previously performed on the yeast genome. Once I have developed a reasonable model for inferring the chromatin landscape, I will extend my results to directly investigate how this configuration of proteins affects gene expression. Specifically, I will predict gene transcription rates using features derived from the adjacent protein landscape. Previous attempts to solve this problem have achieved moderate success, but only within a certain subset of test cases (3). Most of these models have used sequence data such as kmer counts as the sole predictors of gene expression (3). One downside of this approach is that these features lack clear biological significance: it is still difficult to understand the indirect process by which a given kmer’s appearance in the promoter influences the level of gene expression. In my proposed machine learning model, the features, or predictive variables, will be easily interpretable features that represent attributes of the gene’s surrounding protein landscape. These features will include the number and location of TFs and nucleosomes in a gene’s promoter region, and I will obtain them using the model that I developed in the first phase of my project. Thus, the two phases form a pipeline: first I will infer the chromatin architecture, and then I will use the model output as predictors for modeling transcription rates. Developing tools to elucidate chromatin architecture is crucial for understanding gene expression. Differential gene expression drives many critical processes within an organism, allowing cells to adapt to changes in the environment and even become specialized through the process of cell differentiation. Despite our awareness of gene expression’s importance to an organism, the processes regulating gene expression are far from fully understood. My proposed research project will bring us closer to understanding these fundamental cellular dynamics. The DNase-seq data for this project were generated by the Crawford Lab at Duke and the Stam Lab at the University of Washington, and are freely available online. 1) Hesselberth, J. R., et al. (2009). Global mapping of protein-DNA interactions in vivo by digital genomic footprinting. Nature Methods, 6(4), 283-289. 2) Sung, M. H., et al. (2014). DNase Footprint Signatures Are Dictated by Factor Dynamics and DNA Sequence. Molecular Cell. 3) Meyer, P., et al. (2013). Inferring gene expression from ribosomal promoter sequences, a crowdsourcing approach. Genome Research, 23(11), 1928-1937.	HM
191	EvaluatingCausalModelswithBiometricallyInformedData The relationship between poverty and health has been widely covered in the popular media; within the last year, the Atlantic, Time Magazine, and the New York Times have all featured the SES-health gradient at length, drawing strongly upon conventional explanations that material dis- advantage directly (e.g, access to medical care)1 or indirectly (e.g, chronic environmental stress)2 causes health inequalities. Such explanations account for differences between those who have resources and who do not. However, Gottfredson3 argued that situational explanations do not ac- count for the finely stratified health differences that exist across the entire range of SES, whereas individual differences in intelligence and personality do. Indeed, Chapman and colleagues4 found thatpersonalitycharacteristicsaccountfor20%oftheSES-healthgradient. Regardless of whether the person or situation is the “elusive fundamental cause”3 of the SES- healthgradient,thedebatecannotbesettledusingexperiments. Althoughrandomizedstudiessup- port inferring causality for most situational explanations, poverty cannot be randomly as-signed. Norcanwerandomlyassignmostpersoncharacteristics5. Instead,longitudinalquasi-experimental studiesareused,andpotentialconfoundsareincludedascovariates. The typical use of covariates does not provide control for systematically confounded genetic and environmental influences. This approach risks misattributions of causality. Indeed, poverty and individual differences covary with genes and environment, to such a degree that the covariate approach is fundamentally biased (Rowe & Rodgers, 1997)6. Yet, the covariate approach has been theprimarymethodusedtoevaluatethecauseoftheSES-healthgradient. Instead, quasi-experimental designs can be used; such designs support causal inference with- out random assignment7. Sibling-based quasi-experimental models are particularly effective at in- corporating genetic and environmental design elements8. However, such models are underused in psychology (Rodgers et al, 2001)9, tend to focus on environmental confounds1,0 and do not nat- urally incorporate varying levels of relatedness8. My research proposal aims to address both the methodological problem of evaluating person-driven hypotheses and the substantive problem of explainingtheSES-healthgradient,usingandextendingthesiblingcomparisonapproach. KinshipDyads Traditionalsiblingcomparisonmodelsoftenrelyonrareevents(i.e,twins)oradvancedmethod- ology (e.g, propensity score matching, multilevel modeling). As an alternative, my advisor and I have adapted Kenny’s reciprocal standard dyad model1.1 Our adaptation controls for gene and shared environmental influences within a simple regression framework, by taking the difference betweenthetwosiblings(Rodgers,Garrisonetal,2014):12 Y =β +β Y +β X +β X diff 0 1 mean 2 mean 3 diff Y +Y X +X 1i 2i 1i 2i where Y =Y −Y ;Y = ;X =X −X ;X = diff 1i 2i mean diff 1i 2i mean 2 2 In this model, the relative difference in outcomes (Y ) is predicted from the mean level of diff the outcome (Y ), the mean level of the predictor (X ), and the between-sibling predictor mean mean difference (X ). The mean levels support causal inference through at least partial control for diff genes and shared environment. Therefore, we simultaneously evaluate the individual difference (X diff) and the joint contribution of genes and shared environment (Y mean&X mean). Preliminary applicationshavegivenestimatesconsistentwiththeliterature(Garrisonetal,2015)1.3 S.MasonGarrison,VanderbiltUniversity S.MasonGarrison,VanderbiltUniversity ProposedStudies Application I will apply the kinship dyad model to two nationally representative household samples: the National Longitudinal Survey of Youth 1979 (NLSY79) and the NLSY97. The two household sampling techniques have resulted in 15,589 families with 19,374 sibling pairs, which ourresearchteamhasidentifiedandvalidated. Bothsurveysincludemeasuresofconscientiousness andintelligence,themostconsistentpredictorsofhealthandSES.TheNLSY97alsoincludesself- reportedBigFivepersonalityindicators. I will directly test the impacts of SES, personality, and intelligence on health, estimating how much of the SES-health gradient is caused by each (including covariance among the predictors). Then, I will evaluate specific causal mechanisms, such as access to health care, neighborhood quality, and education level. Identifying specific mechanisms will facilitate translating findings into interventions. To ensure that these findings are externally valid, I will compare results across samplestotestwhetherthegradienthaschangedacrosscohorts. Extension After validating the model using real-world data, I will extend the model in three ways: 1extendthemodeltoincludeadditionalhighlycorrelatedpredictors,2evaluatethemodel’s robustnessundermeasurementerror;3incorporatemultiplegenerations. First, although the Big Five are orthogonal by design, intelligence is correlated with multi- ple facets. For my 1st year project, I evaluated the separate impact of intelligence and conscien- tiousness by partialing out the common variance, resulting in uncorrelated and uncontaminated measures of each1.3 Generalizing this approach will enable the model to test more complicated re- lationships between predictors. 2 To evaluate the model’s robustness, I will conduct a series of Monte Carlo simulations, using NSF’s high-performance computing service, XSEDE under var- ious levels of measurement error for both the outcome and predictor variables. Moreover, I will evaluatethemodel’sexternalvalidityinrelationtopreviousresearchthathasprovidedparameters estimates for the relationship between individual differences, health, and SES. If the kinship dyad model correctly estimates the parameters, this further supports that the model has performed cor- rectlyunderreal-worldconditions. 3Themodelcanbefurtherextendedbyexaminingthechildren ofthesiblings,inthesamewaythatthechildrenoftwinsdesignworks–byexploitingthecommon genetictraitsanddissimilarenvironmentaleffectswithinstandardbiometricalmodels. Thiswould allow the model to distinguish between genes and shared environmental causes. Moreover, the effectivenessofthisextensioncanbetestedusingthemultigenerationalstructureoftheNLSY79. Merit&Impact ThisresearchhasthepotentialtohelpuntilttheSES-healthgradient. Directly,byapplyingthis modeltotestthecausesoftheSES-healthgradient,Iwilldeterminewhetherthepersonorsituation is driving the gradient. Identifying specific mechanisms will facilitate translating these findings into actionable interventions. Indirectly, my research will support accessible and parsimonious solutiontomakecausalinferencesaboutperson-drivenhypotheses. Otherresearcherswillbeable toemploythismodelintheirownworkontheSES-healthgradient. To enhance the model’s accessibility to other researchers, I will develop R, SAS, and SPSS syntax, and make it available on my website with detailed tutorials. Moreover, as this approach can be used on many datasets, I will identify and link to compatible files from various academic databases,suchasHarvard’sDataverseandUniversityofMichigan’sICPSR. Refs1Adleretal(1994)AmeriPsych. 2Baumetal(1999)NYAcadSci. 3Gottfredson(2004)JPSP.4Chapmanet al(2009)AmeriJofEpi. 5West(2009)CurDirPsychSci. 6Rowe&Rodgers(1997)DevRev. 7Shadishetal(2002) Wadsworth. 8 Rutter (2007) Persp Psych Sci. 9 Rodgers et al (2000) Ameri Psych. 10 Lahey et al (2010) Cur Dir PsychSci. 11Kennyetal(2001)PsychBul. 12Rodgersetal(2014Jun)BGA.13Garrisonetal(2015Feb)SPSP.	Winner!
193	Computational Design and Structural Analysis of Novel Peptidine Oligomers Key words: Peptidomimetics, rotamer library, foldamers, rational drug desing. Background and Significance Peptides are essential endogenous molecules with intriguing Figure 1. structures that enable them to have innumerable biorelevant functions. Oligomer-based Although peptides have been designed as potential pharmaceutical agents, biopolymers and their poor bioavailabity and poor in vivo stability makes them bad drug mimetics. Depicted candidates. This fact led to the study of peptidomimetics. One result from are structures of these efforts is peptoids, which contain a non-canonical peptidic backbone. peptide, peptoid and Peptoids appear to be resilient to degradation, but their structures possess the proposed more degrees of freedom and thus pay a higher entropic penalty for target peptidine oligomers. binding when compared to peptides (1). Monomer Backbone Peptidines are a novel class of oligomers, structurally derived from differences are peptides and peptoids (see figure 1). These molecules consist of repeating shown in red. units of N- substituted amidines, a functional group found in drugs such as the histamine receptor antagonist cimetidine and ranitidine (Zantac) (2). The unique structure of peptidines enables the duplication of the amount of side chain; this structural feature could confer a unique secondary structure. Peptidine synthesis is a straightforward process that consists of the iterative addition of imidoyl chloride and primary amines in sequence. To the present fifteen trimers, three tetramers and one pentamer have been synthesized in the Spiegel lab at Yale University (an example of one of this successful synthesis is shown in figure 2). However, studies regarding peptidine’s structure are lacking. Gaining insight into peptidine structure will allow further investigation to evaluate these novel oligomers as key [ ] = monomer molecules for the development of peptidine based, more efficient n sequence therapeutics. I propose to study peptidines structure using an array of powerful techniques such as computational modeling software, X-ray Crystallography and Nuclear Magnetic Resonance (NMR). I hypothesize that a) peptidines will have characteristic secondary structures similar to those found in peptides, although their secondary structures will be more rigid than peptoids and peptides due to their higher functional density; furthermore b) peptidine structure will vary with changes in pH, temperature and solvent, but c) by varying computationally the degree of steric hindrance on each side chain I will be able to control peptidine folding patterns in different chemical environments and thus control their function and selectivity once synthesized. Figure 2. Former synthesized peptidine trimer. Peptidines show higher functional density compared to peptides; this characteristic will enable the formation of rigid secondary structures. J. Torres-Robles, 2 Aim 1: To determine experimentally former synthesized peptidine’s structures using X-ray crystallography and multidimensional NMR spectroscopy. I will use X-ray crystallography to determine the electron density of former crystalline peptidines at Yale X-ray crystallography facility. Electron density data will allow me to characterize chemical bonds, electronic properties, dihedral angles and finally the mean position of atoms in each oligomer. To supplement crystallographic studies, I will use H1 NMR NOESY for the characterization of peptide secondary structure (3), these studies will be held at Yale west campus NMR facility. Unlike other 2D NMR techniques (ex. COSY and TOCSY) NOESY detects spin polarization caused by through space dipolar interactions of atoms within 5 Å of distance. Thereby, using this data I can assign and analyze the sequence of interactions through a single oligomer which will lead me to its secondary structure. Using the same technique I will determine how these interactions change with variations in pH, temperature and solvent. Aim 2: To determine computationally peptidine’s energetically favored conformations Recent expansions to the ROSETTA algorithm software allow the study of non-canonical backbones (4). In order to do molecular simulations using this program it is necessary to create a rotamer (energetically favored rotational conformers) library (5). To construct the library, dihedral and torsional angles from side chains and backbone must be determined. Using computational software like Chimera, I can predict torsional angles for a set of side chains. This data will be incorporated to ROSETTA along with dihedral angles to do molecular modeling (Density functional theory (DFT) and molecular mechanics (MM) calculations will be used in case ROSETTA software do not recognizes peptidine primary structure). I will then be able to predict peptidine structural preferences with various side chains. Intramolecular interactions can be studied for different sets of side chains this will be useful to determine folding patterns. Also, intermolecular binding activity will be studied to determine the degree of peptidomimetics in this new type of oligomers by modeling with biological receptors. This data will shed light on peptidine’s function and its relation to their structure. Intellectual merit Studying the structures of peptidines will be a worldwide innovation in the field of peptidomimetic. It will contribute to the fundamental understanding on the relation between structure and function in oligomers that contain intriguing secondary structures that allow them to perform by efficient chemical mechanism in vivo. This infromation will allow the design of molecules with the desired secondary structures in order to improve and control their binding selectivity and function. Fundamentally the prediction of peptidine’s biological interactions will be useful to establish their potential use as better drug leads. Broader impact Because peptidines are expected to have a more rigid structure, in comparison with peptoids and peptides, they will pay a lower entropic penalty for target binding. These novel oligomers are thus potential candidates to develop more efficient drugs by rational drug design to treat a wide variety of diseases. Robust peptidine based libraries can aid in the development of novel therapeutics that will be expected to have better binding efficiency, higher selectivity and vastly improved bioavailability, compared to those of peptoids and peptides. References 1. Josephson,K.; Ricardo, A.; Szostak, J.W. (2014) Drug Discov. Today. 2014, 4, 388-399. 2. Silverman, R.B. and Hollday M.W. the organic chemistry of drug design and drug action. 3rd ed., 2014, pp. 151-155. 3. Stanger, H.E.; Gellman, S., et al. PNAS. 2011, 98, 12015-12020. 4. Drew, K., et al. Plos one. 2013, 8, e67051, 1-17 5. Butterfoss, G.L.,et al. JACS. 2009, 131, 16798-16807.	Winner!
200	Recollection and neurophysiological correlates of fictional memories Keywords: autobiographical memory, fiction, episodic memory, cortical potential This project aims to understand the differences between experienced and fictional memories, from brain processes to behavioral effects. Episodic memories are characterized by a sense of re-living and visual imagery, and form the basis for developing an autobiographical self. Rubin et al. assessed the qualities of autobiographical memories by measuring variables including degree of reliving, visual and auditory imagery, emotions, setting, and belief1. Recent investigations have begun to probe the shared processes of remembering (“I went to the science museum 2 years ago”) versus imagining (“I imagine myself graduating from college in the future”). Absent from the literature is thorough behavioral data on fictional memories: the memory of an imagined experience without an explicit reference to self, derived from fiction (“I can visualize Atticus Finch standing in a Southern courtroom”). Fictional memories are encoded and retrieved with subjective characteristics similar to veridical memories, and can be source of integrated knowledge about the world2; as such, they occupy an interesting and largely unexplored niche in memory research. Conway et al. used electroencephalography (EEG) to record the dynamic process of retrieving true memories from the past3. Using the excellent temporal resolution of EEG, he established a distinct neural signature for what retrieving and maintaining autobiographical memory broadly looks like in the brain. First, there is activation in the prefrontal cortex, followed by additional temporo-occipital activation once the memory is formed. In a different task, subjects constructed future, imagined scenarios that were plausible and involved the self. Conway found that the real and imagined conditions relied heavily on the same brain regions. However, one difference that left prefrontal activation was highest during active maintenance of plausible imagined memories, presumably because this construction task is more effortful. Secondly, he found that temporal and occipital lobe activation is greater in the recall of real autobiographical memories, suggesting that imagined memories elicit stored sensory data, they do so less than real autobiographical memories. In order to gain a theoretical and practical understanding of fictional memories, both behavioral and neurophysiological data are important; my proposed study will investigate these perspectives. I expect that many fictional memories can be vividly re-lived, they may not be associated with a particular time or place. I also expect fictional memories to evidence the dynamic localization of autobiographical memory. And if memories that are explicitly understood to be not real are incorporated into autobiographical memory, then they can influence identity and behavior. Developing a clearer picture of the neurophysiology of fiction and memory could illuminate how fiction-reading contributes to cognitive and affective development, or how fictional sources could be used intentionally by educators. If fictional memories do not show activity aligned to real and imagined autobiographical memories, then we must begin to explain how any episodic-like memory can exist without these networks. I propose a research project to be carried out in two phases. Since there is not existing research on how to cue a fictional memory and it is critical to have reliable and controlled protocols for in the next, EEG-centered phase, I will first establish this protocol, as well as gather behavioral data through surveys. This first phase will also allow me to find and address any unanticipated sources of error in this novel process, and yield data to modify the design of the next phase. I will limit the study to fictional memories generated through the written word (e.g. novels and short stories). Subjects will be given a cue to recall a scene from a written work of fiction that can be strongly recalled; I will seek to gather 30 observations per subject. To gather Brenda Yang Graduate Research Statement NSF GRFP 2015 pilot data, EEG will be used to record cortical scalp potentials (more detailed methods are below). I anticipate that these memories—like veridical episodic memories— will differ in many ways within and between participants, including time since the last experience, personal interest, and amount of rehearsal. These qualities will be assessed via a questionnaire modified from Rubin et al.1, which asks participants to rate their experience on a scale of 1 to 7 for questions that address recollection (like reliving), component processes (like visual imagery, spatial layout and emotions), and reported properties (like importance, rehearsal, and age of memory). The questions will be delivered after each cue through a provided keyboard. In the second phase, I will use an EEG paradigm to examine the temporal dynamics of fictional memory construction. In two conditions, I will record scalp potentials with EEG while subjects recall and maintain (1) true memories of the past and (2) plausible imagined scenarios of the self; these are the scenarios studied by Conway, and will be used as controls. In the third (3) experimental condition, I will elicit the retrieval and maintenance of fictional memories using the protocol established in phase 1. I will seek to gather 5 observations for each condition per participant to balance the need for statistical rigor with maintaining a reasonable length for the study. Each trial will begin with the memory instruction “Real Memory,” “Imagined Memory” or “Fictional Memory” on screen for 3s. A fixation stimulus will be presented for 3s, followed by the cue for one of the three scenarios. The cue will remain on screen until subjects indicate with a bimanual joystick pull that a memory has been successfully retrieved or generated. Participants will communicate that they were unable to retrieve a memory via a keyboard instruction, which will lead to a new trial. After the memory is retrieved, subjects will fixate on the screen and be instructed to hold in mind the memory for 7.5s. Then, the participant will then type a brief description of their memory using the keyboard provided. In designing the cues and trials, it will be critical to balance cues and trials across participants. Subjects with high shifts in voltage throughout the trials will not be analyzed. Statistical significance will be assessed using a 3-way ANOVA involving the electrode levels and the 3 conditions of memory instruction. If fictional memories are experienced as episodic memories, I would expect to find patterns of cortical potential for fictional memories that are similar to that of the imagined future events. That is, the activation of posterior brain regions should be reduced compared to remembrances of real events. Of interest is the degree to which the prefrontal cortex is activated in the absent of a scenario that does not explicitly involve the self. Behaviorally, I expect to find that re-living of fictional memories to be comparable to veridical ones and primarily visual in nature. Of interest are differences in how the event comes to the subject “a coherent story,” the perspective of the experience, and whether the memory comes back “in words” for a fictional memory that was, after all, delivered through language. To support this work, I am seeking graduate programs which would allow me to combine behavioral and brain measures. I have established contact with several institutions where this would be possible and where training and facilities for EEG is available. For example, Elizabeth Marsh at Duke University studies fiction, false memories, and applications to educational practice. Conditional on my acceptance to the program, she has offered guidance for collaboration between her lab and others in the psychology and cognitive neuroscience departments. I am confident that with these supports, my experience designing novel experiments, and solid conceptual background, I can carry out this research within three years. 1Rubin, D.C., Schrauf, R.W., Greenberg, D.L. (2003). Belief and recollection of autobiographical memories. Memory & Cognition, 887-901. 2Marsh, E.J., Meade, M.L., Roediger, H.L. (2009) Learning facts from fiction. Journal of Memory and Language, 519 –536. 3Conway, M. A., Pleydell-Peace, C. W., Whitecross, S. E., & Sharpe, H. (2002). Neurophysiological correlates of memory for experienced and imagined events. Neuropsychologia, 1-8.	Winner!
201	2 fired power plants1. Due to the role of CO in global climate change, reducing emissions is 2 imperative, and its capture and sequestration at the source is highly attractive. While scrubbing technologies are currently employed as the industrial standard, they are inefficient and regeneration of the amine reagent used for CO capture is expensive.1 2 Several classes of adsorbent materials have been proposed as alternatives for CO capture, 2 but metal-organic frameworks (MOFs) are one of the most promising. As porous, crystalline solids, their robustness, chemical tunability, and void space for guest occlusion, i.e. a gas or solvent molecule within the pore, make MOFs highly attractive for such applications. Designing a porous material such as a MOF to replace reagent-based methods is a challenge, as it must exhibit superior selectivity for CO at elevated temperatures and low partial pressures, from a mostly nitrogen-rich 2 atmosphere. In addition to excellent CO adsorption under challenging conditions, the MOF must 2 possess a high tolerance to water, straightforward regeneration, and robustness over thousands of cycles.2 Selective gas or solvent adsorption in a static MOFs is attributed to size exclusion and/or favorable host-guest interactions.2 MOFs that exhibit framework distortion upon guest addition or removal offer an additional route for tuning selectivity. Termed “breathing MOFs”, they were pioneered by Ferey and Kitagawa in the early 2000s.3 Though their body of literature has grown significantly, the ability to rationally design new breathing MOFs has yet to be demonstrated, contrary to their static counterparts.4 Selectivity in breathing MOFs is attributed to a “gate- opening” pressure in which the pore expands, allowing the guest to enter. This unique property could offer a superior method for designing new, more efficient materials for CO capture. 2 Thoroughly understanding how chemical environment, pore aperture, and breathing ability affect selectivity for CO could yield a great leap toward designing an industrially viable material. 2 Objective: Utilizing isoreticular synthesis5, I will develop a series of porous, breathing MOFs tailored for CO capture. Exceptional guest selectivity will be achieved through optimizing a 2 synergistic relationship between functionality, pore availability (i.e. aperture), and breathing motif. By tuning the pore environment in such a way, it will be possible to enhance the initial framework- CO attraction and reduce the available “gate-opening” pressure to that of CO . 2 2 Methodology: Using a set of semi-rigid, organic linkers where size, shape, and functional groups have been altered, I will synthesize 2-pillared, 2D sheet MOFs or 1D tube MOFs, which are also referred to as metal-organic nanotubes (MONTs). Continuing work started in the blank group6, these breathing MOFs will exhibit guest-dependent rotation of the ligand (Figure 1). Characterization using X-ray diffraction methods will be employed. For MOFs in which breathing is a result of rotation of phenyl rings in the ligand, breathing can be verified by 13C CP MAS NMR, as demonstrated in a publication currently under revision by the blank group. Tuning pore size and functionality within a framework Figure 1: Illustration depicting has been well documented5. Triazole ligands of increasing ligand rotation in breathing MOFs. size that adopt a syn-geometry will be incorporated into the frameworks, and isoreticular synthesis will be exploited by adorning the non-triazole moiety with amino, nitrile, hydroxyl, methyl, halide, and ester groups, similar to previous reports (Figure 2).5,7 Experimental evidence show that these alterations can affect a framework’s selectivity for CO 2 over CH , N , and O ; moreover, nitrogen-containing functional groups tend to enhance the 4 2 2 framework-gas attraction via a Lewis acid-base interaction.8 My first aim is to identify the functional groups that enhance framework interaction with CO in breathing MOFs. From there, I 2 will determine the role of pore aperture by systematically increasing the ligand size. The final variable to evaluate if breathing mechanism influences selective CO adsorption. The effects of 2 these alterations can be understood by single and multi-component gas adsorption studies. Selectivity will be evaluated according to previously-published methods that evaluate the role of kinetic favorability in mixed-gas systems.7 Anticipated Results: My work will result in the development of a porous, breathing MOF that will outperform the most highly-selective materials to date for selective CO adsorption in mixed- 2 gas systems.7 Preparation of such a MOF could form a platform for newer, more efficient materials to be developed. My Figure 2: Representative ligands for MOF synthesis. findings will be presented at regional and national conferences, and published in relevant peer-reviewed journal articles. Broader Impacts: Because of the correlation between increased atmospheric CO levels and 2 rising global temperatures, curbing anthropogenic CO emissions is a goal of utmost importance 2 for our society; capturing these emissions at the source is highly attractive. In addition, synthetic chemists are seeking routes to convert CO to valuable commercial commodities like plastics9 and 2 synthetic fuels.10 Efficient capture of CO will help advance these processes, as CO is an abundant 2 2 C -feedstock. Development of a highly selective and efficient porous, adsorbent material for CO 1 2 capture from coal-fired power plants would be invaluable for its environmental and economic benefits. Intellectual Merit: My relevant experience working with gas storage materials at both blank and blank has prepared me well for graduate work at the University of blank, where I will work under Dr. blank blank. I possess strong synthetic skills and a familiarity with an array of characterization techniques (see personal statement). I have presented posters and oral presentations at several regional and national conferences, and was recently published11 as a co-author for my work at blank. As a Latino in science, I hope to mentor young scientists from underrepresented backgrounds in STEM fields through tutoring and educational outreach initiatives. My leadership role at a non-profit organization (see personal statement) has left me with the experience to engage the general public in educational discourse, and I hope to foster an understanding of basic science in our society. Whether I pursue academia, industry, or government-employment, I look forward to joining the generations of scientists who will solve the greatest issues of our age. References [1] Science 2007, 317 (5835), 184-186. [2] Coord. Chem. Rev. 2011, 255 (15–16), 1791-1823. [3] Chem. Soc. Rev. 2009, 38 (5), 1380-1399. [4] Coord. Chem. Rev. 2014, 258–259 (0), 119-136. [5] Science 2002, 295 (5554), 469-472. [6] blank citation blank citation. [7] ProC. Natl. Acad. Sci. USA 2009, 106 (49), 20637-20640. [8] J. Am. Chem. Soc. 2009, 131 (11), 3875-3877. [9]http://www.research.bayer.com/en/CO2.aspx, 2012. [10] Chem. Ing. Tech. 2013, 85 (4), 489-499. [11] blank citation blank citation.	Winner!
202	2 Keywords: carbon capture, amine degradation, aminosilica adsorbents, adsorbent regeneration Hypotheses: Aminosilica adsorbents used for post-combustion carbon capture can be partially regenerated through treatment with acid, transesterification, and a Hoffman rearrangement. Introduction: The most readily available technology to reduce carbon emissions is carbon capture through amine scrubbing followed by geological sequestration. However, amine scrubbing has environmental risks: volatile amines escape into the atmosphere and carcinogenic nitrosamines form from NO .1 x Other technologies exist to capture carbon, but most do not have the fundamental knowledge necessary for pilot testing. Amines covalently tethered to silica supports for adsorption (ASA) use similar chemistry as amine scrubbing to selectively bind CO but avoid the environmental 2 side effects because of a lack of volatility and by binding nitrosamines. The method for attaching amines to silica is common among industry,2 so scaling up ASA production would be more feasible than other novel adsorbents. Similar to amine solvents, flue gas components, NO and x O also reduce effectiveness of ASA. NO preferentially bind to the amine group3 while pure O 2, x 2 oxidizes ASA to form imines, amides, and carboxylic acids, thus degrading the adsorbent.4 Since production of ASA has both environmental and economic costs, reusability is vital for any industrial application. A study by Hallenback and Kitchin explored using NaOH to regenerate adsorbent poisoned with SO ,5 but there have been no studies to evaluate the 2 regeneration capacity of amine adsorbents for carbon capture after exposure from NO or O . I x 2 propose investigating the degradation of ASA from NO and O and developing regeneration x 2 methods to reduce the frequency of adsorbent replacement. More specifically I will 1) Synthesize and characterize the ASA from primary, secondary, and tertiary amines 2) Degrade the ASA using NO and O in a packed bed reactor x 2 3) Regenerate NO treated ASA using aqueous acetic acid and bromide x 4) Regenerate O treated ASA using the Hoffman rearrangement or transesterification 2 1) Synthesis and Characterization: ASA will be created through condensation of MCM-41 mesoporous silica with a chloroalkoxysilane followed by reaction with ammonia, ethylamine, and diethylamine to form primary, secondary, and tertiary ASA. Since the experimental procedures involve toxic chemicals, a risk assessment will be conducted to determine necessary safety precautions. Between sections 1, 2, 3, and 4, ASA will be tested for surface area (using Brunauer-Emmett-Teller method3), pore size (Barrett- Joyner-Halenda method3), elemental analysis (sent for external Primary ASA degradation testing), bonding structure (using C NMR and Fourier Transform 13 from NO and O to form Infrared Spectroscopy4,5), and CO 2 capacity and kinetics (in a nitrosamix ne (uns2 table) and packed bed reactor with live analysis of exiting gas composition5). amide 2) Degradation: The characterized ASA will be placed in packed bed reactors with a mixture of gas containing N , CO , H O, and either NO or O . The NO experiments will be conducted at 2 2 2 x 2 x 50°C, the standard adsorbing temperature, while the O experiments will run at 150°C, the 2 desorption temperature. The ASA will be considered degraded when the exiting concentration of pollutant approaches the inlet concentration. The degraded ASAs will then be characterized. I expect that ASA will initially react with NO and O in a similar manner with amine x 2 solvents: NO will form nitrosamines and nitramines, and O will oxidize the α-carbon. x 2 Furthermore, primary ASA should show a decrease in nitrogen content after exposure to NO x since primary nitrosamines degrade into N . A mixture of alcohols, amides, imides, and acids 2 should result from the O exposure at elevate temperatures.4 These all decrease the capacity of 2 the amine solvent.3–5 3) Regeneration-NO : To remove nitrosamine and nitramine functional groups, the x denitrosation procedure for aliphatic nitrosamines described by Dix et al.6 will be followed. If more stringent conditions are necessary, testing an ASA embedded with 2-amino acetic acid would indicate whether oxidative degradation improves amine denitrosation.6 Regeneration from NO may not work well for primary amines since they degrade into N . Secondary and tertiary x 2 amines treated with NO should increase in capacity as the original amines reform. x 4) Regeneration-O : For primary amines, a Hoffman rearrangement can produce an amine from 2 the degraded amide using mild reagents.7 Secondary and tertiary amines would require too strong reducing agents and cause significant damage to the silica. For these, transesterification with ethanol amine will be employed to recover lost activity. Due to the variety of products formed, regeneration from O exposure may be more difficult than denitrosation. 2 Further Analysis: The effluent from the regeneration steps will be tested for silica to determine the extent of support degradation. Assays requiring acidic treatments will use adsorbent with t- butyl groups near the organosilane bond to protect from hydrolysis. For the methods that show marginal improvement, conditions for regeneration will be altered to improve effectiveness. For these optimized methods, repeated degradation and regeneration cycles will assess durability. Expected Results: These results will indicate the most suitable ASA given different capture conditions. For example, when NO are removed before carbon capture, primary amines may be x most beneficial since the Hoffman rearrangement can easily reform amines from O oxidation. 2 Furthermore, the findings may also correlate well with other solid amine adsorbents, so a general trend in regeneration can be seen regardless of the type of amine adsorbent. Broader Impacts: Reducing CO emissions will help stabilize our planet’s temperature to 2 prevent negative effects of climate change like desertification and a rising sea level which would decrease food supply and increase land scarcity. ASA can isolate CO to reduce emissions and 2 slow climate change. The degradation studies in this project will further characterize ASA, and the regeneration methods developed will help make using ASA more economical. Since amine based adsorbents can capture CO with reduced emissions of toxic amines and carcinogens, this 2 work will also help reduce the negative environmental impact of carbon capture. I will disseminate my results through conferences and publications so other researchers can improve upon and apply the findings towards further development and application. I will mentor undergraduate students and encourage them to develop their own projects so that they gain valuable research skills before graduate school. Since solving climate change requires international cooperation, I plan to collaborate with foreign institutions specializing in carbon capture like the Norwegian Technical University to accelerate the application of CO capture. 2 (1) Jackson, P.; Attalla, M. I. Rapid Commun. mass Spectrom. 2010, 24, 3567–3577. (2) Materne, T.; de Buyl, F.; Witucki, G. L. Organosilane Techonology in Coating Applicaions; 2012. (3) Co, P.; Adsorption, C. S.; Rezaei, F.; Jones, C. W. Ind. Eng. Chem. Res. 2013, 52, 12192–12201. (4) Bollini, P.; Choi, S.; Drese, J. H.; Jones, C. W. Energy & Fuels 2011, 25, 2416–2425. (5) Hallenbeck, A. P.; Kitchin, J. R. Ind. Eng. Chem. Res. 2013, 52, 10788–10794. (6) Dix, L. R.; Oh, S. M. N. Y. F.; Williams, D. L. H. J. Chem. Soc. Perkin Trans. 2 1991, 1099–1104. (7) Patel, I.; Opietnik, M.; Bohmdorfer, S.; Becker, M.; Rosenau, T. Holzforschung 2010, 64, 549–554.	Winner!
204	Key words: Polymer-clay nanocomposite, brominated flame retardants, sustainability Introduction: As a consumer, you hope that the products you use are safe, sustainable, and non- toxic - but that is not guaranteed. Many chemicals in everyday products have unknown or concerning impacts on human health and the environment. At the same time, a growing number of people use electronic products on a daily basis and are exposed to the chemicals contained in them, like brominated flame retardants (BFRs). BFRs are used in electronics because they are extremely effective at reducing the inherent flammability of polymeric materials. However, there is a desire in the flame retardant community to move away from brominated chemicals because of increasing concerns about the impact they may have on human health and the environment, especially during e-waste disposal[1]. BFRs are heavily used in high performance applications like epoxy based printed circuit boards. Over 90% of boards contain tetrabromobisphenol A (TBBPA) reacted with the epoxy matrix [2]. A promising new sustainable flame retardant is montmorillonite or “nanoclay” (flame retardant mechanism is shown in Fig. 1)[3] . Montmorillonite is attractive because just a few weight percent of it in a polymer composite improves the flame retardant characteristics, it is abundant in many different countries, Fig. 1. A layer of clay and char builds and it is a low cost additive. But to date, nanoclay has not during burning, protecting the polymer been demonstrated to be an effective flame retardant on underneath from further degradation. its own, instead being included in a mix of flame retardants used to achieve the desired properties. These mixes include BFRs in lower concentrations than when used without nanoclay and so still pose sustainability problems. Hypothesis: Even dispersion of clay layers and an epoxy-tailored clay surface will improve thermal, mechanical, and fire retardant properties of the epoxy-clay nanocomposite. This material will therefore be suitable for use as a flame retardant, bromine-free plastic. Research Plan: There are four main challenges in literature regarding the use of nanoclay as a sustainable flame retardant in epoxies, stated below with proposed solutions[4]. 1) Weak bonding between the clay and epoxy leads to poor composite properties In order to achieve strong bonding between the two nanocomposite constituents, organically modified montmorillonite (organoclays) will be made. Organoclays have been used to increase the dispersion in polymeric matrices, but the proposed surfactants will also be used to strengthen the interaction between the two phases. Novel surfactants will be grafted onto montmorillonite through well-established methods[4]. Aminosilane and epoxysilane surfactants were chosen because they will bond strongly to the epoxy network and to silica in montmorillonite (Fig. 2). Grafting and bonding strength in the nanocomposite will be determined by Fourier transform infrared spectroscopy (FTIR), thermal gravimetric analysis (TGA) differential scanning calorimetry (DSC), and dynamic mechanical analysis (DMA). 2) Even dispersion of individual clay layers in the epoxy matrix is hard to achieve Montmorillonite consists of ~1nm thick by 50-100μm diameter sheets stacked together with weak charges and Van der Waals forces bonding them together. Processing conditions such as grafting reaction temperature/time, intercalation procedure, and the kinetics of the epoxy curing inside and outside the clay layers will be factors that affect the dispersion of clay layers and will be optimized for this system. Conditions will be assessed by their potential for implementation in large scale manufacturing. Dispersion will be determined by x- ray diffraction (XRD) and transmission electron microscopy (TEM). 3) Nanoclay has not been shown to meet the UL94 standard when used as a flame retardant by itself Flame retardancy of the final nanocomposites will be Fig. 2. Selected structures of proposed novel tested at the Forest Products Lab in Madison, WI, due surfactants. Silane groups will bonds to to the lack of equipment at Purdue University. All montmorillonite and the amine/epoxy groups will bond into the epoxy network. nanocomposites are not expected to be promising, but once a final epoxy-organoclay composition is settled on it will be sent to the Underwriters Laboratory (UL) to receive a rating for the UL94 standard “the Standard for Safety of Flammability of Plastic Materials for Parts in Devices and Appliances testing.” Flammability will be determined by cone calorimetry and limiting oxygen index (LOI). 4) There is no standard assessment of sustainability for flame retardants In order to determine whether our nanocomposite is more sustainable than what is currently used, a comparative Life Cycle Analysis (LCA) will be conducted focusing on energy & water usage and pollutants produced from cradle to grave. Sustainability will be determined through OpenLCA software used with the EPA’s TRACI impact assessments (Tool for the Reduction and Assessment of Chemical and other environmental Impacts). Expected Results: One or more of the surfactants tested will create an epoxy-organoclay nanocomposite with desired thermal, mechanical, and flame retardant properties when made under specific processing conditions. Higher mechanical and thermal properties are also expected from a fully dispersed nanocomposite. A fundamental understanding of silane modifier structure/properties relationships on epoxy-organoclay nanocomposites will be contributed to scientific knowledge. Four papers will be published on: organoclay and nanocomposite manufacturing and properties, nanocomposite fire retardancy, sustainability assessment of nanocomposites, and reaction scheme of silanes bonding to montmorillonite. Conferences will be attended to present these findings including the American Chemical Society National Meeting and the International Symposium of Sustainable Systems and Technologies. Broader Impacts: This nanocomposite can be used in current epoxy applications, replacing the need for BFRs. Due to improved properties of the nanocomposite less material will be needed to achieve the same strength, reducing the environmental impact of a potential epoxy product. These findings will also be shared through outreach programs such as Project Interchange and Innovation to Reality to inform and inspire the next generation of scientists and engineers. [1.] Sjödin, A., Carlsson, H., Thuresson, K., Sjölin, S., Bergman, A., and Ostman, C. “Flame Retardants in Indoor Air at an Electronics Recycling Plant and at Other Work Environments.” Environmental Science & Technology (2001) [2.] EPA. “Flame Retardants in Printed Circuit Boards” Design for the Environment (2008) [3.] Morgan, A. B. and Gilman, J. W. “An Overview of Flame Retardancy of Polymeric Materials : Application, Technology, and Future Directions” Fire Materials (2012) [4.] He, H., Tao, Q., Zhu, J., Yuan, P., Shen, W., and Yang, S. “Silylation of Clay Mineral Surfaces” Applied Clay Science (2013) Statement of originality: I certify that this proposal is my independent and original work.	Winner!
205	Introduction: High latitude permafrost soils contain vast reserves of organic carbon (C) that, with warming, may become a significant source of greenhouse gases (GHGs) due to increased microbial activity. Current climate model predictions for C storage and fluxes in these ecosystems depend heavily on the rate at which soil organic matter (SOM) is broken down.1 At present, there are significant uncertainties regarding how the varying chemical and physical differences among these SOM pools and their interactions with the surrounding environment will affect the rates at which they are being degraded by the local biological community. It has been shown that organic C molecules sorbed to sediment mineral surfaces tend to decompose more slowly, and to a lesser extent than dissolved organic matter (OM).2 This may be due to the physical occlusion of the OM by minerals, reducing its susceptibility to microbial attack, thereby changing its long-term accumulation and translocation in the soil profile.3 Recent radiocarbon (Δ14C) values taken from Alaskan permafrost soils indicate that the carbon dioxide (CO ) and methane (CH ) being released to the atmosphere are derived from older C buried deeper 2 4 in the soil profile.4 Mineral phases and complexation mechanisms responsible for the stabilization of permafrost SOM are largely unknown and the role of these C-mineral interactions in the Arctic ecosystem and global C budgets is not well understood. Research Objectives: The following research questions will be investigated to better quantify and predict GHG emissions from thawing permafrost in response to climate change: R1) What are the major mineral species present in Arctic permafrost soils, and how do they affect SOM distribution in the soil profile? R2) What are the key chemical bonding mechanisms between soil organic C and mineral phases in these systems? R3) How does topographical variation impact C-mineral associations and, in turn, SOM stabilization and GHG emissions? Hypotheses: H1) The major mineral elements I expect to find in these systems are Fe, Ca, and Al. Fe oxides will prove to be an important sorbent in the spatial distribution of SOM-mineral complex formation because of their strong selectivity for aromatic compounds and high molecular weight fractions,2 qualities commonly associated with permafrost soils. H2) OM preservation will largely be controlled by electrostatic interactions and sorption of minerals into micropores along the soil profile. H3) Concentrations of Fe oxides and the relative proportion of OM bound to mineral surfaces will increase at lower topographic regions. Greater portions of OM will be physically protected from the microbial community leading to increased recalcitrance and decreased fluxes of CO and CH to the atmosphere. 2 4 Preliminary Results: Addressing these questions will require close monitoring of permafrost soil dynamics throughout seasonal thaw and at different depths along the soil profile. Working with the NGEE-Arctic biogeochemistry team at Oak Ridge National Lab (ORNL), I have helped design and test a large, temperature-controlled soil column apparatus for intact soil cores. Using a two- stage cooling mechanism, this system successfully mimics or accelerates seasonal thaw patterns in the lab.5 Access ports for the collection of gas and liquid samples allow for continuous, nondestructive monitoring of biogeochemical properties and their changes over time and space, on the order of days to weeks and cm to m, respectively. Study Site: Field observations will take place at the Barrow Environmental Observatory (BEO), located at the northernmost point of the Arctic coastal plain, near the remote, native Iñupiaq village of Barrow, Alaska. This high-latitude ecosystem is characterized by a dynamic landscape Mallory Ladd Graduate Research Statement 11/8/13 dominated by distinct morphological subunits: ice-rich polygonal tundra and drained thaw lake basins (DTLB). The site consists of continuous permafrost with the active layer reaching ~20-55 cm deep. Frozen soil cores (~7.7 cm diameter by 1m depth) will be obtained from low- and high- centered polygons from DTLBs of varying age using a SIPRE auger6 and a hydraulic drill. Experimental Approach: Using cores obtained in the field, (R1) I will first physically and chemically fractionate samples from the organic, mineral, and permafrost horizons and then characterize the mineralogical components with energy-dispersive x-ray (EDX) spectroscopy. Intact soil grains will be examined with micro-Raman and Fourier-transform IR spectroscopies to obtain compositional information, including the relative proportions of polysaccharides, amino sugars, phenols, lignin, and lipids to estimate the mineral interactions with different OM functional groups. (R2) The stability of these interactions will be tested using batch equilibrium techniques, while the aggregate surface area and soil micropore volumes will be determined by scanning electron microscopy (SEM). (R3) Using the soil column mesocosms, I will monitor changes in temperature, moisture, pH, redox potential, and concentrations of gases and solutes throughout a controlled thaw, helping to identify where CO , CH , and dissolved organic C are being generated. 2 4 During my summer field campaigns, I will continuously measure land-surface fluxes of CO and 2 CH using chamber measurements and laser-based infrared gas analyses. The isotopic composition 4 and age of mineralized C and SOM C from each core will be determined at the Center for Accelerator Mass Spectrometry at Lawrence Livermore National Lab. Intellectual Merit: Collectively, this research will provide a deeper mechanistic understanding of the physical and chemical controls on SOM stabilization in permafrost soils, while advancing our fundamental knowledge of the role of C-mineral chemistry in the Arctic ecosystem. These data will help provide a firmer empirical foundation for predicting the most important drivers of C degradation and GHG emissions in these high-latitude regions. As a graduate student at UTK working with researchers at ORNL, I will have full access to the extensive resources provided by both, including all instrumentation mentioned, and the support of expert faculty and scientists who specialize in terrestrial biogeochemistry and can help optimize any necessary techniques. Broader Impacts: Given the importance of climate change to all sectors of society, these results will provide critical data for improving global climate models. Various researchers from a variety of disciplines may use this data to better interpret Arctic systems chemistry and make more informed decisions on current and future governmental policies. My continued involvement with the ACS local section and my tutoring efforts will allow me to actively recruit high school and undergraduate students from underrepresented groups to gain valuable research experience on this project. The interdisciplinary nature of this research will significantly broaden their scientific experience and enhance their understanding of the importance of chemistry in the Arctic to global climate change. I will encourage them to apply for REU support to join me in the field where they will participate in weekly public scientific discussions with native Iñupiaq community members. I am applying to host a PolarTREC teacher, who has recently contacted me from the East Tennessee area, during a summer field campaign where we will connect with her students to share our experiences from the field. Given the close proximity, we will be able to connect more directly with her students through class presentations and scientific demonstrations. On a more regular basis, I am able to share the importance of environmental research, Arctic biogeochemistry, and their impacts on public policy and education, to the broader public, via my website and blog “Think Like a Postdoc” (www.malloryladd.com). References: [1] Jenkinson et al. (2008) Euro. J. of Soil Sci., 59, 400 [2] Gu et al. (1994) Env. Sci. & Tech. 28, 38 [3] Mikutta et al. (2006) Biogeochemistry, 77, 25 [4] Vogel et al. (2009) J of Geophys. Res., 114 [5] Ladd et al. (2013) ORNL Ann WIS Poster Session: http://malloryladd.com/ [6] Bockheim et al. (2007) Soil Soc. of Am. J., 71, 1889	Winner!
206	"Impacts of Radioactive Cs on Marine Bacterioplankton: Effects of the Fukushima Disaster on Hawaii’s Kaneohe Bay Bacterial Communities Introduction Marine bacteria are unmatched in their diversity and abundance. They exhibit mutualism with economically significant organisms, synthesize life-saving natural products, and play a vital role in oceanic nutrient cycling. Despite our dependence on marine bacteria, very little research has been conducted on how they respond to large-scale disasters. One such catastrophe, a tsunami off the coast of Japan, occurred on March 11, 2011. The tsunami caused the Fukushima-Daiichi Nuclear Power Plant to emit 10 PBq of radiation2, the largest ever release of anthropogenic radionuclides into the ocean4. The main pollutant, 137Cs, has a half-life of 30 years and will first hit the US territories at the Hawaiian Pacific Islands in early 2014, diluted by only three orders of magnitude2 (figure 1). While 63 marine species have already exceeded the Japanese limit for radioactive Cs (100 Bq/kg), the impacts of radioactive waste on marine microorganisms are largely unknown6. Due to their short reproductive lifecycle and unicellularity, bacteria evolve faster than most eukaryotes when exposed to radiation, so much so that radiation is used in laboratories to induce mutagenesis. This project aims to assess the impacts of radiation on the bacterioplankton community of Kaneohe Bay in Oahu, Hawaii. The bay is in the direct path of Fukushima’s radioactive waste and has a bacterioplankton community that was well-characterized pre-disturbance1, making it the ideal case study for the microscopic impacts of radioactive pollution. I will compare trends after radiation exposure to previously documented annual/seasonal fluctuations. This is possible because Fukushima bacterial populations were catalogued bimonthly over an 18-month period. Hawaii Hawaii Figure 1: Predicted spread of 137 Cs after 2.5 and 5 years2; color scale shows dilution factor Research Questions 1. How has the bacterioplankton species composition in Kaneohe Bay (as determined by 16S small-subunit ribosomal RNA (SSUrRNA) barcodes) changed since the Fukushima leak? 2. Has there been a significant increase in single nucleotide polymorphisms (SNPs) since the radiation event, as compared to mutation rates that would occur due to random chance? Methods I will work within the Rappé laboratory for aquatic microbial ecology at Hawaii Institute of Marine Biology (University of Hawaii at Manoa), which is equipped with all necessary instruments and sampling materials. Rappé is at the forefront of bacterioplankton ecology, and having established the 2006-2007 baseline1, his lab will provide an excellent knowledge base for collecting comparable data. Seawater will be sampled at a depth of 1m at 2 sites (reef flat and lagoon) separated by 600m near Coconut Island in southern Kaneohe Bay. Samples will be taken twice monthly from January 2015 to July 2018 between 07:00 and 08:30h. In situ measurements of temperature, salinity and pH will be taken at 1m depths using a multi-parameter sonde, and radiation levels + will be monitored with a scintillation probe. Dissolved inorganic nutrient concentrations (NH , 4 – 3– NO2 , PO4 , silicate) will be measured using a continuous segmented flow system. Bacteria will be isolated by filtering 1L of water through a 1.6 µm microfiber membrane pre-filter followed by a 0.2 µm polyethersulfone membrane and stored at –80°C in DNA lysis buffer. Genomic DNA will be extracted using the DNeasy Tissue kit1. Bacterioplankton will be characterized by PCR of SSUrRNA and sequenced in a barcoded Illumina HiSeq run. The bacterial primers 27F-B-FAM and 519R will be used1. OCTUPUS and UC-LUST will be used to process raw reads, which will then be clustered into operational taxonomic units using MegaBLAST3. Mutation and species compositional shifts due to random chance will be determined from the 2006-2007 data1 using a Poisson distribution and extrapolated to determine the number of mutations that should occur from 2015 to 2018. The experimental 2015-2018 community structure and SNP prevalence will be compared against these values to identify changes that are due to radiation. Anticipated Results 1. The bacterial community structure will change significantly more than due to random chance. 2. Post-Fukushima species will have significantly more nonsense and missense mutations in non-essential genes and neutral mutations in housekeeping genes than would have accumulated due to random chance. Broader Impacts This research will help characterize the full repercussions of radioactive pollution at its first outset, providing insight that will allow us to prepare for future radiation leaks and the arrival of the contaminants to the California coast6. It will reduce the knowledge gap of what potential harm radioactivity causes marine microbial communities, and give policy makers the information they need to manage affected ecosystems. In light of the recent shift towards increased nuclear power reliance, this research will inform the tradeoffs of pursuing various energy sources in future development, as well as allow policy makers to establish and enforce adequate safety standards for nuclear power plants. In doing so, this research will protect the ecosystem services that marine bacterioplankton provide for humanity, including the nutrient cycling that supports economically important fisheries and large-scale oceanic biodiversity. Resultant policies will protect the biodiversity of marine microbes, which has already proven itself a priceless source of natural products that combat neurological disorders, infections, and cancer5. This study will also characterize the impact of radiation on pathogenic bacteria in coastal communities, which is crucial to fully assessing the impact of radioactive waste on human and environmental health. Literature Cited 1. Apprill, A. and M. S. Rappé (2011). ""Response of the microbial community to coral spawning in lagoon and reef flat environments of Hawaii, USA."" Aquatic Microbial Ecology 62: 251-266. 2. Behrens, E., et al. (2012). ""Model simulations on the long-term dispersal of 137Cs released into the Pacific Ocean off Fukushima."" Environmental Research Letters 7(3): 034004. 3. Bik, H. M., et al. (2012). ""Sequencing our way towards understanding global eukaryotic biodiversity."" Trends in ecology & evolution 27(4): 233-243. 4. Rossi, V., et al. (2013). ""Multi-decadal projections of surface and interior pathways of the Fukushima Cesium-137 radioactive plume."" Deep Sea Research Part I: Oceanographic Research Papers. 5. Villa, F. A. and L. Gerwick (2010). ""Marine natural product drug discovery: Leads for treatment of inflammation, cancer, infections, and neurological disorders."" Immunopharmacology and immunotoxicology 32(2): 228-237. 6. Wada, T., et al. (2013). ""Effects of the nuclear disaster on marine products in Fukushima."" Journal of environmental radioactivity 124: 246-254."	Winner!
207	Eukaryotic post-translational modification of bacterial effectors Keywords: asparagine hydroxylation, Legionella pneumophila, Yersinia pestis, bacterial effectors Legionella pneumophila, the causative agent of Legionnaire’s disease, has only recently become a human pathogen. Its intracellular lifecycle in amoeba, the natural host, has primed the bacteria for invasion into human alveolar macrophages. Co-evolution within amoeba and horizontal gene transfer has helped shape the near 300 effectors produced by Legionella that are injected into the host cell by the Dot/Icm type IVB translocation system1. A majority of these effectors have eukaryotic-like domains such as: F-box, U-box, Sel-1, ankyrin repeats, leucine- rich repeats, and CaaX motifs, which were likely acquired by horizontal gene transfer2. These domains aid in the hijacking of host processes by L. pneumophila in order to promote growth and replication. Many injected bacterial effectors are modified by the host through various post- translational modifications, however asparagine hydroxylation modification has never been observed. Post-translational asparagine hydroxylation of proteins in mammalian cells is mediated by Factor Inhibiting HIF (FIH). FIH is most commonly studied for its role in asparagine hydroxylation which regulates the Hypoxia inducing Factor (HIF), responsible for the transcription of around 100 hypoxia-related genes. FIH recognizes the amino acid sequence Lxxxx(D/E)ϕNϕ3. This motif can be found in 11 of the injected effectors of L. pneumophila, designated as Hydroxylated Effectors of Legionella (HEL). This motif can also be found in other injected bacterial effectors such as, the Outer Protein M (YopM) of Yersinia pestis, IpaH4.5 ubiquitin ligase of Shigella flexeneri, and an uncharacterized ankyrin protein of Rickettsia felis. It is likely that this motif is present in many other bacterial effectors that have yet to be described. Exploitation of host post-translational modification plays an important role in bacterial pathogenesis by further tuning it with the host, allowing it to manipulate and modulate host functions. Our hypothesis is that pathogens hijack host FIH in order to hydroxylate effector proteins, making them biologically functional. To test this hypothesis, three aims are proposed. Aim 1-Hydroxylation of effector proteins In order to determine if the HEL proteins of L. pneumophila, YopM of Y. pestis, and IpaH4.5 of S. flexeneri are hydroxylated in human cells, HEK293 cells will be transfected with plasmid containing FLAG-tag fusion proteins. Purified proteins will be analyzed by matrix- assisted laser desorption/ionization (MALDI) Mass Spectrometry (MS), to identify a 16 dalton mass shift in the fragment containing the hydroxylation motif. This will be done in collaboration with Dr. Michael Merchant. To confirm the role of FIH in asparagine hydroxylation, FIH inhibitors and FIH silencing by RNAi will be utilized. We have tested and confirmed hydroxylation in this manner for one of the HEL proteins, AnkH. This gives us reason to believe that others may be hydroxylated as well and supports our reasoning for these studies. 1 Ashley Richards Dissertation Proposal 2013 Aim 2- Protein-protein interactions of FIH and effectors Our preliminary studies have shown colocalization of the host FIH and some of the HEL proteins to the Legionella containing vacuole (LCV). Therefore, interaction between FIH and effectors is likely to occur. Due to the transient enzymatic nature of interactions with FIH, Bimolecular Fluorescence Complementation (BiFC) will be used to show the interaction between the proteins of interest and FIH. This system utilizes two plasmids each harboring half of a fluorescent molecule that emits light when brought together by interacting proteins fused to either half. If fluorescence can be detected by confocal microscopy in cells transfected with two plasmids, containing the N-terminal portion of the fluorescent molecule fused to either FIH or effector protein and the C-terminal portion of the fluorescent molecule fused to an effector protein or FIH, then interaction between of the two proteins can be suggested. Aim 3- Role of asparagine hydroxylation motif in the biological function of effectors Generating point-mutations in the asparagine of the hydroxylation motif for each protein will elucidate how hydroxylation of this residue is important to the function of the protein. This will be in comparison to the knock-out mutant, lacking the gene, which will also be generated. These mutants will be used in functional studies in a variety of species and cells such as human derived macrophages, mice, and amoebae. Because L. pneumophila has a plethora of hosts, it is possible that a mutant has an effect in one species or type of cell but not another. This will also be done with Y. pestis YopM mutant in human macrophage cell line, in collaboration with Yersinia researcher Dr. Matthew Lawrenz. Hydroxylation of bacterial proteins has never been shown before. This post-translational modification could be the key to more refined modulation and regulation of the host. This motif seems to be abundant in human pathogens and has implication in convergent evolution of bacterial effectors to better survive in its mammalian host. Not only will this educate us on bacterial host interactions but also provide more insight on FIH, as little is known about the nature of FIH hydroxylation outside of its role in hypoxia. Broader Impacts: These studies would lead into knowledge about effector proteins in the study with unknown function. Ultimately better understanding bacterial effectors and their role in the host could result in potential targets for novel treatments. My research will provide new insights into bacterial protein post-translational modification, and be added to publically accessible databases designed to predict protein structure and function. This will allow others to use this information to elucidate novel functions or regulatory mechanism for proteins in other species. References [1] de Felipe, K.S., Glover, R.T., Charpetier, X., Anderson, O.R., Reyes, M., Pericone, C.D., and Shuman, H.A. (2008) Legionella eukaryotic-like type IV substrates interfere with organelle trafficking. PLoS Pathog 4, e1000117 [2] Al-Quadan, T.P., Price, C.T., and Abu Kwaik, Y. (2012). Exploitation of evolutionarily conserved amoeba and mammalian processes by Legionella. Trends Microbiol 20, 299-306 [3] Wilkins, S. E., Karttunen, S., Hampton-Smith, R. J., Murchland, I., Chapman-Smith, A., & Peet, D. J. (2012). Factor Inhibiting HIF (FIH) Recognizes Distinct Molecular Features within Hypoxia-inducible Factor-α (HIF-α) versus Ankyrin Repeat Substrates. Journal of Biological Chemistry, 287(12), 8769-8781. 2	Winner!
208	Intellectual Merit: Alternative reproductive tactics (ARTs) are phenotypically distinct reproductive strategies that achieve approximately equal fitness (different fitness peaks). As a model system for studying the evolution of variation1, ARTs of males have been extensively studied, characterized by color and/or size, morphology, behavior (i.e. territorial vs. sneaker males), etc.1. By contrast, female ARTs are poorly studied. Female ARTs occur in oviposition site selection, mating behavior, and ontogenetic shifts in female size and fecundity, but many open questions remain1, 2: What selective factors cause divergent female behavior and/or morphology? Are they driven by predator avoidance, developmental limitations, physiology, or did they evolve in other functional contexts, for example, trophic niches1? Have morphological and reproductive behavioral differences evolved as correlated responses to sexual selection, which then impact other life history aspects, such as feeding? Or does natural selection cause feeding dimorphisms that in turn shape morphological and reproductive behavioral differences1? My research will explore phenotypic variation and ecological niches as underlying mechanisms of female alternative reproductive tactics in a novel, model system. Model System: Olive ridley sea turtles (Lepidochelys olivacea) exhibit strikingly divergent female reproductive tactics (Table 1). In the same population, some nest synchronously (SYN) en masse (>10,000 individuals) on a few, distinct, beaches whereas others nest solitarily (SOL) on multiple beaches over thousands of kilometers of coastline3. L. olivacea are the only sea turtle species to exhibit these ARTs, which were not formally recognized until 2002. Virtually nothing is known about why or how the ARTs occur3. I hypothesize that these alternative reproductive tactics are a result of an ecological dimorphism. SYN nesters migrate throughout the E. Pacific and aggregate to mate offshore of SYN nesting beaches to ensure copulation3. I predict SOL nesting females are neritic foragers, allowing them to nest more frequently and find mates more often making SYN aggregations unnecessary. I will sample females at 2 SYN and 3 SOL study site (6 if logistics permit). Table 1: Known characteristics of L. olivacea divergent reproductive tactics Characteristic Synchronous nesters (SYN) Solitary nesters (SOL) Inter-nesting period4 28 days 14 days Nesting phenology3 Rainy season All year Site fidelity 4 High Low Female body & clutch size3 Larger Smaller Eco-morphology AIM 1 AIM 1 Spatially explicit foraging ecology3 Nomadic, pelagic; AIM 2 AIM 2 AIM 1: DEFINE THE MORPHOMETRICS OF SOL AND SYN NESTING L. OLIVACEA. Morphological differences are common attributes of ARTs1. There is some evidence that SYN are larger than SOL nesters3 but basic morphology of these divergent ARTs is unknown. Using morphometric tools I will test my hypothesis that there are significant differences in size, shell depth, shell shape and flipper morphology between the two tactics. Morphological differences relating to foraging behavior are known in other sea turtle species5-7. Ecological dimorphisms have been shown in three populations of Caretta caretta6, 7 where small females forage in pelagic habitats and larger in neritic habitats. In Chelonia mydas, a pelagic population has larger flippers than a neritic one5. Methods: I have defined 10 flipper landmarks related to underlying skeletal and muscle structure. These landmarks and standard sea turtle body measurements8 (i.e. shell width & length, body depth & mass) will be quantified. I will use principal components analysis to test for morphological differences, and if found, to evaluate which attributes drive the variation. I estimated from a power analysis9 (F-test, p =.05, 10% effect size) that a sample of 100 females per study site (N=500) will provide a power of 89% to detect a difference. AIM 2: DEFINE THE FORAGING ECOLOGY OF SYN AND SOL NESTING L. OLIVACEA. Stable nitrogen (δ15N) and carbon (δ13C) isotope ratios, coupled with satellite telemetry, have proven to be effective tools for defining sea turtle ecological dimorphisms in 3 of the 6 other species7, 10, 11. I will utilize these tools to test my hypothesis that SYN nesters are nomadic, pelagic (open ocean) foragers with no localized foraging ground, whereas SOL nesters are neritic foragers with distinct neritic foraging grounds. Methods: Skin and dorsal shell samples will be taken to provide recent (skin) and multi-year (shell) foraging histories12. Samples will be taken during early, mid and late nesting season to account for migrations from various foraging grounds and will be collected, prepared and analyzed using established methods12. The power analysis demonstrated that a sample size of 35 turtles per sample period, per site, for skin and shell tissue (105 per site, total N=524) is sufficient. To examine spatially explicit foraging ecology I will attach satellite tags to randomly assigned females sampled for stable isotopes (10 at each study site, total N=50). Implementing robust state space modeling, I will analyze the data using established protocols13. Sampling from multiple sites and using spatial statistical analyses will account for the possibility of pseudoreplication (spatial autocorrelation in this system). This is the first detailed morphological analysis of L. olivacea ARTs and the first examination of ecological niches as an underlying mechanism driving them. Both aims are feasible; the methods have been successful in other sea turtle studies, I have tested them in the field and I have support of international collaborators. My results will contribute to a meta- analysis creating a stable isotope landscape for the E. Pacific Ocean, headed by a NSF GRF. I am organizing the first L. olivacea working group to address the unknown life history traits, which will have important management applications for this vulnerable species. My field season includes fall semester and at least two are needed. This fellowship is crucial in allowing me to be decoupled from campus and will greatly increase my capacity to do fieldwork. Broader Impacts: Communicating my research is an important part of my career path and professional development. Using social media I share my research and discuss science issues with scientists and lay people. Working with Texas Sea Grant I am developing STEM educational materials, using charismatic sea turtles as flagship species to promote watershed education in K-12 classrooms. I will develop a network of graduate students across Texas to speak with classes about their adventures in pursuit of higher STEM education. I will assemble and train undergraduates (including those in the Texas A&M, NSF- funded, Louis Stokes Alliance for Minority Participation program), Costa Rican community members and personnel from NGOs and national parks to assist in my research. Participants will receive a hands-on opportunity to learn about experimental design, fieldwork, data analyses and ethics of working with animals all while engaging in cultural exchange. I will continue to disseminate my work to the scientific community via presentations and peer-reviewed papers. This fellowship is key in allowing my work to impact the evolutionary understanding of ARTs, life history of an understudied species and a wide nonscientific audience through education and collaboration. References: 1Oliveria et al. 2008 Alt Repro Tactics.2Henson & Warner 1997. Annu Rev Ecol Syst 28:571-92.3Plotkin 2007. Biol and Conserv of Ridley Sea Turtles.4Kalb 1999. Ph.D. Diss. 5Balazs et al. 1997. Proc Ann Sea Turtle Symp.6Hawkes et al. 2006. Curr Biol 16, 990-5.7Hatase et al. 2002. Mar Ecol-Prog Ser 233:273- 281.8Wyneken. 2001 The Anatomy of Sea Turtles.9Cohn1988. Stat Power Analysis for the Behav Sci.10Hatase et al. 2006. Oecologia 149:52-64. 11Caut et al. 2008. PLoS One e1845. 12Reich & Seminoff 2010. Proc Ann Sea Turtle Symp. 13Block et al. 2011. Nature doi:10.103/nature10092.	Winner!
211	Key Words: Peptide Release, Ribosomal Stalling, Gene Regulation, Student Mentoring Abstract: The aim of the proposed research is to gain a mechanistic understanding of peptide release and nascent peptide mediated ribosome stalling by employing both a synthetic and structural approach. This project will broaden our understanding of protein synthesis and gene regulation by the ribosome and promote teaching and learning in all educational levels through mentoring and collaboration. This research will be completed with the guidance of Dr. Scott Strobel at Yale University, with all the requested resources and collaborations available to successfully accomplish the following aims. Background and Significance: Protein synthesis by the ribosome is a fundamental process found in all life. A set of highly conserved nucleotides located in the active site of the large subunit of the ribosome are responsible for two biologically important reactions: peptide bond formation and release. Termination of protein synthesis occurs when one of three stop codons are recognized in the small ribosomal subunit and decoded by release factor proteins (RFs) 1. Upon recognition, a highly ordered water molecule nucleophilically attacks the aminoacyl ester linkage of peptidyl-tRNA hydrolyzing the ester bond which links the nascent polypeptide to the peptidyl-tRNA. As seen in Figure 1a, it is hypothesized that as the ordered water molecule attacks the ester linkage, the carbonyl carbon proceeds through a tetrahedral transition state containing a developing negative charge, an oxyanion. While termination of translation has Figure 1. a) General mechanism of peptide release. b) Generic structure of peptide release been known for transition state analogs. c) Generic structure of peptide formation transition state analogs. some time now2, it is less studied than elongation and the underlining mechanistic processes are only starting to emerge. Thus transition state characterization and structural studies can help define how the ribosome catalyzes this challenging reaction. In addition to catalyzing the formation and release of polypeptides, the ribosome has also been found to have the ability to monitor the structure of the growing polypeptide during elongation, a process which is poorly understood. Accumulating evidence shows that some nascent peptides result in ribosomal stalling due to specific RNA interactions within the exit tunnel of the ribosome. Many of these have been found to play a role in regulating the expression of genes such as erythromycin resistance in bacteria3. Recent cryo-EM reconstructions of the stalled ribosome have suggested that certain interactions within the tunnel are relayed to the peptidyltransferase center (PTC) to arrest translation4. However, how this information is communicated to the PTC is essentially unknown. By further understanding the mechanism of ribosome stalling it may yield insights into the events that regulate gene expression from bacteria to humans, which can lead to the rational design of more efficacious drugs. Specific Aims: Aim 1: To synthesize and measure the binding affinity of a series of transition state analogs. In the Strobel lab, I will create a series of peptide release transition state analogs containing functional groups of varying shapes, charge distributions, and hydrogen-bonding potentials and measure their relative affinity for the ribosome using RNA chemical footprinting (Figure 1b). With this technique, nucleotides in the 23S rRNA will be probed using dimethyl sulfate as a function of inhibitor concentration using established protocols5. All of these inhibitors have the same basic geometry and each is synthesized as a pair of diastereomers that allow both non- bridging oxygens to be characterized independently. Transition state theories predict that enzymes bind the tightest to the transition state of the reactions they catalyze. Therefore, the inhibitors that best complement the electrostatic environment of the active site will bind the tightest, and from changes in the extent of modification of ribosomal residues, the relative affinities will allow us to draw conclusions about the geometry and charge distribution of the active site during release. Aim 2: To gain a structural understanding of peptide release and induced ribosomal stalling. Given the implications of the ribosome in peptide release, its role in translational arrest, and its essential yet understudied role in gene regulation, it will be vital to develop a mechanistic understanding of how the ribosome performs all these actions. Using high-resolution crystal structures I will investigate how important structural features of the ribosome, peptidyl tRNA and release factor proteins position a water molecule for optimal attack of the aminoacyl ester linkage of peptidyl-tRNA. I will also elucidate how specific conformations of the nascent polypeptide chain and subtle conformational changes in the ribosome can feedback inhibit the PTC. I will thus collaborate with the Steitz lab at Yale, which is preeminent in ribosomal X-ray crystallography, to get a crystal structure of release factor 2 bound to the ribosome with the best peptide release transition state analog from Aim 1 (Figure 1b). Using solid phase synthesis, I will also synthesize transition state analogs of peptide bond formation with an attached polypeptide of known stalling ability that can be tethered into the exit tunnel (Figure 1c). By visualizing the peptide-exit tunnel interactions through crystallography together with biochemical and computational data, it is possible to propose a more accurate mechanistic model of nascent polypeptide chain-mediated translational stalling. Aim 3: To promote teaching, mentoring and collaboration in multiple educational levels. In collaboration with Dr. Nicolas Carrasco at Quinnipiac University, I am in the unique position to teach and mentor possible undergraduate students from both Yale and Quinnipiac who wish to participate in this project by helping them experience graduate-level research, and teaching them how to communicate their findings at conferences. These students will work toward the synthesis of various oligonucleotide-peptide conjugates in order to further research stalling peptides and investigate the role of various cofactors in the formation of the stable stalled ribosome complex. I will also use this work as a teaching tool during TA sessions at Yale for science and non-science oriented undergraduate classes by teaching students how to think critically and analytically. Additionally, I will become involved in the New Haven Science Fair Mentor Program (NHSFMP) to help elementary school students and teachers become more excited about science. I will facilitate weekly brainstorming sessions meant to teach students to form a hypothesis, develop an experimental approach, and analyze their results. My long term goal is to help create a better science curriculum to show their students how to become future scientists. References: 1. Weixlbaumer, W. et al. (2008) Science. 322: 953-956. 2. Capecchi, MR. et al. (1967) Proc. Natl. Acad. Sci. 58: 1114-51. 3. Ramu, H. et al. (2011) Mol. Cell. 42: 321-330. 4. Seidelt, B. et al. (2009) Science. 326: 1412-15 5. Parnell, K. et al. (2002) Pro. Natl. Acad. Sci. U. S A. 99: 11658-1166	Winner!
212	gene expression. Transcription factor (TF) activity itself is difficult to measure experimentally in high-throughput; however, many insights can be gained from applying statistical methods to infer activity from the expression of TF target genes. This indirect quantification of TF activity has been made possible by gene expression microarrays, which simultaneously profile thousands of genes. There has been extensive research on how to test for differential expression of a priori defined gene sets such as TF target genes.1 One method recently developed in the Kleinstein lab, Quantitative Set Analysis of Gene Expression (QuSAGE), is unique in that it produces a probability density function for each set by convolution of the expression profiles of individual genes.2 Still, the efficacy of all these methods relies on the chosen TF target gene set. The adjacent figure shows how the NFkB activity inferred after stimulation with TNF (an inducer of NFkB) is highly dependent on the choice of gene set. Thus, there is a need to improve methods for generating and refining such sets. There are numerous ways to generate putative TF target gene sets both computationally and experimentally. Inferred NFkB Activity Computational predictions provide candidate targets by scanning for a specific binding motif in promoter sequences genome-wide. However, this method is known to generate many false-positives. Protein-DNA binding experiments (e.g. ChIP-Seq, ChIP-ChIP) provide experimental evidence for TF-DNA interactions. However, there are a large number of binding interactions observed, many not in the promoter of known genes, and these interactions may be specific to the cell line used. In addition, the accuracy of both of these prediction methods suffers because the occurrence of a TF binding site or the actual binding of a TF to a gene promoter does not necessarily imply transcriptional regulation. Networks from pathway databases (e.g. KEGG) provide some additional information about which genes interact at a transcriptional level. However, the number of interactions in pathways is limited. Currently each TF is associated with a single target gene set. This is problematic because, in reality, TF target genes depend on the cellular and environmental context of the cell. To infer TF activity more accurately, candidate target gene sets from many sources can be refined to include only the genes under a TF's control in the specific context being studied. I will develop a method for generating context-specific transcription factor target gene sets (Aim 1), and apply these gene sets to infer transcription factor activity during infection and vaccination responses (Aim 2). Aim 1: Develop a method for generating context-specific TF target gene sets. I will begin with a large set of proposed candidate TF target genes and then to utilize co- expression patterns from gene expression data to select candidate genes having a similar gene ytivitcA FT 1.5 x 1.0 x 0.5 x 0.0 x x Set 1 Set 2 Set 3 Set 5 Set 4 (149 genes) (206 genes) (301 genes) (2757 genes) (96 genes) 100.0− 500.0− 10.0− 50.0− 1.0− 1 1.0 50.0 10.0 500.0 100.0 TF Target Gene Sets expression pattern. One limitation of current approaches is that each gene is either a candidate of a TF, or it is not. I plan to integrate multiple information sources including computationally predicted binding sites from motif scanning algorithms, protein-DNA binding data, and pathway information to compute a prior probability for each gene being a candidate target of each TF. This prior probability represents the strength of the evidence for a certain gene being a target of a certain TF. Because the relative importance of each of the data sources is not known, I will estimate them as parameters in a Bayesian network. I hypothesize that this extra information will improve correct identification of TF targets. The method will allow for overlap of genes between target sets but I will explore whether this is necessary, since dependence between target sets is often problematic for quantification of TF activity. The proposed method will build on the framework proposed by Fertig et al.3 Some TFs are transcriptionally regulated themselves, allowing for estimation of activity directly from gene expression measurements. I will evaluate my method by comparing the activities inferred from the proposed and published methods for these transcriptionally regulated TFs. Aim 2: Apply these gene sets to infer transcription factor activity during infection and vaccination responses. One natural application of these TF target gene sets is to infer TF activity. Thus, I plan to apply the developed method to specific time-series gene expression data sets of influenza infection and vaccination responses. I have access to these data through the NIAID funded Program for Research on Immune Modeling and Experimentation (PRiME) and the Human Immunology Project Consortium (HIPC). In the case of the influenza infection data, the different contexts correspond to four different strains of in vitro influenza infection; while in the case of vaccination response data, the contexts correspond to vaccine responders or non-responders. Generating context-specific gene sets will allow us to answer two fundamental questions: Are there changes in which genes are regulated by certain TFs across contexts? And how do the activities of TFs differ between contexts? I will answer the first question by applying differential network analysis, a method to identify how the regulatory network is rewired in different contexts. To answer the second question, I will infer activity of each gene set using QuSAGE to find quantitative differences in TF activity between contexts. Significance and Broader Impacts TFs are key regulators in development and disease. The ability to better characterize TF activity thus has implications for understanding disease states and the mechanisms underlying development. The proposed integrative approach to generate context-specific TF target gene sets will improve understanding of transcriptional regulation and allow for a more accurate inference of TF activity. 1. Hung, J.-H., Yang, T.-H., Hu, Z., Weng, Z. & DeLisi, C. Gene set enrichment analysis: performance evaluation and usage guidelines. Brief. Bioinform. 13, 281–91 (2012). 2. Yaari, G., Bolen, C. R., Thakar, J. & Kleinstein, S. H. Quantitative set analysis for gene expression: a method to quantify gene set differential expression including gene-gene correlations. Nucleic Acids Res. 41, e170 (2013). 3. Fertig, E. J., Favorov, A. V & Ochs, M. F. Identifying context-specific transcription factor targets from prior knowledge and gene expression data. IEEE Trans. Nanobioscience 12, 142–9 (2013).	Winner!
213	"MULTI-MODAL DATA ! Keywords: Alzheimer’s Disease, Structural MRI (sMRI), Functional MRI (fMRI), Mild Cognitive Impairment (MCI), GPU computing ! Summary: I propose to use multiple imaging systems, such as structural and functional MRI imaging to provide a temporal evolution of Alzheimer’s disease (AD) with multi-modal data. It will include patients that show no symptoms and patients that have MCI. This temporal evolution will show how different regions of the brain changes and how AD evolves. The hippocampus region will be a main region of interest, but other regions like the temporal lobe will be examined [2,3]. In addition with the imaging system, looking at the cerebrospinal fluid will provide great insight to how AD affects the body [1]. I will also incorporate GPU computing to make it efficient. ! Motivation: AD is ranked as one of the leading diseases in increasing deaths. AD plans to increase among the world, which will have an effect on the economy. It is proposed that the expected cost of AD will rise to $1.2 trillion by 2050 [2]. Some methods have shown how some patients who have MCI evolve into AD [4]. If AD is not detected early, it will lead the patients to be in the latter stages of AD. The latter stages, such as stage 5 create severe cognitive decline and require the AD patient to need assistance in performing routine tasks [2]. Thus, detecting AD early is very crucial. ! Hypothesis: Detecting AD in the early stages could be very beneficial to future patients, but has been a challenge. One group has said that validating imaging biomarkers for AD has brought controversial findings [5]. Now, using multi-modal data, such as fMRI, sMRI, and other imaging systems, could capture how AD progresses through time. With seeing different areas of the brain and CSF, it could provide insight to how AD evolves with patients that show no symptoms and patients that have MCI. ! Research Strategy: To develop an extensive model of what makes AD develop between patients that show no symptoms at all and patients that have MCI. Objective 1: Identify the critical parts/aspects that could lead to Alzheimer’s developing. Objective 2: Examine the different imaging systems that have been used to look for AD, such as fMRI, sMRI, PET, and etc. Objective 3: Extract the regions from the different imaging systems that show where AD could develop and run pattern recognition methods to classify which are likely to develop. Objective 4: Identify which interconnections between regions to show how the disease progresses. ! Research Methodology: This research methodology is based on several pattern recognition approach. Other people have used different image processing techniques to extract regions of interest that show where Alzheimer’s could develop and use pattern recognition to classify the regions. Overall, biomedical image processing and pattern recognition will be the foundation in developing more information about detecting AD in the early stages. ! Anticipated Results: The anticipated result will show how different areas of the brain can show how Alzheimer’s progresses throughout the patient’s age. It will also show evolution between patients with no previous symptoms and patients with MCI. Plus, I plan to incorporate GPU computing so the data could be computed faster because some of the imaging systems, such as fMRI is computationally expensive and extensive [1]. The research that is conducted will be submitted to a journal paper. A journal publication could be sent to IEEE Transactions or Computer Vision and Pattern Recognition. ! Institution: Dr. Alan C. Bovik from University of Texas at Austin is a great professor to conduct research for this project. One of his research areas is biomedical engineering. His work has done detection and diagnosis of breast cancer. The Laboratory for Image and Video Engineering would be a great place to conduct research for this project. ! Intellectual Merit and Broader Impacts: This research could be valuable to the medical field. It would help multiple people in trying to understand more about AD. The major problem with AD is trying to diagnose it earlier in the beginning stages because there people could prepare. When AD is detected at the latter stages, the damage is done and the patients diagnosed with AD will need to be cared for the rest of their life. If a new patient is experiencing common trends compared to a previous patient where it showed the patient’s temporal evolution, then the temporal evolution can provide a model to show if a new patient with similar patterns will develop AD. ! ! 1 2 3 4 5! ! !1 Mason, Emily J., Manus J. Donahue, and Brandon A. Ally. ""Using Magnetic Resonance Imaging in the Early Detection of Alzheimer's Disease."" (2013). !2 Bukhari, Ijaz. ""Early Detection of Alzheimer's-A Crucial Requirement."" arXiv preprint arXiv:1305.2713 (2013). !3 Ahmed, Olfa Ben, et al. ""Alzheimer Disease detection on structural MRI."" Proceedings of ESMRMB 2013 Congress. 30th annual meeting. 2013. !4 Douaud, Gwenaëlle, et al. ""Brain Microstructure Reveals Early Abnormalities more than Two Years prior to Clinical Progression from Mild Cognitive Impairment to Alzheimer's Disease."" The Journal of Neuroscience 33.5 (2013): 2147-2155. !5 Dukart, Juergen, et al. ""Generative FDG-PET and MRI Model of Aging and Disease Progression in Alzheimer's Disease."" PLoS computational biology 9.4 (2013): e1002987."	Winner!
214	Background. Frontal association cortex is a brain region critical for flexible action selection in mammals. In humans and other primates, this area includes the supplementary motor complex (SMC), which has been shown necessary for suppression of inappropriate motor plans--an extreme case being 'alien hand syndrome,' in which SMC damage leads to complex and seemingly purposeful hand movements in the absence of voluntary control.1 Premotor cortex (M2) is thought to be the rodent homolog of SMC. Lesions to M2 selectively disrupt goal-directed behavior2, and in particular the ability to adapt choice of action to changes in reward values.3 Despite the evidence for its causal role in behavioral flexibility, the mechanisms by which neural networks in frontal association cortex realize this vital function remain a mystery. Preliminary Findings. To investigate the neural substrates for flexible action selection in M2, I will use a combination of rodent behavior, in vivo imaging of neural ensemble activity, and local silencing methods. During my first year of PhD research, I developed a two-choice decision task for mice that requires flexible switching between different action selection strategies in order to obtain optimal reward. The first phase of the task requires a cue-guided strategy in which the animal must discriminate between two distinct auditory stimuli that each indicate the availability of water reward at a corresponding lick port on either side of the animal's mouth. In the second phase, an action-guided strategy is necessary: reward is contingent upon licking a specific port regardless of the cue presented. The two phases are alternated many times within a single session without any sensory cue to indicate the phase-switch. Thus, the task requires flexible adaptation of action selection strategy to changing contingencies between cue, action, and reward. In a first step toward understanding the neural basis of behavioral flexibility, I have begun imaging ensembles of M2 neurons at cellular resolution as mice perform this task (Fig. A&B). To measure changes in neural activity, the genetically encoded calcium indicator GCaMP6 was first transduced into layers 2/3 of M2 using an adeno-associated virus (AAV). A cranial window was implanted above M2, and fluorescence traces were recorded using 2-photon microscopy. By aligning the traces recorded from individual neurons to specific events in a trial (e.g., cue or response onset), I have correlated animal behavior with activity changes in single neurons, as well as with the aggregate activity of all neurons in the recorded ensemble. Preliminary analyses have produced two key findings that motivate detailed investigation: (1) A large proportion (>25%) of individual neurons recorded in M2 were choice-selective, i.e., these neurons showed significant differences in activity depending on which port was chosen (Fig. C). Interestingly, the fraction of choice-selective neurons increased following reward delivery, and peaked ~2-sec post-reward (Fig. D). (2) A greater proportion of neurons were choice-selective when the task required an action-guided versus a cue- guided response. Furthermore, a large fraction of neurons showed pre-response choice selectivity when an action-guided strategy was utilized. Aim 1: Test causal role of M2 ensemble dynamics in behavioral flexibility. Our preliminary findings indicate delayed choice-selective activity in M2 that may serve as a feedback signal important for reinforcement of the current action-selection strategy. In order to test this hypothesis, A C E x a m p le C e ll D B G r o u p D a t a n = 5 6(5 m 2 ce llsice ) I will silence M2 in a temporally specific manner using an optogenetic approach. First, the light- sensitive neuronal silencer ArchT will be transduced bilaterally into M2 using an AAV. Light pulses will then be delivered through an optical cannula to inactivate M2 specifically during the 3-sec post reward in order to block delayed choice-selective activity. If such activity is important for reinforcement of action-selection strategy, then this manipulation should increase the number of trials taken to reach a criterion rate of correct response after phase switches, as well the number of perseverative errors. Preliminary observations also reveal early choice-selective activity during action-guided correct trials that may bias action selection toward the appropriate response. If this is the case, silencing M2 during the 3-sec prior to response should increase trials-to-criterion during the action-guided phase while having no effect on performance during the cue-driven phase of the task. Aim 2: Investigate contribution of GABAergic inhibition to M2 ensemble dynamics and determine causal role in flexible action selection. GABAergic inhibition is known to serve essential computational roles in the neocortex. For example, fast-spiking PV+ interneurons (PV- INs) are known to generate the gamma rhythm and sharpen feature selectivity in sensory areas.4,5 However, the function of PV-INs in cognitive areas of cortex remains unexplored. The idea that PV-INs modulate choice selectivity within M2 ensembles is an intriguing hypothesis. To test this possibility, I will modify the in vivo imaging experiment described above to include PV-specific silencing, using a transgenic mouse line (PV-cre) that expresses cre-recombinase only in PV-INs. Because an optogenetic approach would preclude simultaneous imaging, a cre-dependent inhibitory receptor activated by the drug CNO (Gi-DREADD) will be transduced into M2 using an AAV, to allow PV-specific silencing as mice perform the decision task. I hypothesize that silencing PV-INs with CNO will reduce choice selectivity within the imaged ensemble by removing task-related inhibitory control of choice-selective neurons. Additionally, if PV-INs in M2 are critical for strategy reinforcement, then PV-specific silencing should disrupt adaptation of action selection strategy, and thus increase trials-to-criterion and perseverative errors after shifts in cue-action-reward contingencies. Broader Impacts. Research on flexible decision-making will benefit a wide variety of fields that concern human and animal behavior. Economic decisions are essentially a form of goal-directed behavior in which appropriate error signals derived from expectation, reward, and punishment must play a key role. By improving understanding of how decision strategy is adapted, we might develop a more informed view of how markets operate, and possibly reduce the human toll of market dysfunction. Similarly, the justice system requires a nuanced understanding of concepts such as incentive structure, deterrence, and risk, all of which must be rooted in goal-directed behavior. As a complement to the study of normal decision-making, it will also be important to study how flexibility of the system is hampered by stress, distraction, mood, etc. A more mechanistic understanding may lay the foundation for discovery of measures we can all take to optimize our level of cognitive flexibility, in order to make better decisions as individuals and as a society. Finally, flexibility is notoriously difficult to implement in current hardware and software. Since behavioral adaptation is evidently a great talent of animals, biomimetic engineering based upon our own neural wetware may one day deliver some very smart machines. References: 1) Nachev et al. (2008) Functional role of the supplementary and pre-supplementary motor areas. Nat Rev Neurosci, 9(11), 856-869. 2) Gremel et al. (2013) Premotor cortex is critical for goal-directed actions. Frontiers Comp Neurosci, 7. 3) Sul et al. (2011) Role of rodent secondary motor cortex in value-based action selection. Nat Neurosci, 14(9), 1202-1208. 4) Cardin et al. (2009) Driving fast-spiking cells induces gamma rhythm and controls sensory responses. Nature, 459(7247), 663-667. 5) Lee et al. (2012) Activation of specific interneurons improves V1 feature selectivity and visual perception. Nature, 488(7411), 379-383.	Winner!
218	"suffered severe facial trauma. I underwent several procedures to repair the damage, including maxillofacial reconstruction (Figure 1) and extensive dental surgery. During my recovery, I became fascinated with the sophistication of the materials used in my repairs and became deeply interested in orthopedic and dental biomaterials. This interest, combined with my strengths in physics Figure 1. and mathematics, led me to pursue my undergraduate education in bioengineering with an emphasis in materials science at The Pennsylvania State University. Through my educational, teaching, leadership and outreach activities, I have since formulated my career aspiration to become a professor at a research institution. My desires to address biomedical challenges creatively, to advance my education, and to share my knowledge are perfectly suited to this profession. Educational Experiences. In addition to maintaining a high GPA, I have participated in academic activities to prepare myself for graduate study. For example, I joined the Schreyer Honors College my sophomore year to gain experience in writing an honors thesis. Being in the honors college also has afforded me the opportunity to take higher-level courses that incorporate my research interests. For instance, I enrolled in a graduate-level musculoskeletal mechanics course for the spring. I am also a scholar in the Ronald E. McNair Post-Baccalaureate Achievement Program, a federal TRIO program designed to prepare low-income, first generation, and underrepresented students for doctoral study. The program initially appealed to me because it organizes professional development activities for its scholars and financially supports a nine-week summer research internship with a faculty advisor. This program was a unique opportunity to develop my career objectives and gain valuable research experience. In the spring of 2007, I was accepted into the program and joined the laboratory of two faculty advisors whose research aligned with my interest in orthopedic biomaterials: Dr. Erwin Vogler from Materials Science and Engineering and Bioengineering and Dr. Andrea Mastro from Biochemistry and Molecular Biology. These two professors collaborate on studying breast cancer metastasis to bone. Teaching Experiences. As a low-income, first-generation college student, I work to support my education and have sought opportunities where I both can be compensated and can reinforce concepts from my coursework. For instance, I have served as a grader for the Math Department in six differential equations courses over Teaching Experiences two semesters and have volunteered to run • Physiology Teaching Assistant chemistry, physics and calculus study groups for the Women in Engineering Program. During this time, I • Engineering Design Lab Assistant also accepted an internship with the Chemistry • WEP Facilitator – Chemistry, Department, which combined research, volunteer and Calculus, Physics groups teaching components. Under the advisement of Dr. • Chemistry Dept. internship Joseph Keiser, I researched the life and scientific • Grader, Differential equations achievements of George Washington Carver, • 400-level Bioengineering course focusing on his experiments with peanuts. Extracting information from farming bulletins, Carver’s patents and my own laboratory experiments, I developed a laboratory curriculum for students in general chemistry, and implemented the curriculum in the form of a make-up laboratory. The curriculum was later adapted into a calorimetry laboratory for an honors introductory chemistry course. Because I also wanted to be 1 involved in teaching activities more closely related to my academic pursuits in bioengineering, I accepted a position as a laboratory assistant for an engineering design course, helping students in the development and presentation of design projects. As my teaching skills strengthened, I wanted to challenge myself to lead a classroom. For that reason, I became a teaching assistant for an introductory physiology laboratory, a role in which I could teach independent from a faculty instructor. I have taught six sections of this course over the past two years. Each semester, I manage approximately 60 students. As the only TA studying bioengineering, I also mentor bioengineering students from other sections. My responsibilities for the course include preparing and instructing pre-laboratory lectures, developing weekly quiz questions, demonstrating laboratory techniques, leading post-laboratory discussions, and formulating instructions for written reports. I have received Institutional Animal Care and Use Committee (IACUC) training for the use of animals in the laboratory. With the help of another TA, I developed an academic writing tutorial as a supplement for the course. I will further challenge myself in the spring as an assistant for a 400-level bioengineering course by teaching more difficult material. Leadership Activities. In addition to assuming leadership roles in the previously mentioned activities, I currently serve as treasurer for the Penn State chapter of the Biomedical Engineering Society. I help coordinate volunteer activities and professional development workshops for our members, including an American Red Cross blood drive, an H1N1 vaccination clinic, and a graduate school portfolio workshop. Our chapter has also established “Lunch and Learn” activities, which give bioengineering students an opportunity to learn about the research and career paths of department and adjunct faculty members. Additionally, I am presently working with other bioengineering students and faculty to establish a chapter of AEMB, the National Biomedical Engineering Honor Society. Outreach. I have also participated in numerous educational outreach activities. Currently, I work with a program called Engineering Ambassadors to encourage high school students, particularly women, to pursue an education in science or engineering. In this program, I travel to biology, chemistry, and physics courses at high schools across Pennsylvania and give presentations about research emerging at the intersection of science and engineering. I was one of two students to pilot this program over the past summer and present the results of the pilot to administrators in the College of Engineering. Positive feedback from our presentation has allowed the program to expand to include 15 female presenters, with plans for visits to schools across the state throughout the year. While I was taking a physiological systems bioengineering course, I observed that a considerable amount of time was spent reviewing basic physiology because it was being presented from the engineering perspective. This academic year, I am re-designing pre- laboratory PowerPoint presentations used in the physiology course that I teach in order to approach the basic physiology from this engineering perspective. These modified materials may be incorporated into a bioengineering-section of the course in the future. I have also contributed to an engineering communications course by recording a class presentation as an educational web resource for students in a wide variety of engineering disciplines. Additionally, the slides I developed for the class are now used by course instructors as strong examples. As I near the end of my undergraduate career, I look forward to working toward a Ph.D. and a career as an academic. The NSF fellowship would provide me with the resources to achieve this goal. 2 In the spring of 2008, I began undergraduate research in the laboratories of Dr. Erwin Vogler and Dr. Andrea Mastro, who collaboratively study breast cancer metastasis to bone using an engineered three-dimensional bone tissue developed in a compartmentalized bioreactor. This research offered me the opportunity to learn fundamentals of osteobiology and osteopathology that would translate to studies of orthopedic biomaterials later in my career. I was fascinated by the simplicity of the bioreactor design, the sophistication of the tissue it supported and the potential for this in vitro model to enhance the investigation of metastatic bone disease. I have since dedicated myself to this research by working full-time throughout the summers of 2008 and 2009 as well as throughout the past two academic years to complete my honors thesis. Background. The American Cancer Society estimates that one in eight women will be diagnosed with breast cancer in the course of their lifetime. Breast cancer is the second most commonly diagnosed cancer in women in the United States, accounting for nearly 27% of all female cancers in 2009.1 Breast cancer frequently metastasizes to bone (Figure 1), with bone metastases occurring in approximately 70% of patients with advanced breast cancer.2 Breast cancer disrupts normal bone remodeling by suppressing the function of osteoblasts (bone- forming cells) and increasing the activity of osteoclasts (bone-resorbing cells), resulting in increased bone degradation coupled with the release of factors from the bone matrix that support tumor growth. Current Figure 1. therapies target this “vicious cycle” between tumor cells and the skeleton.3 Bisphosphonates are a family of drugs that bind avidly to mineralized bone where they are internalized by osteoclasts and signal osteoclast destruction, resulting in reduced bone degradation. They are often administered alongside chemotherapy drugs (taxanes).4 The interaction of bisphosphonates with osteoclasts is well understood, but little is known about effects of bisphosphonates on osteoblasts. Undergraduate Thesis Research. My research involves the study of these drugs with considerations for their effects on osteoblasts. The purpose of this study is to characterize the effect of a bisphosphonate (zoledronic acid) and a taxane (docetaxel), alone and in combination, on osteoblasts in conventional tissue culture and osteoblasts challenged with metastatic breast cancer cells in the compartmentalized bioreactor. I hypothesize a combination therapy will show synergistic antitumor effects but have little effect on the integrity of osteoblast tissue. In the summer of 2008, I gained valuable skills in cell culture and maintenance by assessing the effects of these drugs on osteoblast proliferation and differentiation in conventional tissue culture. During this time, I was supported by the Ronald E. McNair Summer Undergraduate Research Program and fulfilled all program requirements, which included documenting a minimum 40 hours of research each week, writing a research paper for the Penn State McNair journal, and presenting the research at the annual McNair summer conference. Throughout the 2008-2009 Figure 2. academic year, I became skilled in developing osteoblastic tissue in the bioreactor, challenging that tissue with breast cancer cells, and monitoring cancer progression with confocal microscopy (Figure 2A). I participated in the McNair program again the following summer and was additionally supported by the Undergraduate Summer Discovery Grant awarded through the university. I enhanced my previous data by optimizing cell viability assays and expanded my 1 experiments to the bioreactor, evaluating the effects of zoledronic acid on this model of breast cancer in bone. I discovered a single dose of zoledronic acid at 0.50 µM administered three days after co-culture reduced the formation of breast cancer colonies and disrupted cancer cell alignment with osteoblast tissue (Figure 2B). This finding suggests that, in addition to effects on osteoclasts, zoledronic acid may have a direct antitumor affect on breast cancer cells. These findings will be published in the next McNair journal and were presented at the McNair summer conference in 2009. My abstract was also accepted for poster presentations at two professional conferences, one in the field of engineering and the other in bone metastasis research. The opportunity to engage in conversations with engineers, biologists, and oncologists and receive their feedback was a rewarding experience that taught me the value of being an active member of the scientific community. This academic year, I will complete my thesis research by evaluating the combination bisphosphonate and taxane therapy in the bioreactor model. I am currently supported by the Pennsylvania Space Grant Consortium Sylvia Stein Memorial Scholarship and Federal Work Study. In addition to detailing my research project, a portion of my thesis will be devoted to evaluating the potential for the bioreactor to serve as a system for testing therapeutics. I have also begun a small research project in collaboration with Dr. George Engelmayr, a bioengineering professor, to investigate the effects of surface topography on osteoblast development in the bioreactor. We predict that microfabricated surface topographies will accelerate or enhance maturation of osteoblastic tissue in the bioreactor. While I work alongside my advisors, a graduate student, and a laboratory technician to develop my research skills, my application of these skills is original and my research is independent. Perhaps the most important thing I learned throughout this experience is that approaching a research problem from only one perspective is inefficient. I truly value my capacity to work within a highly interdisciplinary team and my ability to leverage skill sets from different fields to achieve a more holistic understanding of my research problem. Additionally, the experience of working with the bioreactor has taught me that successful strategies in research do not necessarily have to be complex – success can sometimes be achieved through simplicity. Publications and Presentations 1. Miller, Genevieve (forthcoming). “Bisphosphonate effects on breast cancer colonization of three-dimensional osteoblastic tissue.” The Penn State University McNair Scholars Journal. 2. Miller, Genevieve. 2008. “Bisphosphonate and taxane effects on osteoblast proliferation and differentiation.” The Penn State University McNair Scholars Journal. 3. “Bisphosphonate Effects on Breast Cancer Colonization of Three-Dimensional Osteoblastic Tissue.” Poster presented at The IX International Meeting on Cancer-Induced Bone Disease, October 29-30, 2009 and the Biomedical Engineering Society Annual Fall Scientific Meeting, October 8, 2009. 4. “The effect of a bisphosphonate, zoledronic acid, on osteoblasts in vitro.” Presentation offered at Penn State McNair Summer Research Conference, July 18, 2009. 5. “Bisphosphonate and taxane effects on osteoblast proliferation and differentiation.” Presentation offered at Penn State McNair Summer Research Conference, July 19, 2008. --------------------------------------------------------------------------------------------------------------------- 1American Cancer Society, American Cancer 3Steeg P; Theodorescu D, Nature Clinical Society, Inc. 2009, 1-72. Practice Oncology 2008, 5(4): 206-19. 2Coleman RE, Cancer 1997, 80(S8): 1588-94. 4Green J, The Oncologist 2004,9(suppl 4):3-13. 2 Engineered Bone Tissue for the Study of Mechanotransduction in Osteocytes Numerous studies have shown that micro-gravity conditions induce bone loss during long-term inhabitation of space; decreased mechanical stimulation of bone results in uncoupled bone remodeling favoring bone resorption.1 Efforts to prevent or treat micro-gravity induced bone loss typically involve resistive exercise to impart mechanical loads on the skeleton. While some studies indicate that exercise may reduce the uncoupling of bone remodeling, exercise has yet to effectively reduce bone loss.1 Thus, an understanding of the mechanisms by which external loads are sensed by bone cells and translated to cellular signals may elucidate targets for pharmaceutical interventions. Proposed Research. The objective this study is to investigate osteocyte mechanobiology using principles of tissue engineering and to ascertain mechanisms by which osteocytes respond to mechanical loading. I propose to study the effects of small-magnitude, high-frequency fluid shear stresses on the maturation of osteocytes cultured in a compartmentalized bioreactor. Background. Osteocytes are stellate cells abundant in cortical bone that develop from osteoblasts that become entrapped in secreted extracellular matrix (Figure 1). Osteocytes occupy lacunar spaces within bone matrix and are thought to influence their surroundings via cytoplasmic intercellular processes that extend through microscopic canals in the bone called canaliculi.2 It is widely accepted that osteocytes are responsible for sensing mechanical signals (mechanosensation), but the mechanisms by which those mechanical signals Figure 1. are translated to the cells (mechanotransduction) are still unclear.2 Osteocytes are difficult to study in situ because they are embedded in mineralized matrix and relatively inaccessible. Similarly, demineralization of bone matrix for in vitro studies fails to reproduce tissue with an appropriate three-dimensional architecture and a complex lacunocanalicular network.2 Principles of tissue engineering may lend improvements to developing biologically relevant bone models for mechanobiology studies.3 My undergraduate thesis research involved engineering osteoblastic tissue in a compartmentalized bioreactor (Figure 2) and challenging that tissue with breast cancer cells.4 Through this experience, I gained an appreciation for the ability of engineered tissues to serve as in vitro models of osteopathologies and developed an interest in applying Figure 2. these models for studying osteobiology. Krishnan et al. recently reported that pre-osteoblasts proliferate in the bioreactor to form a three-dimensional mineralizing osteoblastic tissue that progressively develops an osteocytic phenotype.5 Cobblestone-shaped osteoblasts mature into stellate cells embedded in a dense matrix with numerous intercellular processes.5 Therefore, the bioreactor provides access to the continuum of osteocyte development in vitro, including the aforementioned lacunocanalicular network. Previous studies have indicated that external forces are transduced to osteocytes by means of “fluid shifts” within the lacunocanalicular network.2 This theory (the poroelastic model) accounts for the effects of low-amplitude, high-frequency forces associated with the majority of daily human activities, such as sitting, standing and changing posture, as well as effects of high-strain activities such as exercise.2,6 While the effects of shear stresses have been evaluated in vivo and on osteocyte-like cell lines such as MLO-Y4, effects of fluid shear on the progressive maturation of osteocytes have yet to be investigated.2 Materials and Methods. Compartmentalized bioreactors based on the principle of simultaneous growth and dialysis7 will be assembled to develop engineered bone tissue that can 1 be mechanically manipulated. Murine calvarial osteoblasts (MC3T3-E1) can be cultured in the bioreactor for up to 10 months to progressively develop an osteocyte-like phenotype.5 Cells can be microscopically monitored throughout the culture interval using confocal microscopy. The bioreactor can be inserted into a mechanical device (Figure 3) that produces small- magnitude deformations in the membrane encasing the device. This external force will generate a pressure gradient within the cell-culture medium, and incorporation of a flow-loop for each compartment will allow the fluid to shift within the lacunocanalicular network. This system, once fully developed, can be used to investigate proposed mechanotransduction pathways and biochemical markers, such as ATP, prostaglandin E and nitric oxide.8 I 2 am particularly interested in quantifying the expression of the protein sclerostin, an osteocyte-specific product of the SOST gene that inhibits Figure 3. bone formation.8 Murine models have shown decreased sclerostin expression following loading, suggesting another potential mechanism for mechanotransduction in osteocytes.9 Agenda. In the first year of my fellowship tenure, I will design and construct the bioreactor and characterize the development of osteocytes in the culture system. Throughout the second year, I will determine a protocol for mechanically loading the bone tissue and investigate proposed mechanotransduction pathways. I will focus on developing quantitative methods of measuring signaling pathways in the bioreactor system. The final year will be devoted to analyzing effects of mechanical stimulation on osteocyte biomarkers and inducing mechanisms of osteocyte mechanotransduction in bone. Broader Impact. The aim of this project is to gain an understanding of the complex mechanisms underlying mechanotransduction in bone. Discovering these mechanisms could contribute to the development of therapies to treat micro-gravity induced bone loss experienced by astronauts during space travel. This research also has implications for the treatment of other bone diseases, such as osteoporosis and Paget’s disease. The collaborative nature of the Harvard- MIT Division of Health Sciences and Technology will allow for numerous perspectives to contribute to this work and my doctoral studies. Furthermore, participation in the Bioastronautics Training Program would guide my research and enhance my understanding of space medicine. I have actively participated in educational outreach activities during my undergraduate studies and will continue to do so throughout my graduate career by mentoring underrepresented students as a McNair alumna and encouraging high school students to pursue careers in science and engineering. As I continue to pursue my aspiration of becoming a professor, I look forward to probing new research questions and sharing my knowledge and experiences with students from a variety of disciplines and backgrounds. --------------------------------------------------------------------------------------------------------------------- 1LeBlanc A.D. et al., J Musculoskelet 6Hwang S.J. et al., Clin Orthop Relat Res Neuronal Interact 2007, 7(1): 33-37. 2009, 467: 1083-1091. 2Allori A.C. et al., Tissue Engineering: Part B 7Rose G.G., Int. Rev. Exp. Pathol. 1966, 5: 2008, 14(3): 285-293. 111-178. 3Freed L.E. et al., Tissue Engineering 2006, 8Riddle R.C.; Donahue H.J., Journal of 12(12): 3285-3305. Orthopaedic Research February 2009, 143- 4Dhurjati R. et al., Tissue Engineering 2006, 149. 12(11): 3045-3054. 9Robling A.G. et al., The Journal of 5Krishnan V. et al., In Vitro Cell.Dev.Biol – Biological Chemistry 2008, 283(9): 5866- Animal 2009, Accepted 3 Sept. 2009. 5875. 2 Rating Sheet 1: Overall Assessment of Intellectual Merit: Very Good Explanation to the applicant: Student's proposed plan of research was well organized and well presented. She documented several awards and scholarships as well as a few publications and presentations Noteworthy that she researched accomplishments of Geo. Washington carver and then developed lab curriculum for students in general chemistry. Also noted that she mentored students in bioeng. and became TA for physiology lab excellent letters of recommendation Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: active in Biomed Engin. Society as treasurer and helped coordinate volunteer outreach activities Sought to motivate more young women to go into engineering and other STEM fields by creating 20-minute presentation served as Engineering Ambassador and presented data from pilot program to administrators which brought about funding for the group. Applicant demonstrated leadership through this initiative and spearheaded visits to her former high school Rating Sheet 2: Overall Assessment of Intellectual Merit: Excellent Explanation to the applicant: excellent academic performance excellent leadership and outreach activities Honors college student Chemsitry intern-includes research, volunteer and teaching components a number of publications and presentations - both oral and poster excellent research experiences innovative course/curriculum slides, example approach basic physiology from engineering perspective and other similar ones good communication skills, very good write-up with suitable figures for previous and proposed research studies Excellent choice of institution excellent references Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: excellent prior accomplishments excellent community outreach-volunteer for Red cross blood drive, H1N1 vaccination clinic, engineering ambassador-to encourge high school students especially girls, to pursue carrers in science/engineering very good future plan excellent individual experience very good inegration research and education very good leadership experiences and skills and got the potential to be a future leader, considering first generation, low-income college student involved in Women In Eng society activiites Designed an innovative lab course which was used for honors degree Rating Sheet 3: Overall Assessment of Intellectual Merit: Excellent Explanation to the applicant: The student is strong academically, with an extremely strong supportive package. She has maintained an excellent standard of work while simultaneous working at many jobs. She does not merely fulfill the obligations of her jobs, but goes above all expectations to develop new processes, teaching materials, etc. The applicant has substantial research experience leading to a poster presentation. The work is with tissue engineering, but not in order to produce replacement tissues. Instead, the engineered tissue is used to study the role of osteoblasts, osteocytes, and pharmaceuticals in diminishing the potential of breast cancer cells to metastasize. Her previous experience will translate directly to her proposed research project. Again, she will use tissue culture as a means of studying bone formation and loss. In this case, she will look at the mechanisms of bone loss and the interaction of exercise with bone loss. The proposal mentions that the mechanisms of cell signaling will be investigated. I would have liked to see those outlined more clearly, and I highly recommend that the student have a clear protocol before beginning the task of building the bioreactor. Overall Assessment of Broader Impacts: Excellent Explanation to the applicant: As previously mentioned, the student has held several jobs where she has not only completed her duties, but has worked to improves processes, teaching materials, etc. I have the opinion that she has done this not to further her own ambitions, but out of extreme conscienciousness. In addition to many hours of work, a rigorous academic schedule, and participation in research, the student has also contributed as a leader in the BMES club and is helping to charter an AEMB Honor Society. She has shown her capability to disseminate scientific results through preparation of a manuscript, presentation of a poster, and being a ""poster child"" of oral presentation skills. I believe that she has the ability to make a meaningful impact in the field of biomedical engineering."	Winner!
220	Introduction: To better understand the impact of global climate change and the effects of increased biogenic and anthropogenic emissions, more research is needed into the unique photochemical processes that take place in the Arctic atmosphere. Although geographically remote, the Arctic has a significant impact on globally important feedbacks to climate change. Since ozone (O ) is the precursor for most of the oxidizing, or self-cleaning, capacity in the 3 troposphere, it generally controls the oxidation potential of the atmosphere. Similar to the discovery of stratospheric ozone depletion, observations of ozone depletion events (ODEs) in the polar boundary layer (BL) were surprising. Springtime episodic depletions of tropospheric O , and the characteristic photochemistry 3 involved, are current areas of considerable research. These sudden and recurrent ODEs during late winter and spring are associated with elevated concentrations of halogenated radicals. The conversion of inert halide salt ions into reactive halogenated species has been shown to deplete O following the onset of Polar Sunrise [e.g. 1,2]. While it is widely accepted that bromine is the 3 primary driver of ODEs via photochemical reactions, the role of iodine chemistry in ozone destruction and oxidizing strength of the atmosphere is not well understood [e.g. 3,4]. The dominance of bromine in Arctic ODEs appears to be a function of its abundance only, as even small perturbations in the iodine concentration significantly impact the rate of ozone depletion in recent models [5]. These models have also shown that interactions with iodine may double the efficiency of bromine in ozone depletion [6]. Problem Statement: To date, few successful in-situ measurements of iodine compounds have been achieved in the Arctic due to the lack of analytical methods. As a result, iodine chemistry is often omitted from current climate models that simulate Arctic ODEs, which may significantly underestimate the rate of ozone depletion. Research Objectives: This study proposes the exploration of new technologies and methods for the selective, quantitative chemical detection of iodine compounds that may play an important role in Arctic ODEs. In this study, I propose to investigate new analytical procedures and instrumentation to observe and quantify I and IO 2 during springtime Arctic ODEs. This work will then allow iodine chemistry to be incorporated with reduced uncertainty into kinetic analyses and computer model simulations. Method Development: To complete these objectives, I will work with Prof. Paul Shepson of Purdue University, a leading scientist in the field of atmospheric halogen chemistry in the Arctic. I will design new laboratory techniques to study the reaction mechanics of Arctic iodine species using chemical ionization mass spectrometry (CIMS), and inductively coupled Argon plasma mass spectrometry (ICP-MS). CIMS has not yet been used to detect I /IO in the High Arctic, 2 however, the Shepson lab has adapted a CIMS for the field and I plan to expand the capabilities of that instrument to detect I and IO in-situ, using SF gas as the ion source. 2 6 Study Site and Field Observations: Field observations of I and IO using CIMS and filter 2 sampling will take place in Barrow, Alaska where Beaufort meets the Chukchi Sea in the Arctic Ocean. This site is ideal for studying an Arctic maritime environment as it is coastal, surrounded by first-year sea ice, and the predominantly northeasterly winds come from clean, undisturbed snow over the sea ice. This ensures that our measurements include only halogenated compounds from the natural environment. Filtered samples from the field site will be transported and analyzed using ICP-MS at the Mass Spectrometry Center at Purdue University. Mallory Ladd Proposed Research 11/14/12 Modeling: Initial models established in the Shepson lab will be expanded to include observations of reactive iodine species (I, IO, HI, HOI, I O , INO ) significant to ODEs in the Arctic. 2 2 x Improvements will be made to a multiphase, zero-dimensional model used previously to study Br and Cl radicals [6], in order to predict the chemistry of iodine species and their involvement during ODEs. Developments to the model needed to expand on the heterogeneous iodine production mechanism and provide a better representation of snowpack and aerosol chemistry are as follows: nitrate and sulfate chemistry will be included; pH, temperature and the availability of oxidants will be varied; and finally, vertical mixing rates will be updated to include a better parameterization of the transfer of halogenated compounds between snow, interstitial air and the boundary layer. This model will aim to quantify the fraction of O depleted 3 by iodine during ODEs, the impact of iodine on the rate and timescale of O depletion, and the 3 effect of iodine on other important atmospheric oxidants such as HO and NO . x x Intellectual Merit: Significant opportunities exist in this proposal to discover fundamental knowledge related to the kinetics of Arctic iodine species and their relationship to ODEs. In addition to the development of a sensitive and selective method for quantifying reactive iodine species from the High Arctic during an ozone depletion event, this research will produce a model of Arctic ozone photochemistry that incorporates the effects of I /IO chemistry in addition to 2 those of Br and Cl. This will lead to a better understanding of the role of iodine chemistry in ODEs and other tropospheric chemical cycles important to the future of global climate change. My previous experience with analytical instrumentation has provided me with a strong foundation for success in developing this method. Undergraduate research and my current position as a lab and field technician have prepared me well for conducting research in an academic setting as well as in the field (see previous research essay). Working in the Shepson lab will be central to the growth of my knowledge about environmental modeling. Having presented multiple research projects, in addition to preparing a manuscript and following it through submission and revision, I am trained in effectively communicating the results of my research. Broader Impacts: I will share this research via publications in peer-reviewed journals for general and specialized audiences, and presentations at both national and international conferences. I will continue my involvement with the ACS and AXΣ (see personal statement) via the ACS Student Affiliates and the AXΣ-Beta Nu chapter at Purdue. In each phase of this project, I will actively recruit high school and undergraduate students from underrepresented groups to gain valuable research experience in our lab. I will encourage students from the ACS Project SEED and the NSF REU program to apply for these positions. The interdisciplinary nature of this research will significantly broaden their scientific experience and enhance their understanding of the importance of chemistry in the Arctic to global climate change. I also plan to be in close contact with the PolarTREC program so that I may host a high school teacher during my field campaign in the Arctic. I plan to partner with them to connect directly to their students back home via a cyber-based platform so that we may share our experiences in the field. Through digital storytelling, I will also be able to communicate the importance of this research to the broader public via a website that Dr. Shepson has previously developed (www.arcticstories.net). Literature Citations: [1] Bottenheim, J.W., et al. (2002) Atmos. Environ., 36, 2535-2544 [2] Helmig, D., et al. (2012) J. Geophys. Res., 117, D20303 [3] Barrie, L.A.; Platt, U. (1997) Tellus 49B,450-454 [4] Mahajan, A., et al. (2010) J. Geophys. Res., 115, D20303 [5] Calvert, J.G.; Lindberg, S.E. (2004) Atmos. Environ., 38, (30), 5087-5104 [6] Stephens, C.R. (2012) PhD Dissertation, Purdue University	HM
221	"Key Words: Proteins, Electrostatics, Folding, Stability, Interactions, 3D Modeling, pK , Small a Angle Neutron Scattering, NMR, ab-initio calculations, molecular dynamics Hypothesis: The combination of experimental methods and computational modeling of the properties of proteins will enable a more fundamental understanding of biophysical principles and allow for innovative and practical solutions to problems in medicine, computational modeling, and bio-engineering. Introduction: The role played by electrostatics on protein folding, stability, and interactions is a crucial element in our understanding of the cell and its functions. Particularly, charged amino acids influence the stability and interactions of proteins, but their charge state depends on their pK . a Therefore, accurate predictions of pK values are critically important for successfully modeling a the pH-dependence of protein folding, stability, and interactions. Accurately modeling these characteristics of proteins can improve the development of practical solutions to problems that arise such as when certain bacteria render an entire set of antibiotics useless (Norris 2007). However, calculating pK values of amino acids in proteins and modeling protein characteristics a is a difficult task and a constant challenge in biophysics. Research Plan: To calculate the pK s of proteins, one must calculate the energy difference between a protein ensembles with ionized and neutral (non-ionized) amino acids to determine the pK shift, a i.e. the change of pK induced by interactions within the protein. Common methods used in these a calculations include the finite difference Poisson-Boltzmann (FDPB) method, the Generalized Born (GB) method, molecular dynamics (MD), empirical methods, and combinations of these techniques. However, with each of these methods there are strengths and weaknesses in predicting experimental data. Problems arise because protein structures are given by X-ray crystallography, and there are various structural artifacts due to the crystallization process. The calculations mentioned above must average over an ensemble of the multiple protein conformations. Since proteins do not have a static structure, the flexibility of proteins introduces more complexities as the charges will fluctuate due to this flexibility. Therefore, successful modeling of energies and pK values requires either (a) explicit modeling of these a conformational ensembles or (b) the development of approaches that can mimic the effect of ensembles. My previous research focused on the second task, as described in my previous essay. My future research will focus on the explicit modeling of conformation ensembles. The problem of modeling these complicated systems can utilize experimental methods as well. By complementing the computational modeling with experimental methods such as NMR spectroscopy and small angle neutron scattering, a stronger understanding of the properties of folding and conformation changes in protein interactions will be possible. At the University of Tennessee, there is continuous collaboration with Oak Ridge National Laboratory, and Oak Ridge has the premier Spallation Neutron Source (SNS). Neutron scattering allows for better spatial resolution of protein structures since neutrons interact with nuclei and the scattering can distinguish hydrogen from deuterium. Also, my work on the Palmetto cluster at Clemson enables me to work with larger supercomputers such as Jaguar and Kraken which are hosted at Oak Ridge. With the resources of NMR spectroscopy at the University of Tennessee and the SNS at Oak Ridge, there is a great potential for more accurate models of proteins and a better understanding of protein interactions. Anticipated Results: The use of ab-initio principles and molecular dynamics will accurately model changes made to ionization sites within proteins as I have demonstrated through the pKa cooperative initiative. With small angle neutron scattering, I will be able to use protein crystals to create more accurate 3D images of the protein. I will use this data to measure the accuracy of the proposed theoretical models and improve these models. Because of prior success, I am hopeful that the availability of new information from the scattering will further improve the accuracy of the models and aid in our understanding of protein folding, stability, and interactions. During my research with Dr. Serpersu, I learned how important it is to utilize various experimental methods such as ITC and NMR to understand properties of proteins. These are not the only experimental tools available, and I will learn and use all of the necessary methods. The use of experimental methods will be used to further verify and understand protein characteristics. It is important to investigate whether or not the addition of ab-initio calculations correctly models pK s. If the initial ab-initio calculations incorrectly model the pK , I propose that ab- a a initio calculations are completed for all sites that are the proximal to the charged amino acid. This will allow for more flexibility in the MD calculations. Using the method proposed above, it is expected that the results obtained will align closely with the experimental results. I have successfully modeled charged amino acids within alpha helices of proteins. In this case, ab-initio calculations rearrange the structure so that the side chain of the titratable group is modeled as facing the surrounding water or other medium. If the modeling of proteins using the aforementioned methods is indeed accurate, then the results of this research will allow scientists to predict the pK of proteins and to model the 3D structure of proteins more accurately. a Broader Impacts: A more fundamental understanding of the properties of proteins will also allow us to take steps towards the development of methods to combat antibiotic resistant bacteria. The results of my research will be published in peer-reviewed scientific journals, and I will present my research to the public to cultivate an interest in science among the general public. My local presentations of research have struck many chords with young students. I mentor high school students throughout the year, and they consistently ask me how they can get involved with research. Many of these students have gone on to research in their undergraduate careers. Through encouraging and communicating with younger students, there is a great potential that they will also become interested in how they can help change the world through the progress of science. Works Cited: Norris, A. and Serpersu, E. “Interactions of coenzyme A with the aminoglycoside acetyltransferase (3)-IIIb and thermodynamics of a ternary system.” Biochemistry. (2010). 49(19): 4036-42 Talley, K. and Alexov, E. “On the pH-optimum of activity and stability of proteins” Proteins: Structure, Function, and Bioinformatics. (2010) Talley, K., Ng, C., Shoppell M., Kundrotas P.J. and Alexov, E.""On the Electrostatic Component of Protein-Protein Binding Free Energy"" PMC Biophys. (2008)"	Winner!
232	Improve Surface Age Determination Ali Bramson Keywords: primary and secondary craters, hierarchical clustering, networks, dendrogram Background: Crater statistics are a fundamental tool for learning about the geology and surfaces of planets, as well as the population of bodies in the solar system doing the impacting. Without physical samples, crater densities remain the only way we can estimate absolute ages of planetary surfaces [1]. Impacts generally are considered to happen randomly in time and space, so the more craters a surface has, the older it is. The size-frequency distribution (SFD) of craters follows an inverse power law [2], so the number of craters increases with decreasing crater diameter. This means smaller craters are more statistically significant when used for calculating surface unit ages [3]; however doubts have recently emerged about how these smaller craters should be interpreted. Secondary craters form when the ejecta from a large crater re-impacts the surface. These secondary craters lead to higher than expected crater counts within a geologic instant [2], which presents a problem for age determination because it makes a surface appear artificially older. Most of these secondaries appear close to their primary in clustered patterns such as rays and so can be excluded from crater counts. However, they often can be hard to distinguish from background small primary craters. This is especially true if they re-impact far from their source (at high velocities), in which case they match the circular morphology and depth to diameter ratios of primary craters. A detailed study of the Europan surface by Bierhaus et al. 2005 shows that secondary craters comprise up to 95% of craters with diameters < 1 km. The ability to extract these secondary craters from the background primary crater density is essential if we wish to accurately constrain the ages of surfaces. The amount and quality of imaging data from recent planetary missions (such as the Mars Reconnaissance Orbiter, Mercury Messenger, and Lunar Reconnaissance Orbiter) drive the need for a technique that can not only identify secondary craters, but also work on the very large data sets we are currently receiving. Fortunately, there is a growing body of work in computer science, devoted to understanding networks and clustering, which can assist in the analysis of these data. In particular, social networking algorithms have been developed to better understand how people in different groups are connected [4], and I have experience from my undergraduate senior thesis in applying such techniques to astronomy (galaxy groups). I propose to combine my interest in computer science and passion for planetary science by applying hierarchical clustering techniques common to network science to find patterns in spatial locations of craters. Applying these techniques to analyze the patterns of crater locations is a logical project that could prove to be very insightful, helping us to distinguish primary craters from secondary. The Algorithm: The hierarchical clustering technique [5] iteratively builds a hierarchy of clusters by starting with all nodes (the craters in question, in my case, or people in the social science setting) as separate groups and connecting them until they are all in the same group (agglomerative). To connect these features, it creates a distance matrix, a matrix of “similarity”, where each element will be the distance between two craters. In a social setting, the distance matrix could use number of friends in common as a measure of similarity. The agglomerative technique then merges the two nodes that are the closest into one cluster. These nodes are removed from the distance matrix while our new cluster is added in, and the process is iterated until all objects have been merged into one cluster. Bierhaus et al. 2005 utilized aspects of this algorithm, combining them with other clustering and statistical techniques in his study of Europa’s surface. However, Europa is a simpler case because it is sparsely cratered compared to other surfaces, making it easier to disentangle primary from secondary craters [3]. I propose to apply this algorithm to the more complicated Martian surface, using the latest database of impact craters on Mars from Robbins & Hynek 2012 [6,7]. Data: I will use a new global Martian (covering >99% of the surface area of the planet) crater database from which is freely available via the U.S. Geological Survey’s (USGS) Mars Crater Consortium, for my study [6,7]. This database contains 384,343 craters and is statistically complete down to diameters ≥ 1km. The database does have secondary crater classification for a select section of the database [1,8]. This will allow me to compare my technique to the manual classification of Robbins and Hynek. This comparison can be used to assess the accuracy of my automatic methods. Expected Results: I will use the output of the hierarchical clustering to yield a tree-like “dendrogram” to show the order in which the craters have been connected. Rosolowsky et al. (2008) have used dendrograms to determine the structure of molecular clouds. Where the branches of their dendrograms correspond to self-gravitating molecular cloud structures, the branches of my dendrograms would correspond to larger networks of craters. The shape of these branches can tell us about the spatial patterns of the craters; if the dendrograms are flat, with many connections at equal distances, the features are distributed homogenously. Long connections high in the dendrogram connecting groups of low-level links indicate the features spot the surface of the planet in a much less uniform manner (i.e. we can automatically identify clusters). I will also create dendrograms for a random distribution of objects of equal density as a comparison. I predict that the comparison to random will extract the primary craters that dot the surface randomly, leaving behind the probable secondaries that will appear in groups of low- level links that deviate from random. The advantage of this technique is that it could be used in any field that involves clustering and connections e.g. creating dendrograms of asteroid proper orbital elements could lead to new asteroid groupings. We might also expect the clustering of surface features on Io to be connected with volcanic events or other asymmetries between the leading and trailing hemispheres of tidally locked moons. The broad applicability across many disciplines is a key part of the broader impact and intellectual merit of this research. While I am passionate about planetary science and this research’s applicability within the field, the algorithms and techniques I use will be valuable to many fields. References: [1] Robbins, S.J. & Hynek, B.M. 2011, GRL, 38, L05201. [2] McEwen, A.S. & Bierhaus, E.B. 2006, Annu. Rev. Earth Planet. Sci., 34, 535. [3] Bierhaus, E.B., Chapman, C.R. & Merline, W.J. 2005, Nature, 437, 1125. [4] Mucha, P.J., et al. 2010, Science, 328, 876. [5] Newman, M.E.J. & Girvan, M. 2004, Phys. Rev. E, 69, 026113. [6] Robbins, S.J. & Hynek, B.M. 2012, JGR, 117, E05004. [7] Robbins, S.J. & Hynek, B.M. 2012, JGR, 117, E06001. [8] Robbins, S.J. & Hynek, B.M. 2011, JGR, 116, E10003. [9] Rosolowsky, E.W. et al. 2008, ApJ, 679, 1338.	Winner!
234	Stress KEYWORDS: personality; cognitive appraisal; emotion; stress reactivity; ecological momentary assessment INTRODUCTION: Everyone experiences stressful life events, but the magnitude of stress experienced in response to the same stressor can vary considerably between two people. For example, certain people tend to express more heightened negative emotion in response to life events and therefore perceive their lives as more stressful.1 Personality factors such as increased neuroticism, behavioral inhibition, and negative attributional style are also implicated in this greater stress reactivity1,2. I propose that these individual differences in response to stressors are strongly influenced by personality, and lead to variation in cognitive-emotional appraisal and processing of the stressful life STRESSOR event. In turn, this cognitive- emotional appraisal contributes to Cognitive Appraisal varying physiological reactivity, Personality -Worry/Rumination as measured by immune, -Neurotic/Anxious -Perceived Control Physiological cardiovascular, and endocrine -Behaviorally- -Perceived Severity Reactivity Inhibited -Immune response (See Figure 1). -Cardiovascular -Negative Cognitive appraisal involves -Endocrine Attribution Style Emotional the perceived severity and Reactivity perceived controllability of the -Negative Affect stressor, as well as the magnitude Figure 1. Proposed model of personality, cognitive appraisal, and frequency of persistent, emotional reactivity, and physiological reactivity in response to stress maladaptive, negative thoughts leading up to and following the stressor (worry/rumination). Emotional reactivity involves the presence of positive or negative affect in response to the stressor (See Figure 1). A better understanding of how subjective cognitive-emotional reactions to stress relate to individual variation in physiological reactivity is needed to better understand key individual differences between people. It is well established that stress relates to immunological dysregulation (e.g. slowed wound healing and increased viral susceptibility).3 Yet the interplay between individual differences in stress reactivity and immune system reactivity has seldom been investigated, particularly with regard to how such changes play out in real time, real-life human contexts.2 One reason for this lack of understanding is that studies of immune reactivity have largely been limited to short-term laboratory studies that use blood draws for serum/plasma-based biomarkers. With less invasive salivary biomarker techniques that are currently in development, more ecologically-valid studies of individual differences in stress reactivity could be conducted. RESEACH AIM 1: To investigate how individual differences in personality relate to differences in physiological reactivity, specifically markers of immune system function. RESEARCH AIM 2: To examine how cognitive appraisal of a stressor and emotional reactivity to a stressor mediate the relationship between personality and physiological reactivity. RESEARCH AIM 3: To determine how relationships between personality, cognitive appraisal, emotional reactivity, and physiological reactivity play out over time in an ecological context. METHODS: I have already independently conducted an extensive literature review to elucidate the connection between salivary and blood measures of inflammation and plan to publish a review article on this work. Along with Penn State investigators who are spear-heading further investigation of salivary immune diagnostics, I hope to incorporate salivary measures of immune reactivity into novel methodological approaches to studying individual variation in the stress response. My research program during my graduate training will culminate in my implementation of three original studies described below. STUDY 1 (S1): I will begin by using an existing data set to examine connections between individual differences and immune responses to stress. My advisor is PI on a longitudinal investigation of the degree to which inflammation mediates connections between stress and cognitive aging among diverse adults. I will be able to use those data to explore new dimensions of how personality (e.g. behavioral inhibition, negative attribution style) is related to cognitive- emotional responses to stress and, consequently, to physiological reactivity over a 4-year period. STUDY 2 (S2): To experimentally model the relationship between personality and immune function, I will expose participants to an acute social lab stressor in order to measure individual differences in stress reactivity through endocrine, immune, and cardiovascular measures and through self-reported assessments of cognitive-emotional state. Specifically, immune function will be measured through circulating inflammatory markers obtained via saliva and blood. STUDY 3 (S3): The data from S1 and S2 will be used to inform a larger naturalistic study which will utilize ecological momentary assessment (EMA), a method whereby participants report what they are feeling and/or how they are behaving in real-time in natural settings. In collaboration with Penn State’s Dynamic Real-Time Ecological Ambulatory Methodologies (DREAM) initiative – a unique program designed to popularize and educate researchers on EMA methods – participants will be given smart phones which will prompt them to fill out assessments of cognitive-emotional states and to give saliva samples at specific time points. State-of-the-art hierarchical linear modeling and structural equation modeling will be utilized for mediation analyses in S1 and S2, as well as to examine between-subject and within-subject variation, including how multiple daily assessments change over time in S3. INTELLECTUAL MERIT and BROADER IMPACTS: With the levels of stress that many in our society are facing, it is imperative to better understand the mechanisms by which some people become more susceptible to the physiological consequences of stress. My program of research has potential implications for improving quality of life, coping strategies, interpersonal relationships, and productivity in the workforce, as well as fostering self- actualization through stress reduction. These three studies will advance the field of psychoneuroimmunology by examining the underutilized combination of less-invasive salivary inflammatory biomarkers with respect to individual differences. The additional assessment of this concept through EMA will allow for greater external validity of results and help popularize more ecologically-valid studies. Pre-existing partnerships with a number faculty who already investigate individual differences in the stress response makes me well-poised to implement the tri-part research initiative I am proposing. In conjunction with the DREAM initiative and Penn State’s Centers for Healthy Aging and “De-Stress Zone” (a biofeedback facility), I also plan to design and hold workshops for education on ecological measurement of stress and stress self-management to implement with diverse populations. With all of the opportunities for research and outreach available at Penn State, my current position places me in an ideal situation to begin explaining the individual variation that is often overlooked in physiological psychology studies. REFERENCES: [1] Suls, J., Green, P., & Hillis, S. (1998). Emotional reactivity to everyday problems, affective inertia, and neuroticism. Pers & Soc Psych Bltn. 24: 127-136. [2] Segerstrom, S.C. (2000). Personality and the immune system: Models, methods, and mechanisms. Annals of Behav Med. 3:180-190. [3] Contrada, R., & Baum, A. (Eds.). (2010). The handbook of stress science: Bio., psych., & health. New York, NY: Springer Publishing Co.	Winner!
242	"Davidow, Juliet Y. I am fond of the saying that research is “me-search”, meaning that some researchers tend to focus their work on issues that are personal or that they are curious about understanding in some way about themselves. I am one of these researchers. My broad interest in how adolescents learn and apply learned information stems from my own learning experiences as well as my professional and research experiences working with different developmental populations. Here I will highlight aspects of my educational experience that have contributed to my interest specifically in learning mechanisms, and in the period of adolescence. Interest in Adolescence: As an undergraduate student, I wanted to gain direct experience to explore my interest in clinical psychology. To this end, I volunteered at St. Luke’s-Roosevelt Hospital in New York City, working with an ethnically and socioeconomically diverse group of patients in the Alternative Adolescent Day Program (AADP) and the Comprehensive Addictions Program for Adolescents (CAPA). This particular set of outpatient programs appealed to me because in addition to therapy and treatment, the patients took classes towards their General Equivalency Diploma or High School Diploma in a curriculum taught by New York Board of Education teachers. I tutored the patients in their class work, helped them complete their homework, as well as facilitated social milieu therapy projects, such as a school newspaper. The patients reminded me of the Montessori philosophy; Adolescence is a period characterized by changes and challenges, and these patients additionally struggled with psychopathology and addiction, and despite all these obstacles were committed to their schooling. Interest in Learning: My own passion for learning developed in the Montessori school I attended from 3 to 13 years of age. Two major principles of the Montessori philosophy are that when children are motivated to learn they will engage in self-directed study and that every child has different optimal strategies for learning. Peer-to-peer-learning is also pivotal, and the Montessori classroom is built around encouraging it, with 3 grade-level groups in a single classroom (e.g. 4th, 5th and 6th graders). This structure allows students to tap multiple resources, from books and other materials to more senior students in the classroom, before turning to the teachers (of which there are usually 2-3 per classroom). This unique early experience of seeking academic help from my classmates, and in turn sharing my knowledge with my peers, instilled in me the desire to learn so that I could in turn teach another. Because of my strong research skills and academic record, I have had the rare opportunity to work under an extraordinary group of mentors. Specifically, Drs. Dima Amso, BJ Casey, and Daphna Shohamy have deeply impressed me as rigorous, female scientists, whose research directly impacts the lives of children and adults, in healthy and other populations. I aspire to be a strong female role model in science, a researcher whose work can improve educational practices across the country, and an educator directly bringing my love of learning to students, and towards these ends I have begun my PhD work at Columbia University. I believe my proposed research holds the potential to expand into a career dedicated to understanding the brain-and-behavior interactions necessary for successful navigation through adolescence. Broader impact in the community: I am beginning to share the knowledge I have gained from my mentors with a rising generation of hopeful scientists. In early December, I will be presenting and discussing my research at a ‘Careers in Science’ fair for middle and high school students, organized by the Women in Science at Columbia (WISC) and the Center for Environmental Research and Conservation (CERC). This event will be attended by a socioeconomically diverse group of students from all over New York City, with a focus on encouraging young women in particular to consider careers in the sciences. This emphasis is of particular importance to me, because I am woman working in the sciences, and my greatest role models and influences to follow this course have been women. Page 1 of 2(cid:1) (cid:1) 2010 NSF Graduate Research Fellowship: Personal Statement Davidow, Juliet Y. A key goal of mine, expressed in my personal statement from my previous submission, was to begin sharing my work directly with adolescents in the community. To this end, I will be making presentations at local high schools, starting with The Calhoun School in Manhattan, speaking in classrooms about what cognitive neuroscience has taught us about changes in behavior and the developing brain during adolescence. I have been mentoring a high school student, Michael, and supervising him through a research project in the lab that will culminate in his applications to the Intel National Science Talent Search, the New York Science and Engineering Fair, and other award programs. To this end I have been teaching Michael the foundations of research methods, including how to survey the existing literature, statistical tools, and the ethical principles researchers must abide by. Michael is interested in how socioeconomic differences in adolescent populations might relate to the learning and generalization mechanisms outlined in my research proposal and the possible role generalization plays in racial stereotyping. Mentoring Michael has been rewarding, and I was excited at the opportunity to interact one-on-one with a student at his level on such a long-term project, given how uncommon it is to have students before the undergraduate level work hands-on on research projects. Graduate Training: My first year of graduate study has been punctuated with acknowledgements of my accomplishments. I was the recipient of the Leo Rubinstein Endowed Fellowship. I applied and was awarded a travel grant from the Kavli Institute for Brain Sciences to present results at the recently past Annual Meeting of the Society for Neuroscience [1]. This study, conducted in my first year of graduate work, tested 74 healthy adults behaviorally, and tested a subset of 53 of these adults in a functional Magnetic Resonance Imaging scan. This sample of adults will be used as a comparison group in the cross-sectional, developmental study that I propose in this application. And perhaps most notably, I was recognized with an Honorable Mention on my first submission of my NSF GRFP application. I was pleased and excited to see the enthusiastic responses from the raters of my essays, and found their feedback to be extremely helpful. The points raised by my raters indentified clear areas where my proposal could be improved, and by directly addressing these issues, I believe my current, revised proposal is stronger. This past semester, I have been taking a course in Methods of Teaching towards improving my skills as a Teaching Assistant (TA) for Undergraduate courses at Columbia University. I will have the opportunity to directly apply my new skills this spring, when I TA for the introductory Developmental Psychology course, a class I am eager to work on, given my strong background in developmental research and methods. With the support of Dr. Shohamy and her expertise in neural networks for learning, I have an ideal mentor for exploring the development of learning processes over the course of adolescence. With cutting edge behavioral and brain imaging facilities at Columbia, I have access to the testing resources my research requires. Situated in Upper Manhattan, I am surrounded by people with diverse ethnic and socioeconomic backgrounds, providing me access to a representative population of potential research participants that will allow me to generalize my results for the benefit of many children. These elements combined with my research background give me confidence that I will be an outstanding student and researcher whose contributions will impact a number of fields within psychology and beyond. Reference: 1. Davidow, J., Kahn, I., & Shohamy, D. (November, 2010). The ability to learn and generalize knowledge is related to intrinsic interactions between multiple memory systems during rest. SfN Annual Meeting, California, USA. Page 2 of 2(cid:1) (cid:1) 2010 NSF Graduate Research Fellowship: Research Proposal Juliet Y. Davidow Learning in adolescence: Neural mechanisms and implications for education. Keywords: Learning, education, development, adolescence, striatum, hippocampus, fMRI. Introduction. There have been significant advances in understanding cognitive and neural mechanisms for learning in adults, with important implications for everyday learning situations [e.g. 1, 2, 3]. However, far less is known about the cognitive and brain mechanisms underlying learning during the critical developmental stage of adolescence. This research program aims to bridge this gap. The proposed research will use behavioral studies combined with functional Magnetic Resonance Imaging (fMRI) in healthy adolescents to delineate the cognitive and neural development of specialized learning systems over the course of adolescence (10-18 years of age). The results will provide a novel understanding of learning mechanisms during adolescence and will inform educational approaches towards learning in the classroom. Background & Rationale. Converging evidence from research in animals, patients and healthy adults demonstrates that there are different kinds of learning that depend on distinct neural systems [2, 3]. Gradual, feedback-based learning of stimulus-response associations – often referred to as “habit” or “incremental” learning - depends on the striatum [4, 5]. This system is sensitive to feedback and results in knowledge that is relatively inflexible and specific to the context in which the learning took place [6]. A distinct and independent “declarative” or “episodic” system supports rapid learning of events and depends on the hippocampus [1, 6]. In contrast to the striatum, the hippocampus is thought to form knowledge that is flexible and easily generalized to novel situations and contexts [1, 6]. It has been shown that individuals who show more hippocampal activation during learning are more likely to integrate and generalize what they learned [5]. Together, these findings indicate that in the healthy adult brain, there is a balance between activity in the striatum and the hippocampus during learning, with consequences for how learned knowledge is used. A key open question is how developmental changes in learning mechanisms relate to changes in the striatum and the hippocampus during adolescence. Some insight into this question comes from longitudinal research of structural brain changes, which revealed differential developmental trajectories in the striatum and the hippocampus [7]. The volume of the striatum peaks around pubertal onset and then diminishes, with reductions continuing into early adulthood [7, 8]. Far less is known about the developmental trajectory of the hippocampus [7, 9]. However, existing results support relatively early maturation of the hippocampus, consistent with adult-level episodic learning performance in children [10]. Together, these findings suggest that the striatum and the hippocampus may develop at different rates during adolescence, with important implications for learning. Given the structural differences in the brain’s learning systems over the course of adolescence, what is the trajectory of different forms of learning (‘incremental’ vs. ‘episodic’)? To address questions about interactions between behavior and brain mechanisms, I will characterize the relationship between learning processes and brain development in human adolescents using a paradigm demonstrated in adults to be a sensitive index of both striatal-dependent incremental learning and hippocampal- dependent episodic learning. These studies will investigate the neural mechanisms in adolescent learning so that educational programs can be informed directly by cognitive neuroscience data from adolescents, instead of making inferences from studies of adults. Experimental methods. I propose three studies that make use of an incremental learning and generalization paradigm (‘acquired equivalence’)[e.g. 5], shown to selectively probe different forms of learning and their neural substrates in adults. Although ideally these studies would be conducted longitudinally, the constraints of my time as a graduate student make cross-sectional and between-subject studies most feasible. The acquired equivalence (AE) task consists of two phases. First, subjects engage in feedback-based learning where they learn to associate a series of (cid:1) Page 1 of 2 2010 NSF Graduate Research Fellowship: Research Proposal Juliet Y. Davidow faces with different objects. This feedback-based learning phase has been shown to depend on the striatum. In the second phase, subjects are tested on their memory for the previously learned associations. Critically, they are also asked to generalize what they learned to novel stimulus combinations. This ability to generalize depends on the hippocampus [e.g. 5]. Figure. ‘Acquired Equivalence’ task structure. a. In Phase 1, subjects learn a set of independent but overlapping associations. Learning is incremental and based on feedback. b. In Phase 2, subjects are tested for their (1) memory on learned pairings (Learned) and for (2) inferential transfer to novel combinations they have not previously seen (Generalized). This paradigm is particularly well suited for studying special populations, including younger subjects. Specifically, it allows as many learning trials as is necessary before the second test phase. This unique feature will permit us to independently explore adolescent learning trajectories for both incremental learning and for episodic generalization.(cid:1)Using fMRI, I will record brain activity at learning and test phases of the task while measuring subjects’ behavioral responses. By using converging methods, I can simultaneously characterize incremental learning from feedback and flexible generalization of learned information in adolescents while exploring individual differences in reliance on neural networks implicated in adult performance. Study 1 will use the AE task with feedback during the learning phase to ask how adolescents learn to associate items and generalize to novel pairings, and delineate the underlying neural networks. Study 2 will use an AE task that compares active learning-with-feedback to passive learning-by-observation, with no feedback. This manipulation has been shown to modulate both hippocampal and striatal contributions [4] and will allow me to compare memory and generalization from active versus passive learning, qualify what kinds of learning are most effective for different kinds of outcomes, and probe the role of active engagement for successful learning. Study 3 will use the AE task and will manipulate whether the feedback involves monetary rewards or not, to explore the impact of heightened reward sensitivity during adolescence [8] on learning behaviors. Together, these studies will build a foundation for understanding (I) learning and generalization behaviors, (II) differential maturation of brain function, and (III) behavior-and-brain interactions over the course of adolescent development. This foundation will be critical in bridging the research fields of neuroscience, development, and education, and also holds potential to impact society with implications for special education, early learning intervention, and psychopathology. Future studies will probe the role of individual differences on learning behaviors, including influences from social environment and genetic phenotypes. Due to the dearth of learning research in healthy adolescents, my proposal promises to put forward novel information about behavior and behavior-and-brain interactions to bear on several fields. Understanding typical adolescent development of learning mechanisms and how knowledge is applied in novel circumstances is imperative to adolescents’ success in the classroom and when faced with challenging decisions. References 1. Eichenbaum, H.E. & Cohen, N.J. From “Conditioning to Conscious Recollection”, 2001. 2. Gabrieli, J.D. Annual Review of Psychology, 1998. 49: 87-115. 3. Knowlton, B.J., Mangels, J.A., & Squire, L.R. Science,1996. 273: p. 1399-1402. 4. Shohamy, D., et al. Brain, 2004. 127: p. 851-859. 5. Shohamy, D., & Wagner, A.D. Neuron, 2008. 60: p. 378-389. 6. Shohamy, D., et al. Journal of Cognitive Neuroscience, 2008. 21(9): p. 1820-1832. 7. Giedd, J.N. Journal of Adolescent Health, 2008. 42: p. 335-343. 8. Somerville, L.H., Jones, R.M., & Casey, B.J. Brain and Cognition, 2010. 72(1): p. 124-133. 9. Gogtay, N., et al. Hippocampus, 2006. 16: p. 664-672. 10. Ofen, N., et al. Nature Neuroscience, 2007. 10(9): p. 1198-1205. (cid:1) Page 2 of 2 Ratings Sheet 1 of 3 Score for Davidow, Juliet Intellectual Merit Criterion Overall Assessment of Intellectual Merit Excellent Explanation to Applicant Thecandidatehasanexcellentrecordofresearchexperience. Thisisevidencedbyhertwoco-authoredpapers and multiple presentations. Also, her references speak to her independent contributions to the research programs. She has presented a very well crafted research proposal. The justification for her hypotheses is compelling and the methods are sound. It will be an interesting and potentially important study. Broader Impacts Criterion Overall Assessment of Broader Impacts Excellent Explanation to Applicant I appreciate the candidate’s recognition of the important role that scientists have in improving science education in their communities. She shows ample evidence of giving back to her community through her mentorship of young people and participation in community projects. She will be an engaged scientist who will continue to help others in her broad community. 2011 NSF GRFP Applicant: Juliet Davidow Applicant ID: 1000100934 Ratings Sheet 2 of 3 Score for Davidow, Juliet Intellectual Merit Criterion Overall Assessment of Intellectual Merit Excellent Explanation to Applicant The applicant demonstrates are strong record of research training through their undergraduate and post- baccalaureate employment including an excellent record of publication/presentation. She has demonstrated both independence and insight. Her broad training is noted. Broader Impacts Criterion Overall Assessment of Broader Impacts Excellent Explanation to Applicant The applicant demonstrates a sustained commitment to the ideals of this criterion through her volunteer, mentoring,andserviceactivities. TheseincludeparticipationinWISCevents,speakingatlocalhighschools about cognitive neuroscience, and mentoring high school students. Her application would be even stronger with concrete plans that address this criterion in the near and far terms. 2011 NSF GRFP Applicant: Juliet Davidow Applicant ID: 1000100934 Ratings Sheet 3 of 3 Score for Davidow, Juliet Intellectual Merit Criterion Overall Assessment of Intellectual Merit Very Good Explanation to Applicant The applicant has considerable experience working in various laboratories and has mastered a large set of research skills that have prepared her to function as a research scientist. She has two journal articles, one article submitted for publication, and several national conference presentations. Thus she understands the process of scientific research from beginning to end. Her proposed line of research is based on a broad appreciation of the literature and is thoughtfully and carefullyplanned. Therewereafewdetailslackingintheresearchplanthatwereimportantinunderstanding the practicality of the proposed effort. Broader Impacts Criterion Overall Assessment of Broader Impacts Excellent Explanation to Applicant Evidencefortheapplicant’senthusiasmforeducationanddiversityissuescanbefoundinherpastvolunteer work, her efforts to mentor students in research, and in her choice of research topics. Theproposedresearch,ifsuccessful,hassomebroadapplicationsthatwereclearlyindicatedbytheapplicant. 2011 NSF GRFP Applicant: Juliet Davidow Applicant ID: 1000100934 file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt Logo-asee-print NSF GRFP RESULTS *Juliet Davidow* | logout </ratingsheets/logout> Rating Sheets * *2010 Rating Sheet 1* * 2010 Rating Sheet 2 </ratingsheets/ratingsheets/84303> * 2010 Rating Sheet 3 </ratingsheets/ratingsheets/84304> Overall Assessment of Intellectual Merit: Very Good Explanation to the applicant: Excellent academic preparation with impressive GRE scores. Sought out undergraduate and postgraduate research experiences and has continued to produce at the graduate level, with one published paper and nine conference papers (one first-authored). her letters of recommendation were very strong and attested to her fine research ability. Her proposed research plan was well-articulated and based on past published research. Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: The broader impacts of her research and her role as a scientist were not articulated directly. She has an excellent record of community service and has demonstrated an understanding of the scientist's roe in communicating findings to the larger community. Articulated an interest in montoring and teaching, OTHER YEARS * *2010* OUTSIDE LINKS * Fastlane <https://www.fastlane.nsf.gov/grfp/> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt (1 of 2) [9/21/11 12:19:11 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt * NSF GRFP Program Announcement <http://www.nsf.gov/grfp> National Science Foundation Graduate Research Fellowship Program Operations Center Administered by: American Society for Engineering Education (ASEE) 1818 N Street NW, Suite 600 Washington, DC 20036 | 866-NSF-GRFP, 866-673-4737 (toll-free from the US and Canada) or 202-331-3542 (international) | info@nsfgrfp.org <mailto:info@nsfgrfp.org> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating1.txt (2 of 2) [9/21/11 12:19:11 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt Logo-asee-print NSF GRFP RESULTS *Juliet Davidow* | logout </ratingsheets/logout> Rating Sheets * 2010 Rating Sheet 1 </ratingsheets/ratingsheets/84302> * *2010 Rating Sheet 2* * 2010 Rating Sheet 3 </ratingsheets/ratingsheets/84304> Overall Assessment of Intellectual Merit: Excellent Explanation to the applicant: The applicant has extensive research experience and a strong academic background. Her proposed research plan is well laid out. She has good communication skills, works well with others and independently. Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: The applicant expresses a desire to be a role model and participate in outreach programs to deliver science to those who might not otherwise be exposed to it. Her research will enhance scientific and technical understanding. Improved understanding of how adolescents learn will do doubt benefit society. The applicant could more explicitly address the broader impact criteria, especially regarding encouraging diversity. OTHER YEARS * *2010* OUTSIDE LINKS * Fastlane <https://www.fastlane.nsf.gov/grfp/> * NSF GRFP Program Announcement <http://www.nsf.gov/grfp> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt (1 of 2) [9/21/11 12:19:55 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt National Science Foundation Graduate Research Fellowship Program Operations Center Administered by: American Society for Engineering Education (ASEE) 1818 N Street NW, Suite 600 Washington, DC 20036 | 866-NSF-GRFP, 866-673-4737 (toll-free from the US and Canada) or 202-331-3542 (international) | info@nsfgrfp.org <mailto:info@nsfgrfp.org> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating2.txt (2 of 2) [9/21/11 12:19:55 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt Logo-asee-print NSF GRFP RESULTS *Juliet Davidow* | logout </ratingsheets/logout> Rating Sheets * 2010 Rating Sheet 1 </ratingsheets/ratingsheets/84302> * 2010 Rating Sheet 2 </ratingsheets/ratingsheets/84303> * *2010 Rating Sheet 3* Overall Assessment of Intellectual Merit: Very Good Explanation to the applicant: Strengths: The applicant has participated in undergraduate research and as a lab manager, followed by work as an RA, and is now enrolled in graduate study. From these experiences, the applicant learned basic psychological research tools, including behavioral study design and analysis, measuring eye movements, ERPs and fMRI. All of these skills will be highly valued in the applicant's ongoing training. The applicant's research efforts have resulted in an honors thesis, one local invited talk, one first authored international conference presentation, authorship on numerous additional conference abstracts, and authorship on two manuscripts in various stages of publication. Weaknesses: The research plan lacks some important information. For example, is there any behavioral evidence that the incremental and episodic learning systems have different developmental trajectories? This is important to establish prior to moving the experiment into the scanner, which isn't going to tell us much about mechanisms, just more about ""where"". I'm assuming that this is a between-subjects design, and not a longitudinal study, because a graduate student career is likely too short to measure longitudinally. However, this is not specified. The applicant does not have any first-authored peer-reviewed manuscipts at this time. Overall Assessment of Broader Impacts: Very Good Explanation to the applicant: The applicant has served both as a peer counselor and as a volunteer file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt (1 of 2) [9/21/11 12:20:15 AM] file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt tutor working with teenagers with drug addiction difficulties. These experiences motivated the applicant to pursue studies on the typical adolescent development. The applicant intends to promote neuroscience in K-12 classrooms with the help of SFN's online materials. OTHER YEARS * *2010* OUTSIDE LINKS * Fastlane <https://www.fastlane.nsf.gov/grfp/> * NSF GRFP Program Announcement <http://www.nsf.gov/grfp> National Science Foundation Graduate Research Fellowship Program Operations Center Administered by: American Society for Engineering Education (ASEE) 1818 N Street NW, Suite 600 Washington, DC 20036 | 866-NSF-GRFP, 866-673-4737 (toll-free from the US and Canada) or 202-331-3542 (international) | info@nsfgrfp.org <mailto:info@nsfgrfp.org> file:///Users/ironoap/Desktop/Davidow_NSF/Feedback_2010/rating3.txt (2 of 2) [9/21/11 12:20:15 AM]"	Winner!
248	KEYWORDS: digitallogic,evolutionarycircuitdesign,geneticmodels,inversion,recombination, systemsbiology,transcriptionalnetworks. Thisprojectisoriginalandofmyowndesign. BACKGROUND: Biologicalsystemsself-regulatevianetworksofinteractingtranscriptionfactors. Such networks produce complex behavior; some theorists have argued that they can produce any behaviordesired.1 However,existenceproofsdonotguaranteethatpractical solutionsexist. One method to test if a desired behavior can be achieved is to run a computer simulation. The computer can test thousands of designs quickly, converging on the best matches. This process has beenusedtoevolvedesignsforoscillators,latches,andotherinterestingbehaviors.2,3 To demonstrate and extend the power of simulated evolution for biological network design, I willevolveageneticallyencodedbinarycounter. Syntheticbiologyhaslongusedbinarycounting as a model system. It is perhaps the simplest behavior which requires a full implementation of digitallogic;thisissignificantbecauseoftherobustnessandsimplicityofdigitalcircuits. Counters have inspired many entries in the iGEM design contests4 as well as high profile experimental attempts,suchastheunary(linear)counterinlastyear’sScience.5 However,tomyknowledge,all thesedesignshaveusednonstandardelementssuchasrecombination-basedgeneticswitches. RESEARCH PLAN: I will perform simulated evolution and compare counter designs with and without switches. Viable designs will be refined by hand and ultimately become physical DNA constructs for testing in vivo. I predict that only designs evolved with switches will be viable. These results will be significant in themselves; they will also inform the synthetic biology design processandimprovethemodelinginfrastructureforfutureresearch. ON COUNTERS: Briefly, a counter element changes state every nth time it is triggered. For a binary counter, n = 2. One could imagine using a binary counter to produce yeast that turn green for every 2nd cell division. More seriously, one could imagine a researcher in aging or cancer connectingfivesuchcounterstomakecellsturngreenwhentheyhavedivided25 =32times. AIM 1: Performselectionforbinarycountinginsimulatedgeneticnetworks. Existing software can simulate evolution in genetic networks. This work will use Genetdes,6 freeopen-sourcesoftwarewhichnativelyusesSBMLformat7todescribethenetworksitactsupon. Asaformermetabolicengineer,IamproficientinSBMLandwithnetworkmodeling. HYPOTHESIS 1: Electronic circuits will evolve into binary counters. The field of evolutionary circuit design began inelectronics, and a mature literature exists on the topic.8,9 Counter design is a common test case, and has been demonstrated a number of times. This will serve as a positive control;Iwilltestandrefinetheselectionmethodsonatargetknowntoexist. HYPOTHESIS 2: Under validated selection conditions, networks of interacting transcription factors will NOT evolve into binary counters. Selection will be performed with Genetdes using a proven fitness function. However, success appears unlikely. No human designer has produced a counternetworkfrompurelytranscriptionallogic,nordoknownexamplesexistinnature. AIM 2: Improvesimulatednetworksbyincludingrecombination-basedgeneticswitches. To test these switches in the context of a modeled network, I will make several changes to the Genetdes simulator and its underlying SBML representations. Current SBML standards permit embeddedtriggersforfunctionsanddiscontinuousevents;thesearekeyfeaturesforimplementing recombination. InthisAim,IwillthereforeextendGenetdestofullSBML-2compliance. Toupdatethemodel,Iwillfirstcreatenewparts: matchedpairsofrecombinaseandtargetsite (Rec and INT), using available kinetic data for the fim system.10 Rec binds INT, then complexes with another bound Rec. This complex triggers a discontinuous event, in which the paired INTs exchange locations. INTs will be context aware, storing the connections made at their genetic location (cis interactions). I will also create a new cell compartment where active INTs and their neighborswillbehiddenduringthisexchange,mimickingDNAblockedbyReccomplex. HYPOTHESIS 3: Networks containing switches will outperform transcription factors alone. Recombination is the favored mechanism for natural behaviors with periodic state changes, such as E. coli virulence10 and S. cerevisiae mating.11 The switches are discrete, leak-free, and fairly efficient-goodtraitsforacounter.5 Iexpectthatswitchingwillimprovethecounterdesigns. AIM 3: Convertevolvedcountermodelstophysicalformandconfirmactivityinvivo. GOAL: Producefunctionalnetworksinalivingcellwithsimulatedevolution. Forhigh-scoring designs, I will perform stochastic simulations on their network to assess noise tolerance. Designs whichsurvivethistestwillbesubjecttosensitivityanalyses,determiningtheirrobustnesstokinetic parameters. TheseanalyseswillbeperformedwiththeSimBiologytoolkitinMATLAB. For the most promising designs, I will manually ensure that each model element has a corre- sponding physical part with the right kinetics. I will also choose the most informative elements to tagwithfluorescentreporterproteins. Designswillbebuiltwithcharacterizedpartslibraries.12,13 Testing and debugging the designs will require dynamic analysis of multiple reporter proteins, ideally in single cells. I plan to work with Prof. Hasty of UCSD, a pioneer of this technique. His grouphasperformedsimilarworkforoscillatorsandcircadianclocks,showingfeasibility.14 AIM 4: Produceeducationalsoftwareusingideasfromnetworkevolutionmodels. ThealgorithmsthatevolveSBMLmodelsinGenetdescanbegeneralizedtouseanystructured input.6 I will refactor existing software to develop a general-purpose evolution simulator; this simulatorwillbeusedtocreateprogramsinTurtle,asimplegraphicslanguageforchildren. ThefitnessofaTurtleprogramwillberatedbyhumanscomparingitsoutputtoatargetimage. Thiswillbeawebgame,whichwillincorporategamedynamicssuchasallowinguserstocompete on how well their scores match the consensus. I will create time-lapse videos of the evolutionary process;thesewillbepostedonYouTubetoprovidevisuallyappealingtoolsforeducation. RESOURCES: Pilot studies for Aims 1 and 4 will be conducted on a small Beowulf cluster15 planned for DIYbio-Boston. Next fall, I plan to enroll in graduate school; I will use university resources(orideally,TeraGrid)forAim2. LabsforAim3canbefoundatUCSDandelsewhere. BROADER IMPACTS: I have dedicated a full Aim to public outreach. For the Turtle project, the tools will be developed in partnership with amateur scientists in DIYbio. Data will be collected throughpublicparticipation,andresultswillbepackagedtoreachthewidestpossibleaudience. I take scientific impacts just as seriously. I will continue to publish and give talks, making suretoreachthebroadergroupwhichcouldexpandonmywork. Forinstance,thepartsdesigners wouldwanttoknowifIwaslimitedbyaspecificgapinthepartslibraries,andthemodelerswanta formalismforrecombination. Withinmysubfield,Ialsohopetouseinterestingresultstomakethe casefordesignpracticeslikesimulatedevolution. Syntheticbiologywantstobetransformative;it couldbe,ifwewerebetteratit. Bylearningtodesignbiologicalsystemsbetter,smarter,andfaster, andbysharingthatknowledge,webuildtheinfrastructurethatwillallowittoreachitspotential. REFERENCES: (1)Buchleretal.2003.ProcNatlAcadSciUSA.(2)RodrigoG&JaramilloA.2007.SystSynth Biol.(3)CaoHetal.2010.SystSynthBiol.(4)igem.org(5)FriedlandAEetal.2009.Science.(6)RodrigoGetal. 2007.Bioinformatics.(7)sbml.org(8)BeielsteinTetal.2002.IEEE-CEC.(9)ShanthiAPetal.2005.IEEE-EH. (10)HamTSetal.2008.PLoSONE.(11)HaberJE.1998.AnnuRevGenet.(12)partsregistry.org(13)biofab.org (14)BennettMR&HastyJ.2009.NatRevGenet(15)beowulf.org	Winner!
249	An Adaptive Chemistry Reduction Method for Detailed Modeling of Advanced Combustion Systems Key words: combustion, mechanism reduction, stiffness removal, CFD Combustion of hydrocarbon fuels provides 85% of energy in the modern United States [1]; the current energy crisis is in reality a fuel crisis. While renewable forms of energy are being pursued to supplement combustion-based sources, hydrocarbon fuels will remain the major component for the next few decades. Currently, there is high demand to improve the efficiency of combustion technology to decrease the amount of fuel consumed and to reduce the emissions in an effort to lessen the environmental impacts; in addition, fuel-flexible designs that can run on both conventional and alternative fuels are desired. Computational modeling drives the design of new combustors and engines for aerospace, transportation, and energy applications, but accurate prediction of fuel consumption and pollutant emissions requires detailed chemical reaction mechanisms. Detailed mechanisms for liquid hydrocarbons of interest contain large numbers of species and reactions; for example, the reaction mechanisms for n-heptane (C H ) and iso-octane (C H ), important molecules for 7 16 8 18 gasoline modeling, contain almost 1000 species and 8000 reactions [1]. Despite rapid advancements in computing power, it is generally formidable to integrate such detailed reaction mechanisms into large-scale computational simulations, in terms of CPU time and memory requirements. In addition, the wide range of time scales (from nanosecond to second) and the nonlinear coupling between species and reactions induces stiffness when governing equations are solved. Due to these computational demands, practical simulations using detailed chemistry are impossible with modern computational tools. Mechanism reduction schemes are used to allow quantitative modeling while keeping realistic chemistry effects. Non-adaptive methods perform reduction based on a predicted range of conditions typically by removing unimportant species and reactions and identifying the species with fast time scales for further reduction, providing a single global mechanism. Most adaptive reduction methods, on the other hand, operate by storing chemical kinetics information and retrieving necessary data during the simulation to avoid direct integration of the governing differential equations; newer techniques use multiple mechanisms reduced prior to the simulation at various points in the flow. I propose the development of a novel adaptive and computationally friendly reduction method that will remove unimportant species and reactions and eliminate stiffness on the fly. I aim to explore and develop new algorithms while using existing reduction methods as a basis. Non-adaptive reduction methods attempt to provide a valid reduced mechanism by predicting the range of conditions (pressure, temperature, mixture composition) of interest in a simulation. However, the size of the reduced mechanism is limited by locations in the computational domain that require more detailed chemistry due to high reaction activity. Many methods have been developed to reduce mechanisms in this manner, but the application of directed relation graph (DRG) theory [2] is particularly useful. In this method, nodes of the DRG represent species and directed edges represent coupling of species. Important target species are defined (e.g. fuel, oxidizer, pollutants) and a graph-searching algorithm finds the dependent sets of species needed to accurately predict the targets’ production rates. Species with contributions below an error threshold are considered unimportant and removed from the mechanism, and the algorithm eliminates reactions containing unimportant species. For further elimination of stiffness in reaction systems, the quasi steady state (QSS) and partial equilibrium (PE) assumptions are applied [3]. QSS species and PE reactions have very short time scales, 1 NSF GRFP 2009 – Proposed Plan of Research – Kyle Evan Niemeyer causing stiffness, and the approximations seek to replace differential equations with algebraic relations to solve for species’ concentrations. Computational singular perturbation (CSP) and intrinsic low dimensional manifold (ILDM) [3] are traditional methods for finding QSS species and PE reactions by separating fast and slow processes. Adaptive reduction methods rely on different approaches to increase computational efficiency during simulations. Approaches such as in situ adaptive tabulation and artificial neural networks [4] perform storage and retrieval of chemical kinetics information to save processing time. Newer adaptive methods such as genetic algorithms [5] and optimization-based approaches [6] use various techniques to provide multiple reduced mechanisms for use during the simulation at different points in the flow. Highly detailed chemistry needs to be considered at locations where reactions are actively occurring, while regions with low reactive activity can use extremely reduced mechanisms. However, all of the methods currently rely on predictive reduction, which will not provide the highest level of accuracy or reduction. I propose the investigation of a new adaptive reduction methodology that will perform on the fly removal of species and reactions and elimination of stiffness. Identification and removal of unimportant species and reactions based on local conditions allows for the highest level of reduction and therefore the least computational demand, while keeping high accuracy. First, I will explore a novel algorithm for species and reaction removal using the DRG concept as a starting point; previous studies [2] based on DRG have shown it to be fast and reliable, suitable characteristics for on the fly application. This work will build directly on the preliminary non- adaptive reduction method I developed [7]. Second, I will investigate efficient methods for identifying fast processes such as QSS species and PE reactions; traditional methods such as CSP and ILDM are time-intensive [3] and therefore not well suited for on the fly stiffness removal. The adaptive reduction method I have proposed can be directly applied to the simulation of combustion processes for aeropropulsion, transportation, and energy applications. The incorporation of detailed chemistry while providing speedy simulation will allow accurate modeling of fuel consumption and emissions and help drive the design of next-generation engines and combustors. A method based on graph theory could also be applied to the modeling of other complex systems; broader applications consist of food web/ecosystem modeling, disease spreading modeling, climate modeling, and biological systems modeling. Also, a new methodology developed to perform mechanism reduction could also be used to collect important information about complex systems. For example, a CSP-based method was used to gather information about explosive processes in a simulation of a hydrogen/air turbulent lifted jet flame [8] - the new method I propose could be used similarly for data mining. References [1] “Basic Research Needs for Clean and Efficient Combustion of 21st Century Fuels,” DOE/BES Workshop Report (2006). [2] T.F. Lu, C.K. Law, Combust. Flame 144 (2006) 24-36. [3] T.F. Lu, C.K. Law, C.S. Yoo, J.H. Chen, Combust. Flame 156 (2009) 1542-1551. [4] J.-Y. Chen, J.A. Blasco, N. Fueyo, C. Dopazo, Proc. Combust. Inst. 28 (2000) 115-21. [5] I. Banerjee, M.G. Ierapetritou, Combust. Flame 144 (2006) 619-33. [6] O.O. Oluwole, P.I. Barton, W.H. Green, Combust. Theory Model. 11 (2007) 127-46. [7] K.E. Niemeyer, C.-J. Sung, M.P. Raju, “Skeletal mechanism generation of surrogate fuels using directed relation graph with error propagation and sensitivity analysis,” Combust. Flame (submitted). [8] T.F. Lu, C.S. Yoo, J.H. Chen, C.K. Law,AIAA 2008-1013 (2008). 2	Winner!
251	The relationship between craft production and the political and economic development of complex societies continues to generate debate in the field of Anthropology. The study remains relevant because craft production is a firmly embedded element of culture (Costin 1991:2). This field of research has broad impacts as it can be expanded to the understanding of socio-economic organization as well as the identification of gender roles (Brumfiel 2006: 862). Ultimately, knowledge of production methods and their roles in a society can lead to a stronger understanding of political organization (Brumfiel and Earle 1987:3-5). Earle‟s (1987:69) work on Hawaiian and pre-Inkan societies indicates that the centralization of production reflected strengthening political control. Further evidence for an increasing complexity in these societies came from emphasis on specialized craft production. Earle‟s work is an excellent example of how archaeological research can tie craft production to the development of complex society. This proposal discusses conducting further research on this topic in the Early Bronze Age (EBA) settlement at Zincirli, Turkey. This site, known primarily for its Iron Age occupation, has an eight hectare walled town dating to 2500 BCE. Excavation conducted in the late 19th century uncovered ceramic and architectural remains which have been used to date the EBA areas (Lehman 1994:106-107). No further excavation on these areas has been attempted since that time, although magnetometry readings have produced evidence for additional architecture. Excavation of the EBA sections at Zincirli will begin in 2010, led by Dr. Christoph Bacchuber as sub-director to Dr. David Schloen, of the University of Chicago. This excavation will provide an opportunity to examine the relationship between craft production and political centralization. A model well suited for the interpretation of this work is the political development model of specialization, exchange, and complex society, as defined by Brumfiel and Earle (1987:1-4). This model suggests that elites control the organization of local craft production and consequently are the primary beneficiaries. The political development model is reflected by evidence of craft specialization, the organization of local production, and the mobilization of goods from producers to elites. To properly address this model and determine its efficacy in understanding the EBA settlement at Zincirli, the following research questions will be addressed: 1. To what degree were craft production activities specialized at this site? 2. How was craft production organized at this site? (What are the units of production?) 3. What manner of mobilization of craft products to elites is present? Costin (1991:4) defines specialization as a “differentiated, regularized, permanent, and perhaps institutionalized production system” in which an individual does not produce all of the goods he/she consumes, but rather is dependent on others to produce certain items. A supportive example of this would be the discovery of a collection of a particular artifact type bearing a high degree of standardization. Conversely, a highly diverse artifact typology suggests a greater number of individuals and thus less specialization (Costin and Hagstrum 1995: 632). Organization can be determined through the distribution of particular artifacts (tools, raw material, waste). Appearing in a centralized location suggests a „work-shop‟ organization, whereas a more diffuse distribution in individual structures suggests household production (Costin 1991:6, 8, 21-29). Control of craft product by elites can be identified through the proximity of production centers to elite residencies (Earle 1987:68-69) and the appearance of specialized craft goods in elite settings (Brumfiel and Earle 1987:3-5). A wide dispersal of specialized craft goods could indicate a lack of elite mobilization. In order to address my research questions, data will be derived through excavation and the entry of piece plotting measurements into a GIS database. After three field seasons with the Chicago team, I anticipate having a database large enough to derive spatial patterns indicating either the presence or absence of centralized craft production. This analysis, similar to that used at Titriş Hoyuk (Hartenberger 2000), will provide the basis for determining the relationship between craft production and political centralization. Titriş Hoyuk, a comparative case study for craft production, is approximately 115km east of Zincirli, and has a contemporaneous EBA habitation. Excavation has yielded a lithic workshop indicating organized, centralized, and specialized craft production (Hartenberger 2000:51). Intellectual Merit The results of this analysis will increase our knowledge of the conditions of this time period and region of Anatolia. The Anatolian Early Bronze Age marks a significant change in economic, political and technological developments (Yenner & Vandiver 1993: 208). This proposed research will provide information on each of these characteristics. One advantage of increasing this knowledge is the ability to link it with the wealth of research that has been conducted in Mesopotamia. This will aid in producing a larger and more regional picture of complex society in the Eastern Mediterranean. My class work, which has trained me in GIS, and my previous excavation experience, especially at Zincirli, Turkey, have given me the tools to conduct this research. My current research for a Bachelor‟s of Philosophy thesis has enabled me to use these tools on a similar archaeological problem. Through my focus on textile production at the EBA site Karataş, in SW Anatolia, I am familiar with much of the relevant literature for this proposed research. Also, I now have experience in using GIS to derive information from artifact distribution and densities. Broader Impact This work builds on previous research conducted on craft production, and will contribute to our understanding of this phenomenon both regionally and theoretically. To disseminate the outcomes of this research, I will actively engage with the academic community by presenting my findings at academic conferences and by producing publications. This work will also contribute to my abilities to teach anthropology and archaeology, by giving me the knowledge and experience to express these concepts to others. References Cited Brumfiel, Elizabeth (2006). Cloth, Gender, Continuity, and Change: Fabricating Unity in Anthropology. American Anthropologist, Vol. 108, No. 4: 862-877. Brumfiel, Elizabeth and Timothy K. Earle (1987). Specialization, Exchange, and Complex Societies. Cambridge University Press: Cambridge. Costin, Cathy Lynne (1991). Craft Specialization: Issues in Defining, Documenting, and Explaining the Organization of Production. Archaeological Method and Theory, Vol. 3: 1-56. Costin, Cathy Lynne and Melissa B. Hagstrum (1995). Standardization, Labor Investment, Skill, and the Organization of Ceramic Production in Late Prehispanic Highland Peru. American Antiquity, Vol. 60, No. 4: 619-639. Earle, Timothy K (1987). Specialization and the Production of Wealth, in Elizabeth Blumfiel and Timothy K. Earle, eds., Specialization, Exchange, and Complex Societies. pp. 64-75. Cambridge University Press: Cambridge. Hartenberger, Britt et al.(2000). The Early Bronze Age Blade Workshop at Titris Hoyuk. Near Eastern Archaeology, Vol.63, No.1:51-58. Lehmann, Gunnar (1994). “Zu den Zerstörungen in Zincirli Während des Frühen 7. Jahrhunderts v. Chr.” Mitteilungen der Deutschen Orient-Gesellschaft zu Berlin, 126:105–122. Yenner, K. Aslihan, and Pamela B. Vandiver (1993) Tin Processing at Göltepe, an Early Bronze Age Site in Anatolia. American Journal of Archaeology, Vol. 97, No. 2:207-238.	Winner!
252	(cid:80)(cid:83)(cid:1)(cid:86)(cid:84)(cid:70)(cid:1)(cid:88)(cid:74)(cid:85)(cid:73)(cid:80)(cid:86)(cid:85)(cid:1)(cid:81)(cid:70)(cid:83)(cid:78)(cid:74)(cid:84)(cid:84)(cid:74)(cid:80)(cid:79)(cid:15) (cid:36)(cid:80)(cid:81)(cid:90)(cid:83)(cid:74)(cid:72)(cid:73)(cid:85)(cid:1)(cid:19)(cid:17)(cid:18)(cid:17)(cid:1)(cid:88)(cid:88)(cid:88)(cid:15)(cid:83)(cid:66)(cid:68)(cid:73)(cid:70)(cid:77)(cid:68)(cid:84)(cid:78)(cid:74)(cid:85)(cid:73)(cid:15)(cid:68)(cid:80)(cid:78)(cid:1)(cid:34)(cid:77)(cid:77)(cid:1)(cid:51)(cid:74)(cid:72)(cid:73)(cid:85)(cid:84)(cid:1)(cid:51)(cid:70)(cid:84)(cid:70)(cid:83)(cid:87)(cid:70)(cid:69)(cid:1)(cid:85)(cid:80)(cid:1)(cid:48)(cid:83)(cid:74)(cid:72)(cid:74)(cid:79)(cid:66)(cid:77)(cid:1)(cid:34)(cid:86)(cid:85)(cid:73)(cid:80)(cid:83)(cid:15)(cid:1)(cid:37)(cid:80)(cid:1)(cid:47)(cid:80)(cid:85)(cid:1)(cid:37)(cid:86)(cid:81)(cid:77)(cid:74)(cid:68)(cid:66)(cid:85)(cid:70) (cid:80)(cid:83)(cid:1)(cid:86)(cid:84)(cid:70)(cid:1)(cid:88)(cid:74)(cid:85)(cid:73)(cid:80)(cid:86)(cid:85)(cid:1)(cid:81)(cid:70)(cid:83)(cid:78)(cid:74)(cid:84)(cid:84)(cid:74)(cid:80)(cid:79)(cid:15)	Winner!
253	Do not duplicate or use without permission www.rachelcsmith.com Making Optimal Decisions for an Uncertain Future: Quantifying the Effects of Anthropogenic Disturbance on Biodiversity and Ecosystem Services Key Words: Disturbance Effects, Biodiversity, Ecosystem Services, Ecosystem Management Introduction: Anthropogenic disturbances negatively impact species, genetic and functional diversities of ecosystems, reducing the essential services they provide. While we know that the presence of diverse functional traits in an ecosystem is directly linked to the successful provisioning of essential services1, it is often easier to quantify an ecosystem’s genetic diversity than to determine its functional diversity. Unfortunately, we lack a fundamental understanding of the interrelationships between these forms of diversity and whether or not one can be used as a proxy for another. This gap in our knowledge impedes our ability to make management decisions that maximize future ecosystem services. My proposed research will: Research Objectives: 1. Determine if and how genetic diversity and functional diversity are related. 2. Determine if and how natural and anthropogenic disturbances alter the relationship between genetic diversity and functional diversity. 3. Create a predictive management tool that allows us to maximize genetic and functional diversity, and thus the provisioning of ecosystem services, in the uncertain future. To accomplish these objectives, I will develop a database of the species present before and after anthropogenic disturbances. The database will contain data from a global range of ecosystems, as well as a subset of data that will be provided by Sierra Pacific Industries (SPI), the largest private landowner in California. I will analyze these data to determine how genetic and functional diversities change after disturbance and then create an easy-to-use online management tool that predicts how future anthropogenic disturbances will alter biodiversity and ecosystem services. Background: Phylogenetic Diversity (PD) is the length of evolutionary pathways connecting taxa, and it is a well-known index used to measure genetic diversity2. When managing an area to conserve overall biodiversity, maximizing PD is likely the best way to hedge our bets and increase the probability of having the right extant species at our disposal in a future of unknown environmental, economic, and medical needs3. Functional diversity (FD), the total branch length of a tree of functional traits, measures the diversity of functional traits in an area4. Maximizing the FD of an area is important because essential ecosystem services are directly tied to the value, range, and abundance of an ecosystem’s functional traits1. It is quickly becoming more practical and economical to determine the PD rather than the FD of an ecosystem because DNA barcoding can reliably complete large taxonomic surveys, while quantifying the functional traits present in an area is still a large undertaking. It is possible that PD can serve as a proxy for FD, which would allow us to assess an ecosystem’s function without quantifying functional traits. However, no large-scale study has ever demonstrated the relationship between genetic and functional diversities. Additionally, we currently have no way to accurately predict the effects of anthropogenic disturbances on PD and FD. SPI owns approximately 1.7 million acres of land and must routinely make management decisions without knowing the exact consequences of their management practices. SPI is interested in learning if their management causes changes to the biodiversity and ecosystem services of their land. Therefore, in addition to studying disturbances in a range of global ecosystems, I will collaborate with SPI to determine the effects of harvesting-related disturbances. NSF-GRFP Proposed Graduate Study Copyright 2010 to Original Author All Rights Reserved. Do not duplicate or use without permission www.rachelcsmith.com Methods: I am currently collecting data from published articles that document changes to the species diversities of a range of organisms after anthropogenic disturbances (e.g. fire, timber extraction) in ecosystems subject to diverse natural disturbance regimes (e.g. fire, flooding). SPI has already compiled a dataset that details the plant species observed before and after clearcutting, replanting, and applying herbicides in 200 forest patches. With this data, I will: 1. Build separate phylogenetic and functional trees of species found before and after human disturbance, using known phylogenies, programs such as Phylomatic and TreeBASE, and lists of functional traits, such as USDA PLANTS and Jepson Herbarium’s Flora Project. 2. Analyze relationships between disturbance, PD, and FD. Using a likelihood and Bayesian approach, I will compare changes to PD and FD after disturbance and changes in the K score of trees, which is the difference in the relative branch lengths and topologies of phylogenetic trees5. If PD and FD change in correlated ways after disturbances, then PD can serve as a proxy for FD. I will also determine if the changes in PD, FD, and K score can be explained by specific disturbance regimes. 3. Organize data and results in an SQL database. Create an accessible online tool that will be available to researchers, landowners, government, and NGOs. It will formulate ecosystem management plans and allow users to predict how their actions will change their land’s biodiversity and ecosystem services. I will also develop a California version for use by SPI. Expected Results: 1. Ecosystems with a high PD will have a correlated high FD. 2. If an ecosystem faces an anthropogenic disturbance that mimics its natural disturbance regime, PD and FD will change in correlated ways, and PD can serve as a proxy for FD. Broader Impacts: Large-scale biodiversity loss directly threatens ecosystem stability and reliability by impairing ecosystem services, such as primary production, carbon storage, and pollination1. Gaining a better understanding of the effects of our ecosystem management decisions will allow us to avoid the irreversible loss of biodiversity and ecosystem functions or to at least limit the scale, frequency, and intensity of anthropogenic disturbances in the future. My research findings will elucidate the relationships between genetic and functional diversities. I will provide the information and tools we need to make economically efficient and socially optimal resource management decisions, ensuring that we conserve the organisms that will provide essential ecosystem services in an uncertain future. I have already started data collection for this project, and I am currently mentoring three undergraduates, including two women who also belong to ethnic minorities, who are aiding in the development of my dissertation project. After developing the management tool, I will hold workshops for landowners throughout California. I will teach them to use the tool, and I will speak on the benefits of managing land to maximize biodiversity and ecosystem function. My research will inform the long-term management decisions of landowners in California. References: [1] Diaz, S et al. 2007. Incorporating plant functional diversity effects in ecosystem service assessments. PNAS 104:20684. [2] Faith, D. 1992. Conservation evaluation and phylogenetic diversity. Biol Cons 61:1–10. [3] Forest, F et al. 2007. Preserving the evolutionary potential of floras in biodiversity hotspots. Nature 445:757-760. [4] Petchey, OL & Gaston, KJ. 2002. Functional diversity (FD), species richness and community composition. Ecol Letters 5:402-411. [5] Soria-Carrasco, V et al. 2007. The K tree score: quantification of differences in the relative branch length and topology of phylogenetic trees. Bioinformatics 23:2954.	Winner!
254	duplicate or reproduce without permission. www.rachelcsmith.com Seasonal Migration Within Aseasonal Tropical Rainforests: A Phenomenon With Immense Implications INTRODUCTION: Tropical rainforests (TRF) are often considered aseasonal, however every TRF studied shows seasonal phenological variations corresponding to precipitation and solar irradiance1. Flushing, flowering, fruiting, and invertebrate biomass have general community- wide peaks during a region’s wet season, with large-fruited trees exhibiting the strongest phenological clumping1,2. Due to local precipitation regimes, phenology peaks vary in different geographical locations, creating spatio-temporal resource shifts1,3,4. Migration, the large-scale seasonal range shifts that occur in response to disparities in regional resources, has not been studied as a faunal survival adaptation within tropical rainforests1. Uncovering how animals move in response to seasonal resource shifts is critical to the conservation of migrating species and the ecological processes they perform5,6,7,8. Furthermore, species dependent on variable resources are the first to face local extinction after forest fragmentation. Moreover, migrating species are particularly threatened by current global climatic changes5,6. I will create a spatio-temporal model of fruiting shifts in SE Asia, then track hornbill movements to test my hypothesis that migration exists in TRF to follow resource shifts. BACKGROUND: Current research on migration as a response to seasonal resource shifts is focused on temperate and highly seasonal tropical regions5. While altitudinal migration that follows seasonal phenological changes does exist in TRF, large-scale seasonal migration that follows regional climatic differences is completely unreported2,6,7. Newton’s comprehensive textbook on migration argues that the increased movements required to cross climatic gradients and the limited resource inequalities between TRF negate the returns for intra-TRF migration7. However, highly mobile TRF frugivores like hornbills can traverse hundreds of kilometers per week, and in SE Asia, seasonality is sufficient to create resource disparities4,8,9. SE Asia is the optimal location to test for migration because monsoons create localized weather patterns in lowland TRF. Variations in wet seasons form a matrix of adjacent landscapes with offset phenologies1,4. Consumers depend on these spatio-temporal rhythms in the food supply1,2,3,8. Local seasonal resource disparities are extreme, exceeding six fold increases in fruiting species during months of peak rainfall. This provides incentive for migration6. Hornbills are large frugivorous birds that are highly mobile and capable of migration. They favor large, ripe, oily fruits in rare canopy/emergent tree species that fruit seasonally2,3,8,10. Hornbills are keystone seed dispersers and the SE Asian equivalent of toucans8,9. Hornbills track resources throughout their home ranges and juveniles are known to roam until they obtain territories, however hornbills are not known to migrate8. A seasonal flock of 3000+ plain pouched hornbills, Aceros subruficollis, has recently been discovered around Lake Tasek Temengor in Peninsular Malaysia11. The hornbills fly north after staying in the region during the two month period of peak rainfall and fruiting3,11. The destination of A. subruficollis is unknown, however, it has never been recorded as a breeding in Malaysia8,11. The seasonal presence of this flock in Malaysia for purposes other than breeding suggests that A. subruficollis is migrating outside of Malaysia, most likely into Thailand. If A. subruficollis is migrating, it would constitute the first documented migration by a TRF species7. Altitudinal migration can be refuted because A. subruficollis vacates from the Lake Tasek Temengor region where elevation changes exceed 1000m within a 30km zone11. A. subruficollisis is currently listed as a vulnerable species due to the rapid decline in small total population. In addition, the details of its range, life history and ecology are unknown8,12. NSF Proposed Research 2010 All Rights Reserved to original author. Do not duplicate or reproduce without permission. www.rachelcsmith.com HYPOTHESIS: i) Rainfall-driven local phenology differences have resulted in significant seasonal resource disparities across space within TRF. ii) A. subruficollis will migrate in order to exploit seasonal resources. Null: i) Resources are homogeneously distributed in time and ii) A. subruficollis movements do not correlate with resource abundance. OBJECTIVE 1: Create a spatio-temporal resource model using GIS mapping techniques to test the relationship between rainfall and phenology of fruiting trees. Then, model optimal migration paths for A. subruficollis based on distance and temporal resource abundances. METHODs: 1) Create regional monthly rainfall/fruiting species database. Precipitation data is available from the Malaysian and Thai Hydrological departments, phenological data is available from the literature1. 2) Model month-by-month rainfall and fruit abundance by region in ArcGIS. 3) Use large-scale layered models to quantify resource disparities across time and space. 4) Model A. subruficollis optimal movements to exploit spatio-temporal resource peaks. OBJECTIVE 2: Test if A. subruficollis migrates to exploit spatio-temporal resource abundances. METHODS: 1) Radio-track 15 A. subruficollis individuals for two years9. Capture birds with pulley-mounted canopy mist-nets at roost in Malaysia and attach satellite-transmitters at the base of the tail feathers9. Monitor their movements with a receiver9. 2) Input A. subruficollis movement data into a GIS spatio-temporal resource model. 3) Determine if there is causal relationship (using spatial auto-correlation) between movements and spatio-temporal resources. CONSEQUENCES: Migrants and species dependent on seasonal resources are particularly vulnerable to climatic changes and forest fragmentation5,7,8. Moreover, concerns about climate change stress the importance of keystone seed dispersers, like hornbills, to help move the trees to more suitable climates8,9. A positive feedback response could develop where keystone migrants disappear from disturbed forests, decreasing ecosystem functioning and future forest resilience. TRF migration also directly challenges Rapaport’s Rule of decreasing animal range size with latitude, a theory based on decreased resource variability in the tropics. The seasonal resource models I will create will bring the degree of variability into question. Migrating frugivores also provide rapid long-range seed dispersal along distinctive corridors and back to roosts, shaping the spatial regeneration patterns and diversity of forest trees3,10. BROADER IMPACTS: I will partner with the Forestry Research Institute of Malaysia (FRIM). Malaysian researchers will aid in all aspects of this project including anticipated co-authorships on publications, and becoming fully trained in the methods and analyses. FRIM helps to manage Malaysia’s natural resources, making it optimal to immediately bridge my research with policy and action. Additionally, this research will locate movement corridors that are critical to conservation efforts for this vulnerable species, which benefits future human generations of all nations12. A. subruficollis is also a charismatic species and important tourism draw in the region8. Finally, Dr. Poonswad at the Mahidol University in Bangkok has enlisted master’s students working on Thailand Hornbill Project (THP) to help track A. subruficollis in Thailand. Working with FRIM and THP will bring together an international team and facilitate the local and broad dissemination of results in English, Malay and Thai. BIBLIOGRAPHY: [1]van Schaik, C.P.,Terborgh, J.W. and Wright, J.S. 1993. Annun. Rev. Ecol. Syst. 24; [2]Walker, J.S. 2006.Biol. Conserv.130; [3]Medway, F.L.S. 1972.Biol. J. Linn. Soc. 4 [4]Kumagai, T. et al. 2009.Water Resources 45; [5]Both, C., et al. 2006. Nature 441; [6]Levey, D.J. 1994. The Auk 111;[7] Newton, I. 2008. Academic Press,London; [8]Kinnaird, M. F. & T. G. O'Brien. 2007. [9]Holbrook, K.M. & T.B. Smith. 2000. Oecologia125; [10] Hardesty, B.D., Hubbell, S.P., et al 2006. Ecology Letters 9.[11] Chew, H.H., & S. Supari. 2000. Forktail 16; Univ. Chicago Press; [12]IUCN 2009. Version 2009.1;	Winner!
255	www.rachelcsmith.com *All Rights Reserved to Original Author 2010 GRFP Research Proposal Native Bee Reproductive Success in Restored Habitats Introduction: Ecological restoration can rehabilitate ecosystem services, but its success depends upon the ability of the restored site to sustain functional populations.1 Restoration has been proposed as a way to promote conservation of native bee populations that have declined due to habitat loss and fragmentation.2 Native bees are effective pollinators of many economically important crops,3 and drastic crashes in managed, non-native honey bee populations due to colony collapse disorder have highlighted systemic vulnerability, as well as the need to diversify on-farm pollinator communities. Within agricultural systems, hedgerows (linear strips of native flowering shrubs planted in fallow field margins) are the preferred restoration method: In 2007, Congress passed the Pollinator Habitat Protection Act (S.1496), incentivizing the creation of pollinator-friendly hedgerows. However, agricultural landscapes have become increasingly simplified due to intensive farming practices, and potential source habitat may be too distant to provide reliable immigration to hedgerows.4 In addition, recent research5 suggests that hedgerows may be sink habitat, where the death rate is greater than the birth rate.6 This research used species richness as a proxy for reproductive success, which is problematic because it gives no indication of long-term population viability within sites. If hedgerows are sinks, pollination services could be threatened.3 Therefore, I propose to directly measure native bee reproductive success in order to assess the sink hypothesis and the conservation potential of hedgerows. Background: Native solitary bees typically have one generation per year, therefore there are two main components that influence reproductive success: per female fecundity and offspring survival. Fecundity may be influenced by proportion of forage (pollen) available for provisioning of brood cells7 at both the local and landscape level.8 Hedgerows often contain low plant diversity (usually between 8 - 15 species); if these resources are inadequate, bees may need to forage in the surrounding landscape to obtain sufficient pollen to meet larval needs.4,8 Limited or patchy landscape resources could reduce success as fewer nests could be created. Larval mortality can be heightened by increased parasitism, and cleptoparasite and parasitoid abundance is often greater in restored sites than in natural areas.10 Additionally, parasitism rates have been correlated with resource availability: in resource-poor environments, bees compensate for floral scarcity by increasing search time, broadening the window for successful parasitism.11 While exposure to herbicides12 and abiotic factors, such as high in-nest moisture and temperature levels,13 can also be fatal to larvae, their effects are difficult to measure; therefore, I will divide causes of mortality into two categories: parasitism and unknown.10 In order to demonstrate the occurrence of source-sink dynamics it is necessary to compare population demographics in multiple habitats.14 Thus, treatments will be in two habitat types, restored (hedgerow), and un-restored (fallow field margins), situated in either complex (heterogeneous) or simple (homogenous) landscapes (n = 18). Additionally, in order to have baseline data against which gauge the success of the restored sites, fecundity and offspring survival will be recorded in natural habitats (n = 4). I will use trap-nesting bees (cavity-nesters) as my study taxon because ninety percent of the native bee species managed for agriculture are trap-nesters, and they readily occupy artificial “trap-nests,” bundles of hollow reeds, that can be lined with removable straw inserts to facilitate monitoring of nest progress.8 Hypotheses: In order to examine the capacity of hedgerows to sustain viable populations of trap- nesting bees, I will measure fecundity and parasitism in two landscape contexts: 1. Fecundity of trap-nesting bees will decline with decreased resources. I hypothesize that landscape complexity will be more important to fecundity than local-level resources. In simple Native Bee Reproductive Success *Do not Reproduce without Permission www.rachelcsmith.com *All Rights Reserved to Original Author 2010 landscapes, I do not expect to find significant differences in fecundity between hedgerows and fallow field margins. In contrast, I predict that in complex landscapes fecundity will in higher in both treatment types, approaching observed levels in natural habitat. However, if fecundity in hedgerows in simple landscapes is higher than in fallow field margins, it would indicate that the local resources they provide are sufficient, bolstering claims that they are an appropriate restoration method in homogenous landscapes. 2. Parasite pressure on larvae will increase with decreasing resources, negatively impacting reproductive success. In simple landscapes, I expect to observe spikes in parasitism levels in both habitat types. I predict that the additional resources provided in heterogeneous landscapes will buffer larvae against heightened parasitism in hedgerows but not in fallow-field margins. Further, I predict that offspring survival in hedgerows and field margins in both landscapes types will be significantly lower than in natural habitat, signifying that disturbed landscapes subject larvae to increased threats from parasitism and other factors shown to increase mortality. Methods: Study Location: This study will take place in Yolo County, an agricultural region in California’s Central Valley. In the study region, complex landscape is a mosaic of natural habitat, riparian corridors, organic farms, and conventional agriculture; simple landscapes are dominated by intensive agriculture (> 80%). Landscape features will be categorized using GIS landsat data. Each site will contain a 300 m transect with a trap-nest in the center, and will be at least 2 km apart to ensure isolation.15 Floral Resources: Vegetation sampling will commence with nest initiation and terminate when nesting ceases. I will record flowering species and number of inflorescence in 1 m2 quadrats along transects. To determine the proportion of local and landscape resources used, I will collect voucher pollen from all flowering plants within a 1500 m radius of trap-nests, and compare it with sub-samples of pollen from nests.8 Parasitism: Once nests are completed, I will x-ray larvae in the lab to ascertain which are parasitized;8 parasitoids will be identified after emergence by Dr. Robbin Thorp, of the UC Davis Bee Biology Lab. Unparasitized pupae will be stored in optimal conditions at the UC Berkeley insectary and monitored for emergence of cleptoparasites. Broader Impacts: Due to the persistent, damaging effects of colony collapse disorder, restoration of native bees is essential for the maintenance of pollination services in agricultural areas.3 These findings could validate hedgerows as an effective restoration method, or illuminate its short-comings. Worldwide, native bees are the most important pollinators in natural systems, and are therefore necessary for preservation of biodiversity.3,16 The result of this study will help identify factors that could contribute to the success of pollinator restoration at larger scales. I will submit papers to scientific journals, present at conferences, and share my results with farmers at annual workshops put on by the Xerces Society, a non-profit dedicated to insect conservation. References: 1. Ormerod, SJ. J. of Applied Ecology 40 (Dec 2003) 2. Dixon, KW. Science 325 (Jul 2009) 3. Kearns, CW. et al. Ann. Review of Ecology and Systematics 29 (1998) 4. Ricketts, TH, et al. Ecology Letters 11 (May 2008) 5. Ockinger, E, HG Smith. J. of Applied Ecology 44 (Feb 2007) 6. Pulliam, HR. American Naturalist 132 (Nov 1988) 7. Muller, A, et al. Biological Conservation 130 (Jul 2006) 8. Williams, NM, C Kremen. Ecological Applications 13 (Apr 2007) 9. Steffan-Dewenter, I. Ecological Entomology 27 (Oct 2002) 10. Exeler, N, et al. J. of Applied Ecology 46 (Oct 2009) 11. Goodell, K. Oecologia 134 (Mar 2003) 12. Freemark, K, C Boutin, Agriculture Ecosystems & Environment 52 (Feb 1995) 13. Hranitz, JM, et al. Environmental Entomology 38 (Apr 2009) 14. Watkinson, AR, WJ Sutherland, J. of Animal ecology 64 (Jan 1995) 15. Gathmann, A, T Tscharntke. J. of Animal Ecology 71 (Sept 2002) 16. Allen-Wardell, G, et al. Conservation Biology 12 (Feb 1998)	Winner!
256	Dynamics of Alliance Formation in Pueblo Societies Keywords: Climate Change, Agent-based modeling, Alliance, Puebloans, Cooperation, Conflict Introduction: Previous work within the Village Ecodynamics Project (VEP) has successfully established a detailed, semi-realistic, household-level model for Puebloan ecodynamics1. I propose to extend this household model to create agent-based models for conflict and cooperation in the context of the 700-year archaeological record of the central Mesa Verde region. Here, the formation of larger groups is linked in a complicated way with conflict, but it is also probable that mutualistic activities not motivated by between-group conflict contributed to these larger group sizes. This model will help us understand the years of peace within the Mesa Verde region, and the circumstances under which Puebloan people resorted to violence, as these cycles have been locally described by Cole2. The models I create will be applicable to other small-scale societies—and elsewhere, with appropriate caution. Background: As resources dwindle, climate change is reshaping the earth, leaving us faced with problems with potentially dire consequences3. Repeated calls have recently been made to apply agent-based modeling to contemporary affairs4, both to understand crises as they unfold, and to anticipate them. In these efforts, archaeology assists by providing a long-term view of the relationship between demography, distribution of human group sizes, environmental factors and violent conflict. My place at Washington State University in the context of the VEP will allow me to address these questions with support from archaeologists, geologists, geographers, computer scientists and economists engaged in VEP empirical and modeling efforts. Hypotheses: My research will investigate how human cooperation affects the demographic success and spread of human groups. Specifically, my research will examine the following hypotheses: 1) both kinship- and non-kinship-based coalitions formed in response to environmental pressures, such as dwindling per-capita resources due to climate change or population growth; 2) coalitions do not form only as a response to external conflict; rather, they are leveraged by humanity’s evolved sociality5 and can serve to provide positive returns to increasing group size; 3) coalitions may fracture when within-group competitive pressures become too great, or when between-group competitive pressures relax. Research Plan: Working with Dr. Timothy Kohler and the VEP, I will participate in ongoing field research in the Mesa Verde region. As an NSF Graduate Research Fellow, I will generate spatial goodness-of-fit measures between VEP simulations and the archaeological record to assess the general fidelity of the simulation to archaeological data from Mesa Verde, as well as to analyze and interpret the residuals. I aim to understand how accurately the existing agent- based models (ABM) predict the spatial distribution of households, subsistence and technology, and to evaluate the extent to which the simulations generate the archaeological record. Moreover, I will explore various methods of assessing spatial goodness-of-fit using over 4,000 archaeological sites in the VEP study area from AD 600 to 1280. Next, I will create a model describing the emergence of alliances based on kinship and economic ties. Currently, the Village simulations do not allow for cooperation beyond that provided by exchange, or conflict beyond that generated through household-level competition for resources. Building upon Dr. Sergey Gavrilets’ (University of Tennessee Knoxville, Biology) framework for alliance formation6, I propose to create a stochastic model describing the emergence of cooperation resulting from between-group competition for key resources. I will gradually add levels of complexity to the unidimensional model as described by Gavrilets, which accounts for alliance formation only through competition for rank or mates. I will introduce a means of incorporating scalar stress in order to generate nested groups, in contrast to the Stefani Crabtree Proposed Research Essay exponential growth of alliances in Gavrilets’ model. An additional shortcoming of the previous model is that it only accounts for alliance-formation as a response to conflict, which ignores altruism and mutually beneficial relationships in coalition formation. Using the experimental test bed provided by the ABM, I will see whether approaches to generating cooperative networks modeled on the sodalities seen in Southwestern societies provide a better fit to the known facts of the archaeological record than do alliances generated out of between-group conflict. In modern Hopi societies, for example, sodalities form around a specific clan, “which own[s] the ceremonies, kivas, and ritual items used by each sodality. However, while sodalities are managed by specific clans, sodality members can come from any clan”7. My benchmark for comparison will be the well-known and precisely-dated archaeological record of the central Mesa Verde region, which provides a dataset that is unparalleled in the Neolithic world. Broader Impacts: This novel approach will provide critical information on the nature of human alliance formation. As my research will analyze how issues such as control of resources influence the formation of alliances, I will be able to determine how these alliances break down when resources become scarce. My results may have widespread applicability as the human population continues to grow worldwide, stretching the resources of our fragile planet. Understanding what lead to the dissolution of civilizations in the Neolithic world may help policymakers anticipate future challenges. My research will inform efforts to understand sociopolitical impacts of climate change. Through agent-based models of the archaeological data, I will analyze how people reacted to fluctuating temperature, reduction of key resources such as woody fuels and water, crop failure, and inter and extra-tribal hostilities, which may have been induced from the changing environment. Additionally, this research will examine the extent to which alliances form out of conflict, or as a means of providing positive per capita return in procurement of resources, and help us to understand not only the years of peace dominating the Mesa Verde region, but also the wave of violence that swept the area in its final years2. Future researchers will be able to build on these models to understand the complex dynamics of human relations in other societies. We are poised at a cross-roads as a civilization, plagued by many of the same issues that our ancestors faced. An understanding of our past will help us make informed decisions about our future. 1 Kohler, T., et. al. 2007. Settlement Ecodynamics in the Prehispanic Central Mesa Verde Region. In The Model-Based Archaeology of Socionatural Systems, edited by T. A. Kohler and S. van der Leeuw, pp. 61-104. SAR Press, Santa Fe. 2 Cole, S. 2007. Population Dynamics and Sociopolitical Instability in the Central Mesa Verde Region, A.D. 600-1280. Unpublished Master’s Thesis, Department of Anthropology, Washington State University, Pullman. 3 Cabrera, D. et al. 2008. What is the crisis? Defining and prioritizing the world’s most pressing problems. Frontiers in Ecology and the Environment 6(9):469–475. 4 for example, see: Buchanan, Mark. 2009. Meltdown modeling: Could agent-based computer models prevent another financial crisis? (News Feature) Nature 460(6):680-682. 5 Henrich, J., et.al. (2005) ‘Economic Man’ in Cross-Cultural Perspective: Ethnography and Experiments from 15 small-scale societies. Behavioral and Brain Sciences, 28, 795-855.. 6 Gavrilets S., et. al. 2008. Dynamics of Alliance Formation and the Egalitarian Revolution. PLoS ONE 3(10): e3293. doi:10.1371/journal.pone.0003293 7 Kantner, J., 2004. Ancient Puebloan Southwest. Cambridge University Press. Cambridge, UK. p. 262	Winner!
257	The First LAI-Linked Predictive Model for Below- and Above-Ground Carbon Sequestration in Quaking Aspen Keywords: vegetative regeneration, carbon modeling, climate change, aspen, conifers Background: Forest ecosystem carbon is a key component to global climate initiatives and forest ecosystems are the primary ecotype used for carbon registration protocols. Although methodologies for estimating above-ground carbon sequestered are well established, using leaf area index (LAI) linked to basal area, below-ground carbon estimates are hampered by the paucity and inconsistency of data for both coarse- and fine-root biomass (1). As soil root carbon is a more stable carbon sink than above-ground carbon, accurate below-ground carbon estimates are indispensable for modeling efforts (2). Objectives: I will measure the total biomass of quaking aspen in representative North American stands. LAI has strong positive correlations to coarse- and fine-root aspen biomass (3),(4). Above- and below-ground biomass and LAI will be determined in order to develop the first predictive model that relates LAI to above- and below-ground biomass for aspen. Quaking aspen (Populus tremuloides) is an excellent candidate for necessary root biomass sampling and carbon modeling. Populus is one of the most widely cultivated northern temperate tree genera in the world, and has both ecological and economic significance; in many ways, it is becoming a ‘model tree’ for biomass and carbon sequestration (5). The most widely distributed species in North America, aspen is extremely important ecologically for water quality and habitat. However, it is in decline in the western United States for reasons not yet fully understood. Aspen root systems are unique; this species regenerates vegetatively from shallow lateral roots, forming large clonal stands which can persist indefinitely, given the correct disturbance regime. These coarse roots, then, persist after removal or decomposition of above- ground biomass, and possibly for several such stand-replacing events. This is very different from roots of seed-regenerated trees, in which the root system dies with stem death and is subject to soil microorganism heterotrophy. Expected Results: This study will generate a predictive model that relates quaking aspen LAI to above- and below-ground carbon allocation. It will also advance understanding of the ecology of a vegetatively reproducing forest species, an important but often overlooked niche (6). This research methodology may be applied to other species and used to explore below- ground relationships of other vegetatively regenerated forest ecosystems. Moreover, an accurate model that related above-ground aspen LAI to below-ground biomass and persistence will be useful economically for forest managers and carbon accreditation. With the basal area: LAI relationship we will develop, this model will be economical and practical across scales and for many interested parties, from small landholders to climate modelers. For small landholders with small forests, income from carbon accreditation can be important in deciding whether to invest in afforestation. A persistent soil carbon stock in aspen stands, if present, would create significant financial incentive for afforestation with and preservation of aspen. Benefits would also accrue for global climate and for areas historically forested with aspen. Broader Impacts: Through my ongoing volunteer work at high schools in Oakland Unified School District with minorities and disadvantaged students, I will integrate the project and its activities with educational activities for a variety of students. Interested students will be given Proposed Plan of Research Benjamin Caldwell the opportunity to involve themselves in the research, with special effort made to employ underrepresented groups as field assistants and involve them in the laboratory. Once developed, our LAI-carbon model will be made available for download free of charge. In addition to the peer-reviewed literature, we will make presentations at conferences for forest managers and environmental organizations. Methodology: I will sample aspen in representative stands in western North America. Stands will be selected on the basis of prior disturbance regime, stand health, and ecotype. I will use several complementary methods to obtain a complete picture of leaf area, course root and above-ground biomass, and fine-root flux. Leaf area: We will determine leaf area relationships by destructive sampling of approximately 30 trees. A random subset of leaves from each tree will be collected and weighed in the field, and subsequently scanned in the laboratory to determine leaf surface area. This will allow the prediction of leaf area at the stand level from basal area. Above-ground biomass: We will use fixed-area inventories from plots we installed to estimate above-ground biomass. Canopy leaf weights and wood density will be used to predict carbon content with allometric equations. Course root biomass: Since aspen roots are typically superficial, they are relatively easy to map. We will map root systems using ground-penetrating radar. This methodology is much less invasive and labor-intensive than excavation, and can be used to map changes in coarse-root biomass over time. Soil cores will be taken to standardize data sets and extrapolate root maps to biomass and carbon (7). Fine root biomass: Fine-root biomass annual flux is best estimated over the course of one year using minirhizotrons (8). We will install minirhizotrons, capped plastic pipes measuring approximately 180 by 5 cm, at a 45 degree angle from the soil surface. We will then take photographs of roots which infiltrate the pipe throughout the course of the year, and determine root length and biomass by analysis with dedicated software. Root biomass, determined from soil cores at the beginning of the sampling period, will be extrapolated to the stand level. Using the initial root biomass and the initial minirhizotron values, we will find fine-root flux for the stand. Analysis: I will use regression analysis to develop leaf area prediction models from tree basal area and height of the crown base in aspen. Tree growth will be predicted from tree leaf area. Using data on root biomass and stand LAI, we will develop predictive equations that relate LAI to above- and below-ground tree carbon. Results will be integrated with MASAM, an existing forest stand health model, to provide tree restoration guidelines, LAI, and carbon estimates (9). Cited Literature 1. Brown, S. Environmental Pollution. 116, 363-72 (2002) 2. Rasse, D. P., Rumpel, C., Dignac, M. Plant and Soil. 269, 341-356 (2005) 3. DesRochers, A., Lieffers, V. Canadian Journal of Forest Research. 31, 1012–1018 (2001) 4. Davis, J., Haines, B., Coleman, Hendrick, R. Forest Ecology and Management. 187, 19-33 5. Taylor, G. Annals of Botany. 90, 681-689 (2002) 6. Bond, W., Midgley, J. Trends in Ecology & Evolution. 16, 45-51 (2001) 7. Butnor, J. et al. Soil Science Society of America Journal. 67, 1607 (2003) 8. Hendricks, J. J. et al. Journal of Ecology. 94, 40-57 (2006) 9. O'Hara, K. Forest Ecology and Management. 118, 57-71 (1999)	Winner!
258	An Empirical Analysis of the Role of Ecological Filters in Grassland Restoration Keywords: restoration, ecological filters, community assembly, invasive plant management Introduction: (cid:39)(cid:72)(cid:74)(cid:85)(cid:68)(cid:71)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:75)(cid:68)(cid:86)(cid:3)(cid:85)(cid:72)(cid:71)(cid:88)(cid:70)(cid:72)(cid:71)(cid:3)(cid:72)(cid:70)(cid:82)(cid:86)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:86)(cid:72)(cid:85)(cid:89)(cid:76)(cid:70)(cid:72)(cid:86)(cid:3)(cid:82)(cid:89)(cid:72)(cid:85)(cid:3)(cid:81)(cid:72)(cid:68)(cid:85)(cid:79)(cid:92)(cid:3)(cid:75)(cid:68)(cid:79)(cid:73)(cid:3)(cid:82)(cid:73)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:40)(cid:68)(cid:85)(cid:87)(cid:75)(cid:182)(cid:86)(cid:3)(cid:79)(cid:68)(cid:81)(cid:71)(cid:3) area, making an understanding of ecological restoration a critical issue for ecology [1]. However, restoration has historically been practiced on an ad hoc basis, without adequate planning or proper application of the scientific method. As a result, most restoration projects fail to achieve lasting change and seldom provide insights that may be broadly applicable and advance restoration theory. Experts in the field of restoration ecology are now calling for studies that apply ecological principles to empirically test basic ecological theories that are pertinent to restoration, such as community assembly and succession and, specifically, the role of trophic interactions [2]. My research attempts to respond to this call by answering questions regarding the interactions of herbivory, seed-predation and biotic soil disturbance in the restoration of a grassland ecosystem in California. Through my efforts, I hope to advance the science and practice of large-scale grassland restoration. Background: The ultimate goal of restoration ecologists is to manipulate assembly and succession in ways that produce the most desirable stable state, either by speeding up natural processes or by overcoming thresholds that might be insurmountable without human intervention. Ecological filters are biotic or abiotic variables that favor the assembly of certain species over others. If well-understood, these filters can be used to favor desirable species while inhibiting the establishment of undesirable ones, thereby directing community assembly towards the most desired state. In grasslands that have been invaded by exotic annual grasses, managers could use ecological filters to promote the re-assembly of native bunchgrasses to improve habitat quality for native plants and animals and improve forage quality for livestock [3]. The role of plant-herbivore interactions such as herbivory, seed-predation, and biotic soil disturbance as potential ecological filters is poorly understood. Evidence suggests that physical soil disturbance caused by burrowing rodents, such as the endangered Giant Kangaroo Rat (Dipodomys ingens) in my study system, promotes the invasion of exotic annual grasses [4]. However, recent work has shown that kangaroo rats also preferentially eat the large seeds of exotics and thus the net effect of their presence on plant recruitment is currently unknown [5]. Another study found that the exotics responded to defoliation with more vigorous regrowth than natives did, and the authors therefore concluded that grazing by large herbivores promotes dominance by exotics [6]. However, the effects of grazing are not limited to defoliation; animals also exhibit preferential selection and alter soil characteristics through compaction and nutrient addition [7]. Thus, the net or synergistic effects of these interactions on native plant restoration remain unclear. Hypotheses: (1) Soil disturbance by kangaroo rats will favor the assembly of exotic grasses while compaction caused by cattle will favor native bunchgrasses. (2) Nutrient addition by both animal species will favor the assembly of exotics and have a greater impact on the re-assembly process than physical soil disturbance. (3) Cattle with help export excess nutrients by selectively grazing nutrient rich vegetation and kangaroo rats will control the abundance of exotics by preferentially consuming their seeds. (4) A combination of cattle and kangaroo rats will be most successful in directing re-assembly towards a state dominated by native bunchgrasses. Research Plan: To test the effects of herbivory, seed-predation, and soil disturbance on re- seeding efforts I will establish 1-m2 restoration plots within an existing framework of nested cattle and kangaroo rat exclosures(cid:178)allowing for the quantification of both the individual and Page 1 of 2 Proposed Plan of Research Christopher M. Gurney combined effects of cattle and rodents. Two plots will be established in each of the three test areas, one on rodent disturbed soil and one on undisturbed soil (n = 10 exclosures). Additionally, soil samples will be taken on and off disturbed soil in each test area and will be analyzed for bulk density and chemical composition. These data will allow for the artificial decoupling of physical soil disturbance from nutrient addition. Additional plots will be established in the kangaroo rat exclosures(cid:178)one to simulate physical soil disturbance, one to simulate nutrient addition, and one to simulate both types of disturbance (for comparison with genuinely disturbed plots) for both animal species. Differences in bulk density will be simulated using a soil corer (to reduce density) or a rammer (to increase density). To simulate nutrient addition, fertilizer will be added to plots in an amount necessary to achieve the observed soil chemical composition where kangaroo rats or cattle have been active. All plots will first be surveyed in the spring using a pinframe method, then sprayed with herbicide and sown with four rows of seeds in the following winter. Each row will be randomly assigned to one of four native species of varying forage quality(cid:178)two were preferred and two were avoided in kangaroo rat feeding trials [5]. Plots will be monitored weekly through the growing season. Soil disturbance, seed germination, and herbivory on seedlings will be recorded. Plant cover will be monitored annually each spring. Data will be analyzed using mixed-model ANOVAs. Logistics and Support: The nested cattle and kangaroo rat exclosures were constructed two years ago as part of a concurrent project. The effectiveness of this experimental framework has already been demonstrated [5], and the kangaroo rat exclosures are checked on a regular basis for evidence of rodent activity. Our partners at the Bureau of Land Management (BLM) are in full support of this project and have generously agreed to provide the required seed and equipment. As a graduate student at UC Berkeley, I will also have access to the resources provided by the Graduate Group in Range Management(cid:178)including the support of expert faculty who specialize in grassland ecology and restoration. Broader Impacts: The results of my research will advance ecological theory by helping to elucidate the roles of herbivory, seed predation, and biotic soil disturbance on plant community assembly. Since many of the issues addressed in my research are ubiquitous throughout grassland ecosystems, my findings could be broadly applied in restoration all over the world. These results will also be useful to land managers and ranchers who hope to reduce the damage caused by invasive plants(cid:178)currently estimated at $2 billion annually in US grasslands [8]. Besides preparing the results for peer-reviewed publication, I will also collaborate with various stakeholders to determine how my findings can best be applied to large-scale management and restoration. At a local level, the joint managing partners of Carrizo Plains National Monument (the BLM and the Nature Conservancy) have already demonstrated a keen interest in applying the results of my research in future restoration projects at the Carrizo Plains(cid:178)(cid:38)(cid:68)(cid:79)(cid:76)(cid:73)(cid:82)(cid:85)(cid:81)(cid:76)(cid:68)(cid:182)(cid:86)(cid:3)(cid:79)(cid:68)(cid:85)(cid:74)(cid:72)(cid:86)(cid:87)(cid:3) remnant grassland and home to 13 endangered species. References: [1] G.C. Daily, Science, 269, 350 (1995). [2] V.K. Temperton et al. Assembly Rules and Restoration Ecology (Island Press, 2004). [3] L.F. Salo, Journal of Arid Environments, 57, 291 (2004). [4] Schiffman, P.M. Biodiversity and Conservation 3, 524 (1994). [5] L.R. Prugh, Carrizo Exclosure Experiment Report (Prepared for The Nature Conservancy, 2008). [6] S. Kimball, and P.M. Schiffman. Conservation Biology 17, 1681 (2003). [7] M.R. Stromberg, J.D. (cid:38)(cid:82)(cid:85)(cid:69)(cid:76)(cid:81)(cid:15)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:38)(cid:17)(cid:48)(cid:17)(cid:3)(cid:39)(cid:182)(cid:36)(cid:81)(cid:87)(cid:82)(cid:81)(cid:76)(cid:82)(cid:17)(cid:3)(cid:3)California Grasslands Ecology and Management (University of California Press, 2007). [8] J.M. DiTomaso, Weed Science, 48, 255 (2000). Page 2 of 2	Winner!
261	Feature Discovery in Link Mining Keywords: link mining, feature discovery, machine learning, graph theory, relational data Background: Traditional data mining approaches attempt to find patterns in a data set characterized by a collection of independent instances of a single relation. This is consistent with the classical statistical inference problem of trying to identify a model given a random sampling of an underlying distribution. A key challenge for machine learning is the problem of mining more richly structured data sets in a way that leverages the linkages between records [1]. In this paradigm, which more accurately resembles real-world data, instances in the data set are relational where different samples are related to each other, either explicitly as typified by friendship relationships in a social network, or on the web by hyperlinks [2]. However, in most large data sets, relationships also exist that are not explicitly annotated. According to Jensen, naively applying traditional machine learning methods to this type of data can lead to inappropriate conclusions [3]. Therefore new approaches are needed to appropriately correlate inherent relationships (i.e. links) in real-world data sets. In recent years, there has been a growing interest in learning from structured, real-world data. This type of data can be described by a graph where the nodes in the graph represent objects, and edges in the graph represent relationships between objects. Perhaps the most famous example of exploiting link structure is the PageRank algorithm [4] employed by the Google search engine. Link mining is situated at the intersection of graph theory, machine learning, and web mining. This research is potentially useful in a wide range of application areas including bio-informatics, bibliographic analysis, financial analysis, national security, social network analysis, and internet search to name a few. While my research is focused more on the theoretical aspects of this topic than in the applicative possibilities, I was happy to see that my work has already been adapted to the bioinformatics domain to study the interactions of proteins [5]. Research: Despite the recent advances in link mining, this topic is still relatively new and there are many fundamental challenges that remain. Unlike more mature fields of research there does not exist any public package or toolkit that provides a standard baseline from which to explore. Therefore, I propose to create a link mining framework that adapts several of the core principles of link and graph mining into a scalable, shared package. This toolkit would be an essential research and teaching tool similar to the University of Waikato’s WEKA toolkit [6] or George Mason’s ECJ system [7]. Initially, this project would only incorporate fundamental and highly- extendable principles of link mining, but most importantly it will serve as a launch-pad for more interesting, collaborative theoretical work. With a core link mining package in place I propose to study the dynamic temporal and graphical nature of relationships within various domains in order to advance the theory of and methodology for determining probabilities of link existence where none are explicitly annotated. This process involves several steps. First a domain must be selected that exhibits the relational attributes applicable to the link mining paradigm. Data from social networks, protein inter- actions, citations, microarrays, etc. all contain necessary attributes; therefore this step is arguably the most straightforward because many real-world data sets are inherently relational [1]. After the domains are defined, features that describe the relationships need to be extracted. For example, friendship in a social network is annotated by the inclusion of the friend’s name on a Tim Weninger 2008 NSF Graduate Fellowship Research Proposal user’s homepage. Pair-dependent features, such as the size of the intersection of interests, etc., offer supplementary evidence for the existence of a friendship. These pair-dependent features will be used to determine the probability for link existence where it is not annotated. Finding the non-obvious pair-dependent features is arguably the most difficult part. Therefore, I propose the use of recent developments in association rule mining and frequent pattern mining by Dr. Jiawei Han et al. [8] to find correlations between data points that best suggest link existence. Furthermore, the general problem of feature selection, extraction and discovery is widely regarded as the most important factor in machine learning [9]. Besides pair-dependent features, I propose to explore the role that graph features have in identifying relationships that lack explicit annotation. In my experience, graph features, such as the shortest path distance between candidate vertices, offer the best support (in terms of entropy) for the existence or absence of links. The major problem with this approach is that extracting graph features is computationally expensive for sufficiently large graphs. Although I have begun work on developing fast, approximate search algorithms I will need to formalize and empirically study these methods. Finally, these features will be used by traditional machine learners to derive information about relationships in data sets. In each step, theories would be tested using the aforementioned link mining toolkit in order to efficiently derive empirical results. I plan to advertise and freely share my toolkit, and continue to present and publish results at refereed conferences and in refereed journals on a regular basis. While my research generally aims to expand the theoretical and computational potential of machine learning, the implications of link mining research can already been seen in the biological, physical and social sciences, and many researchers believe that the application of link mining techniques will continue to grow as more research is conducted. With help from the NSF GRF I intend to study at the University of Illinois Urbana-Champaign (UIUC) where the Data Mining Research Group led by Dr. Jiawei Han (reference letter writer) and I already have a working relationship. I believe that Dr. Han and his colleagues at UIUC are among the best researchers in the world, and they would provide the wisdom and expertise necessary for me to continue my work in this fascinating field. References: [1] Lu, Q., Getoor, L., “Link-based Classification”. ICML'03, Washington DC, 2003. [2] Sen, P., Getoor, L., “Link-based Classification”. University of Maryland CS-TR-4858. 2007. [3] Jensen, D., “Statistical challenges to inductive inference in linked data”. In Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics. 1999. [4] Page, L., Brin, S., Motwani, R. & Winograd, T. “The pagerank citation ranking: Bring order to the web”. Technical Report. Stanford University. 1998. [5] Paradesi, M.S.R., Caragea, D. and Hsu, W.H., “Structural Prediction of Protein-Protein Interactions in Saccharomyces cerevisiae”, IEEE-BIBE'07, vol. 2, Boston, MA, Oct. 2007. [6] Witten, I. H. and Frank, E., “Data Mining: Practical machine learning tools and techniques”, 2nd Edition, Morgan Kaufmann, San Francisco, 2005. [7] ECJ: A Java-based evolutionary computation research system, 2006. http://cs.gmu. edu/eclab/projects/ecj/ [8] Han, J., Pei, J., & Yin, Y., “Mining frequent patterns without candidate generation”, International Conference on Management of Data ACM-SIGMOD'00, pp. 1-12. 2000. [9] Caruana, R, Niculescu-Mizil, A., “An empirical comparison of supervised learning algorithms”. ICML'06, pp. 161-168, Pittsburgh, PA, 2006.	Winner!
262	An Adaptive Chemistry Reduction Method for Detailed Modeling of Advanced Combustion Systems Key words: combustion, mechanism reduction, stiffness removal, CFD Combustion of hydrocarbon fuels provides 85% of energy in the modern United States [1]; the current energy crisis is in reality a fuel crisis. While renewable forms of energy are being pursued to supplement combustion-based sources, hydrocarbon fuels will remain the major component for the next few decades. Currently, there is high demand to improve the efficiency of combustion technology to decrease the amount of fuel consumed and to reduce the emissions in an effort to lessen the environmental impacts; in addition, fuel-flexible designs that can run on both conventional and alternative fuels are desired. Computational modeling drives the design of new combustors and engines for aerospace, transportation, and energy applications, but accurate prediction of fuel consumption and pollutant emissions requires detailed chemical reaction mechanisms. Detailed mechanisms for liquid hydrocarbons of interest contain large numbers of species and reactions; for example, the reaction mechanisms for n-heptane (C H ) and iso-octane (C H ), important molecules for 7 16 8 18 gasoline modeling, contain almost 1000 species and 8000 reactions [1]. Despite rapid advancements in computing power, it is generally formidable to integrate such detailed reaction mechanisms into large-scale computational simulations, in terms of CPU time and memory requirements. In addition, the wide range of time scales (from nanosecond to second) and the nonlinear coupling between species and reactions induces stiffness when governing equations are solved. Due to these computational demands, practical simulations using detailed chemistry are impossible with modern computational tools. Mechanism reduction schemes are used to allow quantitative modeling while keeping realistic chemistry effects. Non-adaptive reduction methods perform reduction based on a predicted range of conditions typically by removing unimportant species and reactions and identifying the species with fast time scales for further reduction, providing a single global mechanism. Most adaptive reduction methods, on the other hand, operate by storing chemical kinetics information and retrieving necessary data during the simulation to avoid direct integration of the differential equations; newer techniques use multiple mechanisms reduced prior to the simulation at various points in the flow. I propose the development of a novel adaptive and computationally friendly reduction method that will remove unimportant species and reactions and eliminate stiffness on the fly. I aim to explore and develop new algorithms while using existing reduction methods as a basis. Non-adaptive reduction methods attempt to provide a valid reduced mechanism by predicting the range of conditions (pressure, temperature, mixture composition) of interest in a simulation. However, the size of the reduced mechanism is limited by the locations in the computational domain that require more detailed chemistry due to high reaction activity. Many methods have been developed to reduce mechanisms in this manner, but the application of directed relation graph (DRG) theory [2] to describe reacting systems is particularly useful. In this method, nodes of the DRG represent species and directed edges represent dependences between species defined by normalized contributions to production rates. Important target species are defined (e.g. fuel, oxidizer, pollutants) and a graph-searching algorithm finds the dependent set of species needed to accurately predict the production rate of targets. Species with contributions below a certain error threshold are removed from the dependent set, and the final reduced mechanism contains the union of all dependent sets. The algorithm then eliminates reactions containing the unimportant species. For further elimination of stiffness in reaction 1 NSF GRFP 2008 – Proposed Plan of Research – Kyle Niemeyer systems, the quasi steady state (QSS) and partial equilibrium (PE) assumptions are applied [3]. QSS species and PE reactions have very short time scales, causing stiffness, and the approximations seek to replace differential equations with algebraic relations to solve for species’ concentrations. Computational singular perturbation (CSP) and intrinsic low dimensional manifold (ILDM) [3] are traditional methods for finding QSS species and PE reactions by separating fast and slow processes. Adaptive reduction methods rely on different approaches to increase computational efficiency during simulations. Approaches such as in situ adaptive tabulation (ISAT) and artificial neural networks (ANN) [4] perform storage and retrieval of chemical kinetics information to save processing time. Newer adaptive methods such as genetic algorithms (GA) [5] and optimization-based approaches [6] use various techniques to provide multiple reduced mechanisms for use during the simulation at different points in the flow. Highly detailed chemistry needs to be considered at locations where reactions are actively occurring, while regions with little reactive activity can use extremely reduced mechanisms. However, all of the methods currently rely on predictive reduction, which will not provide the highest level of accuracy or reduction. I propose the investigation of a new adaptive reduction methodology that will perform on the fly removal of species and reactions and elimination of stiffness. Identification and removal of unimportant species and reactions based on local conditions allows for the highest level of reduction and therefore the least computational demand, while keeping high accuracy. I will explore a novel algorithm for species and reaction removal using the DRG concept as a starting point; previous studies [2] based on DRG have shown it to be fast and reliable, suitable characteristics for on the fly application. Traditional methods for identifying QSS species and PE reactions such as CSP and ILDM are time-intensive [3] and therefore not well suited for on the fly stiffness removal; as such, I will also investigate efficient methods for identifying these fast processes. The adaptive reduction method I have proposed can be directly applied to the simulation of combustion processes for aeropropulsion, transportation, and energy applications. The incorporation of detailed chemistry while providing speedy simulation will allow accurate modeling of fuel consumption and emissions and help drive the design of next-generation engines and combustors. A method based on graph theory could be also be applied to the modeling of other complex systems; broader applications consist of food web/ecosystem modeling, disease spreading modeling, climate modeling, and biological systems modeling. Also, a new methodology developed to perform mechanism reduction could also be used to mine important information about complex systems. For example, a CSP-based method was used to gather information about explosive processes in a simulation of a hydrogen/air turbulent lifted jet flame [7]- the new method I propose could be used similarly for data mining. References [1] “Basic Research Needs for Clean and Efficient Combustion of 21st Century Fuels,” DOE/BES Workshop Report (2006). [2] T.F. Lu, C.K. Law, Combust. Flame 144 (2006) 24-36. [3] T.F. Lu, C.K. Law, J.H. Chen, AIAA 2008-1010 (2008). [4] J.Y. Chen, J.A. Blasco, N. Fueyo, C. Dopazo, Proc. Combust. Inst. 28 (2000) 115-21. [5] I. Banerjee, M.G. Ierapetritou, Combust. Flame 144 (2006) 619-33. [6] O.O. Oluwole, P.I. Barton, W.H. Green, Combust. Theory Model. 11 (2007) 127-46. [7] T.F. Lu, C.S. Yoo, J.H. Chen, C.K. Law, AIAA 2008-1013 (2008). 2	HM
263	permission. CAUSES AND CONSEQUENCES OF BIOCOMPLEXITY Keywords: Adaptation, biocomplexity, biodiversity, natural selection, resilience Conspecific populations often differ in important fitness-related traits. Why might this be? One mechanism driving population differentiation is divergent natural selection, wherein spatial variation in selection drives divergence of populations. Such phenotypic diversity among proximate populations [hereafter “biocomplexity” (1)] is important for the long-term sustainability of the larger population complex because the relative contribution to total production may shift among different life histories depending on the prevailing environmental conditions (1). Despite the intuitive appeal of these ideas, the causes and consequences of biocomplexity are rarely studied. Variation in natural selection is the presumed mechanism generating biocomplexity. For instance, spatial variation in selection has been shown to drive divergence in age-at-maturity of Trinidadian guppies (2). Similarly, temporal variation can drive evolution of phenotypic traits, as evidenced by microevolution of beak size in Darwin’s finches in response to short-term climate perturbations (3). Yet, studies quantifying spatio-temporal variation in selection are exceedingly rare and the ecological circumstances driving selection are rarely understood. The overarching goal of my study is to examine the causes and consequences of biocomplexity among proximate salmon populations. Pacific salmon are an excellent system in which to test these ideas because they form discrete breeding populations, which are then subject to local selection pressures. Reproductive isolation combined with spatially varying selection pressures result in adaptation to local conditions and, ultimately, a diversity of phenotypes among populations. Furthermore, fine-scale variation in environmental conditions also leads to ecologically-driven variation among proximate populations (i.e., phenotypic plasticity). Herein, I propose to study coho salmon (Oncorhynchus kisutch) during their freshwater-rearing (juvenile) stage across multiple sites within the Lagunitas Creek watershed (California). Goal 1: Determine the causes of biocomplexity Hypothesis 1: Physical habitat attributes drive variation in fitness-related traits. Variation in local conditions can lead to phenotypic variation among salmon populations (4). Indeed, recent work has demonstrated that when habitat diversity is lost, specific salmon life history components are also lost (5). I will quantify seasonal variation in stream temperature and flow among ten tributaries of Lagunitas Creek across 3 years using standard methods (6). At each site, I will mark individual fish to monitor their size and growth through time. I will use a formal model comparison approach (7) to determine the physical habitat attributes contributing to phenotypic diversity among these proximate salmon populations. Hypothesis 2: Food web structure drives variation in fitness-related traits. Variation in stream flows has been shown to have a strong impact on food web structure among years (8). Moreover, variation in food-web structure can influence fitness-related traits in fish consumers (9). To investigate the role of food web structure as a driver of biocomplexity among salmon populations, I will characterize variation in food web structure among three tributaries spanning a gradient of stream size. Specifically, I will sample tissues from multiple trophic levels and quantify stable isotopes of carbon and nitrogen to illuminate food web structure (10). I will again use a formal model comparison approach to test the drivers of variation in food web structure among sites using data collected as part of Hypothesis 1, as well as the consequences of variation in food web structure to size and growth of salmon across sites. Hypothesis 3: Spatio-temporal variation in natural selection drives variation in body size. After determining the drivers of variation in size and growth, I plan to quantify natural selection acting Copyright 2008 All Rights Reserved to Original Author. Do not duplicate or use without permission. on these traits. I will use data collected on individual fish across multiple seasons to relate phenotypic traits to survival across focal intervals (e.g., winter). I will use standard approaches (11) for quantifying natural selection across sites and years. Goal 2: Determine the consequences of biocomplexity Hypothesis 4: Variation in smolt size and production differs among sites and years. Many studies have demonstrated the importance of size-at-smolt migration to ocean survival, with larger smolts presumed to survive at higher rates than relatively smaller smolts (12). I propose to determine the consequences of biocomplexity in juvenile growth rates and survival by quantifying variation in smolt size and production among sites by trapping out-migrating smolts from each site, measuring body size, and estimating site-specific smolt production. Analysis of these data will allow me to determine the consequences of variation in the ecological circumstances experienced during the juvenile-rearing stage to smolt size and production, which then influence adult production. This will then allow me to identify production “hotspots” within the system and to distinguish factors that lead to high smolt production in those areas. Anticipated Results: I anticipate that variation among sites of abiotic conditions and food web structure drives biocomplexity among proximate salmon populations. I also expect to find evidence of spatio-temporal variation in the strength and form of selection acting on body size. With the above factors potentially driving biocomplexity, I expect smolt size and production to vary among sites. This underscores the importance of maintaining a diversity of freshwater habitats to maintain biocomplexity among salmon populations as a buffer for future changes. Intellectual Merit and Broader Impacts: With the recent collapse of California salmon stocks, investigations regarding the link between biocomplexity and sustainability are becoming increasingly important. My research seeks to provide insight into the causes and consequences of biocomplexity and may help create more robust management practices that maintain the full diversity of phenotypes in proximate populations, thus ensuring some resilience to future perturbations. To communicate the significance of my original research, I plan to disseminate the findings of this study through various modes including via 1) peer-reviewed, published literature; 2) the Ecological Society of America conference (where I am a student member); and 3) to interested citizens via, for instance, the Point Reyes National Parks Service Podcast. Additionally, I plan to work closely with a local community-based group, the Salmon Protection and Watershed Network (SPAWN), in order to convey my findings to the local community. Finally, as a Burmese-American student in the environmental sciences, I am aware of the lack of ethnic diversity in my field. U.C. Berkeley has an incredibly diverse undergraduate body and I will strive to incorporate students from diverse backgrounds into all aspects of my research. References: 1. R. Hilborn, T. P. Quinn, D. E. Schindler, D. E. Rogers, Proc. Natl. Acad. Sci. U. S. A. 100, 6564 (May, 2003). 2. D. A. Reznick, H. Bryga, J. A. Endler, Nature 346, 357 (Jul, 1990). 3. P. R. Grant, B. R. Grant, Science 296, 707 (Apr, 2002). 4. P. J. Wigington et al., Frontiers in Ecology and the Environment 10, 513 (Dec, 2006). 5. M. M. McClure et al., Evolutionary Applications 1, 300 (2008). 6. F. R. Hauer, G. A. Lamberti, Methods in Stream Ecology. (Academic Press, San Diego, CA, ed. 2nd, 2007). 7. K. P. Burnham, D. R. Anderson, Model Selection and Multimodel Inference: A Practical Information-Theoretical Approach. (Springer, New York, ed. 2nd ed., 2002). 8. M. E. Power, M. S. Parker, W. E. Dietrich, Ecological Monographs 78, 263 (May, 2008). 9. K. B. Suttle, M. E. Power, J. M. Levine, C. McNeely, Ecological Applications 14, 969 (Aug, 2004). 10. J. C. Finlay, S. Khandwala, M. E. Power, Ecology 83, 1845 (Jul, 2002). 11. R. Lande, S. J. Arnold, Evolution 37, 1210 (1983). 12. T. P. Quinn, The Behavior and Ecology of Pacific Salmon and Trout. (University of Washington, Press, Seattle, 2005).	Winner!
264	Crossover Regulation During Caenorhabditis elegans Meiosis Keywords: chromosome structure, condensin, crossover interference, meiosis, recombination Meiosis is essential for the generation of genetic diversity. All sexually-reproducing eukaryotes undergo this specialized cell division, consisting of one round of DNA replication followed by two rounds of chromosome segregation. Successful segregation requires crossover recombination, which is initiated by a programmed double strand break (DSB) that causes the reciprocal exchange of genetic information between homologous chromosomes. Crossovers (COs) provide physical links between homologs, but they also facilitate evolution by culling deleterious mutations and creating novel allelic combinations. Due to their importance, COs are subject to strict regulation that guarantees at least one CO per homolog pair and ensures wide spacing of multiple COs. Additionally, COs preferentially occur on genomic intervals called “hotspots.” These flank haplotype blocks, allelic combinations that tend to be inherited together and are evolutionarily more stable.1 Hotspots determine the evolutionary genomic landscape, but efforts to predict their location have only been partially successful.2 CO hotspots are also hotspots for DSBs, though not all DSBs become COs.3 Therefore, CO regulation affects DSB distribution and DSB resolution into COs or noncrossovers (NCOs). 4,5 The nematode C. elegans provides an elegant system to study this control, for it exhibits complete CO interference: each homolog pair only has one CO per meiosis.6 Chromosomes are structured by a number of protein complexes, one being the highly-conserved condensin complex. C. elegans has three condensins involved in dosage compensation, chromosome compaction, and CO control.7 Disruption of the meiotically-active condensin I complex causes chromosomal axis extension, which alters DSB distribution and thus CO distribution.5 Previously, the condensin II complex was thought to act only in mitosis – but work in the Meyer lab has shown that it is also involved in meiosis, downstream of CO regulation.8 Preliminary data from the lab implicates at least one condensin II subunit earlier in meiosis that affects CO distribution in a way that differs from condensin I. Though CO control is widespread, its precise mechanism remains a mystery. I propose to use C. elegans as a model in which to deepen our understanding of CO regulation by examining how CO distribution is affected by both meiotic condensin complexes. Hypothesis: Condensin II regulates crossover frequency at the level of DSB initiation by lengthening chromosome axes, which changes the binding of DNA to each axis. Mutations in condensin I or II will cause a change in CO frequency manifested by altered hotspot distribution. Aim 1. Do changes in chromosome structure affect CO number by altering DSBs? To determine when CO regulation occurs, I will identify the relationship between DSB formation, DSB resolution, and changes in chromosomal structure as revealed by a lengthened axis. For each experiment proposed below, I will test five condensin II subunit mutants, which we have in lab. Previously-characterized condensin I mutants will serve as a positive control and wild type animals as a negative control. I will also generate animals with mutations in both condensin I and II to uncover interactions between the two complexes. a. Measuring CO frequency. I will score six X chromosome SNPs (single nucleotide polymorphisms) in recombinant individuals generated from crosses between two divergent laboratory strains. In males, CO frequency and distribution can be ascertained along single X chromatids. Preliminary data leads me to expect increased CO frequencies in condensin II mutants, implying that condensin II limits CO formation, but a decreased CO frequency would indicate that condensin II acts to trigger CO formation. b. Measuring DSB frequency. To demonstrate that increased CO frequency is due to increased DSB frequency, I will label DSB position throughout meiosis by immunostaining with RAD-51 antibody, which marks recombination intermediates. Correlation of elevated DSB numbers with higher CO frequencies in condensin II mutants would indicate that additional DSBs provide further substrate for COs, while a lower DSB frequency would Copyright 2008 Original Author implicate involvement at the level of DSB resolution. c. Measuring chromosomal axis length. To measure axis length of X chromosomes, I will use fluorescent in situ hybridization to sequences containing the SNPs from Aim 1a. After immunostaining for DSBs and an antibody to the chromosomal axis protein HTP-3, I can trace labeled X chromosome axes through deconvolved 3D image stacks. Computationally straightening these traces with software present in the lab will allow me to measure axis length within microns and analyze DSB foci on individual X chromatids. Unlike previous lower-resolution studies, this will identify whether sub-chromosomal axis expansions correlate with increased DSB frequency in condensin mutants, demonstrating that changes in chromosome structure affect CO number by creating more DSBs. However, any change in DSB frequencies on altered axis intervals would further bolster a relationship between chromosome structure, DSB initiation, and CO resolution. Aim 2. How do condensins I and II exert effects on chromosome structure? If the condensin complexes affect higher-order chromosome structure by modifying axis length (which I will have determined in Aim 1c), they must also change where DNA attaches to the chromosome axis. To examine whether condensin mutants have these structural changes, I will use ChIP-seq (chromatin immunoprecipation sequencing) to detect the binding of REC-8, a meiosis-specific cohesin that marks DNA-axis attachment, and the axis protein HTP-3. ChIP will isolate specific DNA sequences of protein binding to be identified by high-throughput Solexa sequencing. UC Berkeley has two Solexa sequencers readily available to graduate students in my department. I will analyze REC-8 and HTP-3 binding in condensin I mutants, condensin II mutants, and the double mutant, choosing the subunit mutant conditions that show the strongest CO effect from Aim 1a; wild type animals will serve as a control. Antibodies to both proteins suitable to ChIP have been generated in the lab. If condensin mutants exhibit no change in REC-8 or HTP-3 binding, CO regulation may change axis length without affecting DNA-axis attachment. However, differential DNA-axis binding and altered axis lengths in condensin mutants will reveal a direct association between chromosome structure and CO regulation. Aim 3. What are genome-wide trends of CO in C. elegans? To determine the relationship between DSBs and their resolution into COs or NCOs, I will use microarrays to generate recombination maps for wild type and animals mutant for condensin I, II, and both, again choosing mutant conditions with the strongest CO effect. Previous recombination studies have lacked the resolution to detect NCO formation. To address this, I will use hundreds of SNPs on each chromosome that cause differential hybridization between two divergent laboratory strains, choosing markers that are reproducibly observed on high-throughput tiling arrays.9 Several studies in S. cerevisiae have utilized similar technology,10,11 but few other metazoans will prove as tractable to a genome-wide analysis as C. elegans, due to its small genome, numerous SNPs, and clonal individuals. I will define CO hotspots, and therefore haplotype blocks, using the wild type recombination map. I will also uncover, for the first time, whether NCOs have an effect on overall CO regulation in C. elegans. Additionally, if hotspot architecture changes in condensin mutants, I will have identified a chromosome- wide mode of CO regulation consistent with global control of hotspot activity. This project is fundamentally interesting because it will elucidate a conserved and universal phenomenon for generating diversity, but it will also have a significant impact on our understanding of a basic evolutionary mechanism. Condensins have the ability to exert global effects on chromosome architecture – permitting chromosome-wide communication that could explain the appearance and disappearance of CO hotspots within short spans of time. Determining the mechanism responsible for CO regulation and identifying CO hotspots will be crucial to our understanding of genome organization and evolution. All proposed research is original and of my own design. References: (1) Kauppi L et al. 2007. Prot Natl Acad Sci USA. (2) Petes TD. 2001. Nat Rev Genet. (3) Szostak JW et al. 1983. Cell. (4) Bishop DK & Zickler D. 2004. Cell. (5) Mets DM & Meyer BJ. 2008. MS in preparation. (6) Hillers KJ & Villeneuve AM. 2003. Curr Biol. (7) Tsai CJ et al. 2008. Genes Dev. (8) Chan RC et al. 2004. J Cell Bio. (9) Jones MR et al. 2008. BMC Genomics. (10) Chen SY et al. 2008. Dev Cell. (11) Mancera E et al. 2008. Nature.	Winner!
265	permission. Investigating Informal E-Waste Recycling Methods and Associated Soil Pollution Key words: environmental pollution, heavy metals, e-waste, recycling, informal, Delhi Where does your computer go to die? Electric and electronic waste (e-waste) contains hazardous materials and much of it is processed with few environmental controls. Annually, an estimated 20 to 50 million tons of e-waste is produced worldwide1 and due to the substantial amount of labor involved in the recycling of electronic devices, many e-waste dealers turn to developing economies for processing2. Policies designed to address the movement of e-waste recycling increasingly require robust scientific evidence of toxic leaching and the nascent body of evidence describing the environmental effects of unregulated or informal e-waste recycling is largely anecdotal3, 4. The few empirical studies have operated at an inappropriate scale to make associations between processing categories and associated levels of pollution5; have analyzed policy at a more global scale2, 6; or have focused on pollutant leaching in a laboratory setting as a proxy for environmental leaching7. The recent US Government Accountability Office report (GAO-08-1044, August 2008) criticizing the US EPA’s handling of e-waste highlights the relevance of this research. My study will make a significant contribution to both research and policy by addressing this gap in scale and context and by testing for robust associations between quantified pollutant levels and specific e-waste industrial processes in the field environment. This investigation will classify and map e-waste recycling operations and quantify associated pollutant levels in the soil. Hypothesis: the concentrations of key pollutants in the soil will increase in association with more destructive recycling processes (e.g. repair and resale will be associated with lower toxin concentrations as compared with smashing cathode ray tubes for the copper yokes). Methods: My proposed field site is Delhi, India, due to the city’s established e-waste recycling industry and well-documented specialized processing areas4, 8. I will collaborate with the following non-governmental organizations in the United States and on-site in India: Silicon Valley Toxics Coalition; Toxics Link, Delhi, India; and Chintan Environmental Research and Action Group, Delhi, India. Base maps of Delhi will be collected from map archives, university departments, and government offices. These will include streets and historical land use to assist in navigation and pollutant baseline controls; elevation for surface/hydrological modeling; and geology, soils, and streams for environmental controls. Additional information will be gathered from public records if historical maps are not available. All data will be translated into geographic space, and digitized in a Geographic Information System, with coordinate systems defined, projected and transformed as necessary. Following this, key individuals from local NGOs, government offices, and universities will be interviewed to provide qualitative data for three critical components: types of e-waste recycling operations in Delhi, site locations, and historical land use not captured in the base map construction. Combining the results of these interviews with published literature on recycling processes8, 9, a series of recycling operation categories will be categorized (e.g. one code for gold extraction from printed circuit boards using acid-baths versus another code for cell-phone refurbishment), resulting in approximately 3-5 analytical codes. Information gained in the interviews will be verified and geo-located in the field by surveys assisted by a differentially corrected geographic positioning system (GPS). Using the population of coded recycling operations, a stratified simple random sample of individual recycling sites within each code will be selected. Soil analysis will primarily be performed with a field-portable x-ray fluorescence analyzer (fp-XRF), supplied by Ron Amundson’s lab, based in the Department of Environmental Science, Policy and Management at UC Berkeley. The fp-XRF will be calibrated for lead, Copyright 2008. All rights reserved to original author. Copyright 2008 All rights reserved to original author. Do not duplicate or use in any way without permission. arsenic, cadmium, bromine, mercury, and chromium, toxic elements most commonly associated with e-waste2. All samples collected and fp-XRF readings will be catalogued with geographic coordinates using GPS. The first sampling phase will define pollutant plumes and concentration strata using transect sampling with the fp-XRF. Systematic random sampling will assign sampling locations within each plume strata. In situ soil readings and ex situ samples will be collected during the second phase. To aid in precision and bias control, two methods will be employed: 1) one randomly selected site will provide field calibration values by double sampling: ex situ soil samples and in situ readings 2) 20% of all in situ readings will be accompanied by ex situ samples at remaining sites as a further control for environmental variations10. Potential sources of error include sampling design insensitivity to local variations, sampling obstructions at individual sites, non-soil ground-cover (mitigated by alternate collection protocol for dust using thin-sample), dynamic environmental conditions (mitigated by hydrologic modeling or averaging multiple series), and changing land use at observation sites (mitigated by a study design with more sampling sites than necessary). After ex situ soil samples have been laboratory tested, all soil data will be digitized and geo-referenced. In situ readings will be calibrated against quantitative laboratory results producing a measure of estimated bias. Multivariate Analysis of Variance (MANOVA) will be used to assess the combination of toxin concentrations against the categories of recycling processes. Additional variables will be tested for inclusion in the model such as historical land use and environmental features such as slope and soil type. Weights will be applied in the model for the concentration strata and to control for spatial autocorrelation. Covariance will be addressed in the MANOVA model. Anticipated Results: As the recycling process becomes more destructive, the concentrations of key soil pollutants are expected to increase. The results of this study can aid more precise targeting of particular waste-handling practices for environmental controls, thus facilitating a more nuanced approach to improving e-waste recycling operations. Methods used in this project could also be replicated in other locations to examine environmental contamination associated with formal and informal e-waste recycling. Support: My advisor, Rachel Morello-Frosch (study design and environmental pollution); Ron Amundson (soil sampling methodology); John Radke (spatial analysis and sampling techniques); Alan Hubbard (statistical analysis); and Alastair Iles and Kate O’Neill (hazardous and e-waste trading). Oladele Ogunseitan at UC Irvine, and Jaco Huisman with the UN’s StEP Initiative (e-waste toxicity and recycling processes). 1. Schwarzer, S., et al. in Env Alert Bulletin 4 (UNEP, 2005). 2. Widmer, R., et al. Global perspectives on e-waste. Env Impact Assess Rev 25, 436-458 (2005). 3. Puckett, J., et al. (Basal Action Network, 2005). 4. Puckett, J. B. et al. (Basal Action Network & Silicon Valley Toxics Coalition, 2002). 5. Wong, C. S. C., et al. Evidence of excessive releases of metals from primitive e-waste processing in Guiyu, China. Env Pollution 148, 62-72 (2007). 6. Iles, A. Mapping Environmental Justice in Technology Flows: Computer Waste Impacts in Asia. Global Env Politics 4, 76-107 (2004). 7. Lincoln, J. D., et al. Leaching Assessments of Hazardous Materials in Cellular Telephones. Env Sci & Tech 41, 2572-2578 (2007). 8. Agarwal, R., et al. 1-57 (Toxics Link, 2003). 9. Streicher-Porte, M. et al. Key drivers of the e-waste recycling system: Assessing and modelling e-waste processing in the informal sector in Delhi. Env Impact Assess Rev 25, 472-491 (2005). 10. Kalnicky, D. J. & Singhvi, R. Field portable XRF analysis of environmental samples. J of Haz Materials 83, 93-122 (2001). Copyright 2008. All rights reserved to original author.	Winner!
268	permission. Analysis of Bacterial, archaeal, and viral dispersal between distantly separated acid mine drainage systems through metagenomic analysis Keywords: Acid mine drainage, metagenomics, biogeography Background Acid mine drainage (AMD) is a serious mining-related environmental problem that causes acidification and metal contamination of waters and rivers. The exposure of sulfide minerals such as pyrite to oxygen and water produces sulfuric acid and releases heavy metals into the draining waters (1). It has been shown that microorganisms significantly contribute to this process by catalyzing the limiting step of the involved reaction (1, 2). I propose to study the rates of dispersal of AMD microorganisms, which will provide insight into understanding the processes that initiate and accelerate AMD formation. Microbial communities found in AMD systems are ideal samples to study due to their low diversity and complexity. The most abundant organisms include Leptospirillum group II, Leptospirillum group III, Acidithiobacillus ferrooxidans and Ferroplasma acidamarnus (3). AMD has been widely reported in several countries. The Richmond Mine at Iron Mountain, California, USA is an unusually well studied AMD system (e.g., 2, 3, 4). Metagenomic analysis of microbial biofilms from the Richmond Mine allowed reconstruction of near complete genomes of Leptospirillum group II (4) and Leptospirillum group III (5), among other organisms. On the other side of the equator, Chile is one of the biggest producers of copper in the world, and, due to extensive mining activity, AMD has been found in several abandoned mines along the country. I propose to compare AMD microbial communities at North and South American sites as a function of their geographical separation, using genomics of total community DNA, i.e., metagenomics. It has been reviewed that diversity of microorganisms correlates with distance separation (biogeography) (6). Because AMD organisms are adapted to extreme environments that are separated by long distances, their rates of migration are limited (7). I propose to determine the biogeographic and evolutionary dynamics that can account for strain diversity in the mentioned AMD systems. More specifically, I will determine whether diversification occurred due to contemporary environmental conditions or due to geographical separation, or whether both factors are responsible. This information will provide insights into the rates at which bacteria and archaea responsible for AMD colonize new exposed sites. Prior studies have indicated that similar AMD environments with similar geochemical conditions tend to have similar microbial communities (8, 9). The objective of this proposal, therefore, is to study strain diversification of the most abundant members of these communities. Thus I hypothesize that: 1) Any two strains of a single archaeal species from two different sites will be more evolutionary distant than a pair of bacterial strains from the same two sites. This hypothesis is based on the concept that opportunities for dispersal of acid-adapted microorganisms will be limited (7). Therefore, bacterial and archaeal populations from distantly separated sites should be distinct at the strain level. Archaeal dispersal is more difficult due to the lack of cell wall, leading to more phylogenetic diversity than that of coexisting bacteria, which would disperse more easily. Copyright 2008 Copyright 2008 All rights reserved to original author. Do not copy or use in any way without permission. 2) AMD virus populations will be generally similar at all sites, despite likely rapid evolutionary rates, due to their efficient rates of dispersal. This hypothesis is based on findings that viral populations in other systems are similar at sites separated far apart (10). To test these hypotheses, I propose to sample and analyze microbial communities found in AMD in Chile. Results will be correlated with metagenomic data from the Richmond mine in California available at Dr. Banfield’s lab. Access to sites in Chile will be arranged in collaboration with Drs. David Homes and Raquel Quatrini, from the Millenium Institute in Santiago, and Dr. Cecilia Demergasso from the Northern Catholic University in Antofagasta, Chile. In collaboration with these labs and UC Berkeley, I will apply at the Joint Genome Institute (JGI) for a community-sequencing project. I will collect samples from at least 5 AMD sites in Chile for screening. Initial identification and abundance of organisms will be done using fluorescent in situ hybridization (FISH), a technique I have worked with extensively in the past. I will also do 16S rDNA clone libraries in order to determine initial phylogenetic relationships. I will select two samples that share the majority of species for metagenomic analysis. I will request a total of 70 Mb of metagenomic sequence from JGI. Since complete or near-complete genomes of AMD organisms are available, I will use these genomes to assign DNA fragments to specific organisms. I will assemble genomes using software that our lab has extensive experience with. Then, I will analyze sequence variation within sites and between sites in order to: i) determine whether there is more or less variation within the populations than between them; ii) quantify the extent of diversity within populations of each organism, and iii) for the virus datasets, to determine whether there is evidence for closely related virus populations at the different sites. In summary, I will do a comprehensive genomic and phylogenetic study between microorganisms found in AMD from the US and Chile. My research experience working at Dr. Banfield’s lab, and completing an undergraduate thesis, has well prepared me to do this kind of analysis. Being a coauthor in two publications also demonstrates my ability to work as part of a team. No existing community research project is similar to my proposed work, therefore, this will be a big step in understanding the rates of evolution of microorganisms found in AMD and their relationships within the community. It will serve as a model for studying other microbial communities separated by long distances. My work will be shown at national and international conferences, for both specialized and general audiences. In addition, as a mentor for the SMASH program I will teach my work to minority high school students, hoping to recruit them as future researchers in this field. References (1) Schrenk MO, et al. Science. 1998 Mar 6; 279(5356): 1519-22, (2) Edwards KJ, et al. Chem Geol. 2000 169: 383–397, (3) Bond PL, et al. Appl Environ Microbiol. 2000 Nov; 66: 4962-71, (4) Lo I, Denef VJ, Verberkmoes NC, Shah MB, Goltsman D, et al. Nature. 2007 Mar 29; 446(7135):537-41, (5) Goltsman D, Banfield JF. (In preparation), (6) Hughes Martiny J, et al. Nat Rev Microbiol. 2006 Feb; 4(2): 102-112; (7) Whitaker RJ. Phil Trans R Soc B. 2006 361:1975-1984; (8) Coram N, et al. App Environ Microbiol. 2002 68: 838-845. (9) Dold B, et al. Environ Sci Technol. 2005 39: 2515-2521. (10) Breitbart M, Rohwer F. Trends Microbiol. 2005 Jun;13(6):278-84. Copyright 2008	Winner!
270	Impacts of Climatic Change on the Arid Savanna Fire Regimes of West Africa Keywords: Fire Regime, Climate Change, West Africa, Land Degradation, Arid Savanna Introduction: Changing global conditions may be driving increasingly severe fires that are further deteriorating already highly altered arid savanna ecosystems in West Africa. With the continued rise in anthropogenic fires, vulnerable ecosystems have undergone changes in species dominance, ecosystem structure, and even ecosystem collapse. I hy- pothesize that altered climatic variables are at least partially responsible for increases in fire severity that degrade the West African arid savanna. Background: Arid savannas, dry grasslands with scattered shrubs and drought-resistant trees, are regarded as one of the ecosystems most likely to be affected by climate change (Louppe et al 1995). Though arid savannas require periodic surface fires to maintain their characteristic vegetation, increasingly severe fires have resulted in the annual degradation of more than 200,000 hectares of sub-Saharan arid savanna (Nsiah-Gyabaah 1994). In the past, these losses were attributed either to unsustainable agricultural prac- tices, or the belief that the Sahara was in a period of growth as part of the Earth’s axial obliquity cycle. Today we accept the sophistication of West African land management techniques, and there is evidence that the Sahara is actually in a greening period (Nsiah- Gyabaah 1994). Many suspect that the increase in land degradation is the result of cli- mate change driving fire intensity and frequency increases. The consequences of altered fire regimes are clear, but West African fire regimes are themselves poorly understood. I became interested in studying fire-influenced land degradation when, during a Watson Fellowship studying fire, I returned to my childhood home in Ba’Nso, Camer- oon. Throughout the region, I encountered Fulani herdsmen who had relocated to find better pastureland. The Francophone farmers I met described barren fields, complaining that no amount of burning restored the land’s fertility. Objectives: The study I propose is designed to investigate the impact of altered climatic conditions on fire-induced land degradation, producing some of the first quantitative re- search on fire regimes in West Africa. Fire regimes have been defined using characteris- tics such as frequency, intensity, burning season, and fire size (Goldammer 1988). I will study these physical parameters in West African arid savannas in order to determine if there is a relationship between altered climatic variables and fire severity, and what the impact is on land degradation. After conducting field research in the Ghanaian Accra plains, I will analyze remote sensing data to determine the change at longer timescales. Methodology: Working with Dr. Kwesi Orgle and Forest Resources Management, I will conduct field research on the arid savanna in the Accra Plains on fire plots maintained by the University of Ghana (Swaine 1992). We will analyze three study plots: a control in which fire has been excluded since 1957, another which has been burned annually since 1957, and a plot of extant vegetation which we will burn annually beginning in 2007 and for a subsequent two years. Our research team, including traditional landowners, will col- lect annual data on fires in study plots, measuring consumption of plant biomass, fire ex- tent, and intensity with thermocouples. We will document plant species turnover, the Proposed Plan of Research Smith, Rachel relative proportions of vegetation repopulation, and evaluate changes in rates of land deg- radation through a comparison with data collected since 1957. To identify change over a longer timescale, I will conduct an analysis of NOAA- AVHRR satellite image data captured for West Africa between 1985 and 2005. These previously processed data will be compared to monthly temperature and precipitation datasets by the University of East Anglia’s Climate Research Unit to determine what, if any, relationship exists between changes in numbers of fires, timing, and area burned, and altered precipitation and temperature. Dr. Johann Goldammer, head of the UN-FAO Team of Specialists on Forest Fire has agreed to collaborate with me on this analysis. We’ll use these data to inform a broader scale analysis, using severity indexes and NDVI trends to discern any relationship between changing fire severity and land degradation. Anticipated Results: Climate change may be the dominant influence on the surge in fire- influenced land degradation in West Africa, but the current paucity of quantitative data on fire in this region makes it difficult to understand the relationship between fire and climate. My research will provide the first quantitative measurements of fire regime char- acteristics, identifying fire frequency, size, and intensity thresholds above which ignitions may result in land degradation. Furthermore, it will explore the possibility of interactions between individual fire regime characteristics. I anticipate my research being able to con- clude whether fire-influenced land degradation is caused by climate change. Broader Impacts: This original project will provide critical information on fire regimes and offer insight into the extent to which climatic change is altering fire regimes in West Africa, constituting some of the first quantitative research on basic fire regime parameters in the West African arid savanna. Future researchers will utilize my data in their studies. Traditional landowners, such as the subsistence farmers in Ba’Nso and the Fulani nomadic herders whose livelihoods are tied up in the complex relationships linking fire and land arability, have a vital interest in this research. We will engage key stakeholders as research collaborators and make all literature available in English, French, and Fulbe. Regionally, this project will provide resource managers with ground-truthed data linking fire and climatic change. These data will assist them in formulating management plans and help them anticipate future changes in fire behavior from climate change. Implications of this research stretch beyond Africa to other arid rangelands where fires might be increasing in incidence or severity. My study may help scientists and land managers identify vulnerable areas in other parts of the world. My partnership with The Nature Conservancy and other organizations will make my results widely available. References: Goldammer, J.G. 1988. Rural land-use and fires in the tropics. Agroforestry Systems 6: 235-252. Louppe, D., Ouatara, N. & Coulibaly, A. 1995. Effet des feux de brousse sur la vegeta- tion. Bois et Forêts des Tropiques 245: 59-74. Nsiah-Gyabaah, K. Environmental degradation and desertification in Ghana. Brookfield: Aldershot, 1994. Swaine, M. D., Lieberman, D. and Hall, J. B. 1992. The effect of fire exclusion on savan- nah vegetation at Kpong, Ghana. Biotropica 24,2a: 166-172.	Winner!
271	Game Ranching in Botswana: Effects on Wildlife and Rural Communities Keywords: wildlife conservation, resource tenure, community-based natural resource management (CBNRM), Botswana Objective: To develop an interdisciplinary understanding of the effects of game ranching on wildlife conservation and CBNRM in a livestock-wildlife conflict area Research Focus: In the mid-1990’s, “Use it or lose it” emerged as a controversial wildlife policy slogan, indicating that wildlife would have to pay its way, through consumptive and non-consumptive use, if it were to survive.1 This major shift from existing colonial protectionist strategies is a critical part of today’s African land-use planning discourse. Game ranching, the focus of my research, is the intentional management and maintenance of wild animal populations for subsequent human use (i.e. meat, trophy hunting).1 Touted by its proponents as a sustainable use of land that both conserves biodiversity and enhances livelihoods,2 ranching already is an established industry in South Africa and Namibia. Studies show that game ranching has less impact on land than large-scale cattle ranching,3 yet its viability for wildlife conservation continues to be debated. Furthermore, game ranching’s implications for community- based management of natural resources (CBNRM) has yet to be explored. CBNRM aims to devolve management of and benefits from natural resources to communities so as to create incentives favoring sustainable use.4 However, rights granted under CBNRM do not guarantee that communities will benefit from a given resource.5 In Botswana, communities do not have full control over the key determinants of resource conservation and economic development—hunting quotas, market prices, robustness of wildlife populations, macro-economic/political conditions, and ownership over the land and wildlife itself.6 Therefore, communities rarely invest in natural resource infrastructure and conservation.7 Competition from private game ranches may also threaten CBNRM viability; however, the development of game ranching on communal lands could provide new opportunities for CBNRM projects, as game ranching by definition involves intense management of natural resources. Although game ranching on communal lands is in its infancy in Botswana, a country noted for both conservation and CBNRM initiatives, it merits study given its potential to affect the current community-based conservation model. Social and ecological aspects of environmental phenomena have repeatedly been shown to be interdependent;8 thus, rigorous study of game ranching requires an interdisciplinary approach. The ecological component of my research will take place on private game ranches because there are few community-managed game ranches in Botswana. I will address the question of whether game ranches promote overall conservation of wildlife species at levels similar to that of nearby protected areas (PAs), or merely conserve harvestable species with clear economic value. My sociological research on the implications of game ranching for CBNRM will examine how resource management capabilities and decision-making authority of communities change when game ranching is incorporated as a community-managed program. If game ranching on communal lands increases community security of tenure over wildlife, do communities then invest more in wildlife management? Research Hypotheses: A) Relative to PAs, game ranches (i) maintain similar densities of economically valuable wildlife species (ii) show smaller densities of species with zero or negative economic value. B) Game ranching allows for more management over natural Copyright © 2007 by original author. All rights reserved. resources than do other forms of wildlife use. C) Community-managed game ranches increase security of tenure over wildlife. D) Increased community management and secure wildlife tenure leads to community investment in wildlife management. Methods: My research will combine standard ecological sampling and field methods with the sociological extended case method, which examines interacting effects of external forces on a particular case in order to modify wider theoretical assertions.9 Table 1. Integrated Ecological and Sociological Research Methodology Ecological sampling on private game farms In-depth case study at Dqãe Qare10 (target sample size = 12-14 ranches in central Kalahari) (community-run game ranch in central Kalahari) • Determine distribution & abundance of species with (+), (-), • Interview key informants to determine if and no economic value to game ranches game ranching leads to increased community • Survey methods: a) detection rates along foot transects for control over natural resources compared to direct sightings, track and scats11 b)‘capture’ rates at remote other CBNRM ventures photographic stations12 (to ↑ chance of detecting species, ie. • Indicators of control: a) extent of legal rights elusive carnivores) over land & wildlife b) ability to self- • Compare with parallel data collected from: adjacent determine hunting quotas c) stability of livestock ranches & nearby Central Kalahari Game Reserve revenue (CKGR) to determine game ranching’s impact on local • Conduct structured household surveys & wildlife biodiversity relative to other land uses key informant interviews with community • Other data sources: a) Dept. of Wildlife wildlife population participants in the game ranch on: a)perceived census data in CKGR b) interviews with ranch managers levels of control over natural resources b) about nature and level of ranch management practices (i.e. willingness to invest in wildlife management control strategies for predators, bush clearing, fencing and c) actual levels of investment in wildlife veterinary care) and land-use history management Expected Results: 1) Game ranching’s effects on species’ populations vary depending on the species’ economic value to the ranch 2) Community game ranches have increased level of control over natural resources, stimulating investment in wildlife management. Significance: This research will contribute novel interdisciplinary knowledge that is meaningful to both Botswana and the broader field of conservation science. My study site is ideal because: 1) it encompasses a matrix of land-use types across a continuous landscape, enabling assessment of game ranching’s impact on biodiversity with few confounding factors; 2) I am already familiar with Botswana’s ecology, economics, and socio-politics and have good working relations with key stakeholders; and (3) game ranching is new in Botswana so my results can influence future policy. (I certify this proposal represents my own work and ideas—ACG) 1 Kock, R. A. 1995. Wildlife utilization: use it or lose it—a Kenyan perspective. Biodiversity and Conservation 4: 241-256. 2 Luxmoore, R. 1985. Game farming in South Africa as a force for conservation. Oryx 19 (4): 225-234. 3 Smet M. and D. Ward. 2006. Soil quality gradients around water-points under different management systems in a semi-arid savanna, South Africa. Journal of Arid Environments 64(2): 251-69. 4 Murphree, M. and Hulme D. eds. 2001. African Wildlife and Livelihoods. Cape Town: David Philip. 5 Ribot, J.C. and Peluso, N.L. 2003. A Theory of Access. Rural Sociology 68(2): 153-181. 6 Barnes, J.I. 1999. Economic potential for biodiversity use in southern Africa: empirical evidence. Environment and Development Economics 4: 203–236 7 du Toit, J. et al. 2004. Conserving tropical nature: current challenges for ecologists. Trends in Ecology & Evolution 19(1): 12-17. 8 Blaikie, P. and Brookfield, H. 1987. Land Degradation and Society. New York: Methuen and Co. 9 Burawoy, M. 1991. The Extended Case Method. In Ethnography Unbound. UC Press: Berkeley. 10 I am taking Setswana lessons (national language) and will augment this with a San language course while in Botswana 11 Stander, P. E. 1998. Spoor counts as indices of large carnivore populations: the relationship between spoor frequency, sampling effort and true density. Journal of Applied Ecology 35: 378-385. 12 Carbone, C., S. Christie, K. Conforti, et al. 2001. The use of photographic rates to estimate densities of tigers and other cryptic mammals. Animal Conservation 4: 75-79.	Winner!
275	Evaluation of Resilience Thresholds in Stream Ecosystems Key Words: aquatic ecology; geomorphology; Mediterranean climate; flow regime; salmonids Background: Ecological resilience has been described as the ability of an ecosystem to withstand disturbance and maintain the processes that control its structures4. When a disturbance exceeds the resilience threshold of an ecosystem, organizing processes can shift to an alternative stable state, resulting in changes to community composition and function1. Anthropomorphic alterations to the environment can reduce the resilience of natural ecosystems, resulting in losses of ecological services which in turn can stress human systems7,8. The increasing demand on freshwater resources to meet human water needs has altered the flow of rivers around the world and exemplifies the tension between human and natural systems5,7,8. Where freshwater is limited, increased human demand is coupled with a reduction of flow available to sustain ecological processes, potentially crossing a resilience threshold for the system. This is particularly true of Mediterranean-climate watersheds which experience natural prolonged low flows each year. One indicator of reduced resilience of aquatic ecosystems is the dramatic decline of anadromous fish populations in Pacific Coast streams over the past 150 years. Substantial research effort has been directed toward salmon population recovery, but has largely been focused on reducing fine sediment and improving in-stream habitat structure2. Relatively little attention has been paid to the issue of water quantity as a limiting factor to salmon recovery due to the fact that most salmon research has been conducted in the Pacific Northwest. Rivers of this region are characterized by less seasonal flow variability than in coastal California, due to more mesic climate conditions9 and flow management from dams. Recently developed flow models for tributary streams in California’s Mediterranean climate indicate that dispersed human water extraction has detectably reduced summer flows, resulting in an accelerated and prolonged dry season period3. However, quantitative biophysical data relating changes to the stream hydrograph with decreased habitat suitability and salmon mortality are currently not available. Furthermore, monitoring programs typically fail to capture fine-scale variability in stream flows, including pool characteristics, which act as important refugia for fish in the late summer. Hypotheses: This study will investigate how human-induced modifications to natural flow regimes of northern California streams have affected the resilience of stream ecosystems. Specifically, my research shall test the following two hypotheses: (1) there is a quantifiable resilience threshold for aquatic ecosystems resulting from limited summer stream flows and (2) existing flow models do not adequately capture non-linear, small-scale dynamics of altered stream systems during the dry season. My involvement with a University of California Berkeley watershed research group and previous experiences conducting ecological field studies and stream restoration projects provided the motivation for this original research plan. Research Plan: (1) Resilience Thresholds Analysis: As part of an interdisciplinary team, I will investigate the relationship between flow regime alteration and juvenile salmonid (steelhead trout [Onchorhynchus mykiss] and Coho salmon [O. kisutch]) abundance and survivorship. This research will utilize data from a 10-year study monitoring fish abundance and over-summer survivorship at multiple tributary stream sites within the Russian River watershed, northern California. Stream flow models are currently being developed for these same tributaries to estimate human water demands and determine the availability of water for ecosystem needs. I will integrate the fish survey data with modeled flow over a gradient of flow regime alterations within specific stream reaches to evaluate if flows are a limiting factor for salmonid survivorship. Systematic declines in salmonid abundance associated with low flows will be interpreted as an indication that a resilience threshold necessary for survivorship has been crossed. Sliding regression is an effective method for detecting non-linear threshold responses6 and will be used in the analysis of salmonid mortality under specific flow regimes. I expect to explore alternative regression methods such as ordinal logistic regression to detect threshold responses once the stream flow model and salmonid data are fully integrated. NSF Proposed Plan of Research Copyright 2007 original author, all rights reserved (2) Stream Monitoring: In order to evaluate the ecological effects of stream flow on finer spatial and temporal scales, I propose to collect field data on biotic and physical habitat conditions at specific pools within multiple stream reaches. This field work will validate existing stream flow models to determine their predictive power for water availability in north coast tributary streams and provide small-scale, high resolution information on stream flow and pool dynamics during the dry season. Stream gauges will be installed and flow data collected throughout the summer months. Pool depths and water temperature will also be monitored throughout the low-flow period. The monitoring shall include a comprehensive assessment of habitat variables, including riparian vegetation cover, channel morphology, streambed substrate, and habitat structure (large woody debris and boulders). The effects of reduced dry season flows on streambed composition and other geomorphic variables are of particular interest. The accumulation of fine sediments within salmonid spawning habitat can in part be offset by scouring effects of high flow events which mobilize streambed particles. If the filling of riffles and pools is accelerated by lower summer base flows, a focus on the small-scale dynamics of stream flow and sediment transport will provide insight to ecosystem processes that control habitat quality for salmonids. I will be advised on field methods by a team of experts interested in this research, including V. Resh, P. Moyle, M. Kondolf, and W.E. Dietrich. Anticipated Results and Significance: I expect to find evidence of an ecological resilience threshold (as measured by minimum stream flows) below which juvenile salmonids do not survive. Analyses of stream flow regimes in relation to biotic and geomorphic variables will fill a gap in aquatic systems science by identifying processes that affect ecological integrity of stream ecosystems during low-flow periods5. My research will focus on the effects of dispersed water extraction by individual landowners, which has been much less studied than extraction from central infrastructure (e.g., dams and canals). This dispersed water extraction may be an important impact on aquatic ecosystems that has been overlooked and is likely to grow in many parts of the U.S. experiencing exurban expansion. This study will make an important contribution to our understanding of the effects of land use change on the resilience of stream ecosystems and will evaluate if resilience thresholds can be quantified – a necessary step if such measures are to be used for ecosystem management. Literature Citations: 1. Carpenter, S., Walker, B., Anderies, J.M., and N. Abel. 2001. From metaphor to measurement: Resilience of what to what? Ecosystems 4: 765 – 781. 2. Coey, Robert, Sarah Nossaman-Pierce, Colin Brooks and Zebulon J. Young. 2002. California Department of Fish and Game: Russian River Basin - Fisheries Restoration Plan (Draft). 3. Deitch, M.J., and G.M. Kondolf. 2004 (Submitted). Evaluating the effects of water rights diversions in coastal California streams over spatial and temporal scales. Proceedings of Symposium on Arid Lands, American Society of Civil Engineers, Salt Lake City. 4. Holling, C.S., and L.H. Gunderson. 2002. Resilience and adaptive cycles. In L.H. Gunderson and C.S. Holling, eds. Panarchy: Understanding Transformations in Human and Natural Systems. Island Press, Washington D.C. 5. Nilsson, C., J. E. Pizzuto, G. E. Moglen, M. A. Palmer, E. H. Stanley, and N. E. Bockstael, and L. C. Thompson. 2003. Ecological forecasting and the urbanization of stream ecosystems: challenges for economists, hydrologists, geomorphologists, and ecologists. Ecosystems 6: 659-674. 6. Ourso, R.T. and S.A. Frenzel. 2003. Identification of linear and threshold responses in streams along a gradient of urbanization in Anchorage, Alaska. Hydrobiologia 501: 117 – 131. 7. Postel, S., and B. Richter. 2003. Rivers for Life: Managing water for people and nature. Island Press, Washington, 8. Richter, B.D., J.V. Baumgartner, R. Wigington, and D.P. Braun. 1997. How much water does a river need? Freshwater Biology 37: 231-249. 9. Wolman, M.G., R. Gerson. 1978. Relative scales of time and effectiveness of climate in watershed geomorphology. Earth Surface Processes and Landforms 3(2): 189-208.	
